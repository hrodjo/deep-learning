Deletion-Robust Submodular Maximization:
Data Summarization with ‚Äúthe Right to be Forgotten‚Äù

Baharan Mirzasoleiman 1 Amin Karbasi 2 Andreas Krause 1

Abstract
How can we summarize a dynamic data stream
when elements selected for the summary can be
deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right
to restrict the service provider from using (part
of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization
problem. We develop the first resilient streaming
algorithm, called ROBUST-S TREAMING, with a
constant factor approximation guarantee to the
optimum solution. We evaluate the effectiveness
of our approach on several real-world applications, including summarizing (1) streams of geocoordinates (2); streams of images; and (3) clickstream log data, consisting of 45 million feature
vectors from a news recommendation task.

1. Introduction
Streams of data of massive and increasing volume are generated every second, and demand fast analysis and efficient
storage, including massive clickstreams, stock market data,
image and video streams, sensor data for environmental or
health monitoring, to name a few. To make efficient and reliable decisions we usually need to react in real-time to the
data. However, big and fast data makes it difficult to store,
analyze, or make predictions. Therefore, data summarization ‚Äì mining and extracting useful information from large
data sets ‚Äì has become a central topic in machine learning
and information retrieval.
A recent body of research on data summarization relies on
utility/scoring functions that are submodular. Intuitively,
submodularity (Krause & Golovin, 2013) states that select1

ETH Zurich, Switzerland 2 Yale University, New Haven,
USA. Correspondence to: Baharan Mirzasoleiman <baharanm@inf.ethz.ch>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing any given data point earlier helps more than selecting it
later. Hence, submodular functions can score both diversity
and representativeness of a subset w.r.t. the entire dataset.
Thus, many problems in data summarization require maximizing submodular set functions subject to cardinality
(or more complicated hereditary constraints). Numerous examples include exemplar-based clustering (Dueck &
Frey, 2007), document (Lin & Bilmes, 2011) and corpus
summarization (Sipos et al., 2012), recommender systems
(El-Arini & Guestrin, 2011), search result diversification
(Rakesh Agrawal, 2009), data subset selection (Wei et al.,
2015), and social networks analysis (Kempe et al., 2003).
Classical methods, such as the celebrated greedy algorithm
(Nemhauser et al., 1978) or its accelerated versions (Mirzasoleiman et al., 2015; Badanidiyuru & VondraÃÅk, 2014)
require random access to the entire data, make multiple
passes, and select elements sequentially in order to produce near optimal solutions. Naturally, such solutions cannot scale to large instances. The limitations of centralized methods inspired the design of streaming algorithms
that are able to gain insights from data as it is being collected (Badanidiyuru et al., 2014; Chakrabarti & Kale,
2014; Chekuri et al., 2015; Mirzasoleiman et al., 2017).
While extracting useful information from big data in realtime promises many benefits, the development of more
sophisticated methods for extracting, analyzing and using
personal information has made privacy a major public issue. Various web services rely on the collection and combination of data about individuals from a wide variety of
sources. At the same time, the ability to control the information an individual can reveal about herself in online
applications has become a growing concern.
The ‚Äúright to be forgotten‚Äù (with a specific mandate for
protection in the European Data Protection Regulation
(2012), and concrete guidelines released in 2014) allows
individuals to claim the ownership of their personal information and gives them the authority to their online activities (videos, photos, tweets, etc). As an example, consider a road traffic information system that monitors traffic
speeds, travel times and incidents in real time. It combines
the massive amount of control messages available at the
cellular network with their geo-coordinates in order to gen-

Deletion-Robust Submodular Maximization

erate the area-wide traffic information service. Some consumers, while using the service and providing data, may
not be willing to share information about specific locations
in order to protect their own privacy. With the right to be
forgotten, an individual can have certain data deleted from
online database records so that third parties (e.g., search engines) can no longer trace them (Weber, 2011). Note that
the data could be in many forms, including a) user‚Äôs posts
to an online social media, b) visual data shared by wearable cameras (e.g., Google Glass), c) behavioral patterns or
feedback obtained from clicking on advertisement or news.
In this paper, we propose the first framework that offers instantaneous data summarization while preserving the right
of an individual to be forgotten. We cast this problem as
an instance of robust streaming submodular maximization
where the goal is to produce a concise real-time summary
in the face of data deletion requested by users. We develop
ROBUST-S TREAMING, a method that for a generic streaming algorithm S TREAMING A LG with approximation guarantee Œ±, ROBUST-S TREAMING outputs a robust solution,
against any m deletions from the summary at any given
time, while preserving the same approximation guarantee.
To the best of our knowledge, ROBUST-S TREAMING is the
first algorithm with such strong theoretical guarantees. Our
experimental results also demonstrate the effectiveness of
ROBUST-S TREAMING on several practical applications.

2. Background and Related Work
Several streaming algorithms for submodular maximization have been recently developed. For monotone functions, Gomes & Krause (2010) first developed a multipass algorithm with 1/2‚àí approximation guarantee subject to a cardinality constraint k, using O(k) memory, under strong assumptions on the way the data is generated.
Later, Badanidiyuru et al. (2014) proposed the first single pass streaming algorithm with 1/2 ‚àí  approximation
under a cardinality constraint. They made no assumptions on the order of receiving data points, and only require O(k log k/) memory. Following the same line of inquiry, Chakrabarti & Kale (2014) developed a single pass
algorithm with 1/4p approximation guarantee for handling
more general constraints such as intersections of p matroids. The required memory is unbounded and increases
polylogarithmically with the size of the data. For general
submodular functions, Chekuri et al. (2015) presented a
randomized algorithm subject to a broader range of constraints, namely p-matchoids. Their method gives a (2 ‚àí
o(1))/(8+e)p approximation using O(k log k/2 ) memory
(k is the size of the largest feasible solution). Very recently,
Mirzasoleiman et al. (2017) introduced a (4p ‚àí 1)/4p(8p +
2d ‚àí 1)-approximation algorithm under a p-system and d
knapsack constraints, using O(pk log2 (k)/2 ) memory.

An important requirement, which frequently arises in practice, is robustness. Krause et al. (2008) proposed the
problem of robust submodular observation selection, where
we want to solve max|A|‚â§k mini‚àà[`] fi (A), for normalized monotonic fi . Submodular maximization of f robust
against m deletions can be cast as an instance of the above
problem: max|A|‚â§k min|B|‚â§m f (A\B). The running time,
however, will be exponential in m. Recently, Orlin et al.
(2016) developed an algorithm with an asymptotic guarantee 0.285 for deletion-robust
submodular maximization
‚àö
under up to m = o( k) deletions. The results can be improved for only 1 or 2 deletions.
The aforementioned approaches aim to construct solutions
that are robust against deletions in a batch mode way, without being able to update the solution set after each deletion.
To the best of our knowledge, this is the first to address the
general deletion-robust submodular maximization problem
in the streaming setting. We also highlight the fact that our
method does not require m, the number of deletions, to be
bounded by k, the size of the largest feasible solution.
Very recently, submodular optimization over sliding windows has been considered, where we want to maintain a
solution that considers only the last W items (Epasto et al.,
2017; Jiecao et al., 2017). This is in contrast to our setting,
where the guarantee is with respect to all the elements received from the stream, except those that have been deleted.
The sliding window model can be easily incorporated into
our solution to get a robust sliding window streaming algorithm with the possibility of m deletions in the window.

3. Deletion-Robust Model
We review the static submodular data summarization problem. We then formalize a novel dynamic variant, and constraints on time and memory that algorithms need to obey.
3.1. Static Submodular Data Summarization
In static data summarization, we have a large but fixed
dataset V of size n, and we are interested in finding a
summary that best represents the data. The representativeness of a subset is defined based on a utility function f : 2V ‚Üí R+ where for any A ‚äÇ V the function
f (A) quantifies how well A represents V . We define the
marginal gain of adding an element e ‚àà V to a summary
A ‚äÇ V by ‚àÜ(e|A) = f (A ‚à™ {e}) ‚àí f (A). In many data
summarization applications, the utility function f satisfies
submodularity, i.e., for all A ‚äÜ B ‚äÜ V and e ‚àà V \ B,
‚àÜ(e|A) ‚â• ‚àÜ(e|B).
Many data summarization applications can be cast as an
instance of a constrained submodular maximization:
OPT = max f (A),
A‚ààI

(1)

Deletion-Robust Submodular Maximization

where I ‚äÇ 2V is a given family of feasible solutions.
We will denote by A‚àó the optimal solution, i.e. A‚àó =
arg maxA‚ààI f (A). A common type of constraint is a cardinality constraint, i.e., I = {A ‚äÜ 2V , s.t., |A| ‚â§ k}.
Finding A‚àó even under cardinality constraint is NP-hard,
for many classes of submodular functions (Feige, 1998).
However, a seminal result by Nemhauser et al. (1978) states
that for a non-negative and monotone submodular function
a simple greedy algorithm that starts with the empty set
S0 = ‚àÖ, and at each iteration augments the solution with the
element with highest marginal gain, obtains a (1 ‚àí 1/e) approximation to the optimum solution. For small, static data,
the centralized greedy algorithm or its accelerated variants
produce near-optimal solutions. However, such methods
fail to scale to truly large problems.
3.2. Dynamic Data: Additions and Deletions
In dynamic deletion-robust submodular maximization
problem, the data V is generated at a fast pace and in realtime, such that at any point t in time, a subset Vt ‚äÜ V of
the data has arrived. Naturally, we assume that V1 ‚äÜ V2 ‚äÜ
¬∑ ¬∑ ¬∑ ‚äÜ Vn , with no assumption made on the order or the size
of the datastream. Importantly, we allow data to be deleted
dynamically as well. We use Dt to refer to data deleted by
time t, where again D1 ‚äÜ D2 ‚äÜ ¬∑ ¬∑ ¬∑ ‚äÜ Dn . Without loss
of generality, below we assume that at every time step t exactly one element et ‚àà V is either added or deleted, i.e.,
|Dt \ Dt‚àí1 | + |Vt \ Vt‚àí1 | = 1. We now seek to solve a
dynamic variant of Problem (1)
OPTt = max f (At ) s.t. It = {A : A ‚àà I ‚àß A ‚äÜ Vt \ Dt }.

|Mt | should not depend on t and Vt . Whenever a new element is received, the algorithm can choose 1) to insert it
into its memory, provided that the memory does not exceed a pre-specified capacity bound, 2) to replace it with
one or a subset of elements in the memory (in the preemptive model), or otherwise 3) the element gets discarded and
cannot be used later by the algorithm. If the algorithm receives a deletion request for a subset Dt ‚äÇ Vt at time t
(in which case It will be updated to accommodate this request) it has to drop Dt from Mt in addition to updating
At to make sure that the current solution is feasible (all
subsets A0t ‚äÇ Vt that contain an element from Dt are infeasible, i.e., A0t ‚àà
/ It ). To account for such losses, the streaming algorithm can only use other elements maintained in its
memory in order to produce a feasible candidate solution,
i.e. At ‚äÜ Mt ‚äÜ ((Vt \ Vt‚àí1 ) ‚à™ Mt‚àí1 ) \ Dt . We say that
the streaming algorithm is robust against m deletions, if it
can provide a feasible solution At ‚àà It at any given time t
such that f (At ) ‚â• œÑ OPTt for some constant œÑ > 0. Later,
we show how robust streaming algorithms can be obtained
by carefully increasing the memory and running multiple
instances of existing streaming methods simultaneously.

4. Example Applications
We now discuss three concrete applications, with their
submodular objective functions f , where the size of the
datasets and the nature of the problem often require a
deletion-robust streaming solution.
4.1. Summarizing Click-stream and Geolocation Data

At ‚ààIt

(2)
Note that in general a feasible solution at time t might not
be a feasible solution at a later time t0 . This is particularly
important in practical situations where a subset of the elements Dt should be removed from the solution. We do not
make any assumptions on the order or the size of the data
stream V , but we assume that the total number of deletions
is limited to m , i.e., |Dn | ‚â§ m.
3.3. Dealing with Limited Time and Memory
In principle, we could solve Problem (2) by repeatedly ‚Äì at
every time t ‚Äì solving a static Problem (1) by restricting the
ground set V to Vt \ Dt . This is impractical even for moderate problem sizes. For large problems, we may not even
be able to fit Vt into the main memory of the computing
device (space constraints). Moreover, in real-time applications, one needs to make decisions in a timely manner
while the data is continuously arriving (time constraints).
We hence focus on streaming algorithms which may maintain a limited memory Mt ‚äÇ Vt \ Dt , and must have an
updated feasible solution {At | At ‚äÜ Mt , At ‚àà It } to output at any given time t. Ideally, the capacity of the memory

There exists a tremendous opportunity of harnessing prevalent activity logs and sensing resources. For instance, GPS
traces of mobile phones can be used by road traffic information systems (such as Google traffic, TrafficSense, Navigon) to monitor travel times and incidents in real time. In
another example, stream of user activity logs is recorded
while users click on various parts of a webpage such as ads
and news while browsing the web, or using social media.
Continuously sharing all collected data is problematic for
several reasons. First, memory and communication constraints may limit the amount of data that can be stored and
transmitted across the network. Second, reasonable privacy
concerns may prohibit continuous tracking of users.
In many such applications, the data can be described in
terms of a kernel matrix K which encodes the similarity
between different data elements. The goal is to select a
small subset (active set) of elements while maintaining a
certain diversity. Very often, the utility function boils down
to the following monotone submodular function (Krause &
Golovin, 2013) where Œ± > 0 and KS,S is the principal submatrix of K indexed by the set S.
f (S) = log det(I + Œ±KS,S )

(3)

Deletion-Robust Submodular Maximization

In light of privacy concerns, it is natural to consider participatory models that empower users to decide what portion
of their data could be made available. If a user decides not
to share, or to revoke information about parts of their activity, the monitoring system should be able to update the
summary to comply with users‚Äô preferences. Therefore, we
use ROBUST-S TREAMING to identify a robust set of the k
most informative data points by maximizing Eq. (3).
4.2. Summarizing Image Collections
Given a collection of images, one might be interested
in finding a subset that best summarizes and represents
the collection. This problem has recently been addressed
via submodular maximization. More concretely, Tschiatschek et al. (2014) designed several submodular objectives f1 , . . . , fl , which quantify different characteristics
that good summaries should have, e.g., being representative w.r.t. commonly reoccurring motives. Each function
either captures coverage (including facility location, sumcoverage, and truncated graph cut, or rewards diversity
(such as clustered facility location, and clustered diversity).
Then, they optimize a weighted combination of such functions
l
X
fw (A) =
wi fi (A),
(4)
i=1

where weights are non-negative, i.e., wi ‚â• 0, and learned
via a large-margin structured prediction. We use their
learned mixtures of submodular functions in our image
summarization experiments. Now, consider a situation
where a user wants to summarize a large collection of her
photos. If she decides to delete some of the selected photos in the summary, she should be able to update the result without processing the whole collection from scratch.
ROBUST-S TREAMING can be used as an appealing method.

5. Robust-Streaming Algorithm
In this section, we first elaborate on why naively increasing the solution size does not help. Then, we present our
main algorithm, ROBUST-S TREAMING, for deletion-robust
streaming submodular maximization. Our approach builds
on the following key ideas: 1) simultaneously constructing non-overlapping solutions, and 2) appropriately merging solutions upon deleting an element from the memory.
5.1. Increasing the Solution Size Does Not Help
One of the main challenges in designing streaming solutions is to immediately discover whether an element received from the data stream at time t is good enough to be
added to the memory Mt . This decision is usually made
based on the added value or marginal gain of the new element which in turn depends on the previously chosen elements in the memory, i.e., Mt‚àí1 . Now, let us consider

the opposite scenario when an element e should be deleted
from the memory at time t. Since now we have a smaller
context, submodularity guarantees that the marginal gains
of the elements added to the memory after e was added,
could have only increased if e was not part of the stream
(diminishing returns). Hence, if some elements had large
marginal values to be included in the memory before the
deletion, they still do after the deletion. Based on this intuition, a natural idea is to keep a solution of a bigger size,
say m+k (rather than k) for at most m deletions. However,
this idea does not work as shown by the following example.
Bad Example (Coverage): Consider a collection of n
subsets V = {B1 , . . . , Bn }, where Bi ‚äÜ {1, . . . , n}, and a
coverage function f (A) = | ‚à™i‚ààA Bi |, A ‚äÜ V . Suppose we
receive B1 = {1, . . . , n}, and then Bi = {i} for 2 ‚â§ i ‚â§ n
from the stream. Streaming algorithms that select elements
according to their marginal gain and are allowed to pick
k + m elements, will only pick up B1 upon encounter (as
other elements provide no gain), and return An = {B1 } after processing the stream. Hence, if B1 is deleted after the
stream is received, these algorithms return the empty set
An = ‚àÖ (with f (An ) = 0). An optimal algorithm which
knows that element B1 will be deleted, however, will return
set An = {B2 , . . . , Bk+2 }, with value f (An ) = k + 1.
Hence, standard streaming algorithms fail arbitrarily badly
even under a single deletion (i.e., m = 1), even when we
allow them to pick sets larger than k.
In the following we show how we can solve the above issue
by carefully constructing not one but multiple solutions.
5.2. Building Multiple Solutions
As stated earlier, the existing one-pass streaming algorithms for submodular maximization work by identifying
elements with marginal gains above a carefully chosen
threshold. This ensures that any element received from the
stream which is fairly similar to the elements of the solution set is discarded by the algorithm. Since elements are
chosen as diverse as possible, the solution may suffer dramatically in case of a deletion.
One simple idea is to try to find m (near) duplicates for each
element e in the memory, i.e., find e0 such that f (e0 ) = f (e)
and ‚àÜ(e0 |e) = 0 (Orlin et al., 2016). This way if we face m
deletions we can still find a good solution. The drawback
is that even one duplicate may not exist in the data stream
(see the bad example above), and we may not be able to
recover for the deleted element. Instead, what we will do
is to construct non-overlapping solutions such that once we
experience a deletion, only one solution gets affected.
In order to be robust against m deletions, we run a cascading chain of r instances of S TREAMING A LG s as fol(1)
(2)
(r)
lows. Let Mt = Mt , Mt , . . . , Mt denote the con-

Deletion-Robust Submodular Maximization
Data Stream

(j)

R

11

S

11

22

1

M
R

S

22

2

rr

R

‚Ä¶
S

r

rr

Figure 1: ROBUST-S TREAMING uses r instances of a
generic S TREAMING A LG to construct r non-overlapping
(1)
(2)
(r)
memories at any given time t, i.e., Mt , Mt , . . . , Mt .
(i)
Each instance produces a solution St and the solution returned by ROBUST-S TREAMING is the first valid solution
(i)
(i)
St = {St |i = min j ‚àà [1 ¬∑ ¬∑ ¬∑r], Mt 6= null}.
tent of their memories at time t. When we receive a
new element e ‚àà Vt from the data stream at time t, we
pass it to the first instance of S TREAMING A LG(1) . If
S TREAMING A LG(1) discards e, the discarded element is
cascaded in the chain and is passed to its successive algorithm, i.e. S TREAMING A LG(2) . If e is discarded by
S TREAMING A LG(2) , the cascade continues and e is passed
to S TREAMING A LG(3) . This process continues until either
e is accepted by one of the instances or discarded for good.
Now, let us consider the case where e is accepted by the
i-th instance, S IEVE -S TREAMING(i) , in the chain. As discussed in Section 3.3, S TREAMING A LG may choose to dis(i)
(i)
card a set of points Rt ‚äÇ Mt from its memory before
(i)
(i)
(i)
(i)
inserting e, i.e., Mt ‚Üê Mt ‚à™ {e} \ Rt . Note that Rt
is empty, if e is inserted and no element is discarded from
(i)
(i)
Mt . For every discarded element r ‚àà Rt , we start a new
cascade from (i + 1)-th instance, S TREAMING A LG (i+1) .
Note that in the worst case, every element of the stream can
go once through the whole chain during the execution of the
algorithm, and thus the processing time for each element
scales linearly by r. An important observation is that at any
(1)
(2)
(r)
given time t, all the memories Mt , Mt , ¬∑ ¬∑ ¬∑ ,Mt contain disjoint sets of elements. Next, we show how this data
structure leads to a deletion-robust streaming algorithm.
5.3. Dealing with Deletions
Equipped with the above data structure shown in Fig. 1,
we now demonstrate how deletions can be treated. Assume an element ed is being deleted from the memory
of the j-th instance of S TREAMING A LG(j) at time t, i.e.,
(j)
(j)
Mt ‚Üê Mt \ {ed }. As discussed in Section 5.1, the
solution of the streaming algorithm can suffer dramatically
from a deletion, and we may not be able to restore the quality of the solution by substituting similar elements. Since
there is no guarantee for the quality of the solution after
a deletion, we remove S TREAMING A LG (j) from the chain
(j)
by making Rt = null and for all the remaining elements in

discarded forever

M
M

(j)

(j)

its memory Mt , namely, Rt ‚Üê Mt \ {ed }, we start a
new cascade from j+1-th instance, S TREAMING A LG(j+1) .
The key reason why the above algorithm works is that the
guarantee provided by the streaming algorithm is independent of the order of receiving the data elements. Note that
at any point in time, the first instance i of the algorithm
(i)
with Mt 6= null has processed all the elements from the
stream Vt (not necessarily in the order the stream is originally received) except the ones deleted by time t, i.e., Dt .
Therefore, we can guarantee that S TREAMING A LG (i) provides us with its inherent Œ±-approximation guarantee for
(i)
reading Vt \ Dt . More precisely, f (St ) ‚â• Œ±OPTt , where
OPTt is the optimum solution for the constrained optimization problem (2) when we have m deletions.
In case of adversary deletions, there will be one deletion
from the solution of m instances of S TREAMING A LG in
the chain. Therefore, having r = m + 1 instances, we will
remain with only one S TREAMING A LG that gives us the
desired result. However, as shown later in this section, if
the deletions are i.i.d. (which is often the case in practice),
and we have m deletions in expectation, we need r to be
much smaller than m + 1. Finally, note that we do not need
to assume that m ‚â§ k where k is the size of the largest
feasible solution. The above idea works for arbitrary m ‚â§ n.
The pseudocode of ROBUST-S TREAMING is given in Algorithm 1. It uses r ‚â§ m + 1 instances of S TREAMIN G A LG as subroutines in order to produce r solutions. We
(1)
(1)
(r)
denote by St , St , . . . , St the solutions of the r
S TREAMING A LG s at any given time t. We assume that
an instance i of S TREAMING A LG(i) receive an input ele(i)
ment and produces a solution St based on the input. It
(i)
may also change its memory content Mt , and discard a
(i)
set Rt . Among all the remained solutions (i.e., the ones
that are not ‚Äùnull‚Äù), it returns the first solution in the chain,
i.e. the one with the lowest index.
Theorem 1 Let S TREAMING A LG be a 1-pass streaming
algorithm that achieves an Œ±-approximation guarantee for
the constrained maximization problem (2) with an update
time of T , and a memory of size M when there is no deletion. Then ROBUST-S TREAMING uses r ‚â§ m + 1 instances of S TREAMING A LG s to produce a feasible solution St ‚àà It (now It encodes deletions in addition to constraints) such that f (St ) = Œ±OPTt as long as no more than
m elements are deleted from the data stream. Moreover,
ROBUST-S TREAMING uses a memory of size rM , and has
worst case update time of O(r2 M T ), and average update
time of O(rT ).
The proofs can be found in the appendix. In Table 1 we
combine the result of Theorem 1 with the existing streaming algorithms that satisfy our requirements.

Deletion-Robust Submodular Maximization

Algorithm 1 ROBUST-S TREAMING

6. Experiments

Input: data stream Vt , deletion set Dt , r ‚â§ m+1.
Output: solution St at any time t.
(i)
(i)
1: t = 1, Mt = 0, St = ‚àÖ
‚àÄi ‚àà [1 ¬∑ ¬∑ ¬∑ r]
2: while ({Vt \ Vt‚àí1 } ‚à™ {Dt \ Dt‚àí1 } =
6 ‚àÖ) do
3:
if {Dt \ Dt‚àí1 } =
6 ‚àÖ then
4:
ed ‚Üê {Dt \ Dt‚àí1 }
5:
Delete(ed )
6:
else
7:
et ‚Üê {Vt \ Vt‚àí1 }
8:
Add(1, et )
9:
end if
10:
t=t+
 1(i)
	
(j)
11:
St = St | i = min{j ‚àà [1 ¬∑ ¬∑ ¬∑ r], Mt 6= null}
12: end while

We address the following questions: 1) How much can
ROBUST-S TREAMING recover and possibly improve the
performance of S TREAMING A LG in case of deletions? 2)
How much does the time of deletions affect the performance? 3) To what extent does deleting representative
vs. random data points affect the performance? To this
end, we run ROBUST-S TREAMING on the applications we
described in Section 4, namely, image collection summarization, summarizing stream of geolocation sensor data,
as well as summarizing a clickstream of size 45 million.

13: function Add(i, R)
14:
for e ‚àà R do
(i)
(i)
(i)
15:
[Rt , Mt , St ] =S TREAMING A LG (i) (e)
16:
17:
18:
19:
20:

(i)

if Rt 6= ‚àÖ and i < r then
(i)
Add(i + 1, Rt )
end if
end for
end function

21: function Delete(e)
22:
for i = 1 to r do
(i)
23:
if e ‚àà Mt then
24:
25:
26:
27:
28:
29:
30:

(i)

(i)

Rt = Mt \ {e}
(i)
Mt ‚Üê null
(i)
Add(i + 1, Rt )
return
end if
end for
end function

Theorem 2 Assume each element of the stream is deleted
with equal probability p = m/n, i.e., in expectation we
have m deletions from the stream. Then, with probability
1 ‚àí Œ¥, ROBUST-S TREAMING provides an Œ±-approximation
as long as

k

1
r‚â•
log 1/Œ¥ .
1‚àíp
Theorem 2 shows that for fixed k, Œ¥ and p, a constant number r of S TREAMING A LG s is sufficient to support m = pn
(expected) deletions independently of n. In contrast, for
adversarial deletions, as analyzed in Theorem 1, pn + 1
copies of S TREAMING A LG are required, which grows linearly in n. Hence, the required dependence of r on m is
much milder for random than adversarial deletions. This is
also verified by our experiments in Section 6.

Throughout this section we consider the following streaming algorithms: S IEVE -S TREAMING (Badanidiyuru et al.,
2014), S TREAM -G REEDY (Gomes & Krause, 2010), and
S TREAMING -G REEDY (Chekuri et al., 2015). We allow
all streaming algorithms, including the non-preemptive
S IEVE -S TREAMING, to update their solution after each
deletion. We also consider a stronger variant of S IEVE S TREAMING, called E XT S IEVE, that aims to pick k ¬∑r elements to protect for deletions, i.e., is allowed the same
memory as ROBUST-S TREAMING. After the deletions, the
remaining solution is pruned to k elements.
To compare the effect of deleting representative elements to
the that of deleting random elements from the stream, we
use two stochastic variants of the greedy algorithm, namely,
S TOCHASTIC -G REEDY (Mirzasoleiman et al., 2015) and
R ANDOM -G REEDY (Buchbinder et al., 2014). This way
we introduce randomness into the deletion process in a
principled way. Hence, we have:
S TOCHASTIC -G REEDY (SG): Similar to the the greedy
algorithm, S TOCHASTIC -G REEDY starts with an empty set
and adds one element at each iteration until obtains a solution of size m. But in each step it first samples a random set
R of size (n/m) log(1/) and then adds an element from
R to the solution which maximizes the marginal gain.
R ANDOM -G REEDY (RG): R ANDOM -G REEDY iteratively
selects a random element from the top m elements with
the highest marginal gains, until finds a solution of size m.
For each deletion method, the m data points are deleted
either while receiving the data (where the steaming algorithms have the chance to update their solutions by selecting new elements) or after receiving the data (where there
is no chance of updating the solution with new elements).
Finally, the performance of all algorithms are normalized
against the utility obtained by the centralized algorithm
that knows the set of deleted elements in advance.
6.1. Image Collection Summarization
We first apply ROBUST-S TREAMING to a collection of
100 images from Tschiatschek et al. (2014). We used

Deletion-Robust Submodular Maximization

Algorithm
ROBUST + S IEVE -S TREAMING [BMKK‚Äô14]
ROBUST + f -MSM [CK‚Äô14]

Problem
Mon. Subm.
Mon. Subm.

Constraint
Cardinality
p-matroids

ROBUST + S TREAMING -G REEDY [CGQ‚Äô15]
ROBUST + S TREAMING -L OCAL S EARCH
[MJK‚Äô17]

Non-mon. Subm.

p-matchoid
p-system +
d knapsack

Non-mon. Subm.

Appr. Fact.
1/2 ‚àí 
1
4p
(1‚àíŒµ)(2‚àío(1))
(8+e)p
(1‚àíŒµ)(4p‚àí1)
4p(8p+2d‚àí1)

Memory
O(mk log k/)
O(mk(log |V |)O(1) )
O(mk log k/2 )
O(mpk log2 k/2 )

Table 1: ROBUST-S TREAMING combined with 1-pass streaming algorithms can make them robust against m deletions.
the weighted combination of 594 submodular functions either capturing coverage or rewarding diversity (c.f. Section 4.2). Here, despite the small size of the dataset, computing the weighted combination of 594 functions makes
the function evaluation considerably expensive.

sists of 3,607 geolocations, collected during a one hour bike
ride around Zurich (Fatio, 2015). For each pair of points i
and j we used the corresponding (latitude, longitude) coordinates to calculate their distance in meters di,j and chose
a Gaussian kernel Ki,j = exp(‚àíd2i,j /h2 ) with h = 1500.

Fig. 2a compares the performance of S IEVE -S TREAMING
with its robust version ROBUST-S TREAMING for r = 3 and
solution size k=5. Here, we vary the number m of deletions
from 1 to 20 after the whole stream is received. We see
that ROBUST-S TREAMING maintains its performance by
updating the solution after deleting subsets of data points
imposed by different deletion strategies. It can be seen
that, even for a larger number m of deletions, ROBUSTS TREAMING, run with parameter r < m, is able to return a
solution competitive with the strong centralized benchmark
that knows the deleted elements beforehand. For the image
collection, we were not able to compare the performance of
S TREAM -G REEDY with its robust version due to the prohibitive running time. Fig. 2b shows an example of an updated image summary returned by ROBUST-S TREAMING
after deleting the first image from the summary.

Fig. 3e shows the dataset where red and green triangles show a summary of size 10 found by S IEVE S TREAMING, and the updated summary provided by
ROBUST-S TREAMING with r = 5 after deleting m = 70%
of the datapoints. Fig. 3a and 3c compare the performance of S IEVE -S TREAMING with its robust version when
the data is deleted after or during the stream, respectively.
As we see, ROBUST-S TREAMING provides a solution very
close to the hindsight centralized method. Fig. 3b and
3d show similar behavior for S TREAM -G REEDY. Note
that deleting data points via S TOCHASTIC -G REEDY or
R ANDOM -G REEDY are much more harmful on the quality of the solution provided by S TREAM -G REEDY. We repeated the same experiment by dividing the map into grids
of length 2km. We then considered a partition matroid by
restricting the number of points selected from each grid
to be 1. The red and green triangles in Fig. 3f are the
summary found by S TREAMING -G REEDY and the updated
summary provided by ROBUST-S TREAMING after deleting
the shaded area in the figure.

Normalized objective value

0.98

Robust-RG
Robust-SG

0.96
0.94
0.92

ExtSieve-SG

6.3. Large scale click through prediction

ExtSieve-RG

0.9

Sieve-RG

0.88
0.86

Sieve-SG

5

10

15

Number of deletions

(a) Images

(b) Images

Figure 2: Performance of ROBUST-S TREAMING vs
S IEVE -S TREAMING for different deletion strategies (SG,
RG), at the end of stream, on a collection of 100 images.
Here we fix k = 5 and r = 3. a) performance of ROBUSTS TREAMING and S IEVE -S TREAMING normalized by the
utility obtained by greedy that knows the deleted elements
beforehand. b) updated solution returned by ROBUSTS TREAMING after deleting the first images in the summary.
6.2. Summarizing a stream of geolocation data
Next we apply ROBUST-S TREAMING to the active set selection objective described in Section 4.1. Our dataset con-

For our large-scale experiment we consider again the active
set selection objective, described in Section 4.1. We used
Yahoo! Webscope data set containing 45,811,883 user click
logs for news articles displayed in the Featured Tab of the
Today Module on Yahoo! Front Page during the first ten
days in May 2009 (Yahoo, 2012). For each visit, both the
user and shown articles are associated with a feature vector
of dimension 6. We take their outer product, resulting in a
feature vector of size 36.
The goal was to predict the user behavior for each displayed
article based on historical clicks. To do so, we considered
the first 80% of the data (for the fist 8 days) as our training
set, and the last 20% (for the last 2 days) as our test set.
We used Vowpal-Wabbit (Langford et al., 2007) to train a
linear classifier on the full training set. Since only 4% of
the data points are clicked, we assign a weight of 10 to each

Deletion-Robust Submodular Maximization

ExtSieve-SG

0.955
Sieve-SG

0.95
0.945

Sieve-RG

0.94

0.95

0.85
0.8
Sieve-RG
0.75

0.935

0.7

0.93

0.65

20

40

66

80

100

120

ExtSieve-RG
ExtSieve-SG

0.9

Sieve-SG

0

20

40

60

80

100

120

Number of deletions

Number of deletions

(a) S IEVE -S TREAMING, at end (b) S TREAM -G REEDY, at end
0.82

0.758
Robust-SG

0.756
ExtSieve-RG

0.755

ExtSieve-SG

0.754
0.753
0.752
Sieve-RG

0.751

Robust-RG

0.81

Normalized objective value

Normalized objective value

Robust-SG

Robust-RG

0.757

0.8
ExtSieve-SG

0.79

ExtSieve-RG
0.78
0.77
0.76

Sieve-RG

0.75

Sieve-SG

Sieve-SG
0.75

0

20

40

60

80

100

120

Number of deletions

0.74

0

20

40

60

80

100

120

Number of deletions

(c) S IEVE -S TREAMING, during (d) S TREAM -G REEDY, during

(e) Cardinality constraints

(f) Matroid constraints

Figure 3: ROBUST-S TREAMING vs S IEVE -S TREAMING
and S TREAM -G REEDY for different deletion strategies
(SG, RG) on geolocation data. We fix k = 20 and r = 5.
a) and c) show the performance of robustified S IEVE S TREAMING, whereas b) and d) show performance for robustified S TREAM -G REEDY. a) and b) consider the performance after deletions at the end of the stream, while c)
and d) consider average performance while deletions happen during the stream. e) red and green triangles show a set
of size 10 found by S IEVE -S TREAMING and the updated
solution found by ROBUST-S TREAMING where 70% of the
points are deleted. f) set found by S TREAMING -G REEDY,
constrained to pick at most 1 point per grid cell (matroid
constraint). Here r = 5, and we deleted the shaded area.
clicked vector. The AUC score of the trained classifier on
the test set was 65%. We then used ROBUST-S TREAMING
and S IEVE -S TREAMING to find a representative subset of
size k consisting of k/2 clicked and k/2 not-clicked examples from the training data. Due to the massive size of
the dataset, we used Spark on a cluster of 15 quad-core
machines with 32GB of memory each. We partitioned the
training data to the machines keeping its original order. We

Fig. 4a compares the performance of ROBUSTS TREAMING for a fixed active set of size k = 10, 000,
and r = 2 with random selection, randomly selecting
equal numbers of clicked and not-clicked vectors, and
using S IEVE -S TREAMING for selecting equal numbers of
clicked and not-clicked data points. The y-axis shows the
improvement in AUC score of the classifier trained on a
summary obtained by different algorithms over random
guessing (AUC=0.5), normalized by the AUC score of the
classifier trained on the whole training data. To maximize
fairness, we let other baselines select a subset of r.k
elements before deletions. Fig. 4b shows the same quantity
for r = 5. It can be seen that a slight increase in the
amount of memory helps boosting the performance for all
the algorithms. However, ROBUST-S TREAMING benefits
from the additional memory the most, and can almost
recover the performance of the classifier trained on the full
training data, even after 99% deletion.
1

1

0.9

0.9

Normalized AUC improvement

0.96

ran ROBUST-S TREAMING on each machine to find a summary of size k/15, and merged the results to obtain the final summary of size k. We then start deleting the data uniformly at random until we left with only 1% of the data, and
trained another classifier on the remaining elements from
the summary.

Robust-SG

Normalized AUC improvement

0.965

ExtSieve-RG

Robust-RG

1

Normalized objective value

Normalized objective value

Robust-SG

Robust-RG

0.97

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

Random

RandEqual

ExtSieve

Robust

(a) Yahoo! Webscope, r = 2

0

Random RandEqual ExtSieve

Robust

(b) Yahoo! Webscope, r = 5

Figure 4: ROBUST-S TREAMING vs random unbalanced
and balanced selection and S IEVE -S TREAMING selecting
equal numbers of clicked and not-clicked data points, on
45,811,883 feature vectors from Yahoo! Webscope data.
We fix k = 10, 000 and delete 99% of the data points.

7. Conclusion
We have developed the first deletion-robust streaming algorithm ‚Äì ROBUST-S TREAMING ‚Äì for constrained submodular maximization. Given any single-pass streaming algorithm S TREAMING A LG with Œ±-approximation guarantee, ROBUST-S TREAMING outputs a solution that is robust
against m deletions. The returned solution also satisfies an
Œ±-approximation guarantee w.r.t. to the solution of the optimum centralized algorithm that knows the set of m deletions in advance. We have demonstrated the effectiveness
of our approach through an extensive set of experiments.

Deletion-Robust Submodular Maximization

Acknowledgements
This research was supported by ERC StG 307036, a Microsoft Faculty Fellowship, DARPA Young Faculty Award
(D16AP00046), Simons-Berkeley fellowship and an ETH
Fellowship. This work was done in part while Amin Karbasi, and Andreas Krause were visiting the Simons Institute
for the Theory of Computing.

References
Babaei, Mahmoudreza, Mirzasoleiman, Baharan, Jalili,
Mahdi, and Safari, Mohammad Ali. Revenue maximization in social networks through discounting. Social Network Analysis and Mining, 3(4):1249‚Äì1262, 2013.
Badanidiyuru, Ashwinkumar and VondraÃÅk, Jan. Fast algorithms for maximizing submodular functions. In SODA,
2014.
Badanidiyuru, Ashwinkumar, Mirzasoleiman, Baharan,
Karbasi, Amin, and Krause, Andreas. Streaming submodular maximization: Massive data summarization on
the fly. In KDD, 2014.
Buchbinder, Niv, Feldman, Moran, Naor, Joseph Seffi, and
Schwartz, Roy. Submodular maximization with cardinality constraints. In SODA, 2014.
Chakrabarti, Amit and Kale, Sagar. Submodular maximization meets streaming: Matchings, matroids, and more.
IPCO, 2014.
Chekuri, Chandra, Gupta, Shalmoli, and Quanrud, Kent.
Streaming algorithms for submodular function maximization. In ICALP, 2015.

guidelines.
http://data.consilium.europa.
eu/doc/document/ST-9565-2015-INIT/en/
pdf.
Jiecao, Chen, Nguyen, Huy L., and Zhang, Qin. Submodular optimization over sliding windows. 2017. preprint,
https://arxiv.org/abs/1611.00129.
Kempe, David, Kleinberg, Jon, and Tardos, EÃÅva. Maximizing the spread of influence through a social network. In
KDD, 2003.
Krause, Andreas and Golovin, Daniel.
Submodular
function maximization.
In Tractability: Practical
Approaches to Hard Problems. Cambridge University
Press, 2013.
Krause, Andreas, McMahon, H Brendan, Guestrin, Carlos,
and Gupta, Anupam. Robust submodular observation
selection. Journal of Machine Learning Research, 2008.
Langford, John, Li, Lihong, and Strehl, Alex. Vowpal wabbit online learning project, 2007.
Lin, Hui and Bilmes, Jeff. A class of submodular functions
for document summarization. In NAACL/HLT, 2011.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.
Lazier than lazy greedy. In AAAI, 2015.
Mirzasoleiman, Baharan, Jegelka, Stefanie, and Krause,
Andreas. Streaming non-monotone submodular maximization: Personalized video summarization on the fly.
2017. preprint, https://arxiv.org/abs/1706.
03583.

Dueck, Delbert and Frey, Brendan J. Non-metric affinity
propagation for unsupervised image categorization. In
ICCV, 2007.

Nemhauser, George L., Wolsey, Laurence A., and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functions - I. Mathematical Programming, 1978.

El-Arini, Khalid and Guestrin, Carlos. Beyond keyword
search: Discovering relevant scientificliterature. In KDD,
2011.

Orlin, James B, Schulz, Andreas S, and Udwani, Rajan.
Robust monotone submodular function maximization.
IPCO, 2016.

Epasto, Alessandro, Lattanzi, Silvio, Vassilvitskii, Sergei,
and Zadimoghaddam, Morteza. Submodular optimization over sliding windows. In WWW, 2017.

Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson
Samuel Ieong. Diversifying search results. In WSDM,
2009.

Fatio, Philipe. https://refind.com/fphilipe/
topics/open-data, 2015.

Regulation, European Data Protection. http://ec.
europa.eu/justice/data-protection/
document/review2012/com_2012_11_en.
pdf, 2012.

Feige, Uriel. A threshold of ln n for approximating set
cover. Journal of the ACM, 1998.
Gomes, Ryan and Krause, Andreas. Budgeted nonparametric learning from data streams. In ICML, 2010.

Sipos, Ruben, Swaminathan, Adith, Shivaswamy, Pannaga,
and Joachims, Thorsten. Temporal corpus summarization using submodular word coverage. In CIKM, 2012.

Deletion-Robust Submodular Maximization

Tschiatschek, Sebastian, Iyer, Rishabh K, Wei, Haochen,
and Bilmes, Jeff A. Learning mixtures of submodular
functions for image collection summarization. In NIPS,
2014.
Weber, R. The right to be forgotten: More than a
pandora‚Äôs box?
https://www.jipitec.eu/
issues/jipitec-2-2-2011/3084, 2011.
Wei, Kai, Iyer, Rishabh, and Bilmes, Jeff. Submodularity
in data subset selection and active learning. In ICML,
2015.
Yahoo. Yahoo! academic relations. r6a, yahoo! front page
today module user click log dataset, version 1.0, 2012.
URL http://Webscope.sandbox.yahoo.com.

