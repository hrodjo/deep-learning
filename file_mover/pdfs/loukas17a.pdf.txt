How Close Are the Eigenvectors of the Sample and Actual Covariance
Matrices?
Andreas Loukas 1

Abstract
How many samples are sufficient to guarantee
that the eigenvectors of the sample covariance
matrix are close to those of the actual covariance matrix? For a wide family of distributions,
including distributions with finite second moment and sub-gaussian distributions supported in
a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance
and the number of samples. Our findings imply
non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA
and its applications. For instance, they provide
conditions for separating components estimated
from O(1) samples and show that even few samples can be sufficient to perform dimensionality
reduction, especially for low-rank covariances.

1

Introduction

The covariance matrix C of an n-dimensional distribution
is an integral part of data analysis, with numerous occurrences in machine learning and signal processing. It is
therefore crucial to understand how close is the sample coe estimated from a finite numvariance, i.e., the matrix C
ber of samples m, to the actual covariance matrix. Following developments in the tools for the concentration of
measure, (Vershynin, 2012) showed that a sample size of
m = O(n) is up to iterated logarithmic factors sufficient
for all distributions with finite fourth moment
supported in
‚àö
a centered Euclidean ball of radius O( n). Similar results
hold for sub-exponential (Adamczak et al., 2010) and finite
second moment distributions (Rudelson, 1999).
We take an alternative standpoint and ask if we can do
1
EÃÅcole Polytechnique FeÃÅdeÃÅrale de Lausanne, Switzerland.
Correspondence to: Andreas Loukas <andreas.loukas@epfl.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

better when only a subset of the spectrum is of interest.
Concretely, our objective is to characterize how many samples are sufficient to guarantee that an eigenvector and/or
eigenvalue of the sample and actual covariance matrices
are, respectively, sufficiently close. Our approach is motivated by the observation that methods that utilize the covariance commonly prioritize the estimation of principal
eigenspaces. For instance, in (local) principal component
analysis we are usually interested in the direction of the
first few eigenvectors (Berkmann & Caelli, 1994; Kambhatla & Leen, 1997), where in linear dimensionality reduction one projects the data to the span of the first few eigenvectors (Jolliffe, 2002; Frostig et al., 2016). In the nonasymptotic regime, an analysis of these methods hinges on
characterizing how close are the principal eigenvectors and
eigenspaces of the sample and actual covariance matrices.
Our finding is that the ‚Äúspectral leaking‚Äù occurring in the
eigenvector estimation is strongly concentrated along the
eigenvalue axis. In other words, the eigenvector u
ei of the
sample covariance is far less likely to lie in the span of
an eigenvector uj of the actual covariance when the eigenvalue distance |Œªi ‚àí Œªj | is large, and the concentration of
the distribution in the direction of uj is small. This agrees
with the intuition that principal components are easier to
estimate, exactly because they are more likely to appear in
the samples of the distribution.
We provide a mathematical argument confirming this phenomenon. Under fairly general conditions, we prove that
!
!
kj2
ki2
(1)
m=O
and m = O
(Œªi ‚àí Œªj )2
Œª2i
samples are asymptotically almost surely (a.a.s.) sufficient
ei ‚àíŒªi |/Œªi , respectively, is
to guarantee that |he
ui , uj i| and |Œª
small for all distributions with finite second moment. Here,
kj2 is a measure of the concentration of the distribution in
the direction of uj . We also attain a high probability bound
for sub-gaussian distributions supported in a centered Euclidean ball. Interestingly, our results lead to sample estimates for linear dimensionality reduction, and suggest that
linear reduction is feasible even from few samples.
To the best of our knowledge, these are the first nonasymptotic results concerning the eigenvectors of the sam-

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?
uÃÉ1
uÃÉ4
uÃÉ20
uÃÉ100

0.4

|huÃÉi , uj i|

|huÃÉi , uj i|

0.6

0.8

0.2
0
1
10

1
uÃÉ1
uÃÉ4
uÃÉ20
uÃÉ100

huÃÉ4 , u4 i

0.6
0.4
0.2

10

-1

10

Œªj

(a) m = 10

-3

10

-5

0
1
10

1
uÃÉ1
uÃÉ4
uÃÉ20
uÃÉ100

0.8
0.6
0.4

huÃÉ20 , u20 i

0.2
10

-1

10

-3

10

-5

0
1
10

Œªj

10

-1

10

-3

0.4

uÃÉ1
uÃÉ4
uÃÉ20
uÃÉ100

0.2

huÃÉ100 , u100 i

0.8

|huÃÉi , uj i|

1
huÃÉ1 , u1 i

|huÃÉi , uj i|

1
0.8

10

0.6

0
1
10

-5

10

-1

10

Œªj

(b) m = 100

-3

10

-5

Œªj

(c) m = 500

(d) m = 1000

Figure 1: Inner products he
ui , uj i are localized w.r.t. the eigenvalue axis. The phenomenon is shown for MNIST. Much fewer than
n = 784 samples are needed to approximate u1 and u4 .

ple covariance of non-Normal distributions. Previous studies have intensively investigated the limiting distribution
of the eigenvalues of a sample covariance matrix (Silverstein & Bai, 1995; Bai, 1999), such as the smallest and largest eigenvalues (Bai & Yin, 1993) and the
eigenvalue support (Bai & Silverstein, 1998). Eigenvectors and eigenprojections have attracted less attention; the
main research thrust entails using tools from the theory of
large-dimensional matrices to characterize limiting distributions (Anderson, 1963; Girko, 1996; Schott, 1997; Bai
et al., 2007) and it has limited applicability in the nonasymptotic setting where the sample size m is small and
n cannot be arbitrary large.
Differently, we use techniques from perturbation analysis
and non-asymptotic concentration of measure. However,
in contrast to arguments commonly used to reason about
eigenspaces (Davis & Kahan, 1970; Yu et al., 2015; Huang
et al., 2009; Hunter & Strohmer, 2010), our bounds can
characterize weighted linear combinations of he
ui , uj i2 over
i and j, and do not depend on the minimal eigenvalue
gap separating two eigenspaces but rather on all eigenvalue
differences. The latter renders them useful to many real
datasets, where the eigenvalue gap is not significant but the
eigenvalue magnitudes decrease sufficiently fast.
We also note two recent works targeting the nonassymptotic regime of Normal distributions. Shaghaghi
and Vorobyov recently characterized the first two moments
of the subspace projection error, a result which implies
sample estimates (Shaghaghi & Vorobyov, 2015), but is
restricted to specific projectors. A refined concentration
analysis for spectral projectors of Normal distributions was
also presented in (Koltchinskii & Lounici, 2015). Finally,
we remark that there exist alternative estimators for the
spectrum of the covariance with better asymptotic properties (Ahmed, 1998; Mestre, 2008). Instead, we here focus
on the standard estimates, i.e., the eigenvalues and eigenvectors of the sample covariance.

2

Problem Statement and Main Results

Let x ‚àà Cn be a sample of a multivariate distribution and
denote by x1 , x2 , . . . , xm the m independent samples used

to form the sample covariance, defined as
e=
C

m
X
(xp ‚àí xÃÑ)(xp ‚àí xÃÑ)‚àó

m

p=1

,

(2)

where xÃÑ is the sample mean. Denote by ui the eigenvector
of C associated with eigenvalue Œªi , and correspondingly
ei of C,
e such that
for the eigenvectors u
ei and eigenvalues Œª
Œª1 ‚â• Œª2 ‚â• . . . ‚â• Œªn . We ask:
Problem 1. How many samples are sufficient to guarantee
that the inner product |he
ui , uj i| = |e
u‚àói uj | and the eigene
value gap |Œ¥Œªi | = |Œªi ‚àí Œªi | is smaller than some constant
t with probability larger than ?
Clearly, when asking that all eigenvectors and eigenvalues
of the sample and actual covariance matrices are close, we
will require at least as many samples as needed to ensure
e ‚àí Ck2 ‚â§ t. However, we might do better when
that kC
only a subset of the spectrum is of interest. The reason
is that inner products |he
ui , uj i| are strongly concentrated
along the eigenvalue axis. To illustrate this phenomenon,
let us consider the distribution constructed by the n = 784
pixel values of digit ‚Äò1‚Äô in the MNIST database. Figure 1,
compares the eigenvectors uj of the covariance computed
from all 6742 images, to the eigenvectors u
ei of the same computed from a random subple covariance matrices C
set of m = 10, 100, 500, and 1000 samples. For each
i = 1, 4, 20, 100, we depict at Œªj the average of |he
ui , uj i|
over 100 sampling draws. We observe that: (i) The magnitude of he
ui , uj i is inversely proportional to their eigenvalue
gap |Œªi ‚àí Œªj |. (ii) Eigenvector u
ej mostly lies in the span of
eigenvectors uj over which the distribution is concentrated.
We formalize these statements in two steps.
2.1

Perturbation arguments

First, we work in the setting of Hermitian matrices and notice the following inequality:
e = Œ¥C+C,
Theorem 3.2. For Hermitian matrices C and C
with eigenvectors uj and u
ei respectively, the inequality
|he
ui , uj i| ‚â§

2 kŒ¥Cuj k2
,
|Œªi ‚àí Œªj |

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

ei > sgn(Œªi ‚àí Œªj )(Œªi + Œªj ) and
holds for sgn(Œªi ‚àí Œªj ) 2Œª
Œªi 6= Œªj .
The above stands out from standard eigenspace perturbation results, such as the sin(Œò) Theorem (Davis & Kahan, 1970) and its variants (Huang et al., 2009; Hunter &
Strohmer, 2010; Yu et al., 2015) for three main reasons:
First, Theorem 3.2 characterizes the angle between any
pair of eigenvectors allowing us to jointly bound any linear
combination of inner-products. Though this often proves
handy (c.f. Section 5), it is infeasible using sin(Œò)-type arguments. Second, classical bounds are not appropriate for
a probabilistic analysis as they feature ratios of dependent
random variables (corresponding to perturbation terms). In
the analysis of spectral clustering, this complication was
ei ‚àí Œªj | (Hunter
dealt with by assuming that |Œªi ‚àí Œªj | ‚â§ |Œª
& Strohmer, 2010). We weaken this condition at a cost of
a multiplicative factor. In contrast to previous work, we
also prove that the condition is met a.a.s. Third, previous
bounds are expressed in terms of the minimal eigenvalue
gap between eigenvectors lying in the interior and exterior of the subspace of interest. This is a limiting factor
in practice as it renders the results only amenable to situations where there is a very large eigenvalue gap separating
the subspaces. The proposed result improves upon this by
considering every eigenvalue difference.
2.2

Concentration of measure

The second part of our analysis focuses on the covariance
and has a statistical flavor. In particular, we give an answer
to Problem 1 for various families of distributions.
In the context of distributions with finite second moment,
we prove in Section 4.1 that:
Theorem 4.1. For any two eigenvectors u
ei and uj of the
sample and actual covariance respectively, and for any real
number t > 0:
2
2kj
1 
,
(3)
P(|he
ui , uj i| ‚â• t) ‚â§
m t |Œªi ‚àí Œªj |
subject to the same conditions as Theorem 3.2.
For eigenvalues, we have the following corollary:
ei of C and C,
e
Corollary 2.1. For any eigenvalues Œªi and Œª
respectively, and for any t > 0, we have
!

2
e i ‚àí Œªi |
|Œª
1
ki
P
‚â•t ‚â§
.
Œªi
m Œªi t


Term kj = (E kxx‚àó uj k22 ‚àí Œª2j )1/2 captures the tendency
of the distribution to fall in the span of uj : the smaller the
tail in the direction of uj the less likely we are going to
confuse u
ei with uj .

For normal distributions, we have that kj2 = Œª2j + Œªj tr(C)
and the number of samples needed for |he
ui , uj i| to be small
is m = O(tr(C)/Œª2i ) when Œªj = O(1) and m = O(Œª‚àí2
i )
when Œªj = O(tr(C)‚àí1 ). Thus for normal distributions,
principal components ui and uj with min{Œªi /Œªj , Œªi } =
‚Ñ¶(tr(C)1/2 ) can be distinguished given a constant number
of samples. On the other hand, estimating Œªi with small
relative error requires m = O(tr(C)/Œªi ) samples and can
thus be achieved from very few samples when Œªi is large1 .
In Section 4.2, we also give a sharp bound for the family of
distributions supported within a ball (i.e., kxk ‚â§ r a.s.).
Theorem 4.2. For sub-gaussian distributions supported
within a centered Euclidean ball of radius r, there exists
an absolute constant c s.t. for any real number t > 0,
!
c m Œ¶ij (t)2
, (4)
P(|he
ui , uj i| ‚â• t) ‚â§ exp 1 ‚àí
2
Œªj kxkŒ®2
|Œª ‚àíŒª | t‚àí2Œª

where Œ¶ij (t) = 2 (ri 2 /Œªjj ‚àí1)1/2j ‚àí2 kxkŒ®2 and subject to the
same conditions as Theorem 3.2.
Above, kxkŒ®2 is the sub-gaussian norm, for which we usually have kxkŒ®2 = O(1) (Vershynin, 2010). As such, the
theorem implies that, whenever Œªi  Œªj = O(1), the sample requirement is with high probability m = O(r2 /Œª2i ).
These theorems solidify our experimental findings shown
in Figure 1 and provide a concrete characterization of the
relation between the spectrum of the sample and actual covariance matrix as a function of the number of samples,
the eigenvalue gap, and the distribution properties. As exemplified in Section 5 for linear dimensionality reduction,
we believe that our results carry strong implications for the
non-asymptotic analysis of PCA-based methods.

3

Perturbation Arguments

Before focusing on the sample covariance matrix, it helps
to study he
ui , uj i in the setting of Hermitian matrices. The
presentation of the results is split in three parts. Section 3.1
starts by studying some basic properties of inner products
of the form he
ui , uj i, for any i and j. The results are used in
Section 3.2 to provide a first bound on the angle between
two eigenvectors, and refined in Section 3.3.
3.1

Basic observations

We start by noticing an exact relation between the angle of
a perturbed eigenvector and the actual eigenvectors of C.
Lemma 3.1. For every
Pni and‚àó j in 1,‚àó2, . . . , n, the relation
ei ‚àí Œªj ) (e
(Œª
u‚àói uj ) = `=1 (e
ui u` ) (uj Œ¥Cu` ) holds .
1
Though the same cannot be stated about the absolute error
|Œ¥Œªi |, that is smaller for small Œªi .

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Proof. The proof follows from a modification of a standard
argument in perturbation theory. We start from the definiei u
eu
tion C
ei = Œª
ei and write
(C + Œ¥C) (ui + Œ¥ui ) = (Œªi + Œ¥Œªi ) (ui + Œ¥ui ).

(5)

Expanded, the above expression becomes

+

`=1

+

n
X

= Œªi

Œ≤ij u‚àój u` + Œ¥Œªi u‚àój ui + Œ¥Œªi

`=1

n
X

Œ≤ij u‚àój u` (7)

Cancelling the unnecessary terms and rearranging, we have

+

n
X

Œ≤ij u‚àój Œ¥Cu` .

n
X

(e
u‚àói u` )2

n
X

n
X

(u‚àój Œ¥Cu` )2

`=1

(u‚àój Œ¥Cu` )2 = kŒ¥C uj k22 , (11)

`=1

where in the last step we exploited Lemma 3.2. The proof
concludes by taking a square root at both sides of the inequality.
n
P

(u‚àój Œ¥Cu` )2 = kŒ¥C uj k22 .

Proof. We first notice that u‚àój Œ¥Cu` is a scalar and equal to
its transpose. Moreover, Œ¥C is Hermitian as the difference
of two Hermitian matrices. We therefore have that
n
n
X
X
(u‚àój Œ¥Cu` )2 =
u‚àój Œ¥Cu` u‚àó` Œ¥Cuj

(8)

n
X

(u` u‚àó` )Œ¥Cuj = u‚àój Œ¥CŒ¥Cuj = kŒ¥Cuj k22 ,

`=1

matching our claim.

ei ‚àí Œªj and
At this point, we note that (Œªi + Œ¥Œªi ‚àí Œªj ) = Œª
furthermore that Œ≤ij = u
e‚àói uj ‚àí u‚àói uj . With this in place,
equation (8) becomes
ei ‚àí Œªj ) (e
Œ¥Œªi u‚àój ui + (Œª
u‚àói uj ‚àí u‚àói uj )
n
X
= u‚àój Œ¥Cui +
(e
u‚àói u` ) u‚àój Œ¥Cu` ‚àí u‚àój Œ¥Cui .

`=1

= u‚àój Œ¥C

`=1

(9)

`=1

The proof completes by noticing that, in the left hand side,
e i ‚àí Œªj ) u
all terms other than (Œª
e‚àói uj fall-off, either due to
‚àó
ei ‚àí Œªj , o.w.
ui uj = 0, when i 6= j, or because Œ¥Œªi = Œª
As the expression reveals, he
ui , uj i depends on the orientation of u
ei with respect to all other u` . Moreover, the angles
between eigenvectors depend not only on the minimal gap
between the subspace of interest and its complement space
e i ‚àí Œªj .
(as in the sin(Œò) theorem), but on every difference Œª
This is a crucial ingredient to a tight bound, that will be
retained throughout our analysis.
3.2

=

`=1

Œ¥Œªi u‚àój ui + (Œªi + Œ¥Œªi ‚àí Œªj )Œ≤ij
u‚àój Œ¥Cui

. (10)

`=1

`=1

=

(e
u‚àói u` ) (u‚àój Œ¥Cu` )

We now use the Cauchy-Schwartz inequality

Lemma 3.2.

Œ≤ij u‚àój Œ¥Cu`

`=1
n
X

!2

`=1

(6)

where we used the fact that Cui = Œªi ui P
to eliminate
n
two terms. To proceed, we substitute Œ¥ui = j=1 Œ≤ij uj ,
where Œ≤ij = Œ¥u‚àói uj , into (6) and multiply from the left by
u‚àój , resulting to:
u‚àój Œ¥Cui

n
X

`=1

= Œªi Œ¥ui + Œ¥Œªi ui + Œ¥Œªi Œ¥ui ,

Œ≤ij u‚àój Cu`

ei ‚àí Œªj )2 (e
(Œª
u‚àói uj )2 =

ei ‚àí Œªj )2 (e
(Œª
u‚àói uj )2 ‚â§

CŒ¥ui + Œ¥Cui + Œ¥CŒ¥ui

n
X

Proof. We rewrite Lemma 3.1 as

Bounding arbitrary angles

We proceed to decouple the inner products.
e =
Theorem 3.1. For any Hermitian matrices C and C
Œ¥C + C, with eigenvectors uj and u
ei respectively, we have
ei ‚àí Œªj | |he
that |Œª
ui , uj i| ‚â§ kŒ¥C uj k2 .

3.3

Refinement

As a last step, we move all perturbation terms to the numerator, at the expense of a multiplicative constant factor.
e = Œ¥C+C,
Theorem 3.2. For Hermitian matrices C and C
with eigenvectors uj and u
ei respectively, the inequality
|he
ui , uj i| ‚â§

2 kŒ¥Cuj k2
,
|Œªi ‚àí Œªj |

ei > sgn(Œªi ‚àí Œªj )(Œªi + Œªj ) and
holds for sgn(Œªi ‚àí Œªj ) 2Œª
Œªi 6= Œªj .
Proof. Adding and subtracting Œªi from the left side of the
expression in Lemma 3.1 and from definition we have
(Œ¥Œªi + Œªi ‚àí Œªj ) (e
u‚àói uj ) =

n
X

(e
u‚àói u` ) (u‚àój Œ¥Cu` ).

(12)

`=1

For Œªi 6= Œªj , the above expression can be re-written as


n
P

‚àó
‚àó
‚àó
 (e
ui u` ) (uj Œ¥Cu` ) ‚àí Œ¥Œªi (e
ui uj )

|e
u‚àói uj | = `=1
|Œªi ‚àí Œªj |

Ô£±  n
Ô£º

P ‚àó
‚àó
Ô£¥
Ô£¥


Ô£¥
Ô£¥
(e
u
u
)
(u
Œ¥Cu
)
` 
Ô£≤
i `
j
|Œ¥Œªi | |e
u‚àói uj | Ô£Ω
`=1
‚â§ 2 max
,
. (13)
Ô£¥
|Œªi ‚àí Œªj |
|Œªi ‚àí Œªj | Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£æ

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Let us examine the right-hand side inequality carefully.
Obviously, when the condition |Œªi ‚àí Œªj | ‚â§ 2 |Œ¥Œªi | is not
met, the right clause of (13) is irrelevant. Therefore, for
|Œ¥Œªi | < |Œªi ‚àí Œªj | /2 the bound simplifies to

 n

P ‚àó
ui u` ) (u‚àój Œ¥Cu` )
2  (e
.
(14)
|e
u‚àói uj | ‚â§ `=1
|Œªi ‚àí Œªj |
Similar to the proof of Theorem 3.1, applying the CauchySchwartz inequality we have that
s
n
n
P
P
(u‚àój Œ¥Cu` )2
(e
u‚àói u` )2
2
`=1
`=1
2 kŒ¥Cuj k2
|e
u‚àói uj | ‚â§
=
,
|Œªi ‚àí Œªj |
|Œªi ‚àí Œªj |
(15)
where in the last step we used Lemma 3.2. To finish the
proof we notice that, due to Theorem 3.2, whenever |Œªi ‚àí
ei ‚àí Œªj |, one has
Œªj | ‚â§ | Œª
|e
u‚àói uj | ‚â§

kŒ¥C uj k2
2 kŒ¥Cuj k2
kŒ¥C uj k2
‚â§
<
.
e
|Œªi ‚àí Œªj |
|Œªi ‚àí Œªj |
|Œªi ‚àí Œªj |

(16)

Our bound therefore holds for the union of intervals |Œ¥Œªi | <
ei ‚àí Œªj |, i.e., for Œª
ei > (Œªi +
|Œªi ‚àí Œªj | /2 and |Œªi ‚àí Œªj | ‚â§ |Œª
e
Œªj )/2 when Œªi > Œªj and for Œªi < (Œªi + Œªj )/2 when
Œªi < Œªj .

4

Concentration of Measure

This section builds on the perturbation results of Section 3
to characterize how far any inner product he
ui , uj i and
ei are from the ideal estimates.
eigenvalue Œª
Before proceeding, we remark on some simplifications employed in the following. W.l.o.g., we will assume that the
mean E[x] is zero. In addition, we will assume the perspective of Theorem 3.2, for which the inequality sgn(Œªi ‚àí
ei > sgn(Œªi ‚àíŒªj )(Œªi +Œªj ) holds. This event is shown
Œªj ) 2Œª
to occur a.a.s. when the gap and the sample size are sufficiently large, but it is convenient to assume that it happens
almost surely. In fact, removing this assumption is possible
(see Section 4.1.2), but it is largely not pursued here as it
leads to less elegant and sharp estimates.
4.1

Distributions with finite second moment

Our first flavor of results is based on a variant of the
Tchebichef inequality and holds for any distribution with
finite second moment, though only with moderate probability estimates.
4.1.1

C ONCENTRATION OF EIGENVECTOR ANGLES

We start with the concentration of inner-products |he
ui , uj i|.

Theorem 4.1. For any two eigenvectors u
ei and uj of the
sample and actual covariance respectively, with Œªi 6= Œªj ,
and for any real number t > 0, we have
P(|he
ui , uj i| ‚â• t) ‚â§

2
1  2 kj
m t |Œªi ‚àí Œªj |

ei > sgn(Œªi ‚àí Œªj )(Œªi + Œªj ) and kj =
for sgn(Œªi ‚àí Œªj ) 2Œª


1/2
‚àó
2
E kxx uj k2 ‚àí Œª2j
.
Proof. According to a variant of Tchebichef‚Äôs inequality (Sarwate, 2013), for any random variable X and for any
real numbers t > 0 and Œ±:
P(|X ‚àí Œ±| ‚â• t) ‚â§

Var[X] + (E[X] ‚àí Œ±)2
.
t2

(17)

Setting X = he
ui , uj i and Œ± = 0, we have
2

Var[he
ui , uj i] + E[he
ui , uj i]
P(|he
ui , uj i| ‚â• t) ‚â§
2
t




E he
ui , uj i2
4 E kŒ¥Cuj k22
=
‚â§ 2
, (18)
t2
t (Œªi ‚àí Œªj )2
where the last inequality follows from Theorem 3.2. We
continue by expanding Œ¥C using the definition of the eigenvalue decomposition and substituting the expectation.
h
i


e j ‚àí Œªj uj k2
E kŒ¥Cuj k22 = E kCu
2
h
i
e ‚àí Œªj )(C
e ‚àí Œªj )uj
= E u‚àój (C
h
i
h i
e 2 uj + Œª2 ‚àí 2Œªj u‚àó E C
e uj
= E u‚àój C
j
j
h
i
e 2 uj ‚àí Œª2 .
= E u‚àój C
(19)
j
In addition,
m
X



E (xp x‚àóp )(xq x‚àóq )
E
uj
=
m2
p,q=1

 



m
X E xp x‚àóp E xq x‚àóq
X
E xp x‚àóp xp x‚àóp
‚àó
‚àó
=
uj
uj +
uj
uj
m2
m2
p=1
h

e 2 uj
u‚àój C

i

u‚àój

p6=q

m(m ‚àí 1) 2
1
Œªj + u‚àój E[xx‚àó xx‚àó ] uj
2
m
m
1 2
1 ‚àó
= (1 ‚àí ) Œªj + uj E[xx‚àó xx‚àó ] uj
m
m
=

(20)

and therefore


1
1
E kŒ¥Cuj k22 = (1 ‚àí ) Œª2j + u‚àój E[xx‚àó xx‚àó ] uj ‚àí Œª2j
m
m

E kxx‚àó uj k22 ‚àí Œª2j
u‚àój E[xx‚àó xx‚àó ] uj ‚àí Œª2j
=
=
.
m
m
Putting everything together, the claim follows.

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

The following corollary will be very useful when applying
our results.
Corollary 4.1. For any weights wij and real t > 0:
Ô£´
Ô£∂
X
X
4 wij kj2
,
PÔ£≠
wij he
ui , uj i2 > tÔ£∏ ‚â§
m t (Œªi ‚àí Œªj )2
i6=j

i6=j



1/2
and wij 6= 0 when
where kj = E kxx‚àó uj k22 ‚àí Œª2j
ei > sgn(Œªi ‚àí Œªj )(Œªi + Œªj ).
Œªi 6= Œªj and sgn(Œªi ‚àí Œªj ) 2Œª
Proof. We proceed as in the proof of Theorem 4.1:
hP
i
Ô£´
Ô£∂
2
X
 21
E
w
he
u
,
u
i
ij
i
j
i6=j
PÔ£≠
wij he
ui , uj i2 > tÔ£∏ ‚â§
t2
i6=j


E kŒ¥Cuj k22
4 X
‚â§ 2
(21)
wij
t
(Œªi ‚àí Œªj )2
i6=j



The claim follows by computing E kŒ¥Cuj k22 (as before)
and squaring both terms within the probability.
4.1.2

E IGENVALUE CONCENTRATION

Though perhaps less sharp than what is currently known
(e.g., see (Silverstein & Bai, 1995; Bai & Silverstein, 1998)
for the asymptotic setting), it might be interesting to observe that a slight modification of the same argument can
be used to characterize the eigenvalue relative difference,
and as a consequence the main condition of Theorem 4.1.
ei of C and C,
e
Corollary 4.2. For any eigenvalues Œªi and Œª
respectively, and for any t > 0, we have
!

2
e i ‚àí Œªi |
|Œª
ki
1
P
,
‚â•t ‚â§
Œªi
m Œªi t


where ki = (E kxx‚àó ui k22 ‚àí Œªi )1/2 .
Proof. Directly from the Bauer-Fike theorem (Bauer &
Fike, 1960) one sees that
e i ‚àí Œªi ui k2 = kŒ¥Cui k2 .
|Œ¥Œªi | ‚â§ kCu

4.1.3

T HE INFLUENCE OF THE DISTRIBUTION


As seen by the straightforward inequality E kxx‚àó uj k22 ‚â§
E kxk42 , kj connects to the kurtosis of the distribution.
However, it also captures the tendency of the distribution
to fall in the span of uj .
To see this, we will work with the whitened random vectors Œµ = C +1/2 x, where C + denotes the Moore‚ÄìPenrose
pseudoinverse of C. In particular,
h
i
kj2 = E u‚àój C 1/2 ŒµŒµ‚àó CŒµŒµ‚àó C 1/2 uj ‚àí Œª2j
i
h
= Œªj (E kŒõ1/2 U ‚àó ŒµŒµ‚àó uj k22 ‚àí Œªj )
= Œªj

ei >
Using this, we find that the event E = {sgn(Œªi ‚àíŒªj ) 2Œª
sgn(Œªi ‚àí Œªj )(Œªi + Œªj )} occurs with probability at least


2ki2
ei ‚àí Œªi | < |Œªi ‚àí Œªj | > 1 ‚àí
P(E) ‚â• P |Œª
.
2
m|Œªi ‚àí Œªj |
Therefore, one eliminates the condition from Theorem 4.1‚Äôs statement by relaxing the bound to
P(|he
ui , uj i| ‚â• t) ‚â§ P(|he
ui , uj i| ‚â• t | E) + (1 ‚àí P(E))


2kj2
2
2
+
k
<
i . (23)
m|Œªi ‚àí Œªj | t2 |Œªi ‚àí Œªj |




Œª` E ŒµÃÇ(`)2 ŒµÃÇ(j)2 ‚àí Œªj ,

(24)

`=1

where ŒµÃÇ = U ‚àó Œµ. It is therefore easier to untangle the spaces
spanned by u
ei and uj when the variance of the distribution
along the latter space is small (the expression is trivially
minimized when Œªj ‚Üí 0) or when the variance is entirely
contained along that space (the expression is also small
when Œªi = 0 for all i 6= j). In addition, it can be seen
that distributions with fast decaying tails
 allow
 for better
principal component identification (E ŒµÃÇ(j)4 is a measure
of kurtosis over the direction of uj ).
For the particular case of a Normal distribution, we provide
a closed-form expression.
Corollary 4.3. For a Normal distribution, we have kj2 =
Œªj (Œªj + tr(C)).
Proof. For a centered and normal distribution with identity covariance, the choice of basis is arbitrary and the vector ŒµÃÇ = U ‚àó Œµ is also zero mean with identity
 covariance.

Moreover,
for
every
`
=
6
j
we
can
write
E
ŒµÃÇ(`)2 ŒµÃÇ(j)2 =




E ŒµÃÇ(`)2 E ŒµÃÇ(j)2 = 1. This implies that
n
X




E kxx‚àó uj k22 = Œª2j E ŒµÃÇ(j)4 + Œªj
Œª`
`6=j

=

(22)

The proof is then identical to that of Theorem 4.1.

n
X

Œª2j (3

‚àí 1) + Œªj tr(C) = 2Œª2j + Œªj tr(C) (25)

and, accordingly, kj2 = Œªj (Œªj + tr(C)).
4.2

Distributions supported in a Euclidean ball

Our last result provides a sharper probability estimate for
the family of sub-gaussian distributions supported in a centered Euclidean ball of radius r, with their Œ®2 -norm
kxkŒ®2 = sup khx, yikœà2 ,

(26)

y‚ààS n‚àí1

where S n‚àí1 is the unit sphere and with the œà2 -norm of a
random variable X defined as
p 1/p

kXkœà2 = sup p‚àí1/2 E[|X| ]
p‚â•1

.

(27)

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Our setting is therefore similar to the one used to study covariance estimation (Vershynin, 2012). Due to space constraints, we refer the reader to the excellent review article (Vershynin, 2010) for an introduction to sub-gaussian
distributions as a tool for non-asymptotic analysis of random matrices.
Theorem 4.2. For sub-gaussian distributions supported
within a centered Euclidean ball of radius r, there exists an
absolute constant c, independent of the sample size, such
that for any real number t > 0,
!
c m Œ¶ij (t)2
,
(28)
P(|he
ui , uj i| ‚â• t) ‚â§ exp 1 ‚àí
2
Œªj kxkŒ®2
|Œª ‚àíŒª | t‚àí2Œª

where Œ¶ij (t) = 2 (ri 2 /Œªjj ‚àí1)1/2j ‚àí 2 kxkŒ®2 , Œªi 6= Œªj and
ei > sgn(Œªi ‚àí Œªj )(Œªi + Œªj ).
sgn(Œªi ‚àí Œªj ) 2Œª
Proof. We start from the simple observation that,
for every upper bound B of |he
ui , uj i| the relation
P(|he
ui , uj i| > t) ‚â§ P(B > t) holds. To proceed therefore we will construct a bound with a known tail. As we
saw in Sections 3.3 and 4.1,
2 kŒ¥Cuj k2
|he
ui , uj i| ‚â§
|Œªi ‚àí Œªj |


Pm


2 (1/m) p=1 (xp x‚àóp uj ‚àí Œªj uj )
2
=
|Œªi ‚àí Œªj |

Pm 
2 p=1 xp x‚àóp uj ‚àí Œªj uj 2
‚â§
m |Œªi ‚àí Œªj |
Pm q ‚àó 2 ‚àó
2 p=1 (uj xp ) (xp xp ) ‚àí 2Œªj (u‚àój xp )2 + Œª2j
=
m |Œªi ‚àí Œªj |
Pm q ‚àó 2
2 p=1 (uj xp ) (kxp k22 ‚àí Œªj ) + Œª2j
=
(29)
m |Œªi ‚àí Œªj |
Assuming further that kxk2 ‚â§ r, and since the numerator
is minimized when kxp k22 approaches Œªj , we can write for
every sample x = C 1/2 Œµ:
q
q
(u‚àój x)2 (kxk22 ‚àí Œªj ) + Œª2j ‚â§ (u‚àój x)2 (r2 ‚àí Œªj ) + Œª2j
q
= Œªj (u‚àój Œµ)2 (r2 ‚àí Œªj ) + Œª2j
q
(30)
‚â§ |u‚àój Œµ| Œªj r2 ‚àí Œª2j + Œªj ,
which is a shifted and scaled version of the random variable
|ŒµÃÇ(j)| = |u‚àój Œµ|. Setting a = (Œªj r2 ‚àí Œª2j )1/2 , we have
P(|he
ui , uj i| ‚â• t) ‚â§ P

2

Pm

p=1 (|ŒµÃÇp (j)| a

m |Œªi ‚àí Œªj |

+ Œªj )

!
‚â•t

=P

m
X

!
(|ŒµÃÇp (j)| a + Œªj ) ‚â• 0.5 mt |Œªi ‚àí Œªj |

p=1

!
m (0.5 t |Œªi ‚àí Œªj | ‚àí Œªj )
=P
|ŒµÃÇp (j)| ‚â•
.
a
p=1
m
X

(31)

By Lemma 4.1 however, the left hand side is a sum of independent sub-gaussian variables. Since the summands are
not centered, we expand each |ŒµÃÇp (j)| = zp + E[|ŒµÃÇp (j)|]
in terms of a centered sub-gaussian zp with the same œà2 norm. Furthermore, by Jensen‚Äôs inequality and Lemma 4.1
1/2

2
kxkŒ®2 .
‚â§
E[|ŒµÃÇp (j)|] ‚â§ E ŒµÃÇp (j)2
Œªj
Therefore, if we set Œ¶ij (t) =

(0.5 |Œªi ‚àíŒªj | t‚àíŒªj )
(r 2 /Œªj ‚àí1)1/2

(32)

‚àí 2 kxkŒ®2

!
mŒ¶ij (t)
zp ‚â•
P(|he
ui , uj i| ‚â• t) ‚â§ P
.
Œªj
p=1
m
X

(33)

Moreover, by the rotation invariance principle, the left hand
side of the last inequality
is a sub-gaussian with œà2 -norm
Pm
2
smaller than (c1 p=1 kzp kœà2 )1/2 = (c1 m)1/2 kzkœà2 ‚â§
(c1 m/Œªj )1/2 kxkŒ®2 , for some absolute constant c1 . As a
consequence, there exists an absolute constant c2 , such that
for each Œ∏ > 0:
m 
!
!
X 
c2 Œ∏2 Œªj


zp  ‚â• Œ∏ ‚â§ exp 1 ‚àí
P 
.
(34)
2


m kxkŒ®2
p=1
Substituting Œ∏ = m Œ¶ij (t)/Œªj , we have
P(|he
ui , uj i| ‚â• t) ‚â§ exp 1 ‚àí
= exp 1 ‚àí

c2 m2 Œ¶ij (t)2 Œªj

!

2

mŒª2j kxkŒ®2
!
c2 m Œ¶ij (t)2
2

Œªj kxkŒ®2

,

(35)

which is the desired bound.
Lemma 4.1. If x is a sub-gaussian random vector and Œµ =
C +1/2 x, then for every i, the random variable ‚àö
ŒµÃÇ(i) = u‚àói Œµ
is also sub-gaussian, with kŒµÃÇ(i)kœà2 ‚â§ kxkŒ®2 / Œªi .
Proof. Notice that


X

 n 1/2 ‚àó

‚àó 

kxkŒ®2 = sup khx, yikœà2 = sup 
Œªj (uj y)(uj Œµ)
y‚ààS n‚àí1
y‚ààS n‚àí1 j=1

œà2


X

n


1/2
1/2
‚â•
Œªj (u‚àój ui )ŒµÃÇ(j)

 = Œªi kŒµÃÇ(i)kœà2 , (36)

 j=1
œà2

where, for the last inequality, we set y = ui .

5

sample requirement (% of n)

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Application to Dimensionality Reduction

To emphasize the utility of our results, in the following we
consider the practical example of linear dimensionality reduction. We show that a direct application of our bounds
leads to upper estimates on the sample requirement.
In terms of mean squared error, the optimal way to reduce
the dimension of a sample x of a distribution is by projecting it over the subspace of the covariance with maximum
variance. Denote by Ik the diagonal matrix with the first
k diagonal entries equal to one and the rest zero. When
the actual covariance is known, the expected energy loss
induced by the Pk x = Ik U ‚àó x projection is


P
Œªi
E kxk22 ‚àí kPk xk22
= i>k .
(37)
loss(Pk ) =
2
E[kxk2 ]
tr(C)
e ‚àó is constructed
However, when the projector Pek = Ik U
from the sample covariance, we have
h
i
E kxk22 ‚àí kPek xk22
loss(Pek ) =
E[kxk22 ]
Pn
e)
e ‚àó U ŒõU ‚àó U
Œªi ‚àí tr(Ik U
= i=1
tr(C)
Pn
P
Œª
‚àí
u‚àói uj )2 Œªj
i=1 i
i‚â§k,j (e
=
(38)
tr(C)
with the expectation taken over the to-be-projected vectors
x, but not the samples used to estimate the covariance. After slight manipulation, one finds that
P
(e
u‚àói uj )2 (Œªi ‚àí Œªj )
i‚â§k,j6=i
loss(Pek ) = loss(Pk ) +
. (39)
tr(C)
The loss difference has an intuitive interpretation: when reducing the dimension with Pek one looses either by discarding useful energy (terms j > k), or by displacing kept components within the permissible eigenspace (terms j ‚â§ k).
Note also that all terms with j < i are negative and can
be excluded from the sum if we are satisfied we an upper
estimate2 .
It is an implication of (39) and Corollary 4.1 that, when its
conditions hold, for any distribution and t > 0

 X
4 kj2
t
P loss(Pek ) > loss(Pk ) +
‚â§
.
tr(C)
mt |Œªi ‚àí Œªj |
i‚â§k
j>i

Observe that the loss difference becomes particularly small
whenever k is small: (i) the terms in the sum are fewer and
(ii) the magnitude of each term decreases (due to |Œªi ‚àíŒªj |).
2

A similar approach could also be utilized to derive a lower
bound of the quantity loss(Pek ) ‚àí loss(Pk ).

100
tolerance = 0.02
tolerance = 0.05
tolerance = 0.1

80
60
40
20

2

4

8

16

32

64

k

Figure 2: The figure depicts for each k, the sample size needed
such that the loss difference loss(Pek )‚àíloss(Pk ) becomes smaller
than some tolerance. We can observe that, in MNIST, linear dimensionality reduction works with fewer than n = 725 samples
when the size k of the reduced dimension is small.

This phenomenon is also numerically verified in Figure 2
for the distribution of the images featuring digit ‚Äò3‚Äô in
MNIST (total 6131 images with n = 784 pixels each).
The figure depicts for different k how many samples are
required such that the loss difference is smaller than a tolerance threshold, here 0.02, 0.05, and 0.1. Each point in the
figure corresponds to an average over 10 sampling draws.
The trends featured in these numerical results agree with
our theoretical intuition. Moreover they illustrate that for
modest k the sample requirement is far smaller than n.
It is also interesting to observe that for covariance matrices that are (approximately) low-rank, we obtain estimates
reminiscent of compressed sensing (CandeÃÄs et al., 2011),
in the sense that the sample requirement becomes a function of the non-zero eigenvalues. Though intuitive, with the
exception of (Koltchinskii et al., 2016), this dependency of
the estimation accuracy on the rank was not transparent in
known results for covariance estimation (Rudelson, 1999;
Adamczak et al., 2010; Vershynin, 2012).

6

Conclusions

The main contribution of this paper was the derivation
of non-asymptotic bounds for the concentration of innerproducts |he
ui , uj i| involving eigenvectors of the sample
and actual covariance matrices. We also showed how these
results can be extended to reason about eigenvalues and we
applied them to the non-asymptotic analysis of linear dimensionality reduction.
We have identified two interesting directions for further research. The first has to do with obtaining tighter estimates.
Especially with regards to our perturbation arguments, we
believe that our current bounds on inner products could be
sharpened by at least a constant multiplicative factor. The
second direction involves using our results for the analysis of methods that utilize the eigenvectors of the covariance, such that principal component projection and regression (Jolliffe, 1982; Frostig et al., 2016).

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

References
Adamczak, Rados≈Çaw, Litvak, Alexander, Pajor, Alain, and
Tomczak-Jaegermann, Nicole. Quantitative estimates of
the convergence of the empirical covariance matrix in
log-concave ensembles. Journal of the American Mathematical Society, 23(2):535‚Äì561, 2010.
Ahmed, SE. Large-sample estimation strategies for eigenvalues of a wishart matrix. Metrika, 47(1):35‚Äì45, 1998.
Anderson, Theodore Wilbur. Asymptotic theory for principal component analysis. The Annals of Mathematical
Statistics, 34(1):122‚Äì148, 1963.
Bai, ZD. Methodologies in spectral analysis of large dimensional random matrices, a review. Statistica Sinica,
pp. 611‚Äì662, 1999.
Bai, ZD and Yin, YQ. Limit of the smallest eigenvalue of a
large dimensional sample covariance matrix. The annals
of Probability, pp. 1275‚Äì1294, 1993.
Bai, ZD, Miao, BQ, Pan, GM, et al. On asymptotics of
eigenvectors of large sample covariance matrix. The Annals of Probability, 35(4):1532‚Äì1572, 2007.
Bai, Zhi-Dong and Silverstein, Jack W. No eigenvalues
outside the support of the limiting spectral distribution of
large-dimensional sample covariance matrices. Annals
of probability, pp. 316‚Äì345, 1998.
Bauer, Friedrich L and Fike, Charles T. Norms and exclusion theorems. Numerische Mathematik, 2(1):137‚Äì141,
1960.
Berkmann, Jens and Caelli, Terry. Computation of surface
geometry and segmentation using covariance techniques.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(11):1114‚Äì1116, 1994.

Huang, Ling, Yan, Donghui, Taft, Nina, and Jordan,
Michael I. Spectral clustering with perturbed data. In
Advances in Neural Information Processing Systems, pp.
705‚Äì712, 2009.
Hunter, Blake and Strohmer, Thomas.
Performance
analysis of spectral clustering on compressed, incomplete and inaccurate measurements. arXiv preprint
arXiv:1011.0997, 2010.
Jolliffe, Ian. Principal component analysis. Wiley Online
Library, 2002.
Jolliffe, Ian T. A note on the use of principal components
in regression. Applied Statistics, pp. 300‚Äì303, 1982.
Kambhatla, Nandakishore and Leen, Todd K. Dimension
reduction by local principal component analysis. Neural
computation, 9(7):1493‚Äì1516, 1997.
Koltchinskii, Vladimir and Lounici, Karim. Normal approximation and concentration of spectral projectors of
sample covariance. arXiv preprint arXiv:1504.07333,
2015.
Koltchinskii, Vladimir, Lounici, Karim, et al. Asymptotics
and concentration bounds for bilinear forms of spectral
projectors of sample covariance. In Annales de l‚ÄôInstitut
Henri PoincareÃÅ, ProbabiliteÃÅs et Statistiques, volume 52,
pp. 1976‚Äì2013. Institut Henri PoincareÃÅ, 2016.
Mestre, Xavier. Improved estimation of eigenvalues and
eigenvectors of covariance matrices using their sample
estimates. IEEE Transactions on Information Theory,
54(11), 2008.
Rudelson, Mark. Random vectors in the isotropic position.
Journal of Functional Analysis, 164(1):60‚Äì72, 1999.

CandeÃÄs, Emmanuel J., Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM, 58(3):11:1‚Äì11:37, June 2011.

Sarwate,
Dilip.
Two-sided chebyshev inequality for event not symmetric around the
mean?
Mathematics Stack Exchange, 2013.
URL:http://math.stackexchange.com/q/144675 (version:
2012-05-13).

Davis, Chandler and Kahan, William Morton. The rotation
of eigenvectors by a perturbation. III. SIAM Journal on
Numerical Analysis, 7(1):1‚Äì46, 1970.

Schott, James R. Asymptotics of eigenprojections of correlation matrices with some applications in principal components analysis. Biometrika, pp. 327‚Äì337, 1997.

Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal component projection without principal component analysis. In Proceedings of
The 33rd International Conference on Machine Learning, pp. 2349‚Äì2357, 2016.

Shaghaghi, Mahdi and Vorobyov, Sergiy A. Subspace
leakage analysis of sample data covariance matrix. In
ICASSP, pp. 3447‚Äì3451. IEEE, 2015.

Girko, V. Strong law for the eigenvalues and eigenvectors
of empirical covariance matrices. 1996.

Silverstein, Jack W and Bai, ZD. On the empirical distribution of eigenvalues of a class of large dimensional
random matrices. Journal of Multivariate analysis, 54
(2):175‚Äì192, 1995.

How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?

Vershynin, Roman. Introduction to the non-asymptotic
analysis of random matrices. arXiv:1011.3027, 2010.
Vershynin, Roman. How close is the sample covariance
matrix to the actual covariance matrix? Journal of Theoretical Probability, 25(3):655‚Äì686, 2012.
Yu, Yi, Wang, Tengyao, Samworth, Richard J, et al. A useful variant of the davis‚Äìkahan theorem for statisticians.
Biometrika, 102(2):315‚Äì323, 2015.

