Density Level Set Estimation on Manifolds with DBSCAN

Heinrich Jiang 1

Abstract
We show that DBSCAN can estimate the connected components of the Î»-density level set {x :
f (x) â‰¥ Î»} given n i.i.d. samples from an unknown density f . We characterize the regularity of the level set boundaries using parameter
Î² > 0 and analyze the estimation error under the
Hausdorff metric. When the data lies in RD we
e âˆ’1/(2Î²+D) ), which matches
obtain a rate of O(n
known lower bounds up to logarithmic factors.
When the data lies on an embedded unknown ddimensional manifold in RD , then we obtain a
e âˆ’1/(2Î²+dÂ·max{1,Î²}) ). Finally, we prorate of O(n
vide adaptive parameter tuning in order to attain
these rates with no a priori knowledge of the intrinsic dimension, density, or Î².

1. Introduction
DBSCAN (Ester et al., 1996) is one of the most popular clustering algorithms amongst practitioners and has had
profound success in a wide range of data analysis applications. However, despite this, its statistical properties have
not been fully understood. The goal of this work is to give
a theoretical analysis of the procedure and to the best of
our knowledge, provide the first analysis of density levelset estimation on manifolds. We also contribute ideas to
related areas that may be of independent interest.
DBSCAN aims at discovering clusters which turn out to
be the high-density regions of the dataset. It takes in two
hyperparameters: minPts and Îµ. It defines a point as a
core-point if there are at least minPts sample points in its Îµradius neighborhood. The points within the Îµ-radius neighborhood of a core-point are said to be directly reachable
from that core-point. Then, a point q is reachable from a
core-point p if there exists a path from q to p where each
point is directly reachable from the next point. It is now
clear that this definition of reachable gives a partitioning of
1
Google.
Correspondence to:
rich.jiang@gmail.com>.

Heinrich Jiang <hein-

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

the dataset (and remaining points not reachable from any
core-point are considered noise). This partitioning is the
clustering that is returned by DBSCAN.
The problem of analyzing DBSCAN has recently been explored in (Sriperumbudur & Steinwart, 2012). Their analysis is for a modified version of DBSCAN and is not focused
on estimating a fixed density level. Their results have many
desirable properties, but are not immediately applicable for
what this paper tries to address. Using recent developments
in topological data analysis along with some tools we develop in this paper, we show that it is now possible to analyze the original procedure.
The clusters DBSCAN aims at discovering can be viewed
as approximations of the connected components of the level
sets {x : f (x) â‰¥ Î»} where f is the density and Î» is
some density level. We provide the first comprehensive
analysis in tuning minPts and Îµ to estimate the density
level set for a particular level. Here, the density level Î»
is known to the algorithm while the density remains unknown. Density level set estimation has been studied extensively. e.g., (Carmichael et al., 1968; Hartigan, 1975;
Polonik, 1995; Cuevas & Fraiman, 1997; Walther, 1997;
Tsybakov et al., 1997; BaÄ±llo et al., 2001; Cadre, 2006; Willett & Nowak, 2007; Biau et al., 2008; Rigollet & Vert,
2009; Maier et al., 2009; Singh et al., 2009; Rinaldo &
Wasserman, 2010; Steinwart, 2011; Rinaldo et al., 2012;
Steinwart et al., 2015; Chen et al., 2016; Jiang, 2017).
However approaches that obtain state-of-art consistency results are largely unpractical (i.e. unimplementable). Our
work shows that in actuality, DBSCAN, a procedure known
for decades and has since been used widely, can achieve
the strongest known results. Also, unlike much of the existing work, we show that DBSCAN can also recover the
connected components of the level sets separately and bijectively.
Our work begins with the insight that DBSCAN behaves
like an Îµ-neighborhood graph, which is different from, but
related to the k-nearest neighbor graph. The latter has
been heavily used for cluster-tree estimation (Chaudhuri &
Dasgupta, 2010; Stuetzle & Nugent, 2010; Kpotufe & von
Luxburg, 2011; Chaudhuri et al., 2014; Jiang & Kpotufe,
2017) and in this paper we adapt some of these ideas for
Îµ-neighborhood graphs.

Density Level Set Estimation on Manifolds with DBSCAN

Cluster-tree estimation aims at discovering the hierarchical tree structure of the connected-components as the levels
vary. Balakrishnan et al. (2013) extends results by Chaudhuri & Dasgupta (2010) to the setting where the data lies
on a lower dimensional manifold and provide consistency
results depending on the lower dimension and independent
of the ambient dimension. Here we are instead interested
in how to set minPts and Îµ in order to estimate a particular level and provide rates on the Hausdorff distance error. This is different from works on cluster tree estimation
which focuses on how to recover the tree structure rather
than recovering a particular level. In that regard, we also
require density estimation bounds in order to get a handle
on the true density-levels and the empirical ones.
Dasgupta & Kpotufe (2014) gives us optimal highprobability finite-sample k-NN density estimation bounds
which hold uniformly; this is key to obtaining optimal
level-set estimation rates under the Hausdorff error. Much
of the previous works on density level-set estimation, e.g.
(Rigollet & Vert, 2009) provide rates under risk measures
such as symmetric set difference. These metrics are considerably weaker than the Hausdorff metric; the latter is a uniform guarantee. There are such bounds for the histogram
density estimator. This allowed Singh et al. (2009) to obtain optimal rates under Hausdorff metric, while having
a fully adaptive procedure. This was a significant breakthrough for level set estimation, as discussed by Chazal
et al. (2015). We believe this to be the strongest consistency results obtained thus far. However, a downside is that
the histogram density estimator has little practical value.
Here, aided with the desired bounds on the k-NN density
estimator, we can actually obtain similar results to Singh
et al. (2009) but with the clearly practical DBSCAN.
We extend the k-NN density estimation results of Dasgupta
& Kpotufe (2014) to the manifold case, as the bulk our
analysis is about the more general case that the data lies on
a manifold. Density-based procedures perform poorly in
high-dimensions since the number of samples required increases exponentially in the dimensionâ€“ the so called curse
of dimensionality. Thus, the consequences of handling the
manifold case are of practical significance. Since the estimation rates we obtain depend only on the intrinsic dimension, it explains why DBSCAN can do well in high dimensions if the data has low intrinsic dimension (i.e. the manifold hypothesis). Given the modern capacity of systems
to collect data of increasing complexity, it has become ever
more important to understand the feasibility of practical
algorithms in high dimensions.
To analyze DBSCAN, we write minPts and Îµ in terms of
the d, unknown manfold dimension; k, which controls the
density estimator; and Î», which determines which level to
estimate. We assume knowledge of Î» with the goal of es-

timating the Î»-level set of the density. We give a range of
k in terms of n and corresponding consistency guarantees
and estimation rates for such choices. We then adaptively
tune d and k in order to attain close to optimal performance
with no a priori knowledge of the distribution. Adaptivity
is highly desirable because it allows for automatic tuning of
the hyper-parameters, which is a core tenet of unsupervised
learning. To solve for the unknown dimension, we use an
estimator from Farahmand et al. (2007), which we show to
have considerably better finite-sample behavior than previously thought. More details and discussion of related works
is in the main text. We then provide a new method of choosing k such that it will asymptotically approach a value that
provides near-optimal level set estimation rates.

2. Overview
We start by analyzing the procedure under the manifold
assumption. The end of the paper will discuss the fulldimensional setting. The bulk of our contribution lies in
analyzing the former situation, while the analysis of the latter uses a subset of those techniques.
â€¢ Section 3 proves that the clusters returned by DBSCAN are close to the connected components of certain Îµ-neighborhood graphs (Lemma 2). This is significant because these graphs can be shown to estimate
density level sets.
â€¢ Section 4 introduces the manifold setting and provides
supporting results including k-nearest neighbor density estimation bounds (Lemma 5 and Lemma 6) that
are useful later on.
â€¢ Section 5 provides a range of parameter settings under which for each true cluster, there exists a corresponding cluster returned by DBSCAN (Lemma 7 and
Lemma 8), and a rate for the Hausdorff distance between them (Theorem 1).
â€¢ Section 6 shows how one can apply DBSCAN a second time to remove false clusters from the first application, thus completing a bijection between the estimates and the true clusters (Theorem 2).
â€¢ Section 7 explains how to adaptively tune the parameters so that they fall within the theoretical ranges. The
main contributions of this section are a stronger result
about a known k-nearest neighbor based approach to
estimating the unknown dimension (Theorem 3) and a
new way to tune k to approach an optimal choice of k
(Theorem 4).
â€¢ Section 8 gives the result when the data lives in RD
without the manifold assumption.

Density Level Set Estimation on Manifolds with DBSCAN

3. The connection to neighborhood graphs
This section is dedicated towards the understanding of
the clusters produced by DBSCAN. The algorithm can be
found in (Ester et al., 1996) and is not shown here since
Lemma 1 characterizes what DBSCAN returns.
We have n i.i.d. samples X = {x1 , ..., xn } drawn from a
distribution F over RD .
Definition 1. Define the k-NN radius of x âˆˆ RD as
rk (x) := inf{r > 0 : |X âˆ© B(x, r)| â‰¥ k},
where B(x, r) denotes the Euclidean ball of radius r centered at x. Let G(k, Îµ) denote the Îµ-neighborhood level
graph of X with vertices {x âˆˆ X : rk (x) â‰¤ Îµ} and an
edge between x and x0 iff ||x âˆ’ x0 || â‰¤ Îµ.
Remark 1. This is slightly different from Îµ-neighborhood
graph, which includes all vertices. Here we exclude vertices below certain empirical density level (i.e. rk (x) > Îµ).
The next definition is relevant to DBSCAN and is from (Ester et al., 1996) but in the notation of Definition 1.
Definition 2. The following is with respect to fixed Îµ > 0
and minPts âˆˆ N.

Proof. Take any K âˆˆ K. Each point in K is a core-point
and by Lemma 1 and the definition of density-reachable,
each point in K belongs to the same C âˆˆ C. Thus, K âŠ†
{x âˆˆ C : rk (x) â‰¤ Îµ}. Next we show that K = {x âˆˆ C :
rk (x) â‰¤ Îµ}.
Suppose there exists core-point x âˆˆ C but x âˆˆ
/ K and let
y âˆˆ K. By Lemma 1, there exists core-point c âˆˆ C such
that all points in C are directly reachable from c. Then
there exists a path of core-points from x to c with pairwise
edges of length at most Îµ. The same holds for c to y. Thus
there exists such a path of core-points from x to y, which
means that x, y are in the same CC of G(minP ts, Îµ), contradicting the assumption that x âˆˆ
/ K and y âˆˆ K. Thus,
in fact K = {x âˆˆ C : rk (x) â‰¤ Îµ}. The result now follows since C consists of points that are at most Îµ from its
core-points.
We can now see that DBSCANâ€™s clusterings can be viewed
as the connected components (CCs) of an appropriate neighborhood level graph. Using a neighborhood graph to
approximate the level-set has been studied in (Rinaldo &
Wasserman, 2010). The difference is that they use a kernel
density estimator instead of a k-NN density estimator and
study the convergence properties under different settings.

â€¢ p is a core-point if rminPts (p) â‰¤ Îµ.

4. Manifold Setting

â€¢ q is directly density-reachable from p if |p âˆ’ q| â‰¤ Îµ
and p is a core-point.

4.1. Setup

â€¢ q is density-reachable from p if there exists a sequence
q = p1 , p2 , ..., pm = p such that pi is directly densityreachable from pi+1 for i = 1, .., m âˆ’ 1.

We make the following regularity assumptions which are
standard among works on manifold learning e.g. (Baraniuk
& Wakin, 2009; Genovese et al., 2012; Balakrishnan et al.,
2013).

The following result is paraphrased from Lemmas 1 and 2
from (Ester et al., 1996), which characterizes the clusters
learned by DBSCAN.
Lemma 1. (Ester et al., 1996) Let C be the clusters returned by DBSCAN(minPts, Îµ). For any core-point x, there
exists C âˆˆ C with x âˆˆ C. On the other hand, for any
C âˆˆ C, there exists core-point x such that C = {x0 :
x0 is density-reachable from x}.
We now show the following result relating the Îµneighborhood level graphs and the clusters obtained from
DBSCAN. Such an interpretation of DBSCAN has been
given in previous works such as Campello et al. (2015).
Lemma 2 (DBSCAN and Îµ-neighborhood level graphs).
Let C be the clusters obtained from DBSCAN(minP ts, Îµ)
on X.
Let K be the connected components of
G(minP ts, Îµ). Then, there exists a one-to-one correspence between C and K such that if C âˆˆ C and K âˆˆ K
correspond, then
K âŠ† C âŠ† âˆªxâˆˆK B(x, Îµ) âˆ© X.

Assumption 1. F is supported on M where:
â€¢ M is a d-dimensional smooth compact Riemannian
manifold without boundary embedded in compact subset X âŠ† RD .
â€¢ The volume of M is bounded above by a constant.
â€¢ M has condition number 1/Ï„ , which controls the curvature and prevents self-intersection.
Let f be the density of F with respect to the uniform measure on M .
Assumption 2. f is continuous and bounded.
4.2. Basic Supporting Bounds
The following result bounds the empirical mass of Euclidean balls to the true mass under f . It is a direct consequence of Lemma 6 of Balakrishnan et al. (2013).
Lemma 3 (Uniform convergence of empirical Euclidean
balls (Lemma 6 of Balakrishnan et al. (2013))). Let N be
a minimal fixed set such that each point in M is at most distance 1/n from some point in N . There exists a universal

Density Level Set Estimation on Manifolds with DBSCAN

constant C0 such that the following holds with probability
at least 1 âˆ’ Î´. For all x âˆˆ X âˆª N ,
âˆš
d log n
F(B) â‰¥ CÎ´,n
â‡’ Fn (B) > 0
nâˆš
k
k
k
â‡’ Fn (B) â‰¥
F(B) â‰¥ + CÎ´,n
n
n
n
âˆš
k
k
k
F(B) â‰¤ âˆ’ CÎ´,n
â‡’ Fn (B) < .
n
n
n
âˆš
where CÎ´,n = C0 log(2/Î´) d log n, Fn is the empirical
distribution, and k â‰¥ CÎ´,n .
Remark 2. For the rest of the paper, many results are qualified to hold with probability at least 1âˆ’Î´. This is precisely
the event in which Lemma 3 holds.
Remark 3. If Î´ = 1/n, then CÎ´,n = O((log n)3/2 ).
Next, we need the following bound on the volume of the
intersection Euclidean ball and M ; this is required to get
a handle on the true mass of the ball under F in later arguments. The upper and lower bounds follow from Chazal
(2013) and Lemma 5.3 of Niyogi et al. (2008). The proof
is given in the appendix.
Lemma 4 (Ball Volume). If 0 < r < min{Ï„ /4d, 1/Ï„ },
and x âˆˆ M then
vd rd (1 âˆ’ Ï„ 2 r2 ) â‰¤ vold (B(x, r) âˆ© M ) â‰¤ vd rd (1 + 4dr/Ï„ ).
d

where vd is the volume of a unit ball in R and vold is the
volume w.r.t. the uniform measure on M .
4.3. k-NN Density Estimation
Here, we establish density estimation rates for the k-NN
density estimator in the manifold setting. This builds on
work in density estimation on manifolds e.g. (Hendriks,
1990; Pelletier, 2005; Ozakin & Gray, 2009; Kim & Park,
2013; Berry & Sauer, 2017); thus, it may be of independent
interest. The estimator is defined as follows
Definition 3 (k-NN Density Estimator).
fk (x) :=

k
.
n Â· vd Â· rk (x)d

The following extends previous work of Dasgupta &
Kpotufe (2014) to the manifold case. The proofs can be
found in the appendix.
Lemma 5 (fk upper bound). Suppose that Assumptions 1
and 2 hold. Define the following which charaterizes how
much the density increases locally in M :
(
)
rÌ‚(, x) := sup r :

sup
x0 âˆˆB(x,r)âˆ©M

f (x0 ) âˆ’ f (x) â‰¤  .

2
Fix Î»0 > 0 and Î´ > 0 and suppose that k â‰¥ CÎ´,n
. Then
there exists constant C1 â‰¡ C1 (Î»0 , d, Ï„ ) such that if
2d/(2+d)

k â‰¤ C1 Â· CÎ´,n

Â· n2/(2+d) ,

then the following holds with probability at least 1 âˆ’ Î´ uniformly in  > 0 and x âˆˆ X with f (x) +  â‰¥ Î»0 :


CÎ´,n
Â· (f (x) + ),
fk (x) < 1 + 3 Â· âˆš
k
âˆš

provided k satisfies vd Â· rÌ‚(, x)d Â·(f (x)+) â‰¥ nk âˆ’CÎ´,n nk .
Lemma 6 (fk lower bound). Suppose that Assumptions 1
and 2 hold. Define the following which charaterizes how
much the density decreases locally in M :
(
)
rÌŒ(, x) := sup r :

f (x) âˆ’ f (x0 ) â‰¤  .

sup
x0 âˆˆB(x,r)âˆ©M

Fix Î»0 > 0 and 0 < Î´ < 1 and suppose k â‰¥ CÎ´,n . Then
there exists constant C2 â‰¡ C2 (Î»0 , d, Ï„ ) such that if
2d/(4+d)

k â‰¤ C2 Â· CÎ´,n

Â· n4/(4+d) ,

then with probability at least 1 âˆ’ Î´, the following holds
uniformly for all  > 0 and x âˆˆ X with f (x) âˆ’  â‰¥ Î»0 :


CÎ´,n
fk (x) â‰¥ 1 âˆ’ 3 Â· âˆš
Â· (f (x) âˆ’ ),
k
provided
k satisfies
vd Â· rÌŒ(, x)d Â· (f (x) âˆ’ )

âˆš 
4 k
k
3 n + CÎ´,n n .

â‰¥

Remark 4. We will often bound the density of points with
low density. In low-density regions, there is less data and
thus we require more points to get a tight bound. However,
in many cases a tight bound is not necessary; thus the purposes of  is to allow some slack. The higher the , the
easier it is for the lemma conditions to be satisified. In particular, if f is Î±-HoÌˆlder continuous (i.e. |f (x) âˆ’ f (x0 )| â‰¤
CÎ± |x âˆ’ x0 |Î± ), we have rÌ‚(, x), rÌŒ(, x) â‰¥ (/CÎ± )1/Î± .

5. Consistency and Rates
5.1. Level-Set Conditions
Much of the results will depend on the behavior of level
set boundaries. Thus, we require sufficient drop-off at the
boundaries, as well as separation between the CCs at a particular level set. We give the following notion of separation.
Definition 4. A, A0 are r-separated in M if there exists a
set S such that every path from A to A0 intersects S and
supxâˆˆM âˆ©(S+B(0,r)) f (x) < inf xâˆˆAâˆªA0 f (x).
Define the following shorthands for distance from a point to
a set, the intersection of M with a neighborhood around a
set under the Euclidean distance, and the largest Euclidean
distance from a point in a set to its closest sample point.

Density Level Set Estimation on Manifolds with DBSCAN

Definition 5. d(x, A) := inf x0 âˆˆA |x âˆ’ x0 |, C âŠ•r := {x âˆˆ
M : d(x, C) â‰¤ r}, rn (C) := supcâˆˆC d(c, X).
We have the following mild assumptions which ensures
that the CCs can be separated from the rest of the density
by sufficiently wide valleys and there is sufficient decay
around the level set boundaries.
Assumption 3 (Separation Conditions). Let Î» > 0 and
CÎ» be a CCs of {x âˆˆ M : f (x) â‰¥ Î»}. There exists
CÌŒÎ² , CÌ‚Î² , Î², rs , rc > 0 and 0 < Î»0 < Î» such that the following holds:
For each C âˆˆ CÎ» , there exists AC , a connected component
of M Î»0 := {x âˆˆ M : f (x) â‰¥ Î»0 } such that:
â€¢ AC separates C by a valley: AC does not intersect
with any other CC in CÎ» ; AC and M Î»0 \AC are rs separated by some SC .
â€¢ C âŠ•rc âŠ† AC .
â€¢ Î²-regularity: For x âˆˆ C âŠ•rc \C, we have
CÌŒÎ² Â· d(x, C)Î² â‰¤ Î» âˆ’ f (x) â‰¤ CÌ‚Î² Â· d(x, C)Î² .
Remark 5. We can choose any 0 < Î² < âˆ. The Î²regularity assumption appears in e.g. (Singh et al., 2009).
This is very general and also allows us to make a separate
global smoothness assumption.
Remark 6. We currently characterize the smoothness w.r.t.
the Euclidean distance. One could alternatively use the
geodesic distance on M , dM (p, q). It follows from Proposition 6.3 of Niyogi et al. (2008) that when |p âˆ’ q| < Ï„ /4,
we have |p âˆ’ q| â‰¤ dM (p, q) â‰¤ 2|p âˆ’ q|. Since the distances we deal in our analysis with are of such small order,
these distances can thus essentially be treated as equivalent. We use the Euclidean distance throughout the paper
for simplicity.
Remark 7. For the rest of this paper, it will be understood
that Assumptions 1, 2, and 3 hold.
We can define a region which isolates C away from other
clusters of {x âˆˆ M : f (x) â‰¥ Î»}.
Definition 6. XC := {x : âˆƒ a path P from x to x0 âˆˆ
C such that P âˆ© SC = âˆ…}.
5.2. Parameter Settings
Fix Î» > 0 and Î´ > 0. Let k satisfy the following
Kl Â· (log n)2 â‰¤ k â‰¤ Ku Â· (log n)2d/(2+d) Â· n2Î²
0

0

/(2Î² 0 +d)

,

where Î² := min{1, Î²}, and Kl and Ku are positive
constants depending on Î´, CÌŒÎ² , CÌ‚Î² , Î², Ï„, d, ||f ||âˆ , Î»0 , rs , rc
which are implicit in the proofs later in this section.

The remainder of this section will be to show that DBSCAN(minPts, Îµ) with
!1/d
k
âˆš
minPts = k, Îµ =
2 / k)
n Â· vd Â· (Î» âˆ’ Î» Â· CÎ´,n
will consistently estimate each CC of {x âˆˆ M : f (x) â‰¥
cÎ» as the clusters reÎ»}. Throughout the text, we denote C
turned by DBSCAN under this setting.
5.3. Separation and Connectedness
Take C âˆˆ CÎ» . We show that DBSCAN will return an estib such that C
b does not contain any points outmated CC C,
b contains all the sample
side of XC . Then, we show that C
points in C. The proof ideas used are similar to that of standard results in cluster trees estimation; they can be found
in the appendix.
Lemma 7 (Separation). There exists Kl sufficiently large
and Ku sufficiently small such that the following holds with
bâˆˆC
cÎ»
probability at least 1 âˆ’ Î´. Let C âˆˆ CÎ» . There exists C
b
such that C âŠ† XC .
Lemma 8 (Connectedness). There exists Kl sufficiently
large and Ku sufficiently small such that the following
holds with probability at least 1 âˆ’ Î´. Let C âˆˆ CÎ» . If there
bâˆˆC
cÎ» such that C
b âŠ† XC , then C âŠ•rn (C) âˆ© X âŠ† C.
b
exists C
Remark 8. These results allow C to have any dimension
between 0 to d since we reason with C âŠ•rn (C) , which contains samples, instead of simply C.
5.4. Hausdorff Error
We give the estimation rate under the Hausdorff metric.
Definition 7 (Hausdorff Distance).
dHaus (A, A0 ) = max{sup d(x, A0 ), sup d(x0 , A)}.
x0 âˆˆA0

xâˆˆA

Theorem 1. There exists Kl sufficiently large and Ku sufficiently small such that the following holds with probability
bâˆˆC
cÎ» such
at least 1 âˆ’ Î´. For each C âˆˆ CÎ» , there exists C
that
b â‰¤ 2 Â· (4Î»/CÌŒÎ² )1/Î² Â· C 2/Î² Â· k âˆ’1/2Î² .
dHaus (C, C)
Î´,n
Proof. For Kl and Ku appropriately chosen, we have
Lemma 7 and Lemma 8 hold. Thus we have for C âˆˆ CÎ» ,
bâˆˆC
cÎ» such that
there exists C
[
bâŠ†
B(x, Îµ) âˆ© M.
C âŠ•rn (C) âˆ© X âŠ† C
xâˆˆXC âˆ©X
fk (x)â‰¥Î»âˆ’

2
CÎ´,n
âˆš
k

Î»


1/Î²
4Î»Â·C 2
b â‰¤ rÌ„,
Define rÌ„ := CÌŒ Â·âˆšÎ´,n
. We show that dHaus (C, C)
k
Î²
which involves two directions to show from the Hausdroff

Density Level Set Estimation on Manifolds with DBSCAN

b â‰¤
metric: that maxxâˆˆCb d(x, C) â‰¤ rÌ„ and supxâˆˆC d(x, C)
rÌ„.
We start by proving maxxâˆˆCb d(x, C) â‰¤ rÌ„. Define r0 =
rÌ„/2. We have
1
r0 =
2

2
4 Â· CÎ´,n
âˆš
CÌŒÎ² Â· k

!1/Î²


â‰¥

k
vd nÎ»0

1/d
â‰¥ Îµ,

where the first inequality holds when Ku is chosen sufficiently small, and the last inequality holds because Î»0 <
C2

Î» âˆ’ âˆšÎ´,n
Î». Hence r0 + Îµ â‰¤ rÌ„. Therefore, it suffices to
k
show
sup
xâˆˆ(XC \C âŠ•r0 )âˆ©X

2
CÎ´,n

fk (x) < Î» âˆ’ âˆš Î».
k

We have that for x âˆˆ (XC \C âŠ•r0 /2 ) âˆ© X, f (x) â‰¤ Î» âˆ’
CÌŒÎ² (r0 /2)Î² := Î»0 . Thus, for any x âˆˆ (XC \C âŠ•r0 ) âˆ© X and
letting  = Î»0 âˆ’ f (x), we have
âˆš
rÌ‚(, x) â‰¥ r0 /2 â‰¥ (4Î»0 CÎ´,n /( k Â· CÌŒÎ² ))1/Î² /2.
For Ku chosen sufficiently small, the last equation will be
large enough (i.e. of order (k/vd nÎ»)1/d ) so that the conditions of Lemma 5 hold. Thus, applying this for each
x âˆˆ (XC \C âŠ•r0 ) âˆ© X, we obtain


CÎ´,n
(Î» âˆ’ CÌŒÎ² (r0 /2)Î² ).
sup
fk (x) < 1 + 3 âˆš
k
xâˆˆ(XC \C âŠ•r0 )âˆ©X

show how a second application of DBSCAN (Algorithm 1)
can remove the false clusters discovered by the first application of DBSCAN with no additional parameters. This
gives us the other direction, that each estimate in CbÎ» corresponds to a true CC in CÎ» , and thus DBSCAN can identify
with a one-to-one correspondence each CC of the level-set.
Algorithm 1 DBSCAN False CC Removal
As in Section 5.2, let minPts = k and

1/d
k
.
Îµ = nÂ·v Â·(Î»âˆ’Î»Â·C 2 /âˆšk)
d
Î´,n

1/d
k
.
Define ÎµÌƒ := nÂ·v Â·(Î»âˆ’Î»Â·C 2 / âˆš
3
k)
d

Î´,n

Let CbÎ» be the clusters returned by DBSCAN(minPts, Îµ).
bÎ» be the clusters returned by DBSCAN(minPts, ÎµÌƒ).
Let D
e
Let CÎ» be the clusters obtained by merging clusters from
bÎ» which are subsets of the same cluster in D
bÎ» .
C
e
Return CÎ» .
We state our result below. The proof is less involved and is
in the appendix.
Theorem 2 (Removal of False CC Estimates). Define Î³ =
Î» âˆ’ supxâˆˆM \(âˆªCâˆˆC XC ) f (x), which is positive. There exÎ»
ists Kl sufficiently large and Ku sufficiently small depending on Î³ in addition to the constants mentioned in Section 5.2 so that the following holds with probability at least
b âˆˆ CeÎ» , there exists C âˆˆ CÎ» such that
1 âˆ’ Î´. For all C
2/Î²

âˆ’1/2Î²
b â‰¤ 2 Â· (4Î»/CÌŒÎ² )1/Î² Â· C
dHaus (C, C)
.
Î´,n Â· k

C2

We have the r.h.s. is at most Î» âˆ’ âˆšÎ´,n
Î» for Ku chosen
k
appropriately and the first direction follows.
b â‰¤
We now turn to the other direction, that supxâˆˆC d(x, C)
rÌ„. Let x âˆˆ C. Then there exists sample point x0 âˆˆ
b
B(x, rn (C)) by definition of rn and we have that x0 âˆˆ C.
Finally, rn (C) â‰¤ rÌ„ for Kl sufficiently large, and thus
|x0 âˆ’ x| â‰¤ rÌ„. The result follows.
0

0

Remark 9. When taking k â‰ˆ n2Î² /(2Î² +d) , we obtain the
b â‰ˆ nâˆ’1/(2Î²+dÂ·max{1,Î²}) , ignoring
error rate of dHaus (C, C)
logarithmic factors. When 0 < Î² â‰¤ 1, this matches the
known lower bound established in Theorem 4 of Tsybakov
et al. (1997). However, we do not obtain this rate when
Î² > 1. In this case, the density estimation error will be of
order at least nâˆ’1/(2+d) due in part to the error from resolving the geodesic balls with Euclidean balls. This does
not arise in the full dimensional setting, which will be described later.

6. Removal of False Clusters
The result of Theorem 1 guarantees us that for each C âˆˆ
b âˆˆ CbÎ» that estimates it. In this section, we
CÎ» , there exists C

7. Adaptive Parameter Tuning
In this section, we show how to obtain the near optimal
rates by estimating d and adaptively choosing k such that
0
0
k â‰ˆ n2Î² /(2Î² +d) without knowledge of Î².
7.1. Determining d
Knowing the manifold dimension d is necessary to tune the
parameters as described in Section 5.2. There has been
much work done on estimating the intrinsic dimension as
many learning procedures (including this one) require d as
an input. Such work in intrinsic dimension estimation include (Kegl, 2002; Levina & Bickel, 2004; Hein & Audibert, 2005). Pettis et al. (1979) and more recently Farahmand
et al. (2007) take a k-nearest neighbor approach. We work
with the estimate of a dimension at a point proposed in the
latter work:
Ë† =
d(x)

log 2
.
log(r2k (x)/rk (x))

The main result of Farahmand et al. (2007) gives a highprobability bound for a single sample X1 âˆˆ X. Here we

Density Level Set Estimation on Manifolds with DBSCAN

give a high-probability bound under more mild smoothness
assumptions which hold uniformly for all samples above
some density-level given our new knowledge of k-NN density estimation rates. This may be of independent interest.
Theorem 3. Suppose that f is Î±-HoÌˆlder continuous for
some 0 < Î± â‰¤ 1. Choose Î»Ì„0 > 0 and Î´ > 0. Then there exists constants C1 , C2 depending on Î´, CÎ± , Î±, Ï„, d, Î»Ì„0 such
that if k satisfies
C1 Â· (log n)2 â‰¤ k â‰¤ C2 Â· n2Î±/(2Î±+d) ,
then with probability at least 1 âˆ’ Î´,
Ë† âˆ’ d| â‰¤ 20d Â· ||f ||âˆ Â· CâˆšÎ´,n ,
|d(x)
k
uniformly for all x âˆˆ X with fk (x) â‰¥ Î»Ì„0 .
Proof. We have for x âˆˆ X such that if fk (x) â‰¥ Î»Ì„0 , then
f (x) â‰¥ Î»0 := Î»Ì„0 /2 by Lemma 5 for C1 chosen appropriately large and C2 chosen appropriately small.
Ë† =
d(x)

log 2
d log 2
=
.
log(r2k (x)/rk (x))
log 2 + log(fk (x)/f2k (x))

We now try to get a handle on fk (x)/f2k (x) and show it
is sufficiently close to 1. Applying Lemma 5 and 6 with
C
f (x) and C1 , C2 appropriately chosen so that the
 = âˆšÎ´,n
k
conditions for the two Lemmas hold (remember that here
we have rÌ‚(, x), rÌŒ(, x) â‰¥ (/CÎ± )1/Î± ), we obtain
âˆš
âˆš
fk (x)
(1 âˆ’ 3CÎ´,n / k)(1 âˆ’ CÎ´,n / k) Â· f (x)
âˆš
âˆš
â‰¥
f2k (x)
(1 + 3CÎ´,n / k)(1 + CÎ´,n / k) Â· f (x)
CÎ´,n
â‰¥1âˆ’9Â· âˆš ,
k
where the last inequality âˆš
holds when C1 is chosen sufficiently large so that CÎ´,n / k is sufficiently small. On the
other hand, we similarly obtain (for C1 and C2 appropriately chosen):
âˆš
âˆš
fk (x)
(1 + 3CÎ´,n / k)(1 + CÎ´,n / k) Â· f (x)
âˆš
âˆš
â‰¤
f2k (x)
(1 âˆ’ 3CÎ´,n / k)(1 âˆ’ CÎ´,n / k) Â· f (x)
CÎ´,n
â‰¤1+9Â· âˆš .
k
It is now clear that by the expansion log(1 âˆ’ r) = âˆ’r âˆ’
r2 /2 âˆ’ r3âˆš
/3 âˆ’ Â· Â· Â· , and for Kl chosen sufficently large so
that CÎ´,n / k is sufficiently small, we have
 



log fk (x)  â‰¤ 10 Â· CâˆšÎ´,n .

f2k (x) 
k
The result now follows by combining this with the earlier
Ë†
established expression for d(x),
as desired.
Remark 10. In Farahmand et al. (2007), it is the case that
Î± = 1; under this setting, we match their bound with an
error rate of n1/(2+d) with k â‰ˆ n2/(2+d) being the optimal
choice for k (ignoring log factors).

7.2. Determining k
After determining d, the next parameter we look at is k.
In particular, to obtain the optimal rate, we must choose
0
0
k â‰ˆ n2Î² /(2Î² +d) without knowledge of Î². We present a
consistent estimator for Î².
We need the following definition. The first characterizes how much f varies in balls of a certain radius along
the boundaries of the Î»-level set (where âˆ‚CÎ» denotes the
boundary of CÎ» ). The second is meant to be an estimate of
the first, which can be computed from the data alone. The
final is our estimate of Î².
Dr =
DÌ‚r,k =

inf

sup

x0 âˆˆâˆ‚CÎ» xâˆˆB(x0 ,r)

min

|Î» âˆ’ f (x)|
max

x0 âˆˆX
xâˆˆB(x0 ,r)âˆ©X
B(x0 ,r)âˆ©X6=âˆ…

|Î» âˆ’ fk (x)|

Î²Ì‚ = logr (DÌ‚r,k )
The next is a result of how DÌ‚r,k estimates Dr .
Lemma 9. Suppose that f is Î±-HoÌˆlder continuous
âˆš for
some 0 < Î± â‰¤ 1. Let k = b(log n)5 c and r = 1/ log n.
Then there exists positive constants CÌƒ and N depending on
d, Ï„, Î±, CÎ± , Î»0 , ||f ||âˆ , rc such that when n â‰¥ N , then the
following holds with probability at least 1 âˆ’ 1/n.
|Dr âˆ’ DÌ‚r,k | â‰¤ CÌƒ/(log n)2 .
Proof sketch. Suppose that the value of Dr is attained at
x0 = p and the value of DÌ‚r,k is attained at x0 = q. Let
y, z be the points that maximize |Î» âˆ’ f (x)| on B(p, r) and
B(q, r), respectively. Let yÌ‚, zÌ‚ be the sample points that
maximize |Î» âˆ’ fk (x)| on B(p, r) and B(q, r), respectively.
Now, we have
Dr âˆ’ DÌ‚r,k = |Î» âˆ’ f (y)| âˆ’ |Î» âˆ’ fk (zÌ‚)|
â‰¤ |Î» âˆ’ f (z)| âˆ’ |Î» âˆ’ fk (zÌ‚)| â‰¤ |f (z) âˆ’ fk (zÌ‚)|
â‰¤ max{f (z) âˆ’ fk (z), fk (zÌ‚) âˆ’ f (zÌ‚)}.
Now let z 0 be the closest sample point to z in B(q, r). Then,
â‰¤ max{f (z 0 ) âˆ’ fk (z 0 ), fk (zÌ‚) âˆ’ f (zÌ‚)} + |f (z) âˆ’ f (z 0 )|
+ |fk (z) âˆ’ fk (z 0 )| â‰¤

|f (x) âˆ’ fk (x)|

max
xâˆˆX,f (x)â‰¥Î»0

+ CÎ± |z âˆ’ z 0 |Î± + |fk (z) âˆ’ fk (z 0 )|.
On the other hand, we have
DÌ‚r,k âˆ’ Dr = |Î» âˆ’ fk (zÌ‚)| âˆ’ |Î» âˆ’ f (y)|
â‰¤ |Î» âˆ’ fk (yÌ‚)| âˆ’ |Î» âˆ’ f (y)| â‰¤ |f (y) âˆ’ fk (yÌ‚)|
â‰¤ max{f (y) âˆ’ fk (y), fk (yÌ‚) âˆ’ f (yÌ‚)}.
Let y 0 be the closest sample point to y in B(p, r). Then,
â‰¤ max{f (y 0 ) âˆ’ fk (y 0 ), fk (yÌ‚) âˆ’ f (yÌ‚)} + |f (y) âˆ’ f (y 0 )|
+ |fk (y) âˆ’ fk (y 0 )| â‰¤

max
xâˆˆX,f (x)â‰¥Î»0

|f (x) âˆ’ fk (x)|

+ CÎ± |y âˆ’ y 0 |Î± + |fk (y) âˆ’ fk (y 0 )|.

Density Level Set Estimation on Manifolds with DBSCAN

Thus it suffices to bound maxxâˆˆX,f (x)â‰¥Î»0 |f (x) âˆ’
fk (x)|, |y âˆ’ y 0 |, |z âˆ’ z 0 |, |fk (y) âˆ’ fk (y 0 )|, |fk (z) âˆ’ fk (z 0 )|.
First take Î´ = 1/n and use Lemma 5 and 6 for
maxxâˆˆX,f (x)â‰¥Î»0 |f (x) âˆ’ fk (x)|. Using Lemma 3, we can
show that rn := |y âˆ’ y 0 | . (log n/n)1/d . Next we bound
|fk (y) âˆ’ fk (y 0 )|. y 0 âˆˆ X so we have guarantees on its fk
value. Note that rk (y 0 ) âˆ’ rn â‰¤ rk (y) â‰¤ rk (y 0 ) + rn .
Let rk = rk (y 0 ). This implies that fk (y 0 )(rk /(rk +
rn ))d â‰¤ fk (y) â‰¤ fk (y 0 )(rk /(rk âˆ’ rn ))d . Now since
rk â‰ˆ (k/n)1/d , we have |fk (y) âˆ’ fk (y 0 )| . log n/k. The
same holds for the bounds related to z, z 0 .
Theorem 4 (Î²Ì‚ â†’ Î² in probability). Suppose f is Î±HoÌˆlder continuous for some âˆšÎ± with 0 < Î± â‰¤ Î² 0 . Let
k = b(log n)5 c and r = 1/ log n. Then for all  > 0,


lim P |Î²Ì‚ âˆ’ Î²| â‰¥  = 0.
nâ†’âˆ

Proof. Based on the Î²-regularity assumption, we have for
r < rc :
CÌŒÎ² rÎ² â‰¤ Dr â‰¤ CÌ‚Î² rÎ² .
Combining this
âˆš with Lemma 9, we have with probability at
least 1 âˆ’ 1/ n that
CÌŒÎ² rÎ² âˆ’ CÌƒ/(log n)2 â‰¤ DÌ‚r,k â‰¤ CÌ‚Î² rÎ² + CÌƒ/(log n)2 .
Thus with probability at least 1 âˆ’ 1/n,
Î² âˆ’ Î²Ì‚ â‰¥

log(1 âˆ’ CÌƒ/(DÌ‚r,k Â· (log n2 ))) log CÌ‚Î²
âˆ’
log r
log r

Î² âˆ’ Î²Ì‚ â‰¤

log(1 + CÌƒ/(DÌ‚r,k Â· (log n2 ))) log CÌŒÎ²
+
.
log r
log r

It is clear that these expressions go to 0 as n â†’ âˆ and the
result follows.
Ë†0

Ë†0

Remark 11. We can then take k = nÎ² /(2Î² +d) with
Î²Ì‚ 0 = min{1, Î²Ì‚ âˆ’ 0 } for some 0 > 0 so that Î²Ì‚ 0 < Î² 0 for n
sufficiently large and thus k lies in the allowed ranges described in Section 5.2 asymptotically. The settings of Îµ and
MinPts are implied by this choice of k and our estimate of
d.
7.3. Rates with Data-driven Tuning
Putting this all together, along with Theorems 1 and 2,
gives us the following consequence about level set recovery with adaptive tuning. It shows that we can obtain rates
arbitrarily close to those obtained as if the smoothness parameter Î² and intrinsic dimension were known.
Corollary 1. Suppose that 0 < Î´ < 1 and f is Î±HoÌˆlder continuous for some 0 < Î± â‰¤ 1 and suppose
the data-driven choices of parameters described in Remark 11 are used for DBSCAN. For any  > 0, there exists

N,Î´,f â‰¡ N (, Î´, f ) and CÎ´ â‰¡ CÎ´ (Î´, f ) such that the following holds. If n â‰¥ N,Î´,f , then with probability at least
bâˆˆC
cÎ»
1 âˆ’ Î´ simulatenously for each C âˆˆ CÎ» , there exists C
such that
1
+
b â‰¤ CÎ´ Â· nâˆ’ 2Î²+d max{1,Î²}
dHaus (C, C)
.

Moreover, using Algorithm 2, there is a one-to-one correcÎ» .
spondence between CÎ» and C

8. Full Dimensional Setting
Here we instead take f to be the density of F over the uniform measure on RD . Let
minPts = k, Îµ =

k
âˆš
2 / k)
n Â· vD Â· (Î» âˆ’ Î» Â· CÎ´,n

!1/D
,

where k satisfies
Kl Â· (log n)2 â‰¤ k â‰¤ Ku Â· (log n)2D/(2+D) Â· n2Î²/(2Î²+D) ,
and Kl and Ku are positive constants depending
Î´, CÌŒÎ² , CÌ‚Î² , Î², Ï„, D, ||f ||âˆ , Î»0 , rs , rc .
Then Theorem 1 and 2 hold (replacing d with D in Algorithm 1) for this setting of DBSCAN and thus taking
k â‰ˆ n2Î²/(2Î²+D) gives us the optimal estimation rate of
O(nâˆ’1/(2Î²+D) ). A straightforward modification of Corollary 1 also holds. This is discussed further in the Appendix.

9. Conclusion
We proved that DBSCAN can obtain Hausdorff level-set
e âˆ’1/(2Î²+D) ) when the data is in RD ,
recovery rates of O(n
e âˆ’1/(2Î²+dÂ·max{1,Î²}) ) when the data lies on an emand O(n
bedded d-dimensional manifold. The former rate is optimal up to log factors and the latter matches known ddimensional lower bounds for 0 < Î² â‰¤ 1 up to log factors. Moreover, we provided a fully data-driven procedure
to tune the parameters to attain these rates.
This shows that the procedureâ€™s ability to recover density
level sets matches the strongest known consistency results
attained for this problem. Furthermore, we developed the
necessary tools and give the first analysis of density levelset estimation on manifolds, let alone with a practical procedure such as DBSCAN.
Our density estimation errors however cannot converge
e âˆ’1/(2+d) ), which is due in part to the error
faster than O(n
from resolving geodesic balls with Euclidean balls. Thus
it remains an open problem whether the manifold level-set
rates are minimax optimal when Î² > 1.

Density Level Set Estimation on Manifolds with DBSCAN

Acknowledgements
The author is grateful to Samory Kpotufe for insightful discussions and to the anonymous reviewers for their useful
feedback.

References
BaÄ±llo, Amparo, Cuesta-Albertos, Juan A, and Cuevas, Antonio. Convergence rates in nonparametric estimation of
level sets. Statistics & probability letters, 53(1):27â€“35,
2001.
Balakrishnan, S., Narayanan, S., Rinaldo, A., Singh, A.,
and Wasserman, L. Cluster trees on manifolds. In Advances in Neural Information Processing Systems, pp.
2679â€“2687, 2013.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Association, (just-accepted), 2016.
Cuevas, A. and Fraiman, R. A plug-in approach to support
estimation. Annals of Statistics, pp. 2300â€“2312, 1997.
Dasgupta, S. and Kpotufe, S. Optimal rates for k-nn density
and mode estimation. In Advances in Neural Information
Processing Systems, 2014.
Ester, M., Kriegel, H., Sander, J., and Xu, X. A densitybased algorithm for discovering clusters in large spatial
databases with noise. KDD, 96(34):226â€“231, 1996.
Farahmand, A., Szepesvari, C., and Audibert, J. Manifoldadaptive dimension estimation. ICML, 2007.

Baraniuk, Richard G and Wakin, Michael B. Random projections of smooth manifolds. Foundations of computational mathematics, 9(1):51â€“77, 2009.

Genovese,
Christopher,
Perone-Pacifico,
Marco,
Verdinelli, Isabella, and Wasserman, Larry. Minimax manifold estimation. Journal of machine learning
research, 13(May):1263â€“1291, 2012.

Berry, Tyrus and Sauer, Timothy. Density estimation on
manifolds with boundary. Computational Statistics &
Data Analysis, 107:1â€“17, 2017.

Hartigan, J. Clustering algorithms. Wiley, 1975.

Biau, GeÌrard, Cadre, BenoÄ±Ì‚t, and Pelletier, Bruno. Exact
rates in density support estimation. Journal of Multivariate Analysis, 99(10):2185â€“2207, 2008.
Cadre, BenoÄ±t. Kernel estimation of density level sets.
Journal of multivariate analysis, 97(4):999â€“1023, 2006.
Campello, Ricardo JGB, Moulavi, Davoud, Zimek, Arthur,
and Sander, JoÌˆrg. Hierarchical density estimates for
data clustering, visualization, and outlier detection.
ACM Transactions on Knowledge Discovery from Data
(TKDD), 10(1):5, 2015.
Carmichael, J., George, G., and Julius, R. Finding natural
clusters. Systematic Zoology, 1968.
Chaudhuri, K. and Dasgupta, S. Rates for convergence for
the cluster tree. In Advances in Neural Information Processing Systems, 2010.

Hein, Matthias and Audibert, Jean-Yves. Intrinsic dimensionality estimation of submanifolds in rd. ICML, 2005.
Hendriks, Harrie. Nonparametric estimation of a probability density on a riemannian manifold using fourier expansions. The Annals of Statistics, pp. 832â€“849, 1990.
Jiang, Heinrich. Uniform convergence rates for kernel density estimation. International Conference on Machine
Learning (ICML), 2017.
Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation with an application to clustering. Proceedings of the
20th International Conference on Artificial Intelligence
and Statistics (AISTATS), 2017.
Kegl, Balazs. Intrinsic dimension estimation using packing
numbers. NIPS, 2002.
Kim, Yoon Tae and Park, Hyun Suk. Geometric structures arising from kernel density estimation on riemannian manifolds. Journal of Multivariate Analysis, 114:
112â€“126, 2013.

Chaudhuri, K., Dasgupta, S., Kpotufe, S., and von
Luxburg, U. Consistent procedures for cluster tree estimation and pruning. IEEE Transactions on Information
Theory, 2014.

Kpotufe, S. and von Luxburg, U. Pruning nearest neighbor
cluster trees. In International Conference on Machine
Learning, 2011.

Chazal, F. An upper bound for the volume of geodesic balls
in submanifolds of euclidean spaces. 2013.

Levina, Elizaveta and Bickel, Peter J. Maximum likelihood
estimation of intrinsic dimension. NIPS, 2004.

Chazal, FreÌdeÌric, Glisse, Marc, LabrueÌ€re, Catherine, and
Michel, Bertrand. Convergence rates for persistence diagram estimation in topological data analysis. Journal of
Machine Learning Research, 16:3603â€“3635, 2015.

Maier, Markus, Hein, Matthias, and von Luxburg, Ulrike.
Optimal construction of k-nearest-neighbor graphs for
identifying noisy clusters. Theoretical Computer Science, 410(19):1749â€“1764, 2009.

Density Level Set Estimation on Manifolds with DBSCAN

Niyogi, Partha, Smale, Stephen, and Weinberger, Shmuel.
Finding the homology of submanifolds with high confidence from random samples. Discrete & Computational
Geometry, 39(1-3):419â€“441, 2008.
Ozakin, Arkadas and Gray, Alexander G. Submanifold
density estimation. pp. 1375â€“1382, 2009.
Pelletier, Bruno. Kernel density estimation on riemannian
manifolds. Statistics & probability letters, 73(3):297â€“
304, 2005.
Pettis, K., Bailey, T., and Jain, A. An intrinsic dimensionality estimator from near-neighbor information. IEEE
Transactions on PAMI, 1979.
Polonik, Wolfgang. Measuring mass concentrations and
estimating density contour clusters-an excess mass approach. The Annals of Statistics, pp. 855â€“881, 1995.
Rigollet, P. and Vert, R. Fast rates for plug-in estimators of
density level sets. Bernoulli, 15(4):1154â€“1178, 2009.
Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678â€“
2722, 2010.
Rinaldo, Alessandro, Singh, Aarti, Nugent, Rebecca, and
Wasserman, Larry. Stability of density-based clustering. Journal of Machine Learning Research, 13(Apr):
905â€“948, 2012.
Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive hausdorff estimation of density level sets. The Annals of Statistics, 37(5B):2760â€“2782, 2009.
Sriperumbudur, Bharath K and Steinwart, Ingo. Consistency and rates for clustering with dbscan. pp. 1090â€“
1098, 2012.
Steinwart, I. Adaptive density level set clustering. In 24th
Annual Conference on Learning Theory, 2011.
Steinwart, Ingo et al. Fully adaptive density-based clustering. The Annals of Statistics, 43(5):2132â€“2167, 2015.
Stuetzle, Werner and Nugent, Rebecca. A generalized single linkage method for estimating the cluster tree of a
density. Journal of Computational and Graphical Statistics, 19(2):397â€“418, 2010.
Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):948â€“
969, 1997.
Walther, Guenther. Granulometric smoothing. The Annals
of Statistics, pp. 2273â€“2299, 1997.
Willett, RM and Nowak, Robert D. Minimax optimal levelset estimation. IEEE Transactions on Image Processing,
16(12):2965â€“2979, 2007.

