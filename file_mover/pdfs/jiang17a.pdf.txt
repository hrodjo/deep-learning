Density Level Set Estimation on Manifolds with DBSCAN

Heinrich Jiang 1

Abstract
We show that DBSCAN can estimate the connected components of the λ-density level set {x :
f (x) ≥ λ} given n i.i.d. samples from an unknown density f . We characterize the regularity of the level set boundaries using parameter
β > 0 and analyze the estimation error under the
Hausdorff metric. When the data lies in RD we
e −1/(2β+D) ), which matches
obtain a rate of O(n
known lower bounds up to logarithmic factors.
When the data lies on an embedded unknown ddimensional manifold in RD , then we obtain a
e −1/(2β+d·max{1,β}) ). Finally, we prorate of O(n
vide adaptive parameter tuning in order to attain
these rates with no a priori knowledge of the intrinsic dimension, density, or β.

1. Introduction
DBSCAN (Ester et al., 1996) is one of the most popular clustering algorithms amongst practitioners and has had
profound success in a wide range of data analysis applications. However, despite this, its statistical properties have
not been fully understood. The goal of this work is to give
a theoretical analysis of the procedure and to the best of
our knowledge, provide the first analysis of density levelset estimation on manifolds. We also contribute ideas to
related areas that may be of independent interest.
DBSCAN aims at discovering clusters which turn out to
be the high-density regions of the dataset. It takes in two
hyperparameters: minPts and ε. It defines a point as a
core-point if there are at least minPts sample points in its εradius neighborhood. The points within the ε-radius neighborhood of a core-point are said to be directly reachable
from that core-point. Then, a point q is reachable from a
core-point p if there exists a path from q to p where each
point is directly reachable from the next point. It is now
clear that this definition of reachable gives a partitioning of
1
Google.
Correspondence to:
rich.jiang@gmail.com>.

Heinrich Jiang <hein-

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

the dataset (and remaining points not reachable from any
core-point are considered noise). This partitioning is the
clustering that is returned by DBSCAN.
The problem of analyzing DBSCAN has recently been explored in (Sriperumbudur & Steinwart, 2012). Their analysis is for a modified version of DBSCAN and is not focused
on estimating a fixed density level. Their results have many
desirable properties, but are not immediately applicable for
what this paper tries to address. Using recent developments
in topological data analysis along with some tools we develop in this paper, we show that it is now possible to analyze the original procedure.
The clusters DBSCAN aims at discovering can be viewed
as approximations of the connected components of the level
sets {x : f (x) ≥ λ} where f is the density and λ is
some density level. We provide the first comprehensive
analysis in tuning minPts and ε to estimate the density
level set for a particular level. Here, the density level λ
is known to the algorithm while the density remains unknown. Density level set estimation has been studied extensively. e.g., (Carmichael et al., 1968; Hartigan, 1975;
Polonik, 1995; Cuevas & Fraiman, 1997; Walther, 1997;
Tsybakov et al., 1997; Baıllo et al., 2001; Cadre, 2006; Willett & Nowak, 2007; Biau et al., 2008; Rigollet & Vert,
2009; Maier et al., 2009; Singh et al., 2009; Rinaldo &
Wasserman, 2010; Steinwart, 2011; Rinaldo et al., 2012;
Steinwart et al., 2015; Chen et al., 2016; Jiang, 2017).
However approaches that obtain state-of-art consistency results are largely unpractical (i.e. unimplementable). Our
work shows that in actuality, DBSCAN, a procedure known
for decades and has since been used widely, can achieve
the strongest known results. Also, unlike much of the existing work, we show that DBSCAN can also recover the
connected components of the level sets separately and bijectively.
Our work begins with the insight that DBSCAN behaves
like an ε-neighborhood graph, which is different from, but
related to the k-nearest neighbor graph. The latter has
been heavily used for cluster-tree estimation (Chaudhuri &
Dasgupta, 2010; Stuetzle & Nugent, 2010; Kpotufe & von
Luxburg, 2011; Chaudhuri et al., 2014; Jiang & Kpotufe,
2017) and in this paper we adapt some of these ideas for
ε-neighborhood graphs.

Density Level Set Estimation on Manifolds with DBSCAN

Cluster-tree estimation aims at discovering the hierarchical tree structure of the connected-components as the levels
vary. Balakrishnan et al. (2013) extends results by Chaudhuri & Dasgupta (2010) to the setting where the data lies
on a lower dimensional manifold and provide consistency
results depending on the lower dimension and independent
of the ambient dimension. Here we are instead interested
in how to set minPts and ε in order to estimate a particular level and provide rates on the Hausdorff distance error. This is different from works on cluster tree estimation
which focuses on how to recover the tree structure rather
than recovering a particular level. In that regard, we also
require density estimation bounds in order to get a handle
on the true density-levels and the empirical ones.
Dasgupta & Kpotufe (2014) gives us optimal highprobability finite-sample k-NN density estimation bounds
which hold uniformly; this is key to obtaining optimal
level-set estimation rates under the Hausdorff error. Much
of the previous works on density level-set estimation, e.g.
(Rigollet & Vert, 2009) provide rates under risk measures
such as symmetric set difference. These metrics are considerably weaker than the Hausdorff metric; the latter is a uniform guarantee. There are such bounds for the histogram
density estimator. This allowed Singh et al. (2009) to obtain optimal rates under Hausdorff metric, while having
a fully adaptive procedure. This was a significant breakthrough for level set estimation, as discussed by Chazal
et al. (2015). We believe this to be the strongest consistency results obtained thus far. However, a downside is that
the histogram density estimator has little practical value.
Here, aided with the desired bounds on the k-NN density
estimator, we can actually obtain similar results to Singh
et al. (2009) but with the clearly practical DBSCAN.
We extend the k-NN density estimation results of Dasgupta
& Kpotufe (2014) to the manifold case, as the bulk our
analysis is about the more general case that the data lies on
a manifold. Density-based procedures perform poorly in
high-dimensions since the number of samples required increases exponentially in the dimension– the so called curse
of dimensionality. Thus, the consequences of handling the
manifold case are of practical significance. Since the estimation rates we obtain depend only on the intrinsic dimension, it explains why DBSCAN can do well in high dimensions if the data has low intrinsic dimension (i.e. the manifold hypothesis). Given the modern capacity of systems
to collect data of increasing complexity, it has become ever
more important to understand the feasibility of practical
algorithms in high dimensions.
To analyze DBSCAN, we write minPts and ε in terms of
the d, unknown manfold dimension; k, which controls the
density estimator; and λ, which determines which level to
estimate. We assume knowledge of λ with the goal of es-

timating the λ-level set of the density. We give a range of
k in terms of n and corresponding consistency guarantees
and estimation rates for such choices. We then adaptively
tune d and k in order to attain close to optimal performance
with no a priori knowledge of the distribution. Adaptivity
is highly desirable because it allows for automatic tuning of
the hyper-parameters, which is a core tenet of unsupervised
learning. To solve for the unknown dimension, we use an
estimator from Farahmand et al. (2007), which we show to
have considerably better finite-sample behavior than previously thought. More details and discussion of related works
is in the main text. We then provide a new method of choosing k such that it will asymptotically approach a value that
provides near-optimal level set estimation rates.

2. Overview
We start by analyzing the procedure under the manifold
assumption. The end of the paper will discuss the fulldimensional setting. The bulk of our contribution lies in
analyzing the former situation, while the analysis of the latter uses a subset of those techniques.
• Section 3 proves that the clusters returned by DBSCAN are close to the connected components of certain ε-neighborhood graphs (Lemma 2). This is significant because these graphs can be shown to estimate
density level sets.
• Section 4 introduces the manifold setting and provides
supporting results including k-nearest neighbor density estimation bounds (Lemma 5 and Lemma 6) that
are useful later on.
• Section 5 provides a range of parameter settings under which for each true cluster, there exists a corresponding cluster returned by DBSCAN (Lemma 7 and
Lemma 8), and a rate for the Hausdorff distance between them (Theorem 1).
• Section 6 shows how one can apply DBSCAN a second time to remove false clusters from the first application, thus completing a bijection between the estimates and the true clusters (Theorem 2).
• Section 7 explains how to adaptively tune the parameters so that they fall within the theoretical ranges. The
main contributions of this section are a stronger result
about a known k-nearest neighbor based approach to
estimating the unknown dimension (Theorem 3) and a
new way to tune k to approach an optimal choice of k
(Theorem 4).
• Section 8 gives the result when the data lives in RD
without the manifold assumption.

Density Level Set Estimation on Manifolds with DBSCAN

3. The connection to neighborhood graphs
This section is dedicated towards the understanding of
the clusters produced by DBSCAN. The algorithm can be
found in (Ester et al., 1996) and is not shown here since
Lemma 1 characterizes what DBSCAN returns.
We have n i.i.d. samples X = {x1 , ..., xn } drawn from a
distribution F over RD .
Definition 1. Define the k-NN radius of x ∈ RD as
rk (x) := inf{r > 0 : |X ∩ B(x, r)| ≥ k},
where B(x, r) denotes the Euclidean ball of radius r centered at x. Let G(k, ε) denote the ε-neighborhood level
graph of X with vertices {x ∈ X : rk (x) ≤ ε} and an
edge between x and x0 iff ||x − x0 || ≤ ε.
Remark 1. This is slightly different from ε-neighborhood
graph, which includes all vertices. Here we exclude vertices below certain empirical density level (i.e. rk (x) > ε).
The next definition is relevant to DBSCAN and is from (Ester et al., 1996) but in the notation of Definition 1.
Definition 2. The following is with respect to fixed ε > 0
and minPts ∈ N.

Proof. Take any K ∈ K. Each point in K is a core-point
and by Lemma 1 and the definition of density-reachable,
each point in K belongs to the same C ∈ C. Thus, K ⊆
{x ∈ C : rk (x) ≤ ε}. Next we show that K = {x ∈ C :
rk (x) ≤ ε}.
Suppose there exists core-point x ∈ C but x ∈
/ K and let
y ∈ K. By Lemma 1, there exists core-point c ∈ C such
that all points in C are directly reachable from c. Then
there exists a path of core-points from x to c with pairwise
edges of length at most ε. The same holds for c to y. Thus
there exists such a path of core-points from x to y, which
means that x, y are in the same CC of G(minP ts, ε), contradicting the assumption that x ∈
/ K and y ∈ K. Thus,
in fact K = {x ∈ C : rk (x) ≤ ε}. The result now follows since C consists of points that are at most ε from its
core-points.
We can now see that DBSCAN’s clusterings can be viewed
as the connected components (CCs) of an appropriate neighborhood level graph. Using a neighborhood graph to
approximate the level-set has been studied in (Rinaldo &
Wasserman, 2010). The difference is that they use a kernel
density estimator instead of a k-NN density estimator and
study the convergence properties under different settings.

• p is a core-point if rminPts (p) ≤ ε.

4. Manifold Setting

• q is directly density-reachable from p if |p − q| ≤ ε
and p is a core-point.

4.1. Setup

• q is density-reachable from p if there exists a sequence
q = p1 , p2 , ..., pm = p such that pi is directly densityreachable from pi+1 for i = 1, .., m − 1.

We make the following regularity assumptions which are
standard among works on manifold learning e.g. (Baraniuk
& Wakin, 2009; Genovese et al., 2012; Balakrishnan et al.,
2013).

The following result is paraphrased from Lemmas 1 and 2
from (Ester et al., 1996), which characterizes the clusters
learned by DBSCAN.
Lemma 1. (Ester et al., 1996) Let C be the clusters returned by DBSCAN(minPts, ε). For any core-point x, there
exists C ∈ C with x ∈ C. On the other hand, for any
C ∈ C, there exists core-point x such that C = {x0 :
x0 is density-reachable from x}.
We now show the following result relating the εneighborhood level graphs and the clusters obtained from
DBSCAN. Such an interpretation of DBSCAN has been
given in previous works such as Campello et al. (2015).
Lemma 2 (DBSCAN and ε-neighborhood level graphs).
Let C be the clusters obtained from DBSCAN(minP ts, ε)
on X.
Let K be the connected components of
G(minP ts, ε). Then, there exists a one-to-one correspence between C and K such that if C ∈ C and K ∈ K
correspond, then
K ⊆ C ⊆ ∪x∈K B(x, ε) ∩ X.

Assumption 1. F is supported on M where:
• M is a d-dimensional smooth compact Riemannian
manifold without boundary embedded in compact subset X ⊆ RD .
• The volume of M is bounded above by a constant.
• M has condition number 1/τ , which controls the curvature and prevents self-intersection.
Let f be the density of F with respect to the uniform measure on M .
Assumption 2. f is continuous and bounded.
4.2. Basic Supporting Bounds
The following result bounds the empirical mass of Euclidean balls to the true mass under f . It is a direct consequence of Lemma 6 of Balakrishnan et al. (2013).
Lemma 3 (Uniform convergence of empirical Euclidean
balls (Lemma 6 of Balakrishnan et al. (2013))). Let N be
a minimal fixed set such that each point in M is at most distance 1/n from some point in N . There exists a universal

Density Level Set Estimation on Manifolds with DBSCAN

constant C0 such that the following holds with probability
at least 1 − δ. For all x ∈ X ∪ N ,
√
d log n
F(B) ≥ Cδ,n
⇒ Fn (B) > 0
n√
k
k
k
⇒ Fn (B) ≥
F(B) ≥ + Cδ,n
n
n
n
√
k
k
k
F(B) ≤ − Cδ,n
⇒ Fn (B) < .
n
n
n
√
where Cδ,n = C0 log(2/δ) d log n, Fn is the empirical
distribution, and k ≥ Cδ,n .
Remark 2. For the rest of the paper, many results are qualified to hold with probability at least 1−δ. This is precisely
the event in which Lemma 3 holds.
Remark 3. If δ = 1/n, then Cδ,n = O((log n)3/2 ).
Next, we need the following bound on the volume of the
intersection Euclidean ball and M ; this is required to get
a handle on the true mass of the ball under F in later arguments. The upper and lower bounds follow from Chazal
(2013) and Lemma 5.3 of Niyogi et al. (2008). The proof
is given in the appendix.
Lemma 4 (Ball Volume). If 0 < r < min{τ /4d, 1/τ },
and x ∈ M then
vd rd (1 − τ 2 r2 ) ≤ vold (B(x, r) ∩ M ) ≤ vd rd (1 + 4dr/τ ).
d

where vd is the volume of a unit ball in R and vold is the
volume w.r.t. the uniform measure on M .
4.3. k-NN Density Estimation
Here, we establish density estimation rates for the k-NN
density estimator in the manifold setting. This builds on
work in density estimation on manifolds e.g. (Hendriks,
1990; Pelletier, 2005; Ozakin & Gray, 2009; Kim & Park,
2013; Berry & Sauer, 2017); thus, it may be of independent
interest. The estimator is defined as follows
Definition 3 (k-NN Density Estimator).
fk (x) :=

k
.
n · vd · rk (x)d

The following extends previous work of Dasgupta &
Kpotufe (2014) to the manifold case. The proofs can be
found in the appendix.
Lemma 5 (fk upper bound). Suppose that Assumptions 1
and 2 hold. Define the following which charaterizes how
much the density increases locally in M :
(
)
r̂(, x) := sup r :

sup
x0 ∈B(x,r)∩M

f (x0 ) − f (x) ≤  .

2
Fix λ0 > 0 and δ > 0 and suppose that k ≥ Cδ,n
. Then
there exists constant C1 ≡ C1 (λ0 , d, τ ) such that if
2d/(2+d)

k ≤ C1 · Cδ,n

· n2/(2+d) ,

then the following holds with probability at least 1 − δ uniformly in  > 0 and x ∈ X with f (x) +  ≥ λ0 :


Cδ,n
· (f (x) + ),
fk (x) < 1 + 3 · √
k
√

provided k satisfies vd · r̂(, x)d ·(f (x)+) ≥ nk −Cδ,n nk .
Lemma 6 (fk lower bound). Suppose that Assumptions 1
and 2 hold. Define the following which charaterizes how
much the density decreases locally in M :
(
)
ř(, x) := sup r :

f (x) − f (x0 ) ≤  .

sup
x0 ∈B(x,r)∩M

Fix λ0 > 0 and 0 < δ < 1 and suppose k ≥ Cδ,n . Then
there exists constant C2 ≡ C2 (λ0 , d, τ ) such that if
2d/(4+d)

k ≤ C2 · Cδ,n

· n4/(4+d) ,

then with probability at least 1 − δ, the following holds
uniformly for all  > 0 and x ∈ X with f (x) −  ≥ λ0 :


Cδ,n
fk (x) ≥ 1 − 3 · √
· (f (x) − ),
k
provided
k satisfies
vd · ř(, x)d · (f (x) − )

√ 
4 k
k
3 n + Cδ,n n .

≥

Remark 4. We will often bound the density of points with
low density. In low-density regions, there is less data and
thus we require more points to get a tight bound. However,
in many cases a tight bound is not necessary; thus the purposes of  is to allow some slack. The higher the , the
easier it is for the lemma conditions to be satisified. In particular, if f is α-Hölder continuous (i.e. |f (x) − f (x0 )| ≤
Cα |x − x0 |α ), we have r̂(, x), ř(, x) ≥ (/Cα )1/α .

5. Consistency and Rates
5.1. Level-Set Conditions
Much of the results will depend on the behavior of level
set boundaries. Thus, we require sufficient drop-off at the
boundaries, as well as separation between the CCs at a particular level set. We give the following notion of separation.
Definition 4. A, A0 are r-separated in M if there exists a
set S such that every path from A to A0 intersects S and
supx∈M ∩(S+B(0,r)) f (x) < inf x∈A∪A0 f (x).
Define the following shorthands for distance from a point to
a set, the intersection of M with a neighborhood around a
set under the Euclidean distance, and the largest Euclidean
distance from a point in a set to its closest sample point.

Density Level Set Estimation on Manifolds with DBSCAN

Definition 5. d(x, A) := inf x0 ∈A |x − x0 |, C ⊕r := {x ∈
M : d(x, C) ≤ r}, rn (C) := supc∈C d(c, X).
We have the following mild assumptions which ensures
that the CCs can be separated from the rest of the density
by sufficiently wide valleys and there is sufficient decay
around the level set boundaries.
Assumption 3 (Separation Conditions). Let λ > 0 and
Cλ be a CCs of {x ∈ M : f (x) ≥ λ}. There exists
Čβ , Ĉβ , β, rs , rc > 0 and 0 < λ0 < λ such that the following holds:
For each C ∈ Cλ , there exists AC , a connected component
of M λ0 := {x ∈ M : f (x) ≥ λ0 } such that:
• AC separates C by a valley: AC does not intersect
with any other CC in Cλ ; AC and M λ0 \AC are rs separated by some SC .
• C ⊕rc ⊆ AC .
• β-regularity: For x ∈ C ⊕rc \C, we have
Čβ · d(x, C)β ≤ λ − f (x) ≤ Ĉβ · d(x, C)β .
Remark 5. We can choose any 0 < β < ∞. The βregularity assumption appears in e.g. (Singh et al., 2009).
This is very general and also allows us to make a separate
global smoothness assumption.
Remark 6. We currently characterize the smoothness w.r.t.
the Euclidean distance. One could alternatively use the
geodesic distance on M , dM (p, q). It follows from Proposition 6.3 of Niyogi et al. (2008) that when |p − q| < τ /4,
we have |p − q| ≤ dM (p, q) ≤ 2|p − q|. Since the distances we deal in our analysis with are of such small order,
these distances can thus essentially be treated as equivalent. We use the Euclidean distance throughout the paper
for simplicity.
Remark 7. For the rest of this paper, it will be understood
that Assumptions 1, 2, and 3 hold.
We can define a region which isolates C away from other
clusters of {x ∈ M : f (x) ≥ λ}.
Definition 6. XC := {x : ∃ a path P from x to x0 ∈
C such that P ∩ SC = ∅}.
5.2. Parameter Settings
Fix λ > 0 and δ > 0. Let k satisfy the following
Kl · (log n)2 ≤ k ≤ Ku · (log n)2d/(2+d) · n2β
0

0

/(2β 0 +d)

,

where β := min{1, β}, and Kl and Ku are positive
constants depending on δ, Čβ , Ĉβ , β, τ, d, ||f ||∞ , λ0 , rs , rc
which are implicit in the proofs later in this section.

The remainder of this section will be to show that DBSCAN(minPts, ε) with
!1/d
k
√
minPts = k, ε =
2 / k)
n · vd · (λ − λ · Cδ,n
will consistently estimate each CC of {x ∈ M : f (x) ≥
cλ as the clusters reλ}. Throughout the text, we denote C
turned by DBSCAN under this setting.
5.3. Separation and Connectedness
Take C ∈ Cλ . We show that DBSCAN will return an estib such that C
b does not contain any points outmated CC C,
b contains all the sample
side of XC . Then, we show that C
points in C. The proof ideas used are similar to that of standard results in cluster trees estimation; they can be found
in the appendix.
Lemma 7 (Separation). There exists Kl sufficiently large
and Ku sufficiently small such that the following holds with
b∈C
cλ
probability at least 1 − δ. Let C ∈ Cλ . There exists C
b
such that C ⊆ XC .
Lemma 8 (Connectedness). There exists Kl sufficiently
large and Ku sufficiently small such that the following
holds with probability at least 1 − δ. Let C ∈ Cλ . If there
b∈C
cλ such that C
b ⊆ XC , then C ⊕rn (C) ∩ X ⊆ C.
b
exists C
Remark 8. These results allow C to have any dimension
between 0 to d since we reason with C ⊕rn (C) , which contains samples, instead of simply C.
5.4. Hausdorff Error
We give the estimation rate under the Hausdorff metric.
Definition 7 (Hausdorff Distance).
dHaus (A, A0 ) = max{sup d(x, A0 ), sup d(x0 , A)}.
x0 ∈A0

x∈A

Theorem 1. There exists Kl sufficiently large and Ku sufficiently small such that the following holds with probability
b∈C
cλ such
at least 1 − δ. For each C ∈ Cλ , there exists C
that
b ≤ 2 · (4λ/Čβ )1/β · C 2/β · k −1/2β .
dHaus (C, C)
δ,n
Proof. For Kl and Ku appropriately chosen, we have
Lemma 7 and Lemma 8 hold. Thus we have for C ∈ Cλ ,
b∈C
cλ such that
there exists C
[
b⊆
B(x, ε) ∩ M.
C ⊕rn (C) ∩ X ⊆ C
x∈XC ∩X
fk (x)≥λ−

2
Cδ,n
√
k

λ


1/β
4λ·C 2
b ≤ r̄,
Define r̄ := Č ·√δ,n
. We show that dHaus (C, C)
k
β
which involves two directions to show from the Hausdroff

Density Level Set Estimation on Manifolds with DBSCAN

b ≤
metric: that maxx∈Cb d(x, C) ≤ r̄ and supx∈C d(x, C)
r̄.
We start by proving maxx∈Cb d(x, C) ≤ r̄. Define r0 =
r̄/2. We have
1
r0 =
2

2
4 · Cδ,n
√
Čβ · k

!1/β


≥

k
vd nλ0

1/d
≥ ε,

where the first inequality holds when Ku is chosen sufficiently small, and the last inequality holds because λ0 <
C2

λ − √δ,n
λ. Hence r0 + ε ≤ r̄. Therefore, it suffices to
k
show
sup
x∈(XC \C ⊕r0 )∩X

2
Cδ,n

fk (x) < λ − √ λ.
k

We have that for x ∈ (XC \C ⊕r0 /2 ) ∩ X, f (x) ≤ λ −
Čβ (r0 /2)β := λ0 . Thus, for any x ∈ (XC \C ⊕r0 ) ∩ X and
letting  = λ0 − f (x), we have
√
r̂(, x) ≥ r0 /2 ≥ (4λ0 Cδ,n /( k · Čβ ))1/β /2.
For Ku chosen sufficiently small, the last equation will be
large enough (i.e. of order (k/vd nλ)1/d ) so that the conditions of Lemma 5 hold. Thus, applying this for each
x ∈ (XC \C ⊕r0 ) ∩ X, we obtain


Cδ,n
(λ − Čβ (r0 /2)β ).
sup
fk (x) < 1 + 3 √
k
x∈(XC \C ⊕r0 )∩X

show how a second application of DBSCAN (Algorithm 1)
can remove the false clusters discovered by the first application of DBSCAN with no additional parameters. This
gives us the other direction, that each estimate in Cbλ corresponds to a true CC in Cλ , and thus DBSCAN can identify
with a one-to-one correspondence each CC of the level-set.
Algorithm 1 DBSCAN False CC Removal
As in Section 5.2, let minPts = k and

1/d
k
.
ε = n·v ·(λ−λ·C 2 /√k)
d
δ,n

1/d
k
.
Define ε̃ := n·v ·(λ−λ·C 2 / √
3
k)
d

δ,n

Let Cbλ be the clusters returned by DBSCAN(minPts, ε).
bλ be the clusters returned by DBSCAN(minPts, ε̃).
Let D
e
Let Cλ be the clusters obtained by merging clusters from
bλ which are subsets of the same cluster in D
bλ .
C
e
Return Cλ .
We state our result below. The proof is less involved and is
in the appendix.
Theorem 2 (Removal of False CC Estimates). Define γ =
λ − supx∈M \(∪C∈C XC ) f (x), which is positive. There exλ
ists Kl sufficiently large and Ku sufficiently small depending on γ in addition to the constants mentioned in Section 5.2 so that the following holds with probability at least
b ∈ Ceλ , there exists C ∈ Cλ such that
1 − δ. For all C
2/β

−1/2β
b ≤ 2 · (4λ/Čβ )1/β · C
dHaus (C, C)
.
δ,n · k

C2

We have the r.h.s. is at most λ − √δ,n
λ for Ku chosen
k
appropriately and the first direction follows.
b ≤
We now turn to the other direction, that supx∈C d(x, C)
r̄. Let x ∈ C. Then there exists sample point x0 ∈
b
B(x, rn (C)) by definition of rn and we have that x0 ∈ C.
Finally, rn (C) ≤ r̄ for Kl sufficiently large, and thus
|x0 − x| ≤ r̄. The result follows.
0

0

Remark 9. When taking k ≈ n2β /(2β +d) , we obtain the
b ≈ n−1/(2β+d·max{1,β}) , ignoring
error rate of dHaus (C, C)
logarithmic factors. When 0 < β ≤ 1, this matches the
known lower bound established in Theorem 4 of Tsybakov
et al. (1997). However, we do not obtain this rate when
β > 1. In this case, the density estimation error will be of
order at least n−1/(2+d) due in part to the error from resolving the geodesic balls with Euclidean balls. This does
not arise in the full dimensional setting, which will be described later.

6. Removal of False Clusters
The result of Theorem 1 guarantees us that for each C ∈
b ∈ Cbλ that estimates it. In this section, we
Cλ , there exists C

7. Adaptive Parameter Tuning
In this section, we show how to obtain the near optimal
rates by estimating d and adaptively choosing k such that
0
0
k ≈ n2β /(2β +d) without knowledge of β.
7.1. Determining d
Knowing the manifold dimension d is necessary to tune the
parameters as described in Section 5.2. There has been
much work done on estimating the intrinsic dimension as
many learning procedures (including this one) require d as
an input. Such work in intrinsic dimension estimation include (Kegl, 2002; Levina & Bickel, 2004; Hein & Audibert, 2005). Pettis et al. (1979) and more recently Farahmand
et al. (2007) take a k-nearest neighbor approach. We work
with the estimate of a dimension at a point proposed in the
latter work:
ˆ =
d(x)

log 2
.
log(r2k (x)/rk (x))

The main result of Farahmand et al. (2007) gives a highprobability bound for a single sample X1 ∈ X. Here we

Density Level Set Estimation on Manifolds with DBSCAN

give a high-probability bound under more mild smoothness
assumptions which hold uniformly for all samples above
some density-level given our new knowledge of k-NN density estimation rates. This may be of independent interest.
Theorem 3. Suppose that f is α-Hölder continuous for
some 0 < α ≤ 1. Choose λ̄0 > 0 and δ > 0. Then there exists constants C1 , C2 depending on δ, Cα , α, τ, d, λ̄0 such
that if k satisfies
C1 · (log n)2 ≤ k ≤ C2 · n2α/(2α+d) ,
then with probability at least 1 − δ,
ˆ − d| ≤ 20d · ||f ||∞ · C√δ,n ,
|d(x)
k
uniformly for all x ∈ X with fk (x) ≥ λ̄0 .
Proof. We have for x ∈ X such that if fk (x) ≥ λ̄0 , then
f (x) ≥ λ0 := λ̄0 /2 by Lemma 5 for C1 chosen appropriately large and C2 chosen appropriately small.
ˆ =
d(x)

log 2
d log 2
=
.
log(r2k (x)/rk (x))
log 2 + log(fk (x)/f2k (x))

We now try to get a handle on fk (x)/f2k (x) and show it
is sufficiently close to 1. Applying Lemma 5 and 6 with
C
f (x) and C1 , C2 appropriately chosen so that the
 = √δ,n
k
conditions for the two Lemmas hold (remember that here
we have r̂(, x), ř(, x) ≥ (/Cα )1/α ), we obtain
√
√
fk (x)
(1 − 3Cδ,n / k)(1 − Cδ,n / k) · f (x)
√
√
≥
f2k (x)
(1 + 3Cδ,n / k)(1 + Cδ,n / k) · f (x)
Cδ,n
≥1−9· √ ,
k
where the last inequality √
holds when C1 is chosen sufficiently large so that Cδ,n / k is sufficiently small. On the
other hand, we similarly obtain (for C1 and C2 appropriately chosen):
√
√
fk (x)
(1 + 3Cδ,n / k)(1 + Cδ,n / k) · f (x)
√
√
≤
f2k (x)
(1 − 3Cδ,n / k)(1 − Cδ,n / k) · f (x)
Cδ,n
≤1+9· √ .
k
It is now clear that by the expansion log(1 − r) = −r −
r2 /2 − r3√
/3 − · · · , and for Kl chosen sufficently large so
that Cδ,n / k is sufficiently small, we have
 



log fk (x)  ≤ 10 · C√δ,n .

f2k (x) 
k
The result now follows by combining this with the earlier
ˆ
established expression for d(x),
as desired.
Remark 10. In Farahmand et al. (2007), it is the case that
α = 1; under this setting, we match their bound with an
error rate of n1/(2+d) with k ≈ n2/(2+d) being the optimal
choice for k (ignoring log factors).

7.2. Determining k
After determining d, the next parameter we look at is k.
In particular, to obtain the optimal rate, we must choose
0
0
k ≈ n2β /(2β +d) without knowledge of β. We present a
consistent estimator for β.
We need the following definition. The first characterizes how much f varies in balls of a certain radius along
the boundaries of the λ-level set (where ∂Cλ denotes the
boundary of Cλ ). The second is meant to be an estimate of
the first, which can be computed from the data alone. The
final is our estimate of β.
Dr =
D̂r,k =

inf

sup

x0 ∈∂Cλ x∈B(x0 ,r)

min

|λ − f (x)|
max

x0 ∈X
x∈B(x0 ,r)∩X
B(x0 ,r)∩X6=∅

|λ − fk (x)|

β̂ = logr (D̂r,k )
The next is a result of how D̂r,k estimates Dr .
Lemma 9. Suppose that f is α-Hölder continuous
√ for
some 0 < α ≤ 1. Let k = b(log n)5 c and r = 1/ log n.
Then there exists positive constants C̃ and N depending on
d, τ, α, Cα , λ0 , ||f ||∞ , rc such that when n ≥ N , then the
following holds with probability at least 1 − 1/n.
|Dr − D̂r,k | ≤ C̃/(log n)2 .
Proof sketch. Suppose that the value of Dr is attained at
x0 = p and the value of D̂r,k is attained at x0 = q. Let
y, z be the points that maximize |λ − f (x)| on B(p, r) and
B(q, r), respectively. Let ŷ, ẑ be the sample points that
maximize |λ − fk (x)| on B(p, r) and B(q, r), respectively.
Now, we have
Dr − D̂r,k = |λ − f (y)| − |λ − fk (ẑ)|
≤ |λ − f (z)| − |λ − fk (ẑ)| ≤ |f (z) − fk (ẑ)|
≤ max{f (z) − fk (z), fk (ẑ) − f (ẑ)}.
Now let z 0 be the closest sample point to z in B(q, r). Then,
≤ max{f (z 0 ) − fk (z 0 ), fk (ẑ) − f (ẑ)} + |f (z) − f (z 0 )|
+ |fk (z) − fk (z 0 )| ≤

|f (x) − fk (x)|

max
x∈X,f (x)≥λ0

+ Cα |z − z 0 |α + |fk (z) − fk (z 0 )|.
On the other hand, we have
D̂r,k − Dr = |λ − fk (ẑ)| − |λ − f (y)|
≤ |λ − fk (ŷ)| − |λ − f (y)| ≤ |f (y) − fk (ŷ)|
≤ max{f (y) − fk (y), fk (ŷ) − f (ŷ)}.
Let y 0 be the closest sample point to y in B(p, r). Then,
≤ max{f (y 0 ) − fk (y 0 ), fk (ŷ) − f (ŷ)} + |f (y) − f (y 0 )|
+ |fk (y) − fk (y 0 )| ≤

max
x∈X,f (x)≥λ0

|f (x) − fk (x)|

+ Cα |y − y 0 |α + |fk (y) − fk (y 0 )|.

Density Level Set Estimation on Manifolds with DBSCAN

Thus it suffices to bound maxx∈X,f (x)≥λ0 |f (x) −
fk (x)|, |y − y 0 |, |z − z 0 |, |fk (y) − fk (y 0 )|, |fk (z) − fk (z 0 )|.
First take δ = 1/n and use Lemma 5 and 6 for
maxx∈X,f (x)≥λ0 |f (x) − fk (x)|. Using Lemma 3, we can
show that rn := |y − y 0 | . (log n/n)1/d . Next we bound
|fk (y) − fk (y 0 )|. y 0 ∈ X so we have guarantees on its fk
value. Note that rk (y 0 ) − rn ≤ rk (y) ≤ rk (y 0 ) + rn .
Let rk = rk (y 0 ). This implies that fk (y 0 )(rk /(rk +
rn ))d ≤ fk (y) ≤ fk (y 0 )(rk /(rk − rn ))d . Now since
rk ≈ (k/n)1/d , we have |fk (y) − fk (y 0 )| . log n/k. The
same holds for the bounds related to z, z 0 .
Theorem 4 (β̂ → β in probability). Suppose f is αHölder continuous for some √α with 0 < α ≤ β 0 . Let
k = b(log n)5 c and r = 1/ log n. Then for all  > 0,


lim P |β̂ − β| ≥  = 0.
n→∞

Proof. Based on the β-regularity assumption, we have for
r < rc :
Čβ rβ ≤ Dr ≤ Ĉβ rβ .
Combining this
√ with Lemma 9, we have with probability at
least 1 − 1/ n that
Čβ rβ − C̃/(log n)2 ≤ D̂r,k ≤ Ĉβ rβ + C̃/(log n)2 .
Thus with probability at least 1 − 1/n,
β − β̂ ≥

log(1 − C̃/(D̂r,k · (log n2 ))) log Ĉβ
−
log r
log r

β − β̂ ≤

log(1 + C̃/(D̂r,k · (log n2 ))) log Čβ
+
.
log r
log r

It is clear that these expressions go to 0 as n → ∞ and the
result follows.
ˆ0

ˆ0

Remark 11. We can then take k = nβ /(2β +d) with
β̂ 0 = min{1, β̂ − 0 } for some 0 > 0 so that β̂ 0 < β 0 for n
sufficiently large and thus k lies in the allowed ranges described in Section 5.2 asymptotically. The settings of ε and
MinPts are implied by this choice of k and our estimate of
d.
7.3. Rates with Data-driven Tuning
Putting this all together, along with Theorems 1 and 2,
gives us the following consequence about level set recovery with adaptive tuning. It shows that we can obtain rates
arbitrarily close to those obtained as if the smoothness parameter β and intrinsic dimension were known.
Corollary 1. Suppose that 0 < δ < 1 and f is αHölder continuous for some 0 < α ≤ 1 and suppose
the data-driven choices of parameters described in Remark 11 are used for DBSCAN. For any  > 0, there exists

N,δ,f ≡ N (, δ, f ) and Cδ ≡ Cδ (δ, f ) such that the following holds. If n ≥ N,δ,f , then with probability at least
b∈C
cλ
1 − δ simulatenously for each C ∈ Cλ , there exists C
such that
1
+
b ≤ Cδ · n− 2β+d max{1,β}
dHaus (C, C)
.

Moreover, using Algorithm 2, there is a one-to-one correcλ .
spondence between Cλ and C

8. Full Dimensional Setting
Here we instead take f to be the density of F over the uniform measure on RD . Let
minPts = k, ε =

k
√
2 / k)
n · vD · (λ − λ · Cδ,n

!1/D
,

where k satisfies
Kl · (log n)2 ≤ k ≤ Ku · (log n)2D/(2+D) · n2β/(2β+D) ,
and Kl and Ku are positive constants depending
δ, Čβ , Ĉβ , β, τ, D, ||f ||∞ , λ0 , rs , rc .
Then Theorem 1 and 2 hold (replacing d with D in Algorithm 1) for this setting of DBSCAN and thus taking
k ≈ n2β/(2β+D) gives us the optimal estimation rate of
O(n−1/(2β+D) ). A straightforward modification of Corollary 1 also holds. This is discussed further in the Appendix.

9. Conclusion
We proved that DBSCAN can obtain Hausdorff level-set
e −1/(2β+D) ) when the data is in RD ,
recovery rates of O(n
e −1/(2β+d·max{1,β}) ) when the data lies on an emand O(n
bedded d-dimensional manifold. The former rate is optimal up to log factors and the latter matches known ddimensional lower bounds for 0 < β ≤ 1 up to log factors. Moreover, we provided a fully data-driven procedure
to tune the parameters to attain these rates.
This shows that the procedure’s ability to recover density
level sets matches the strongest known consistency results
attained for this problem. Furthermore, we developed the
necessary tools and give the first analysis of density levelset estimation on manifolds, let alone with a practical procedure such as DBSCAN.
Our density estimation errors however cannot converge
e −1/(2+d) ), which is due in part to the error
faster than O(n
from resolving geodesic balls with Euclidean balls. Thus
it remains an open problem whether the manifold level-set
rates are minimax optimal when β > 1.

Density Level Set Estimation on Manifolds with DBSCAN

Acknowledgements
The author is grateful to Samory Kpotufe for insightful discussions and to the anonymous reviewers for their useful
feedback.

References
Baıllo, Amparo, Cuesta-Albertos, Juan A, and Cuevas, Antonio. Convergence rates in nonparametric estimation of
level sets. Statistics & probability letters, 53(1):27–35,
2001.
Balakrishnan, S., Narayanan, S., Rinaldo, A., Singh, A.,
and Wasserman, L. Cluster trees on manifolds. In Advances in Neural Information Processing Systems, pp.
2679–2687, 2013.

Chen, Yen-Chi, Genovese, Christopher R, and Wasserman,
Larry. Density level sets: Asymptotics, inference, and
visualization. Journal of the American Statistical Association, (just-accepted), 2016.
Cuevas, A. and Fraiman, R. A plug-in approach to support
estimation. Annals of Statistics, pp. 2300–2312, 1997.
Dasgupta, S. and Kpotufe, S. Optimal rates for k-nn density
and mode estimation. In Advances in Neural Information
Processing Systems, 2014.
Ester, M., Kriegel, H., Sander, J., and Xu, X. A densitybased algorithm for discovering clusters in large spatial
databases with noise. KDD, 96(34):226–231, 1996.
Farahmand, A., Szepesvari, C., and Audibert, J. Manifoldadaptive dimension estimation. ICML, 2007.

Baraniuk, Richard G and Wakin, Michael B. Random projections of smooth manifolds. Foundations of computational mathematics, 9(1):51–77, 2009.

Genovese,
Christopher,
Perone-Pacifico,
Marco,
Verdinelli, Isabella, and Wasserman, Larry. Minimax manifold estimation. Journal of machine learning
research, 13(May):1263–1291, 2012.

Berry, Tyrus and Sauer, Timothy. Density estimation on
manifolds with boundary. Computational Statistics &
Data Analysis, 107:1–17, 2017.

Hartigan, J. Clustering algorithms. Wiley, 1975.

Biau, Gérard, Cadre, Benoı̂t, and Pelletier, Bruno. Exact
rates in density support estimation. Journal of Multivariate Analysis, 99(10):2185–2207, 2008.
Cadre, Benoıt. Kernel estimation of density level sets.
Journal of multivariate analysis, 97(4):999–1023, 2006.
Campello, Ricardo JGB, Moulavi, Davoud, Zimek, Arthur,
and Sander, Jörg. Hierarchical density estimates for
data clustering, visualization, and outlier detection.
ACM Transactions on Knowledge Discovery from Data
(TKDD), 10(1):5, 2015.
Carmichael, J., George, G., and Julius, R. Finding natural
clusters. Systematic Zoology, 1968.
Chaudhuri, K. and Dasgupta, S. Rates for convergence for
the cluster tree. In Advances in Neural Information Processing Systems, 2010.

Hein, Matthias and Audibert, Jean-Yves. Intrinsic dimensionality estimation of submanifolds in rd. ICML, 2005.
Hendriks, Harrie. Nonparametric estimation of a probability density on a riemannian manifold using fourier expansions. The Annals of Statistics, pp. 832–849, 1990.
Jiang, Heinrich. Uniform convergence rates for kernel density estimation. International Conference on Machine
Learning (ICML), 2017.
Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation with an application to clustering. Proceedings of the
20th International Conference on Artificial Intelligence
and Statistics (AISTATS), 2017.
Kegl, Balazs. Intrinsic dimension estimation using packing
numbers. NIPS, 2002.
Kim, Yoon Tae and Park, Hyun Suk. Geometric structures arising from kernel density estimation on riemannian manifolds. Journal of Multivariate Analysis, 114:
112–126, 2013.

Chaudhuri, K., Dasgupta, S., Kpotufe, S., and von
Luxburg, U. Consistent procedures for cluster tree estimation and pruning. IEEE Transactions on Information
Theory, 2014.

Kpotufe, S. and von Luxburg, U. Pruning nearest neighbor
cluster trees. In International Conference on Machine
Learning, 2011.

Chazal, F. An upper bound for the volume of geodesic balls
in submanifolds of euclidean spaces. 2013.

Levina, Elizaveta and Bickel, Peter J. Maximum likelihood
estimation of intrinsic dimension. NIPS, 2004.

Chazal, Frédéric, Glisse, Marc, Labruère, Catherine, and
Michel, Bertrand. Convergence rates for persistence diagram estimation in topological data analysis. Journal of
Machine Learning Research, 16:3603–3635, 2015.

Maier, Markus, Hein, Matthias, and von Luxburg, Ulrike.
Optimal construction of k-nearest-neighbor graphs for
identifying noisy clusters. Theoretical Computer Science, 410(19):1749–1764, 2009.

Density Level Set Estimation on Manifolds with DBSCAN

Niyogi, Partha, Smale, Stephen, and Weinberger, Shmuel.
Finding the homology of submanifolds with high confidence from random samples. Discrete & Computational
Geometry, 39(1-3):419–441, 2008.
Ozakin, Arkadas and Gray, Alexander G. Submanifold
density estimation. pp. 1375–1382, 2009.
Pelletier, Bruno. Kernel density estimation on riemannian
manifolds. Statistics & probability letters, 73(3):297–
304, 2005.
Pettis, K., Bailey, T., and Jain, A. An intrinsic dimensionality estimator from near-neighbor information. IEEE
Transactions on PAMI, 1979.
Polonik, Wolfgang. Measuring mass concentrations and
estimating density contour clusters-an excess mass approach. The Annals of Statistics, pp. 855–881, 1995.
Rigollet, P. and Vert, R. Fast rates for plug-in estimators of
density level sets. Bernoulli, 15(4):1154–1178, 2009.
Rinaldo, Alessandro and Wasserman, Larry. Generalized
density clustering. The Annals of Statistics, pp. 2678–
2722, 2010.
Rinaldo, Alessandro, Singh, Aarti, Nugent, Rebecca, and
Wasserman, Larry. Stability of density-based clustering. Journal of Machine Learning Research, 13(Apr):
905–948, 2012.
Singh, Aarti, Scott, Clayton, Nowak, Robert, et al. Adaptive hausdorff estimation of density level sets. The Annals of Statistics, 37(5B):2760–2782, 2009.
Sriperumbudur, Bharath K and Steinwart, Ingo. Consistency and rates for clustering with dbscan. pp. 1090–
1098, 2012.
Steinwart, I. Adaptive density level set clustering. In 24th
Annual Conference on Learning Theory, 2011.
Steinwart, Ingo et al. Fully adaptive density-based clustering. The Annals of Statistics, 43(5):2132–2167, 2015.
Stuetzle, Werner and Nugent, Rebecca. A generalized single linkage method for estimating the cluster tree of a
density. Journal of Computational and Graphical Statistics, 19(2):397–418, 2010.
Tsybakov, Alexandre B et al. On nonparametric estimation
of density level sets. The Annals of Statistics, 25(3):948–
969, 1997.
Walther, Guenther. Granulometric smoothing. The Annals
of Statistics, pp. 2273–2299, 1997.
Willett, RM and Nowak, Robert D. Minimax optimal levelset estimation. IEEE Transactions on Image Processing,
16(12):2965–2979, 2007.

