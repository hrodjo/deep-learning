Faster Principal Component Regression
and Stable Matrix Chebyshev Approximation

Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2

Abstract
We solve principal component regression (PCR),
up to a multiplicative accuracy 1+γ, by reducing
e −1 ) black-box calls of ridge
the problem to O(γ
regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale
PCR instances. In contrast, previous result ree −2 ) such black-box calls. We obtain
quires O(γ
this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques
may be of independent interests, especially when
designing iterative methods.

1

Introduction

In machine learning and statistics, it is often desirable to
represent a large-scale dataset in a more tractable, lowerdimensional form, without losing too much information.
One of the most robust ways to achieve this goal is through
principal component projection (PCP):
PCP: project vectors onto the span of the top principal components of the a matrix.
It is well-known that PCP decreases noise and increases efficiency in downstream tasks. One of the main applications
is principal component regression (PCR):
PCR: linear regression but restricted to the subspace of top principal components.
Classical algorithms for PCP or PCR rely on a principal
component analysis (PCA) solver to recover the top principal components first; with these components available, the

tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.
Unfortunately, PCA solvers demand a running time that
at least linearly scales with the number of top principal
components chosen for the projection. For instance, to
project a vector onto the top 1000 principal components of
a high-dimensional dataset, even the most efficient Krylovbased (Musco & Musco, 2015) or Lanczos-based (AllenZhu & Li, 2016a) methods require a running time that is
proportional to 1000 × 40 = 4 × 104 times the input matrix
sparsity, if the Krylov or Lanczos method is executed for
40 iterations. This is usually computationally intractable.
1.1

Approximating PCP Without PCA

In this paper, we propose the following notion of PCP ap0
proximation. Given a data matrix A ∈ Rd ×d (with singular values no greater than 1) and a threshold λ > 0, we
say that an algorithm solves (γ, ε)-approximate PCP if —
informally speaking and up to a multiplicative 1±ε error—
it projects (see Def. 3.1 for a formal definition)


1. eigenvector ν of A> A with value in λ(1 + γ), 1 to ν,


2. eigenvector ν of A> A with value in 0, λ(1 − γ) to ~0,

3. eigenvector ν of A> A with value in λ(1 − γ), λ(1 +

γ) to “anywhere between ~0 and ν.”
Such a definition also extends to (γ, ε)-approximate PCR
(see Def. 3.2).
It was first noticed by Frostig et al. (Frostig et al., 2016)
that approximate PCP and PCR be solved with a running
time independent of the number of principal components
above threshold λ. More specifically, they reduced (γ, ε)approximate PCP and PCR to

O γ −2 log(1/ε) black-box calls of
any ridge regression subroutine

*

Equal contribution . Future version of this paper shall
be found at https://arxiv.org/abs/1608.04773.
1
Microsoft Reseaerch 2 Princeton University. Correspondence
to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li
<yuanzhil@cs.princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

where each call computes (A> A + λI)−1 u for some vector u.1 Our main focus of this paper is to quadratically
improve this performance and reduce PCP and PCR to
1
Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG (Johnson &
Zhang, 2013), one can usually solve ridge regression to an 10−8
accuracy with at most 40 passes of the data.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation


O γ −1 log(1/γε) black-box calls of
any ridge regression subroutine
where each call again computes (A> A + λI)−1 u.
Remark 1.1. Frostig et al. only showed their algorithm
satisfies the properties 1 and 2 of (γ, ε)-approximation
(but not the property 3), and thus their proof was
only
p A with no singular value in the range
p for matrix
[ λ(1 − γ), λ(1 + γ)]. This is known as the eigengap
assumption, which is rarely satisfied in practice (Musco &
Musco, 2015). In this paper, we prove our result both with
and without such eigengap assumption. Since our techniques also imply the algorithm of Frostig et al. satisfies
property 3, throughout the paper, we say Frostig et al. solve
(γ, ε)-approximate PCP and PCR.
1.2

From PCP to Polynomial Approximation

The main technique of Frostig et al.
is to construct a polynomial to approximate the sign function
sgn(x) : [−1, 1] → {±1}:

+1, x ≥ 0;
sgn(x) :=
−1, x < 0.
In particular, given any polynomial g(x) satisfying


g(x) − sgn(x) ≤ ε ∀x ∈ [−1, −γ] ∪ [γ, 1] ,


g(x) ≤ 1 ∀x ∈ [−γ, γ] ,

(1.1)
(1.2)

the problem of (γ, ε)-approximate PCP can be reduced to
computing the matrix polynomial g(S) for S := (A> A +
λI)−1 (A> A − λI) (cf. Fact 7.1). In other words,
• to project any vector χ ∈ Rd to top principal components, we can compute g(S)χ instead; and
• to compute g(S)χ, we can reduce it to ridge regression
for each evaluation of Su for some vector u.
Remark 1.2. Since the transformation from A> A to S is
not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A> A. We
restrict to polynomial choices of g(·) because in this way,
the final rational function has all the denominators being
A> A + λI, thus reduces to ridge regressions.
Remark 1.3. The transformation from A> A to S ensures
that all the eigenvalues of A> A in the range (1 ± γ)λ
roughly map to the eigenvalues of S in the range [−γ, γ].
Main Challenges. There are two main challenges regarding the design of polynomial g(x).
• E FFICIENCY. We wish to minimize the degree n =
deg(g(x)) because the computation of g(S)χ usually
requires n calls of ridge regression.
• S TABILITY. We wish g(x) to be stable; that is, g(S)χ
must be given by a recursive formula where if we make
ε0 error in each recursion (due to error incurred from
ridge regression), the final error of g(S)χ must be at
most ε0 × poly(d).

Remark 1.4. Efficient routines such as SVRG (Johnson &
Zhang, 2013) solve ridge regression and thus compute Su
for any u ∈ Rd , with running times only logarithmically in
1/ε0 . Therefore, by setting ε0 = ε/poly(d), one can blow
up the running time by a small factor O(log(d)) in order to
obtain an ε-accurate solution for g(S)χ.
The polynomial g(x) constructed by Frostig et al.
comes from truncated
Taylor expansion. It has degree

O γ −2 log(1/ε) and is stable. This γ −2 dependency limits the practical performance of their proposed PCP and
PCR algorithms, especially in a high accuracy regime. At
the same time,
• the optimal degree for a polynomial
to satisfy even

only (1.1) is Θ γ −1 log(1/ε) (Eremenko & Yuditskii,
2007; 2011).
Frostig et al. were unable to find a stable polynomial
matching this optimal degree and left it as open question.2
1.3

Our Results and Main Ideas

We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(γ −1 log(1/γε)). At a high level, we construct a
−1/2
polynomial q(x) that approximately equals 1+κ−x
2
for some κ = Θ(γ 2 ); then we set g(x) := x·q(1+κ−2x2 )
which approximates sgn(x).
−1/2
To construct q(x), we first note that 1+κ−x
has
2
no singular point on [−1, 1] so we can apply Chebyshev approximation theory to obtain some q(x) of degree
O(γ −1 log(1/γε)) satisfying

 1 + κ − x −1/2 


q(x) −
 ≤ ε for every x ∈ [−1, 1] .
2


This can be shown to imply g(x) − sgn(x) ≤ ε for every
x ∈ [−1, −γ] ∪ [γ, 1], so (1.1) is satisfied.
In order to prove (1.2) , we prove a separate lemma:3
 1 + κ − x −1/2
q(x) ≤
for every x ∈ [1, 1 + κ] .
2
Note that this does not follow from standard Chebyshev
theory because Chebyshev approximation guarantees are
only with respect to x ∈ [−1, 1].
This proves the “EFFICIENCY” part of the main challenges
discussed earlier. As for the “STABILITY” part, we prove a
general theorem regarding any weighted sum of Chebyshev
polynomials applied to matrices. We provide a backward
recurrence algorithm and show that it is stable under noisy
2
Using degree reduction, Frostig et al.found an explicit polynomial g(x) of degree O γ −1 log(1/γε) satisfying (1.1). However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front
of each monomial. Furthermore, it is not clear if their polynomial
satisfies the (1.2).
3
We proved a general lemma which holds for any function
whose all orders of derivatives are non-negative at x = 0.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

computations. This may be of independent interest.
For interested readers, we compare our polynomial q(x)
with that of Frostig et al. in Figure 1.
1.4

Related Work

There are a few attempts to reduce the cost of PCA when
solving PCR, by for instance approximating the matrix
APλ where Pλ is the PCP projection matrix (Chan &
Hansen, 1990; Boutsidis & Magdon-Ismail, 2014). However, they cost a running time that linearly scales with the
number of principal components above λ.
A significant number of papers have focused on the lowrank case of PCA (Musco & Musco, 2015; Allen-Zhu &
Li, 2016a; 2017) and its online variant (Allen-Zhu & Li,
2016b). Unfortunately, all of these methods require a running time that scales at least linearly with respect to the
number of top principal components.
More related to this paper is work on matrix sign function, which plays an important role in control theory and
quantum chromodynamics. Several results have addressed
Krylov methods for applying the sign function in the socalled Krylov subspace, without explicitly constructing
any approximate polynomial (van den Eshof et al., 2002;
Schilders et al., 2008). However, Krylov methods are not
(γ, ε)-approximate PCP solvers, and there is no supporting
stability theory behind them.4 Other iterative methods have
also been proposed, see Section 5 of textbook (Higham,
2008). For instance, Schur’s method is a slow one and
also requires the matrix to be explicitly given. The Newton’s iteration and its numerous variants (e.g. (Nakatsukasa
& Freund, 2016)) provide rational approximations to the
matrix sign function as opposed to polynomial approximations. Our result and Frostig et al. (Frostig et al., 2016) differ from these cited works, because we have only accessed
an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its
stability are crucial.
Using matrix Chebyshev polynomials to approximate matrix functions is not new. Perhaps the most celebrated example is to approximate S−1 using polynomials on S, used
in the analysis of conjugate gradient (Shewchuk, 1994). Independent from this paper,5 Han et al. (Han et al., 2016)
used Chebyshev polynomials to approximate the trace of
the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the
4
We anyways have included Krylov method in our empirical
evaluation section and shall discuss its performance there, see the
full version of this paper.
5
Their paper appeared online two months before us, and we
became aware of their work in March 2017.
6
In particular, their degree of the Chebyshev
polynomial is

O γ −1 (log2 (1/γ) + log(1/γ) log(1/ε)) in the language of this

paper; in contrast, we have degree O γ −1 log(1/γε) .

case when the matrix-vector multiplication oracle is only
approximate (like we do in this paper), or the case when S
has eigenvalues in the range [−γ, γ].
Roadmap. In Section 2, we provide notions for this paper and basics for Chebyshev polynomials. In Section 3,
we formally define approximate PCP and PCR, and reduce
PCR to PCP. In Section 4, we show a general lemma for
Chebyshev approximations. In Section 5, we design our
polynomial approximation to sgn(x). In Section 6, we
show how to stably compute Chebyshev polynomials. In
Section 7, we state our main theorems regarding PCP and
PCR. In Section 8, we provide empirical evaluations.

2

Preliminaries

We denote by 1[e] ∈ {0, 1} the indicator function for event
e, by kvk or kvk2 the Euclidean norm of a vector v, by M†
the Moore-Penrose pseudo-inverse of a symmetric matrix
M, and by kMk2 its spectral norm. We sometimes use ~v
to emphasize that v is a vector.
Given a symmetric d × d matrix M and any f : R →
R, f (M) is the matrix function applied to M, which
is equal to Udiag{f (D1 ), . . . , f (Dd )}U> if M =
Udiag{D1 , . . . , Dd }U> is its eigendecomposition.
Throughout the paper, matrix A is of dimension d0 × d.
We denote by σmax (A) the largest singular value of A.
Following the tradition of (Frostig et al., 2016) and keeping
the notations light, we assume without loss of generality
that σmax (A) ≤ 1. We are interested in PCP and PCR
problems with an eigenvalue threshold λ ∈ (0, 1).
Throughout the paper, we denote by λ1 ≥ · · · ≥ λd ≥
0 the eigenvalues of A> A, and by ν1 , . . . , νd ∈ Rd
the eigenvectors of A> A corresponding to λ1 , . . . , λd .
We denote by Pλ the projection matrix Pλ :=
(ν1 , . . . , νj )(ν1 , . . . , νj )> where j is the largest index satisfying λj ≥ λ. In other words, Pλ is a projection matrix
to the eigenvectors of A> A with eigenvalues ≥ λ.
Definition 2.1. The principal component projection (PCP)
of χ ∈ Rd at threshold λ is ξ ∗ = Pλ χ.
Definition 2.2. The principal component regression (PCR)
0
of regressand b ∈ Rd at threshold λ is
x∗ = arg miny∈Rd kAPλ y − bk2
∗

>

†

or equivalently

>

x = (A A) Pλ (A b) .
2.1

Ridge Regression

Definition 2.3. A black-box algorithm ApxRidge(A, λ, u)
is an ε-approximate ridge regression solver, if for every u ∈
Rd , it satisfies kApxRidge(A, λ, u)−(A> A+λI)−1 uk ≤
εkuk.
Ridge regression is equivalent to solving well-conditioned
linear systems, or minimizing strongly convex and smooth
objectives f (y) := 21 y > (A> A + λI)y − u> y.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

-1.0

1.0

1.0

1.0

0.5

0.5

0.5

0.5

-0.5

1.0 -1.0

-0.5

0.5

-0.5

1.0 -1.0

0.5

-0.5

-0.5

-1.0

-0.5

-1.0

(a) degree 21

1.0

-1.0

(b) degree 41

(c) degree 101

Figure 1: Comparing our polynomial g(x) (orange solid curve) with that of Frostig et al. (blue dashed curve).

Remark 2.4. There is huge literature on efficient algorithms
solving ridge regression. Most notably,
(1) Conjugate gradient (Shewchuk, 1994) or accelerated
gradient descent (Nesterov, 2004) gives fastest fullgradient methods;
(2) SVRG (Johnson & Zhang, 2013) and its acceleration
Katyusha (Allen-Zhu, 2017) give the fastest stochasticgradient method; and
(3) NUACDM (Allen-Zhu et al., 2016) gives the fastest
coordinate-descent method.
The running time of (1) is O(nnz(A)λ−1/2 log(1/ε))
where nnz(A) is time to multiply A to any vector. The
running times of (2) and (3) depend on structural properties of A and are always faster than (1).
Because the best complexity of ridge regression depends
on the structural properties of A, following Frostig et al.,
we only compute our running time in terms of the “number
of black-box calls” to a ridge regression solver.
2.2

Chebyshev Polynomials

2 − 1[k = 0]
π

Z

1

f (x)Tk (x)
√
dx
1 − x2
−1
n


2 − 1[k = 0] X
ck :=
f xj Tk xj .
n+1
j=0
(j+0.5)π 
∈ [−1, 1] is the j-th ChebyAbove, xj := cos
n+1
shev point of order n.
where

ak :=

The following lemma is known as the aliasing formula for
Chebyshev coefficients:
Lemma 2.8 (cf. Theorem 4.2 of (Trefethen, 2013)). Let
f be Lipschitz continuous on [−1, 1] and {ak }, {ck } be defined in Def. 2.7, then
c0 = a0 +a2n +a4n +... , cn = an +a3n +a5n +...

, and

for every k ∈ {1, 2, . . . , n − 1},
ck = ak +(ak+2n +ak+4n +...)+(a−k+2n +a−k+4n +...)
Definition 2.9. For every ρ > 0, let Eρ be the ellipse E of
foci ±1 with major radius 1 + ρ. (This is
palso known as
Bernstein ellipse with parameter 1 + ρ + 2ρ + ρ2 .)

The following lemma is the main theory regarding Chebyshev approximation:
T0 (x) := 1, T1 (x) := x,
Tn+1 (x) := 2x · Tn (x) − Tn−1 (x) Lemma 2.10 (cf. Theorem 8.1 and 8.2 of (Trefethen,
2013)). Suppose f (z) is analytic on Eρ and |f (z)| ≤ M
U0 (x) := 1, U1 (x) := 2x, Un+1 (x) := 2x · Un (x) − Un−1 (x)
on Eρ . Let pn (x) and qn (x) be the degree-n Chebyshev
d
truncated series and Chebyshev interpolation of f (x) on
Fact 2.6 ((Trefethen, 2013)). It satisfies dx
Tn (x) =
[−1, 1]. Then,
nUn−1 (x) for n ≥ 1 and
Definition 2.5. Chebyshev polynomials of 1st and 2nd kind
are {Tn (x)}n≥0 and {Un (x)}n≥0 where


 cos(n arccos(x)),
cosh(n arccosh(x)),
∀n ≥ 0 : Tn (x) =
 (−1)n cosh(n arccosh(−x)),

if |x| ≤ 1;
if x ≥ 1;
if x ≤ −1.

In particular, when x ≥ 1,
p
p
n
n 
1
Tn (x) =
x − x2 − 1 + x + x2 − 1
2
p
p

n+1
n+1 
1
Un (x) = √
x + x2 − 1
− x − x2 − 1
2
2 x −1

Definition 2.7. For function f (x) whose domain contains [−1, 1], its degree-n Chebyshev truncated series and
degree-n Chebyshev interpolation are respectively
n
n
X
X
pn (x) :=
ak Tk (x) and qn (x) :=
ck Tk (x) ,
k=0

k=0

•

max |f (x)−pn (x)| ≤

x∈[−1,1]

•

max |f (x)−qn (x)| ≤

x∈[−1,1]

√2M
ρ+

2ρ+ρ2

√4M
ρ+

2ρ+ρ2

• |a0 | ≤ M and |ak | ≤ 2M 1 + ρ +

3

1+ρ+

p

1+ρ+

2ρ + ρ2

p

−n

2ρ + ρ2

;

−n

.

p
−k
2ρ + ρ2
for k ≥ 1.

Approximate PCP and PCR

We formalize our notions of approximation for PCP and
PCR, and provide a reduction from PCR to PCP.
3.1

Our Notions of Approximation

Recall that Frostig et al. (Frostig et al., 2016) work
only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

p
p
[ λ(1 − γ), λ(1 + γ)]. Their approximation guarantees are very straightforward:
• an output ξ is ε-approximate for PCP on vector χ if
kξ − ξ ∗ k ≤ εkχk;
• an output x is ε-approximate for PCR with regressand
b if kx − x∗ k ≤ εkbk.
Unfortunately, these notions are too strong and impossible
to satisfy for matrices that do not have a large eigengap
around the projection threshold λ.
In this paper we propose the following more general (but
yet very meaningful) approximation notions.
Definition 3.1. An algorithm B(χ) is (γ, ε)-approximate
PCP for threshold λ, if for every χ ∈ Rd



1. P(1+γ)λ B(χ) − χ  ≤ εkχk.


2. (I − P(1−γ)λ )B(χ) ≤ εkχk.


3. ∀i such that λi ∈ (1 − γ)λ, (1 + γ)λ , it satisfies
|hνi , B(χ) − χi| ≤ |hνi , χi| + εkχk.
Intuitively, the first property above states that, if projected
to the eigenspace with eigenvalues above (1 + γ)λ, then
B(χ) and χ are almost identical; the second property states
that, if projected to the eigenspace with eigenvalues below
(1 − γ)λ, then B(χ) is almost zero; and the third property
states that, for each eigenvector νi with eigenvalue in the
range [(1 − γ)λ, (1 + γ)λ], the projection hνi , B(χ)i must
be between 0 and hνi , χi (but up to an error εkχk).

3.2

Reductions from PCR to PCP

If the PCP solution ξ = Pλ (A> b) is computed exactly,
then by definition one can compute (A> A)† ξ which gives
a solution to PCR by solving a linear system. However, as
pointed by Frostig et al. (Frostig et al., 2016), this computation is problematic if ξ is only approximate. The following approach has been proposed to improve its accuracy by
Frostig et al.
• “compute p((A> A + λI)−1 )ξ where p(x) is a polynox
.”
mial that approximates function 1−λx
This is a good approximation to (A> A)† ξ because the
x
1
composition of functions 1−λx
and 1+λx
is exactly x−1 .
Pm t−1 t
Frostig et al. picked p(x) = pm (x) = t=1 λ x which
is a truncated Taylor series, and used the following procedure to compute sm ≈ pm ((A> A + λI)−1 )ξ:
s0 = B(A> b),

s1 = ApxRidge(A, λ, s0 ),

∀k ≥ 1 : sk+1 = s1 + λ · ApxRidge(A, λ, sk ) . (3.1)
Above, B is an approximate PCP solver and ApxRidge is
an approximate ridge regression solver. Under eigengap
assumption, Frostig et al. (Frostig et al., 2016) showed

Naturally, Pλ (χ) itself is a (0, 0)-approximate PCP.

Lemma 3.4 (PCR-to-PCP). For fixed λ, γ, ε ∈ (0, 1),
let
A be a  matrix
values lie in
 p
p whose singular

0, (1 − γ)λ ∪
(1 − γ)λ, 1 . Let ApxRidge be any
O( mε2 )-approximate ridge regression solver, and let B be
ελ
7
any (γ, O( m
2 ))-approximate PCP solver . Then, procedure (3.1) satisfies

We propose the following notion for approximate PCR:

ksm −(A> A)† Pλ A> bk ≤ εkbk

Definition 3.2. An algorithm C(b) is (γ, ε)-approximate
0
PCR for threshold λ, if for every b ∈ Rd


1. (I − P(1−γ)λ )C(b) ≤ εkbk.

Unfortunately, the above lemma does not hold without
eigengap assumption. In this paper, we fix this issue by
proving the following analogous lemma:

2. kAC(b) − bk ≤ kAx∗ − bk + εkbk.
∗

>

†

>

where x = (A A) P(1+γ)λ A b is the exact PCR solution for threshold (1 + γ)λ.
The first notion states that the output x = C(b) has nearly
no correlation with eigenvectors below threshold (1 − γ)λ;
and the second states that the regression error should be
nearly optimal with respect to the exact PCR solution but
at a different threshold (1 + γ)λ.
Relationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.:
Fact
3.3. p If A has no
p
[ λ(1 − γ), λ(1 + γ)], then

singular

value

in

• Def. 3.1 is equivalent to kB(χ) − Pλ (χ)k ≤ O(ε)kχk.
• Def. 3.2 implies kC(χ) − x∗ k ≤ O(ε/λ)kbk and
kC(χ) − x∗ k ≤ O(ε)kbk implies Def. 3.2.
∗

>

†

>

Above, x = (A A) Pλ A b is the exact PCR solution.

if

m = Θ(log(1/εγ)) .

Lemma 3.5 (gap free PCR-to-PCP). For fixed λ, ε ∈
(0, 1) and γ ∈ (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any
O( mε2 )-approximate ridge regression solver, and B be
ελ
any (γ, O( m
2 ))-approximate PCP solver. Then, procedure (3.1) satisfies,
(

k(I − P(1−γ)λ )sm k ≤ εkbk , and

)

kAsm − bk ≤ kA(A> A)† P(1+γ)λ A> b − bk + εkbk

if m = Θ(log(1/εγ)
Note that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2. The proof
of Lemma 3.5 is not hard, but requires a very careful case
analysis by decomposing vectors b and each sk into three
components, each corresponding to eigenvalues of A> A in
the range [0, (1−γ)λ], [(1−γ)λ, (1+γ)λ] and [(1+γ)λ, 1].
7

Recall from Fact 3.3 that this √requirement is equivalent to
saying that kB(χ) − Pλ χk ≤ O( εm2λ )kχk.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

We defer the details to the full version.

4

Chebyshev Approximation Outside [−1, 1]

Classical Chebyshev approximation theory (such as
Lemma 2.10) only talks about the behaviors of pn (x) or
gn (x) on interval [−1, 1]. However, for the purpose of this
paper, we must also bound its value for x > 1. We prove
the following general lemma in the full version, and believe
it could be of independent interest: (we denote by f (k) (x)
the k-th derivative of f at x)
Lemma 4.1. Suppose f (z) is analytic on Eρ and for every k ≥ 0, f (k) (0) ≥ 0. Then, for every n ∈ N, letting
pn (x) and qn (x) be be the degree-n Chebyshev truncated
series and Chebyshev interpolation of f (x), we have
∀y ∈ [0, ρ] :

5

0 ≤ pn (1 + y), qn (1 + y) ≤ f (1 + y) .

Our Polynomial Approximation of sgn(x)

For fixed κ ∈ (0, 1], we consider
the degree-n ChebyPn
shev interpolation qn (x) = k=0 ck Tk (x) of the function
−1/2
on [−1, 1]. Def. 2.7 tells us that
f (x) = 1+κ−x
2
n
 k(j + 0.5)π 
2 − 1[k = 0] X √
2 cos
n+1
n+1
j=0

 (j + 0.5)π −1/2
× 1 + κ − cos
.
n+1
Our final polynomial to approximate sgn(x) is therefore

ck :=

gn (x) = x·qn (1+κ−2x2 ) and

it satisfies |f (x) − qn (x)| ≤ ε.

Theorem 5.1. For every α ∈ (0, 1], ε ∈ (0, 1/2), choosing κ = 2α2 , our function gn (x) := x · qn (1 + κ − 2x2 )
satisfies that as long as n ≥ √12α log εα3 2 , then (see also
Figure 1)
• |gn (x) − sgn(x)| ≤ ε for every x ∈ [−1, α] ∪ [α, 1].
• gn (x) ∈ [0, 1] for every x ∈ [0, α] and gn (x) ∈
[−1, 0] for every x ∈ [−α, 0].

Note that our degree n = O α−1 log(1/αε) is nearoptimal, because the minimum degree for a polynomial
to

satisfy even only the first item is Θ α−1 log(1/ε) (Eremenko & Yuditskii, 2007; 2011). However, the results of
(Eremenko & Yuditskii, 2007; 2011) are not constructive,
and thus may not lead to stable matrix polynomials.
We prove Theorem 5.1 by first establishing two simple
lemmas. The following lemma is a consequence of
Lemma 2.10:
Lemma 5.2. For every
 ε ∈ (0, 1/2) and κ ∈ (0, 1], if
n ≥ √1κ log κ1 + log 4ε then
|f (x) − qn (x)| ≤ ε .



The next lemma an immediate consequence of our
−0.5
:
Lemma 4.1 with f (z) = 1+κ−z
2
Lemma 5.3. For every ε ∈ (0, 1/2), κ ∈ (0, 1], n ∈ N,
and x ∈ [0, κ], we have
 κ − x −1/2
0 ≤ qn (1 + x) ≤
.
2
Proof of Theorem 5.1. We are now ready to prove
Theorem 5.1.
• When x ∈ [−1, α] ∪ [α, 1], it satisfies 1 + κ − 2x2 ∈
[−1, 1]. Therefore, applying Lemma 5.2 we have when6
ever n ≥ √1κ log εκ
= √12α log εα3 2 it satisfies |f (1 +
κ−2x2 )−qn (1+κ−2x2 )|∞ ≤ ε. This further implies
|gn (x)−sgn(x)| = |xqn (1+κ−2x2 )−xf (1+κ−2x2 )|
≤ |x||f (1 + κ − 2x2 ) − qn (1 + κ − 2x2 )| ≤ ε .
• When |x| ≤ α, it satisfies 1 + κ − 2x2 ∈ [1, 1 + κ].
Applying Lemma 5.3 we have for all x ∈ [0, α],
0 ≤ gn (x) = x · qn (1 + κ − 2x2 ) ≤ x · (x2 )−1/2 = 1
and similarly for x ∈ [−α, 0] it satisfies 0 ≥ gn (x) ≥
−1.


deg(gn (x)) = 2n+1 .

We prove the following theorem in this section:

∀x ∈ [−1, 1],

−0.5
,
Proof of Lemma 5.2. Denoting by f (z) = 1+κ−z
2
we know that f (z) is analytic on
ellipse
E
with
ρ
=
ρ
p
κ/2, and it satisfies |f (z)| ≤
2/κ in Eρ . Applying
Lemma 2.10, we know that when n ≥ √1κ log κ1 + log 4ε

A Bound on Chebyshev Coefficients. We also give an
upper bound to the coefficients of polynomial qn (x). Its
proof can be found in the full version, and this upper bound
shall be used in our final stability analysis.
Lemma 5.4 (coefficients of qn ).
Let qn (x) =
P
n
c
T
(x)
be
the
degree-n
Chebyshev
interpolation
k=0 k k

1+κ−x −1/2
on [−1, 1]. Then, for all i ∈
of f (x) =
2
{0, 1, . . . , n},
p
−i
p
e 32(i + 1) 
1 + κ + 2κ + κ2
|ci | ≤
κ

6

Stable Computation of Matrix Chebyshev
Polynomials

In this section we show that any polynomial that is
a weighted summation of Chebyshev polynomials with
bounded coefficients, can be stably computed when applied
to matrices with approximate computations. We achieve
so by first generalizing Clenshaw’s backward method to
matrix case in Section 6.1 in order to compute a matrix
variant of Chebyshev sum, and then analyze its stability in
Section 6.2 with the help from Elloit’s forward-backward
transformation (Elliott, 1968).

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Remark 6.1. We wish to point out that although Chebyshev
polynomials are known to be stable under error when computed on scalars (Gil et al., 2007), it is not immediately
clear why it holds also for matrices. Recall that Chebyshev polynomials satisfy Tn+1 (x) = 2xTn (x) − Tn−1 (x).
In the matrix case, we have Tn+1 (M)χ = 2MTn (M)χ −
Tn−1 (M)χ where χ ∈ Rd is a vector. If we analyzed this
formula coordinate by coordinate, error could blow up by a
factor d per iteration.
In addition, we need to ensure that the stability theorem
holds for matrices M with eigenvalues that can exceed 1.
This is not standard because Chebyshev polynomials are
typically analyzed only on domain [−1, 1].
6.1

Clenshaw’s Method in Matrix Form

Consider any computation of the form
~sN :=

N
X

Tk (M)~ck ∈ Rd

(6.1)

k=0

where M ∈ Rd×d is symmetric and each ~ck is in Rd . (Note
that for PCP and PCR purposes, we it suffices to consider
~ck = c0k χ where c0k ∈ R is a scalar and χ ∈ Rd is a fixed
vector for all k. However, we need to work on this more
general form for our stability analysis.)
Vector sN can be computed using the following procedure:
Lemma 6.2 (backward recurrence). ~sN = ~b0 −M~b1 where
~bN +1 := ~0, ~bN := ~cN , and
∀r ∈ {N − 1, . . . , 0} : ~br := 2M~br+1 − ~br+2 + ~cr ∈ Rd .
6.2

Inexact Clenshaw’s Method in Matrix Form

We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably
computed. We define the following model to capture the
error with respect to matrix-vector multiplications.

0 satisfying ∀k ∈ {0, 1, . . . , N } :
n
^
ρk k~ck k ≤ Cc
∀x ∈ [a, b] :

|Tk (x)|≤CT ρk
|Uk (x)|≤CU ρk

o

.

Then, if the inexact backward recurrence in Def. 6.3 is
applied with ε ≤ 4N1CU , we have
kb
sN − ~sN k ≤ ε · 2(1 + 2N CT )N CU Cc .

7

Algorithms and Main Theorems for PCP
and PCR

We are now ready to state our main theorems for PCP and
PCR. We first note a simple fact:
where S := 2(A> A +
Fact 7.1. (Pλ )χ = I+sgn(S)
2
−1 >
>
−1
λI) A A − I = (A A + λI) (A> A − λI).
In other words, for every vector χ ∈ Rd , the exact
PCP solution Pλ (χ) is the same as computing (Pλ )χ =
I+sgn(S)
χ. Thus, we can use our polynomial gn (x) intro2
duced in Section 5 and compute gn (S)χ ≈ sgn(S)χ. Finally, in order to compute gn (S), we need to multiply S to
deg(gn ) vectors; whenever we do so, we call perform ridge
regression once.
Since the high-level structure of our PCP algorithm is very
clear, due to space limitation, we present the pseudocodes
of our PCP and PCR algorithms in the full version.
7.1

Our Main Theorems

We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that
of Frostig et al. (Frostig et al., 2016).
0

Theorem 7.2 (eigengap assumption). Given A ∈ Rd ×d
and λ, γ ∈ (0, 1), assume
that thep
singular values of A
p
are in the range [0, (1 − γ)λ] ∪ [ (1 + γ)λ, 1]. Given
0
χ ∈ Rd and b ∈ Rd , denote by

Definition 6.3 (inexact backward recurrence). Let M be
an approximate algorithm that satisfies kM(u) − Muk2 ≤
ξ ∗ = Pλ χ and x∗ = (A> A)−1 Pλ A> b
d
εkuk2 for every u ∈ R . Then, define inexact backward
the exact PCP and PCR solutions, and by ApxRidge any
recurrence to be
ε0 -approximate ridge regression solver. Then,
bbN +1 := 0, bbN := ~cN , and
• QuickPCP outputs
ξ satisfying kξ ∗ − ξk ≤ εkχk with


d
1
−1
∀r ∈ {N − 1, . . . , 0} : bbr := 2M bbr+1 − bbr+2 + ~cr ∈ R ,
O γ log γε oracle calls to ApxRidge as long as

1
log(1/ε0 ) = Θ log γε
.
and define the output as sbN := bb0 − M(bb1 ).
The following theorem gives an error analysis to our inexact backward recurrence. We prove it in full version, and
the main idea of our proof is to convert each error vector of
a recursion of the backward procedure into an error vector
corresponding to some original ~ck .
Theorem 6.4 (stable Chebyshev sum). For every N ∈
N∗ , suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥

∗
• QuickPCR outputs
 x satisfying kx − x k ≤ εkbk with
1
−1
O γ log γλε oracle calls to ApxRidge, as long as

1
log(1/ε0 ) = Θ log γλε
.

In contrast, the number of ridge-regression oracle calls was
1
1
Θ(γ −2 log γε
) for PCP and Θ(γ −2 log γλε
) for PCR in
(Frostig et al., 2016). We include the proof of Theorem 7.2
in the full version.
We state our theorem without the eigengap assumption.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Theorem 7.3 (gap-free). Given A ∈ Rd ×d , λ ∈ (0, 1),
and γ ∈ (0, 2/3], assume that kAk2 ≤ 1. Given
0
χ ∈ Rd and b ∈ Rd , and suppose ApxRidge is an ε0 approximate ridge regression solver, then

Allen-Zhu, Zeyuan and Li, Yuanzhi. Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecomposition. In Proceedings of the 34th International Conference on Machine Learning, ICML ’17, 2017.

• QuickPCP outputs ξ that is (γ, ε)-approximate PCP
1
oracle calls to ApxRidge as long
with O γ −1 log γε

1
0
as log(1/ε ) = Θ log γε
.

Allen-Zhu, Zeyuan, Richtárik, Peter, Qu, Zheng, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In ICML, 2016.

0

• QuickPCR outputs xthat is (γ, ε)-approximate PCR
1
oracle calls to ApxRidge as
with O γ −1 log γλε

1
0
long as elog(1/ε ) = Θ log γλε
.
We make a final remark here regarding the practical usage
of QuickPCP and QuickPCR.
Remark 7.4. Since our theory is for (γ, ε)-approximations
that have two parameters, the user in principle has to feed
in both γ and n where n is the degree of the polynomial
approximation to the sign function. In practice, however, it
is usually sufficient to obtain (ε, ε)-approximate PCP and
PCR. Therefore, our pseudocodes allow users to set γ = 0
and thus ignore this parameter γ; in such a case, we shall
use γ = log(n)/n which is equivalent to setting γ = Θ(ε)
because n = Θ(γ −1 log(1/γε)).

8

Experiments

We provide empirical evaluations in the full version of this
paper.

9

Conclusion

We summarize our contributions.
• We put forward approximate notions for PCP and PCR
that do not rely on any eigengap assumption. Our notions reduce to standard ones under the eigengap assumption.
• We design near-optimal polynomial approximation
g(x) to sgn(x) satisfying (1.1) and (1.2).
• We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary, our g(x)
can be applied to matrices in a stable manner.
• We obtain faster, provable PCA-free algorithms for
PCP and PCR than known results.

References
Allen-Zhu, Zeyuan. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.

Boutsidis, Christos and Magdon-Ismail, Malik. Faster
SVD-truncated regularized least-squares. In 2014 IEEE
International Symposium on Information Theory, pp.
1321–1325. IEEE, 2014.
Chan, Tony F and Hansen, Per Christian. Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations. SIAM Journal
on Scientific and Statistical Computing, 11(3):519–530,
1990.
Elliott, David. Error analysis of an algorithm for summing
certain finite series. Journal of the Australian Mathematical Society, 8(02):213–221, 1968.
Eremenko, Alexandre and Yuditskii, Peter. Uniform approximation of sgn x by polynomials and entire functions. Journal d’Analyse Mathématique, 101(1):313–
324, 2007.
Eremenko, Alexandre and Yuditskii, Peter. Polynomials of
the best uniform approximation to sgn (x) on two intervals. Journal d’Analyse Mathématique, 114(1):285–315,
2011.
Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal Component Projection Without Principal Component Analysis. In ICML, 2016.
Gil, Amparo, Segura, Javier, and Temme, Nico M. Numerical Methods for Special Functions. Society for
Industrial and Applied Mathematics, jan 2007. ISBN
978-0-89871-634-4. doi: 10.1137/1.9780898717822.
URL
http://epubs.siam.org/doi/
abs/10.1137/1.9780898717822http:
//epubs.siam.org/doi/book/10.1137/
1.9780898717822.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016a.

Han, Insu, Malioutov, Dmitry, Avron, Haim, and Shin,
Jinwoo. Approximating the spectral sums of largescale matrices using chebyshev approximations. arXiv
preprint arXiv:1606.00942, 2016.

Allen-Zhu, Zeyuan and Li, Yuanzhi. First Efficient Convergence for Streaming k-PCA: a Global, Gap-Free, and
Near-Optimal Rate. ArXiv e-prints, abs/1607.07837,
July 2016b.

Higham, N. Functions of Matrices. Society for Industrial and Applied Mathematics, 2008. doi: 10.1137/1.
9780898717778. URL http://epubs.siam.org/
doi/abs/10.1137/1.9780898717778.

Faster Principal Component Regression and Stable Matrix Chebyshev Approximation

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance reduction.
In Advances in Neural Information Processing Systems,
NIPS 2013, pp. 315–323, 2013.
Musco, Cameron and Musco, Christopher. Randomized
block krylov methods for stronger and faster approximate singular value decomposition. In NIPS, pp. 1396–
1404, 2015.
Nakatsukasa, Yuji and Freund, Roland W. Computing
fundamental matrix decompositions accurately via the
matrix sign function in two iterations: The power of
zolotarev’s functions. SIAM Review, 58(3):461–493,
2016.
Nesterov, Yurii. Introductory Lectures on Convex Programming Volume: A Basic course, volume I. Kluwer Academic Publishers, 2004. ISBN 1402075537.
Schilders, Wilhelmus H.A., Van der Vorst, Henk A., and
Rommes, Joost. Model order reduction: theory, research
aspects and applications, volume 13. Springer, 2008.
Shewchuk, Jonathan Richard. An introduction to the conjugate gradient method without the agonizing pain, 1994.
Trefethen, Lloyd N. Approximation Theory and Approximation Practice. SIAM, 2013.
van den Eshof, Jasper, Frommer, Andreas, Lippert, Th,
Schilling, Klaus, and van der Vorst, Henk A. Numerical
methods for the qcdd overlap operator. i. sign-function
and error bounds. Computer Physics Communications,
146(2):203–224, 2002.

