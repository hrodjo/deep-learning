Selective Inference for Sparse High-Order Interaction Models

Shinya Suzumura 1 Kazuya Nakagawa 1 Yuta Umezu 1 Koji Tsuda 2 3 Ichiro Takeuchi 1 3

Abstract
Finding statistically significant high-order interactions in predictive modeling is important but
challenging task because the possible number of
high-order interactions is extremely large (e.g.,
> 1017 ). In this paper we study feature selection and statistical inference for sparse highorder interaction models. Our main contribution
is to extend recently developed selective inference framework for linear models to high-order
interaction models by developing a novel algorithm for efficiently characterizing the selection
event for the selective inference of high-order interactions. We demonstrate the effectiveness of
the proposed algorithm by applying it to an HIV
drug response prediction problem.

1. Introduction
Finding statistically reliable high-order interaction features
in predictive modeling has been important challenging task.
For example, in a biomedical study, co-occurrence of multiple mutations in multiple genes may have a significant influence on a response to a drug even if occurrence of single
mutation in each of these genes has no influence (Manolio & Collins, 2006; Cordell, 2009). A major challenge
in prediction modeling with high-order interaction features
is the exponentially expanded feature space. If one has a
dataset with d original variables and takes into account

Pr ind
teractions up to order r, the model has D :=
ρ=1 ρ
features (e.g., for d = 10, 000, r = 5, D > 1017 ). Unless
both d and r are fairly small, D is extremely large. Feature selection and statistical inference in such an extremely
high-dimensional model are challenging both computationally and statistically.
A common approach to high-dimensional modeling is to
consider a sparse model, i.e., a model only with a selected
1

Nagoya Institute of Technology, Nagoya, Japan 2 University
of Tokyo, Tokyo, Japan 3 RIKEN, Tokyo, Japan. Correspondence
to: Ichiro Takeuchi <takeuchi.ichiro@nitech.ac.jp>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Example of the tree structure among high-order interaction features when d = 4 and r = 3.

subset of features. In the past two decades, considerable
amount of studies have been done on sparse modeling and
feature selection in high-dimensional models. In these
studies, a variety of feature selection algorithms such as
marginal screening (Fan & Lv, 2008), orthogonal matching pursuit (Pati et al., 1993), LASSO (Tibshirani, 1996),
and their various extensions have been developed. On the
other hand, statistical inference for sparse models (hypothesis testing or confidence interval computation of the fitted coefficients) have not been deeply studied until very
recently. The main challenge in statistical inference of
sparse models is that, if the data is used for selecting a
subset of features, this selection event must be taken into
account in the following inference stage. Otherwise, the
inference results are distorted by so-called selection bias,
and false positive errors cannot be controlled at desired
levels. This problem is refereed to as selective inference
or post selection inference (Benjamini & Yekutieli, 2005;
Benjamini et al., 2009; Berk et al., 2013). After the seminal
work by Lee et al. (2016), significant progress has been recently made on selective inference for sparse linear models
(Fithian et al., 2014b; Lee & Taylor, 2014; Fithian et al.,
2015; Tian & Taylor, 2015; Taylor & Tibshirani, 2016;
Yang et al., 2016; Barber & Candès, 2016).
In this paper, we study feature selection and statistical inference for sparse high-order interaction models. Unfortunately, neither existing feature selection methods nor existing selective inference methods can be applied to sparse
high-order interaction models because the computational
costs of these existing methods at least linearly depend on
the number of features D. The main contribution in this
paper is to develop computationally efficient algorithms for
these two tasks when the original variables are represented
in [0, 1]d . Our main idea is to exploit the underlying tree
structure of high-order interaction features as depicted in

Selective Inference for Sparse High-Order Interaction Models

Figure 1. In feature selection tasks, it allows us to efficiently identify interaction features that have no chance to
be selected. In statistical inference tasks, it allows us to efficiently identify interaction features that do not affect the
results of the selective inference.
We demonstrate the effectiveness of the proposed methods
through numerical experiments both on synthetic and real
datasets. In the latter, we apply the proposed method to
HIV dataset in (Rhee et al., 2003), where the goal is to identify statistically significant high-order interactions of multiple gene mutations that are significantly associated with
HIV drug responses.

Related works and our contributions Methods for efficiently finding high-order interaction features and properly
evaluating their statistical significances have long been desired in many scientific studies.
In the past decade, feature selection for interaction models has been studied in the context of sparse learning (Choi
et al., 2010; Hao & Zhang, 2014; Bien et al., 2013). None
of these works have a special computational trick for handling exponentially large number of interaction features,
which makes their empirical evaluations restricted up-to
second order interactions. One commonly used heuristic
in the context of interaction modeling is to introduce a
prior knowledge such as strong heredity assumption where,
e.g., an interaction term z1 z2 would be selected only when
both of z1 and z2 are selected. Such a heuristic restriction is helpful for reducing the number of interaction terms
to be considered. However, in many scientific studies, researchers are primarily interested in finding interactions
even when their main effects alone do not have any association with the response. The idea of considering a tree structure among interaction features has been commonly used n
data mining literature (Kudo et al., 2005; Saigo et al., 2006;
Nakagawa et al., 2016). However, it is difficult to properly
assess the statistical significances of the selected features
by these mining techniques.
One traditional approach to assessing the statistical significances of selected features is multiple testing correction
(MTC). In the context of DNA microarray studies, many
MTC procedures for high-dimensional data have been proposed (Tusher et al., 2001; Dudoit et al., 2003). An MTC
approach for statistical evaluation of high-order interaction features was recently studied in (Terada et al., 2013;
Llinares-López et al., 2015). A main drawback of MTC is
that they are highly conservative when the number of candidate features increases. Another common approach is datasplitting (DS). (Fithian et al., 2014a). In DS approach, we
split the data into two subsets, and use one for feature selection and another for statistical inference, which enables
us to remove the selection bias. However, performances

of DS approach is clearly weak both in selection and inference stages because only a part of the available data is
used in each stage. In addition, it is quite annoying that
different set of features would be selected if data is splitted
differently. Recently, much attention has been paid to selective inference for sparse linear models. The basic idea
of selective inference is to make inferences conditional on
a feature selection event. Lee et al. (2016) recently proposed a practical selective inference framework for a class
of feature selection algorithms.
The main contribution in this paper is to extend the selective inference framework into sparse high-order interaction
models by introducing novel computational algorithms. To
the best of our knowledge, there are no other existing works
for sparse high-order interaction models in which the statistical significances of the fitted coefficients are properly
evaluated in non-asymptotic sense.

Notations We use the following notations in the remainder. For any natural number n, we define [n] :=
{1, . . . , n}. A vector and a matrix is denoted such as
v ∈ Rn and M ∈ Rn×m , respectively. The index function is written as 1{z} which returns 1 if z is true, and 0
otherwise. The sign function is written as sgn(z) which
returns 1 if z ≥ 0, and −1 otherwise. An n × n identity
matrix is denoted as In .

2. Preliminaries
2.1. Problem setup
Consider a regression problem with a response Y ∈ R and
d-dimensional original covariates z = [z1 , . . . , zd ]> by the
following high-order interaction model up to r-th order

Y =

X

X

αj1 zj1 +

j1 ∈[d]

+ ··· +

αj1 ,j2 zj1 zj2

(j1 ,j2 )∈[d]×[d]
j1 6=j2

X

αj1 ,...,jr zj1 · · · zjr + ε,

(1)

(j1 ,...,jr )∈[d]r
j1 6=...6=jr

where αs are the coefficients and ε is a random noise. We
assume that each original covariate zj , j ∈ [d] is defined in
a domain [0, 1]. Here, values 1 and 0 respectively might
be interpreted as the existence and the non-existence of
a certain property, and values between them indicate the
“degree” of existence. High-order interaction features thus
represent co-existence of multiple properties. For example,
if we are interested in interactions among age, body mass
index (BMI), and a mutation in a certain gene, we may code

Selective Inference for Sparse High-Order Interaction Models

some covariates as

 1
(BMI − 15)/(30 − 15)
zj1 :=

0

if BMI > 30,
if BMI ∈ [15, 30],
if BMI < 15,

zj2 := 1{mutation in the gene}.
Then, e.g., an interaction term zj1 zj2 represents the coexistence of high BMI and a mutation in the gene.
The
interaction model Eq.(1) has in total D :=

P high-order
d
features.
Let us write the mapping from the
ρ∈[r] ρ
original covariates z := [z1 , . . . , zd ]> ∈ Rd to the highorder interaction features x := [x1 , . . . , xD ]> ∈ RD as
φ : [0, 1]d → [0, 1]D , z 7→ x,, i.e.,
x := φ(z) = [z1 , . . . , zd , z1 z2 , . . . , zd−1 zd ,
. . . , z1 ···zk , . . . , zd−r+1 ···zd ]>
Then, the high-order interaction model Eq.(1) is simply
written as a D-dimensional linear model
y = β > x = β1 x 1 + · · · + βD x D ,
where β1 , . . . , βD are D coefficients corresponding to
αj1 , . . . , αj1 ,...,jr in Eq.(1). Since a high-order interaction
feature is a product of original covariates defined in [0, 1],
the range of each feature xj , j ∈ [D] is also [0, 1].
The original training set is denoted as {(zi , yi ) ∈ [0, 1]d ×
R}i∈[n] , while the expanded training set is written as
{(xi , yi ) ∈ [0, 1]D × R}i∈[n] . The latter is also denoted as
(X, y) ∈ [0, 1]n×D × Rn where each row of X is xi ∈ Rd
and each element of y is yi . Furthermore, the j-th column
of X is written as x·j , j ∈ [D]. We denote the pseudo
inverse of X as X + := (X > X)−1 X > .
Our goal is to identify statistically significant high-order interaction terms that have large impacts on the response Y
by identifying regression coefficients αs which are significantly deviated from zero. Unfortunately, since the number of coefficients αs to be fitted would be far greater than
the sample size n, traditional least-square estimation theory cannot be used for making statistical inferences on the
fitted model. We thus consider first to perform feature selection and then to make statistical inference only for the
selected features based on selective inference approach.
2.2. Selective inference for sparse linear models
In this section, we briefly review the selective inference
framework for sparse linear models developed by Lee et al.
(2016). Selective inference is developed for two stage
methods, where a subset of features is selected in the first
stage, and inferences are made only on the selected features
in the second stage. A key finding by Lee et al. (2016) is

that, if the first selection stage is described as a linear selection event, then exact statistical inference of the fitted
coefficients conditional on the selection event can be done.
Consider a linear regression model y = Xβ ∗ + ε, where
β ∗ ∈ RD is the true coefficients and ε is distributed according to N(0, σ 2 I) with known variance σ 2 .
Feature selection stage Suppose that, in the first feature
selection stage, a subset of features S ⊆ [D] are selected.
The selective inference framework in Lee et al. (2016) can
be applied to feature selection algorithms whose selection
process can be characterized by a set of linear inequalities
in the form of Ay ≤ b with a certain matrix A and a certain vector b that do not depend on y. This type of selection event is called a linear selection event. In the selective
inference framework, inferences are made conditional on
the selection event. It means that, in the case of a linear
selection event, we only care about the cases where y is
observed in a polytope Pol(S) := {y ∈ Rn | Ay ≤ b}.
In Lee & Taylor (2014) and Lee et al. (2016), marginal
screening, OMP and LASSO are shown to be linear selection events, indicating that the selective inference framework can be applied to statistical testing of the selected
features by these algorithms.
Statistical inference stage Consider a hypothesis testing
for the j-th selected feature in S
∗
∗
H0,j : βS,j
= 0 vs. H1,j : βS,j
6= 0.

(2)

The least-square estimator of the linear model only with the
selected features S is written as β̂S = (XS> XS )−1 XS> y.
If we consider the case where S is NOT selected from the
data, i.e., independent of y, then, under the null hypothesis
H0 , the sampling distribution of each fitted coefficient is
2
2
β̂S,j ∼ N(0, σS,j
), where σS,j
:= σ 2 (XS> XS )−1
jj .

(3)

For two-sided test at level α, if the critical values `α/2 and
uα/2 are chosen to be the lower and the upper α/2 points
of the sampling distribution in Eq.(3), then the type I error
at level α is controlled as
Pr(β̂S,j ∈
/ [`α/2 , uα/2 ]) ≤ α

(4)

On the other hand, when S is selected from the data as
we consider here, we would like to control the following
selective type I error
(S,j)

(S,j)

(S,j)

(S,j)

Pr(β̂S,j ∈
/ [`α/2 , uα/2 ] | {S is selected})
=Pr(β̂S,j ∈
/ [`α/2 , uα/2 ] | y ∈ Pol(S)) ≤ α

(5)
(S,j)

by appropriately selecting the adjusted critical values `α/2
(S,j)

and uα/2 , where the selection event {S is selected} is

Selective Inference for Sparse High-Order Interaction Models

written as y ∈ Pol(S) in the case of a linear selection
event. Lee et al. (2016) derived how to compute these
adjusted critical values as formally stated in the following
lemma.
Lemma 1. If the critical values are computed as
(S,j)

[L(S,j),U (S,j)] −1

`α/2 := (F0,σ2

)

(α/2),

(6a)

(1 − α/2),

(6b)

(S,j)

`↵/2

S,j

(S,j)

[L(S,j),U (S,j)] −1

uα/2 := (F0,σ2

)

S,j

y

L(S, j)

U (S, j)
(S,j)
u↵/2

then the selective type I error is controlled as in Eq. (5),
[L,U ]
where Fµ,σ2 is the cumulative distribution function of a
truncated Normal distribution TN(µ, σ 2 , L, U ), i.e.,
[L,U ]

Fµ,σ2 (x) =

Φ((x − µ)/σ) − Φ((L − µ)/σ)
,
Φ((U − µ)/σ) − Φ((L − µ)/σ)

and the truncation points are obtained, by using the observed β̂S,j and y, as
L(S, j) = β̂S,j + θL (XS> XS )−1
jj ,
where θL := min θ s.t. y +
θ∈R

U (S, j) = β̂S,j +

θ(XS+ )> ej

(7a)
∈ Pol(S),

θU (XS> XS )−1
jj ,

where θU := max θ s.t. y +
θ∈R

θ(XS+ )> ej

(7b)
∈ Pol(S).

The proof of Lemma 1 is is presented in Appendix A although it is easily proved by using the results in Lee et al.
(2016). See Lee et al. (2016) for more general statement
about the selective inference framework.
Eq.(7) indicates that the truncation points are obtained by
considering the interval where the test statistic β̂S,j can
move within the polyhedron Pol(S). Figure 2 schematically illustrates that, when we make inferences conditional
on a linear selection event S, the sampling distribution is
defined within the polytope Pol(S), and it follows a truncated normal distribution when y is normally distributed.

✓(XS+ )> ej

Figure 2. An illustration of polyhedral lemma. The polyhedron
represents the selection event and truncation points can be computed by optimizing θ along with the direction (XS+ )> ej . In addition, critical values can be obtained by computing Eq.(6), and
the red region shows the rejection region of the test in Eq.(2).

3.1. MS for interaction models
Consider selecting the top k interaction features from all
the D interaction features that have marginal strong correlations with the response. Noting that each feature is
defined in [0, 1] and the value indicates (the degree of)
the existence of a certain property, we consider a score
x>
·j y, j ∈ [D] for each of the D features, and select the top
k features according to their absolute scores |x>
·j y|. We denote the index set of the selected k features by S, and that
of the unselected k̄ := D − k features by S̄ := [D] \ S.
Since D is extremely large, we cannot compute the score
for each interaction feature. We exploit the tree structure
among interaction patterns as depicted in Figure 1.
Definition 2. (Descendant features) For each j ∈ [D], let
Des(j) ⊆ [D] be the set of features corresponding to the
descendant nodes in the tree including j itself.

Unfortunately, we cannot directly apply this selective inference framework to high-order interaction models because
the polytope Pol(S) is characterized by extremely large
number of linear inequalities, and the optimization problems in Eq.(7) are hard to solve.

Lemma 3. Consider an interaction feature x·j , j ∈ [D],
whose indices are represented in a tree structure as depicted in Figure1. Then, for any node j ∈ [D] in the tree,


 X

X
|x·j̃ y| ≤ max
xij yi , −
xij yi
(8)



3. Feature selection for interaction models

for all j̃ ∈ Des(j).

In this section, we present two feature selection algorithms
for high-order interaction models. Since the number of features D is extremely large, existing feature selection algorithms for linear models cannot be directly applied to interaction models. In this paper, we study marginal screening
(MS) and orthogonal matching pursuit (OMP) as examples
of feature selection algorithms.

The proof of Lemma 3 is presented in Appendix A.
Lemma 3 tells that, for a descendant feature x·j̃ , (j, j̃) ∈
S × Des(j), an upper bound of the absolute score |x>
y|
·j̃
can be computed based on its parent feature x·j .

i:yi >0

i:yi <0

We note that this simple upper bound has been used in some
data mining studies such as Saigo et al. (2006); Kudo et al.

Selective Inference for Sparse High-Order Interaction Models

(2004); Nakagawa et al. (2016). When we search over the
tree, if the upper bound in Eq.(8) is smaller than the current k-th largest score at a certain node j, then we can quit
searching over its descendant nodes j̃ ∈ Des(j).
As pointed out in Lee & Taylor (2014), feature selection
processes of marginal screening is a linear selection event,
i.e., characterized by a set of linear constraints. The event
that k features in S are selected, and k̄ features in S̄ are
>
0
not selected is rephrased as |x>
·j y| ≥ |x·j 0 y|, ∀ (j, j ) ∈
S × S̄. Let sj := sgn(x>
·j y), j ∈ S. Then, the above
feature selection event is rewritten with the sign constraints
of the selected features by the following 2k k̄+k constraints
(−sj x·j − x·j 0 )> y ≤ 0, ∀ (j, j 0 ) ∈ S × S̄,

(9a)

(−sj x·j + x·j 0 ) y ≤ 0, ∀ (j, j ) ∈ S × S̄,

(9b)

>

−sj x>
·j y

0

≤ 0, ∀ j ∈ S.

(9c)

These constraints are written as Ay ≤ 0 with a matrix
A ∈ R(2kk̄+k)×n . Unfortunately, finding θmin and θmax
by naively solving the optimization problems in Eq.(7) is
computationally difficult because the polyhedron Pol(S) is
characterized by the extremely large number of constraints.
For example, when d = 10, 000, r = 5, k = 10, the
number of linear inequalities that defines the polyhedron
Pol(S) is 2k k̄ + k > 1019 .
3.2. OMP for interaction models

upper bound of |x>
r | is given as
·j̃ h
|x>
r | ≤ max
·j̃ h


 X


xij rh,i , −

i:rh,i >0

X

xij rh,i

i:rh,i <0




.



At each iteration, when we search over the tree, if the upper
bound is smaller than the current largest correlation, then,
in the same way as the case of MS, we can quit searching
over its descendant nodes j 0 ∈ Des(j).
It is also pointed out in Lee & Taylor (2014) that a feature
selection process of OMP is linear selection event. At step
h, the event that the (h)-th feature is selected is formulated
>
0
as |x>
·(h) rh | ≥ |x·j 0 rh |, for all j ∈ S̄h . Let PSh := In −
+
ΓSh ΓSh . Then, the above selection event is rewritten as a
set of linear inequalities with respect to y
(−s(h) x·(h) − x·j 0 )> PSh y ≤ 0, ∀ j 0 ∈ S̄h ,

(10a)

(−s(h) x·(h) + x·j 0 ) PSh y ≤ 0, ∀ j ∈ S̄h ,

(10b)

>

−s(h) x>
·(h) PSh y

0

≤ 0,

(10c)

where s(h) = sgn(x>
·(h) rh ). By combining all the linear
selection events in k steps, the
P entire selection event of
the OMP is characterized by h∈[k] (2(D − h) + 1) linear inequalities in Rn . In practice, it is computationally intractable to handle these extremely large number of linear
inequalities.

Orthogonal matching pursuit (OMP) is a well-known iterative feature selection method (Pati et al., 1993). At each
iteration, the most correlated feature with the residual of
the current model which is fitted via least-squares method
by using the features selected in earlier steps.

4. Selective inference for interaction models

Consider again selecting k interaction features by OMP.
Let [(1), . . . , (h)] be the sequence of the indices of the selected features from step 1 to step h for h ∈ [k], and define Sh := {(1), . . . , (h)}. Before step h + 1, we have
already selected h features x·j , j ∈ Sh . Using these h features,
P the current n-dimensional model output is written as
j∈[h] β̂Sh ,(j) x·(j) , where the coefficients β̂Sh ,(j) , j ∈ [h]
are estimated by least-squares method. Denoting by ΓSh
the n×h matrix whose j-th column is x·(j) , the least square
estimates are written as β̂Sh := [β̂Sh ,(1) , . . . , β̂Sh ,(h) ]> =
(ΓSh )+ y. Then, at the h + 1 step, we consider the correlation between the residual vector rh := y − ΓSh β̂Sh and
a feature x·j 0 for j 0 ∈ S̄h , and find the one that maximizes
the absolute correlation |x>
·j 0 rh | among them. Here, since
the number of remaining features |S̄h | = D − h is still
extremely large, it is hard to compute all these D − h correlations. To overcome this difficulty, we can simply use
Lemma 3 just by replacing y with the current residual rh .
Specifically, for a descendant feature x·j̃ , j̃ ∈ Des(j), an

The discussion in §3 suggests that it would be hard to compute critical values for selective inference in Eq.(6) because
the selection event y ∈ Pol(S) is characterized by extremely large number of inequalities. Our basic idea for addressing this computational difficulty is to note that most of
the inequalities actually do not affect the results of the selective inference, and a large portion of them can be identified by exploiting the anti-monotonicity properties defined
in the tree structure among high-order interaction features.

In this section, we present an efficient selective inference
algorithm for high-order interaction models, which is our
main contribution.

4.1. Marginal screening
We consider k trees for each of the k selected features.
Each tree consists of a set of nodes corresponding to each of
the non-selected features j 0 ∈ S̄. For a pair (j, j 0 ) ∈ S × S̄,
the j 0 -th node in the j-th tree corresponds to the linear inequalities Eqs.(9a) and (9b). When we search over these k
trees, we introduce a novel pruning strategy by deriving a
condition such that, if the j 0 -th node in the j-th tree satisfies
certain conditions, then all the (j, j̃ 0 )-th inequalities for all

Selective Inference for Sparse High-Order Interaction Models

j̃ 0 ∈ Desj (j 0 ) are guaranteed to be irrelevant to the selective inference results because they do not affect the optimal
solutions in Eq.(7), where we define Desj (j 0 ) be all the
features corresponding to the descendant node of j 0 in the
j-th tree.

Theorem 6. (i) Consider solving the optimization problem
(a)
in Eq.(11a), and let θ̂L be the current optimal solution,
(a)
i.e., we know that the optimal θL is at least no greater
(a)
than θ̂L . If

Lemma 4. Let η := (XS+ )> ej . The solutions of the optimization problems in (7) are respectively written as

{UD < 0} ∪ {LD > 0, LE < 0, LE /LD > θ̂L }

(a)

(b)

(a)

min
0

(j,j )∈S×S̄,
(sj x·j +x·j 0 )> η>0

(a)

θU :=
(b)

θL :=
(b)
θU

(c)

:=

θL :=

(sj x·j + x·j 0 )> y
,
(sj x·j + x·j 0 )> η

(11a)

(sj x·j + x·j 0 ) y
,
(sj x·j + x·j 0 )> η

(11b)

min

(sj x·j − x·j 0 )> y
,
(sj x·j − x·j 0 )> η

(11c)

max

(sj x·j − x·j 0 )> y
,
(sj x·j − x·j 0 )> η

(j,j 0 )∈S×S̄,
(sj x·j −x·j 0 )> η<0

min

j∈S,
sj x>
·j η>0

sj x>
·j y

sj x>
·j η

(c)

, θU :=

max

j∈S,
sj x>
·j η<0

(11d)
sj x>
·j y

sj x>
·j η

Lemma 5. For any triplet (j, j 0 , j̃ 0 ) ∈ S × S̄ × Desj (j 0 ),
X
(a)
LE := sj x>
xij 0 yi ≤ (sj x·j + x·j̃ 0 )> y, (12a)
·j y +
i:yi <0

:=

sj x>
·j y

+

i:yi >0

(a)

LD := sj x>
·j η +
(a)
UD

:= sj x>
·j η +

(b)

LE := sj x>
·j y −
(b)

UE := sj x>
·j y −
(b)
LD
(b)
UD

X

:=

sj x>
·j η

−

:=

sj x>
·j η

−

X
i:ηi <0

X
i:ηi >0

X
i:yi >0

X
i:yi <0

X
i:ηi >0

X
i:ηi <0

(a)

(a)

(a)

(a)

(a)

(a)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

xij 0 yi ≥ (sj x·j + x·j̃ 0 )> y, (12b)
xij 0 ηi ≤ (sj x·j + x·j̃ 0 )> η, (12c)
xij 0 ηi ≥ (sj x·j + x·j̃ 0 )> η, (12d)
xij 0 yi ≤ (sj x·j − x·j̃ 0 )> y, (12e)
xij 0 yi ≥ (sj x·j − x·j̃ 0 )> y, (12f)
xij 0 ηi ≤ (sj x·j − x·j̃ 0 )> η, (12g)
xij 0 ηi ≥ (sj x·j − x·j̃ 0 )> η. (12h)

The proof of Lemma 5 is presented in Appendix A.

∪ {LD > 0, LE > 0, LE /UD > θ̂L }
is true, then the (j, j̃ 0 )-th constraint in Eq. (9b) for any
(j, j 0 , j̃ 0 ) ∈ S × S̄ × Desj (j 0 ) does not affect the optimal
solution in Eq.(11c).
(iii) Furthermore, consider solving the optimization prob(a)
lem in Eq.(11b), and let θ̂U be the current optimal solution. If
(a)

.

The proof of Lemma 4 is presented in Appendix A.

(a)
UE

(a)

(ii) Next, consider solving the optimization problem in
(b)
Eq.(11c), and let θ̂L be the current optimal solution. If
(b)

max
0

(j,j 0 )∈S×S̄,
(sj x·j −x·j 0 )> η>0

(a)

{UD < 0} ∪ {LD > 0, LE < 0, LE /LD > θ̂L }

>

(j,j )∈S×S̄,
(sj x·j +x·j 0 )> η<0

(a)

is true, then the (j, j̃ 0 )-th constraint in Eq. (9a) for any
(j, j 0 , j̃ 0 ) ∈ S × S̄ × Desj (j 0 ) does not affect the optimal
solution in Eq.(11a).

(a) (b) (c)
− max{θU , θU , θU },

where
θL :=

(a)

∪ {LD > 0, LE > 0, LE /UD > θ̂L }

(c)

θL = − min{θL , θL , θL },
θU =

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

{LD > 0} ∪ {UD < 0, LE < 0, LE /UD < θ̂U }
∪ {UD < 0, LE > 0, LE /LD < θ̂U }
is true, then the (j, j̃ 0 )-th constraint in Eq. (9a) for any
(j, j 0 , j̃ 0 ) ∈ S × S̄ × Desj (j 0 ) does not affect the optimal
solution in Eq.(11b).
(iv) Finally, consider solving the optimization problem in
(b)
Eq.(11d), and let θ̂U be the current optimal solution. If
(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

{LD > 0} ∪ {UD > 0, LE < 0, LE /UD < θ̂U }
∪ {UD > 0, LE < 0, LE /LD < θ̂U }
is true, then the (j, j̃ 0 )-th constraint in Eq. (9b) for any
(j, j 0 , j̃ 0 ) ∈ S × S̄ × Desj (j 0 ) does not affect the optimal
solution in Eq.(11d).
The proof of Theorem 6 is presented in Appendix. Note
that all the conditions in Theorem 6 can be checked at the
j 0 -th node in each tree. If the conditions are satisfied as
the j 0 -th node, then one can skip searching over its subtree. It allows us to perform selective inference for highorder interaction models even the number of constraints
that defines the selection event is extremely large. As we
demonstrate in the experiment section, these pruning conditions are quite effective in practice. For example, we can
perform selective inference for an interaction models with
d = 10, 000, r = 5, k = 10 in a few seconds.

Selective Inference for Sparse High-Order Interaction Models
1.1

0.95
0.1
0.05
0
50

5.1.1. FALSE POSITIVE RATES
Figure 3 shows the FW-FPRs when varying the number of
transactions n ∈ {50, 100, . . . , 250}, the number of original covariates d ∈ {50, 100, . . . , 250}. In all cases, the
FW-FPRs of naive were far greater than the desired significance level α = 0.05, indicating that the selection bias

1
0.95
0.1
0.05
0

250

50

100

150

200

250

The number of features d

1.1

naive
select
split

1.05

Family-wise error (FW-FPRs)

Family-wise error (FW-FPRs)

We demonstrate the performance of the selective inference
for high-order sparse interaction models by numerical experiments on synthetic datasets and a real dataset.

The synthetic dataset was generated as follows. In the experiments for comparing FW-FPRs, we generated the training instances (zi , yi ) ∈ [0, 1]d ×R independently at random
for each i ∈ [n]. The original covariates zi were randomly
generated so that it contains d(1 − ζ) 1s on average, where
ζ ∈ [0, 1] is an experimental parameter for representing
the sparsity of the dataset, while the response yi was randomly generated from a Normal distribution N (0, σ 2 ). In
the experiments for comparing true positive rates (TPRs)
the response yi was randomly generated from a Normal
distribution N (µ(X), σ 2 I), where, for each row of µ(X)
is defined as µ(zi ) = 2z1 z2 z3 in the experiments for MS,
µ(zi ) = 0.5z1 − 2z2 z3 + 3z4 z5 z6 in the experiments for
OMP. We investigated the performances by changing various experimental parameters. We set the baseline parameters as n = 100, d = 100, k = 5, r = 5, α = 0.05,
σ = 0.5, and ζ = 0.6.

100
150
200
The number of instances n

naive
select
split

1.05

(a) n ∈ {50, . . . , 250} (b) d ∈ {50, . . . , 250}
MS

5. Experiments

1
0.95
0.1
0.05
0
50

100
150
200
The number of instances n

250

naive
select
split

1.05
1
0.95
0.1
0.05
0
50

100

150

200

250

The number of features d

(c) n ∈ {50, . . . , 250} (d) d ∈ {50, . . . , 250}
OMP

1
0.8

select
split

True positive rates (TPRs)

True positive rates (TPRs)

Figure 3. False positive rates (FPRs).

0.6
0.4
0.2
0

1
0.8

select
split

0.6
0.4
0.2
0

50

100
150
200
250
The number of instances n

50

100

150

200

250

The number of features d

(a) n ∈ {50, . . . , 250}

(b) d ∈ {50, . . . , 250}
MS

True positive rates (TPRs)

First, we compared selective inference (select) with
naive (naive) and data-splitting (split) on synthetic
datasets. In naive, the critical values of the selected k
features were naively computed without any selection bias
correction mechanisms as in Eq. (4). In split, the dataset
was first divided into two equally sized sets, and one of
them was used for selection stage, and the other was used
for inference stage. Note that the errors controlled by these
methods are individual false positive rate for each of the
selected features (although naive actually cannot control
it), we applied Bonferroni correction within the k selected
features, i.e., we reject the hypothesis in Eq. (2) with the
significance level α/k where α = 0.05, and we refer this
error as family-wise false positive rates (FW-FPRs).

Family-wise error (FW-FPRs)

1

1.1

5.1. Experiments on synthetic datasets

1.1

naive
select
split

1.05

1
0.8

select
split

0.6
0.4
0.2
0

True positive rates (TPRs)

As we discuss in the previous section, the selection event
at each iteration of OMP has same form as MS. Therefore,
we can derive similar pruning conditions as in Theorem 6
for OMP. Due to the space limitation, we deffer the corresponding lemma and the theorem for OMP in Appendix B.

Family-wise error (FW-FPRs)

4.2. Orthogonal matching pursuit (OMP)

1
0.8

select
split

0.6
0.4
0.2
0

50

100
150
200
250
The number of instances n

50

100

150

200

250

The number of features d

(c) n ∈ {50, . . . , 250} (d) d ∈ {50, . . . , 250}
OMP
Figure 4. True positive rates (TPRs).

is harmful. The FW-FPRs of the other two approaches
select and split were successfully controlled.
5.1.2. T RUE POSITIVE RATES
Figure 4 shows the TPRs of select and split (we omit
naive because it cannot control FPRs). Here, TPRs are
defined as the probability of finding truly correlated interaction features. In all the setups, the TPRs of select
were much greater than split. Note that the performances of split would be worse than select both in
the selection and the inference stages. The risk of failing to
select truly correlated features in split would be higher
than select because only half of the data would be used
in the selection stage. Similarly, the statistical power in the
inference stage in split would be smaller than select
because the sample size is smaller.

Selective Inference for Sparse High-Order Interaction Models
Table 1. Computation times [sec]
MS
with computational trick

n
100
500
1000
5000
10000
d
100
500
1000
5000
10000

ζ = 0.8
4.68 × 10−2
1.74 × 10−1
3.38 × 10−1
2.33 × 100
5.04 × 100
ζ = 0.8
4.40 × 10−2
5.06 × 10−1
1.23 × 100
1.53 × 101
3.70 × 101

ζ = 0.9
1.80 × 10−2
9.07 × 10−2
1.54 × 10−1
6.61 × 10−1
1.55 × 100
ζ = 0.9
1.77 × 10−2
1.64 × 10−1
3.74 × 10−1
2.88 × 100
6.16 × 100

OMP
ζ = 0.8
1.37 × 102
1.80 × 102
2.65 × 102
1.05 × 103
2.06 × 103
ζ = 0.8
1.47 × 102
≥ 1 day
≥ 1 day
≥ 1 day
≥ 1 day

ζ = 0.9
1.31 × 102
1.36 × 102
1.41 × 102
2.57 × 102
5.12 × 102
ζ = 0.9
1.31 × 102
≥ 1 day
≥ 1 day
≥ 1 day
≥ 1 day

Table 2. The numbers of significant high-order interactions of
multiple mutations in HIV datasets.
Data
dlv(n = 732)
efv(n = 734)
nvp(n = 746)
3tc(n
abc(n
azt(n
d4t(n
ddi(n
tdf(n

=
=
=
=
=
=

633)
628)
630)
630)
632)
353)

apv(n
atv(n
idv(n
lpv(n
nfv(n
rtv(n
sqv(n

=
=
=
=
=
=
=

768)
329)
827)
517)
844)
795)
826)

with computational trick

without computational trick

MS
OMP
1st 2nd 3rd 4th Time[s] 1st 2nd 3rd 4th Time[s]
NNRTI (d = 371)
1
.495
2
18.0
.732
5
13.7
4
1
.774
8
17.4
NRTI (d = 348)
1
2
.257
4
15.1
5
13
7
2
.238
9
11.7
2
5
3
1
.231
5
17.5
4
11
6
1
.215
7
1
3
13.7
2
1
.234
6
12.1
.230
3
1
26.4
PI (d = 225)
3
6
1
.188
9
6.5
1
3
2
.150
3
1
5.0
1
6
3
.437
9
6.2
4
4
1
.275
11
6.1
5
7
1
.455
15
5.8
5
7
2
.183
10
1
5.6
1
3
2
.623
7
1
7.8

ζ = 0.8
2.33 × 10−1
1.01 × 100
3.18 × 100
6.20 × 101
1.24 × 102
ζ = 0.8
2.41 × 10−1
3.52 × 101
3.01 × 102
≥ 1 day
≥ 1 day

without computational trick

ζ = 0.8
8.83 × 102
1.33 × 103
2.15 × 103
1.00 × 104
1.98 × 104
ζ = 0.8
8.86 × 102
≥ 1 day
≥ 1 day
≥ 1 day
≥ 1 day

ζ = 0.9
8.28 × 102
8.60 × 102
9.07 × 102
2.05 × 103
4.63 × 103
ζ = 0.9
8.20 × 102
≥ 1 day
≥ 1 day
≥ 1 day
≥ 1 day

Database (Rhee et al., 2003). The goal here is to find statistically significant high-order interactions of multiple mutations (up to r = 5 order interactions) that are highly associated with the drug resistances. We selected k = 30
features, and evaluated the statistical significances of these
features by the selective inference framework. Table 2
shows the numbers of 1st, 2nd, 3rd and 4th order interactions that were statistically significant after Bonferroni
correction, i.e., significance level is set to be α/k with
α = 0.05. (there were no statistically significant 5th order interactions).
Figure 5 shows the degree of significances in the form of
adjusted p-values after Bonferroni correction in increasing
order on idv and d4t datasets by MS and OMP scenario,
respectively. These results indicate that the selective inference approach could successfully identify statistically significant high-order interactions of multiple mutations.

Adjusted p-value

1
1e-05
1e-10
1e-15
1e-20
1e-25
1e-30
1e-35
1e-40
1e-45

,90M
63P
46I,
90M
46I,
63P
46I,
46I
46I
10I,
84V
,90M
63P ,90M
63P
10I,
90M
90M
10I,
63P
63P
10I, A
,82
54V
82A
10I
,82A
63P A
,82
71V
71V
36I,
54V
10I,
54V P
,63
54V V
,71
54V
71V P,71V
,63
54V
,90M
,71V
63P M
,90
71V M
,90
54V
,71V
63P
71V
10I, ,71V
63P
10I,

1
0.1
0.01
0.001
0.0001
1e-05
1e-06
1e-07
1e-08
1e-09

Adjusted p-value

5.1.3. C OMPUTATIONAL EFFICIENCY

(a) idv dataset (MS)

E
,207
67N
V
,142
41L
70R
77L
75I,
8H
E,22
122
35T
62V
0A
E,19
122
Y
Q
,208
,219
70R D,122E
,69
67N
F
,215
41L 3E
,12
67N 9E
,21
70R
E
,122 19R
Y
68G
,215
Y,2
,215 N,210W
41L
,67
,43E
41L
F
215
65R
5T
69i
E,13
,122
41L
5Y
75M 10W,21
N,2
15Y
103
W,2
,210
41L 0W
I,21
118
Y
215
E
122
67N
M
151
41L
W
210

Table 1 shows the computation times in seconds for the selective inference approach with and without the computational tricks described in §4 for various values of the number of transactions n ∈ {100, . . . , 10, 000}, the number of
original covariates d ∈ {100, . . . , 10, 000}, and the sparsity rates ζ ∈ {0.8, 0.9} (we terminated the search if the
time exceeds 1 day). It can be observed from the table that,
if we use the computational trick, the selective inferences
can be conducted with reasonable computational costs except for d ≥ 5, 000 and ζ = 0.8 cases with OMP. When the
computational trick was not used, the cost was extremely
large. Especially when the number of original covariates d
is larger than 100, we could not complete the search within
1 day. From the results, we conclude that computational
trick described in §4 is indispensable for selective inferences for sparse high-order interaction models.

ζ = 0.9
5.85 × 10−2
3.74 × 10−1
7.27 × 10−1
3.48 × 100
9.00 × 100
ζ = 0.9
6.02 × 10−2
9.83 × 100
1.66 × 102
1.92 × 103
5.98 × 104

(b) d4t dataset (OMP)

5.2. Application to HIV drug resistance data
We applied the selective inference approach to HIV-1 sequence data obtained from Stanford HIV Drug Resistance

Figure 5. The list of Bonferroni-adjusted selective p-values of
k = 30 selected high-order interactions of multiple mutations
on two HIV datasets.

Selective Inference for Sparse High-Order Interaction Models

Acknowledgements
This work was partially supported by MEXT KAKENHI
(17H00758, 16H06538), JST CREST (JPMJCR1302,
JPMJCR1502), RIKEN Center for Advanced Intelligence Project, and JST support program for starting up
innovation-hub on materials research by information integration initiative.

References
Barber, Rina Foygel and Candès, Emmanuel J. A knockoff filter for high-dimensional selective inference. arXiv
preprint arXiv:1602.03574, 2016.
Benjamini, Yoav and Yekutieli, Daniel. False discovery
rate–adjusted multiple confidence intervals for selected
parameters. Journal of the American Statistical Association, 100(469):71–81, 2005.
Benjamini, Yoav, Heller, Ruth, and Yekutieli, Daniel. Selective inference in complex research. Philosophical
Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 367(1906):
4255–4271, 2009.
Berk, Richard, Brown, Lawrence, Buja, Andreas, Zhang,
Kai, and Zhao, Linda. Valid post-selection inference.
The Annals of Statistics, 41(2):802–837, 2013.
Bien, J., Taylor, J. E., and Tibshirani, R. A LASSO for hierarchical interactions. Journal of The Royal Statistical
Society B, 41:1111–1141, 2013.
Choi, N.H., Li, W., and Zhu, J. Variable selection with the
strong heredity constraint and its oracle property. Journal of the American Statistical Association, 105:354–
364, 2010.
Cordell, Heather J. Detecting gene–gene interactions that
underlie human diseases. Nature Reviews Genetics, 10
(6):392–404, 2009.
Dudoit, Sandrine, Shaffer, Juliet Popper, and Boldrick, Jennifer C. Multiple hypothesis testing in microarray experiments. Statistical Science, pp. 71–103, 2003.
Fan, J. and Lv, J. Sure independence screening for ultrahigh dimensional feature space. Journal of The Royal
Statistical Society B, 70:849–911, 2008.
Fithian, William, Sun, Dennis, and Taylor, Jonathan. Optimal inference after model selection. arXiv preprint
arXiv:1410.2597, 2014a.
Fithian, William, Sun, Dennis, and Taylor, Jonathan. Optimal inference after model selection. arXiv preprint
arXiv:1410.2597, 2014b.

Fithian, William, Taylor, Jonathan, Tibshirani, Robert, and
Tibshirani, Ryan. Selective sequential model selection.
arXiv preprint arXiv:1512.02565, 2015.
Hao, Ning and Zhang, Hao Helen. Interaction screening
for ultrahigh-dimensional data. Journal of the American
Statistical Association, 109(507):1285–1301, 2014.
Kudo, T., Maeda, E., and Matsumoto, Y. An application of
boosting to graph classification. In Advances in Neural
Information Processing Systems, 2005.
Kudo, Taku, Maeda, Eisaku, and Matsumoto, Yuji. An application of boosting to graph classification. In Advances
in neural information processing systems, pp. 729–736,
2004.
Lee, Jason D and Taylor, Jonathan E. Exact post model
selection inference for marginal screening. In Advances
in Neural Information Processing Systems, pp. 136–144,
2014.
Lee, Jason D, Sun, Dennis L, Sun, Yuekai, Taylor,
Jonathan E, et al. Exact post-selection inference, with
application to the lasso. The Annals of Statistics, 44(3):
907–927, 2016.
Llinares-López, Felipe, Sugiyama, Mahito, Papaxanthos,
Laetitia, and Borgwardt, Karsten. Fast and memoryefficient significant pattern mining via permutation testing. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 725–734. ACM, 2015.
Manolio, Teri A and Collins, Francis S. Genes, environment, health, and disease: facing up to complexity. Human heredity, 63(2):63–66, 2006.
Nakagawa, Kazuya, Suzumura, Shinya, Karasuyama,
Masayuki, Tsuda, Koji, and Takeuchi, Ichiro. Safe pattern pruning: An efficient approach for predictive pattern
mining. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 1785–1794. ACM, 2016.
Pati, Yagyensh Chandra, Rezaiifar, Ramin, and Krishnaprasad, PS. Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet decomposition. In Signals, Systems and Computers, 1993.
1993 Conference Record of The Twenty-Seventh Asilomar Conference on, pp. 40–44. IEEE, 1993.
Rhee, Soo-Yon, Gonzales, Matthew J, Kantor, Rami, Betts,
Bradley J, Ravela, Jaideep, and Shafer, Robert W. Human immunodeficiency virus reverse transcriptase and
protease sequence database. Nucleic acids research, 31
(1):298–303, 2003.

Selective Inference for Sparse High-Order Interaction Models

Saigo, H., Uno, T., and Tsuda, K. Mining complex genotypic features for predicting hiv-1 drug resistance. Bioinformatics, 24:2455—2462, 2006.
Taylor, Jonathan and Tibshirani, Robert. Post-selection
inference for l1-penalized likelihood models. arXiv
preprint arXiv:1602.07358, 2016.
Terada, Aika, Okada-Hatakeyama, Mariko, Tsuda, Koji,
and Sese, Jun. Statistical significance of combinatorial
regulations. Proceedings of the National Academy of
Sciences, 110(32):12996–13001, 2013.
Tian, Xiaoying and Taylor, Jonathan. Asymptotics of selective inference. arXiv preprint arXiv:1501.03588, 2015.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1996.
Tusher, Virginia Goss, Tibshirani, Robert, and Chu,
Gilbert. Significance analysis of microarrays applied to
the ionizing radiation response. Proceedings of the National Academy of Sciences, 98(9):5116–5121, 2001.
Yang, Fan, Barber, Rina Foygel, Jain, Prateek, and Lafferty,
John. Selective inference for group-sparse linear models.
arXiv preprint arXiv:1607.08211, 2016.

