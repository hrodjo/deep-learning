Random Fourier Features for Kernel Ridge Regression:
Approximation Bounds and Statistical Guarantees
Haim Avron 1 Michael Kapralov 2 Cameron Musco 3
Christopher Musco 3 Ameya Velingker 2 Amir Zandieh 2

Abstract

In the above, K 2 Rn⇥n is the kernel matrix or Gram
matrix defined by Kij ⌘ k(xi , xj ) and y ⌘ [y1 · · · yn ]T is
the vector of responses. The KRR estimator can be derived
by minimizing a regularized square loss objective function
over a hypothesis space defined by the reproducing kernel
Hilbert space associated with k(·, ·); however, the details
are not important for this paper.

Random Fourier features is one of the most popular techniques for scaling up kernel methods,
such as kernel ridge regression. However, despite impressive empirical results, the statistical
properties of random Fourier features are still not
well understood. In this paper we take steps toward filling this gap. Specifically, we approach
random Fourier features from a spectral matrix
approximation point of view, give tight bounds
on the number of Fourier features required to
achieve a spectral approximation, and show how
spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.

While simple, KRR is a powerful technique that is well
understood statistically and capable of achieving impressive empirical results. Nevertheless, the method has a
key weakness: computing the KRR estimator can be prohibitively expensive for large datasets. Solving (1) generally requires ⇥(n3 ) time and ⇥(n2 ) memory. Thus, the design of scalable methods for KRR (and other kernel based
methods) has been the focus of intensive research in recent
years (Zhang et al., 2015; Alaoui & Mahoney, 2015; Musco
& Musco, 2016; Avron et al., 2016).

1. Introduction
Kernel methods constitute a powerful paradigm for devising non-parametric modeling techniques for a wide range
of problems in machine learning. One of the most elementary is Kernel Ridge Regression (KRR). Given training data
(x1 , y1 ), . . . , (xn , yn ) 2 X ⇥ Y, where X ✓ Rd is an input
domain and Y ✓ R is an output domain, a positive definite
kernel function k : X ⇥ X ! R, and a regularization parameter > 0, the response for a given input x is estimated
as:
n
X
f¯(x) ⌘
k(xj , x)↵j
j=1

where ↵ = (↵1 · · · ↵n )T is the solution of the equation
(K + In )↵ = y.

(1)

*
Equal contribution 1 School of Mathematical Sciences, Tel
Aviv University, Israel 2 School of Computer and Communication Sciences, EPFL, Switzerland 3 Computer Science and
Artificial Intelligence Laboratory, MIT, USA. Correspondence
to: Haim Avron <haimav@post.tau.ac.il>, Michael Kapralov
<michael.kapralov@epfl.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

One of the most popular approaches to scaling up kernel
based methods is random Fourier features sampling, originally proposed by Rahimi & Recht (2007). For shiftinvariant kernels (e.g. the Gaussian kernel), Rahimi &
Recht (2007) presented a distribution D on functions from
X to Cs (s is a parameter) such that for every x, z 2 Rd
k(x, z) = E'⇠D ['(x)⇤ '(z)] .
The idea is to sample ' from D and use k̃(x, z) ⌘
'(x)⇤ '(y) as a surrogate kernel. The resulting approximate KRR estimator can be computed in O(ns2 ) time and
O(ns) memory (see §2.2 for details), giving substantial
computational savings if s ⌧ n.
This approach naturally raises the question: how large
should s be to ensure a high quality estimator? Or, using
the exact KRR estimator as a natural baseline: how large
should s be for the random Fourier features estimator to be
almost as good as the exact KRR estimator? Answering
this question can help us determine when random Fourier
features can be useful, whether the method needs to be improved, and how to go about improving it.
The original random Fourier features analysis (Rahimi
& Recht, 2007) bounds the point-wise distance between

Random Fourier Features for Kernel Ridge Regression

k(·, ·) and k̃(·, ·) (for other approaches for analyzing random Fourier features, see §2.3). However, the bounds do
not naturally lead to an answer to the aforementioned question. In contrast, spectral approximation bounds on the entire kernel matrix, i.e. of the form

(1

)(K + In )

K̃ + In

(1 + )(K + In ) , (2)

naturally have statistical and algorithmic implications. Indeed, in §3 we show that when (2) holds we can bound the
excess risk introduced by the random Fourier features estimator when compared to the KRR estimator. We also show
that K̃ + In can be used as an effective preconditioner for
the solution of (1). This motivates the study of how large s
should be as a function of for (2) to hold.
In this paper we rigorously analyze the relation between
the number of random Fourier features and the spectral approximation bound (2). Our main results are the following:
We give an upper bound on the number of random features needed to achieve (2) (Theorem 7). This bound, in
conjunction with the results in §3, positively shows that
random Fourier features can give guarantees for KRR
under reasonable assumptions.
We give a lower bound showing that our upper bound is
tight for the Gaussian kernel (Theorem 8).
We show that the upper bound can be improved dramatically by modifying the sampling distribution used
in classical random Fourier features (§4). Our sampling
distribution is based on an appropriately defined leverage function of the kernel, closely related to so-called
leverage scores frequently encountered in the analysis of
sampling based methods for linear regression. Unfortunately, it is unclear how to efficiently sample using the
leverage function.
To address the lack of an efficient way to sample using the leverage function, we propose a novel, easy-tosample distribution for the Gaussian kernel which approximates the true leverage function distribution and allows random Fourier features to achieve a significantly
improved upper bound (Theorem 10). The bound has an
exponential dependence on the data dimension, so it is
only applicable to low dimensional datasets. Nevertheless, it demonstrate that classic random Fourier features
can be improved for spectral approximation and motivates further study. As an application, our improved
understanding of the leverage function yields a novel
asymptotic bound on the statistical dimension of Gaussian kernel matrices over bounded datasets, which may
be of independent interest (Corollary 15).

2. Preliminaries
2.1. Setup and Notation
The complex conjugate of x 2 C is denoted by x⇤ . For a
vector x or a matrix A, x⇤ or A⇤ denotes the Hermitian
transpose. The l ⇥ l identity matrix is denoted Il . We use
the convention that vectors are column-vectors.
A Hermitian matrix A is positive semidefinite (PSD) if
x⇤ Ax 0 for every vector x. It is positive definite (PD) if
x⇤ Ax > 0 for every vector x 6= 0. For any two Hermitian
matrices A and B of the same size, A
B means that
B A is PSD.
We use L2 (d⇢) = L2 (Rd , d⇢) to denote the space of
complex-valued square-integrable functions with respect to
some measure ⇢(·). L2 (d⇢) is a Hilbert space equipped
with the inner product
Z
hf, giL2 (d⇢) =
f (⌘)g(⌘)⇤ d⇢(⌘)
d
ZR
=
f (⌘)g(⌘)⇤ p⇢ (⌘)d⌘ .
Rd

In the above, p⇢ (·) is the density associated with ⇢(·).
We denote the training set by (x1 , y1 ), . . . , (xn , yn ) 2
X ⇥ Y ✓ Rd ⇥ R. Note that n denotes the number of
training examples, and d their dimension. We denote the
kernel, which is a function from X ⇥ X to R, by k. We
denote the kernel matrix by K, with Kij ⌘ k(xi , xj ).
The associated reproducing kernel Hilbert space (RKHS)
is denoted by Hk , and the associated inner product by
(·, ·)Hk . Some results are stated for the Gaussian kernel
k(x, z) = exp( kx zk22 /2 2 ) for some bandwidth parameter .
We use
= n to denote the ridge regularization parameter. While for brevity we omit the n subscript, the
choice of regularization parameter generally depends on n.
Typically, n = !(1) and n = o(n). See Caponnetto
& De Vito (2007) and Bach (2013) for discussion on the
asymptotic behavior of n , noting that in our notation, is
scaled by an n factor as compared to those works. As the
ratio between n and will be an important quantity in our
bounds, we denote it as n ⌘ n/ .
The statistical dimension or effective degrees of freedom is
denoted by s (K) ⌘ Tr (K + In ) 1 K .
2.2. Random Fourier Features
2.2.1. C LASSICAL R ANDOM F OURIER F EATURES
Random Fourier features (Rahimi & Recht, 2007) is an
approach to scaling up kernel methods for shift-invariant
kernels. A shift-invariant kernel is a kernel of the form
k(x, z) = k(x z) where k(·) is a positive definite func-

Random Fourier Features for Kernel Ridge Regression

tion (we abuse notation by using k to denote both the kernel
and the defining positive definite function).
The underlying observation behind random Fourier features is a simple consequence of Bochner’s Theorem: for
every shift-invariant kernel for which k(0) = 1 there is a
probability measure µk (·) and a corresponding probability
density function pk (·), both on Rd , such that
Z
T
k(x, z) =
e 2⇡i⌘ (x z) dµk (⌘)
d
ZR
T
=
e 2⇡i⌘ (x z) pk (⌘)d⌘ .
(3)
Rd

In other words, the inverse Fourier transform of the kernel
k(·) is a probability density function, pk (·). For simplicity
we typically drop the k subscript, writing µ(·) = µk (·) and
p(·) = pk (·), with the associated kernel function clear from
context.
If ⌘ 1 , . . . , ⌘ s ⇣are drawn according to p(·),
⌘⇤ and we define
1
2⇡i⌘ T
x
2⇡i⌘ T
x
1
s
p
'(x) ⌘ s e
,··· ,e
, then it is not
hard to see that

Let q(·) be any probability density function whose support
includes that of p(·). If we sample ⌘ 1 , . . . , ⌘ s using q(·),
and define
s
s
!⇤
1
p(⌘ 1 ) 2⇡i⌘T1 x
p(⌘ s ) 2⇡i⌘Ts x
'(x) ⌘ p
e
,··· ,
e
q(⌘ 1 )
q(⌘ s )
s
we still have k(x, z) = E' ['(x)⇤ '(z)]. We refer to this
method as modified random Fourier features and remark
that it can be viewed as a form of importance sampling.
2.2.3. A DDITIONAL N OTATIONS AND I DENTITIES
Now that we have defined (modified) random Fourier features, we can introduce some additional notation and identities that shall prove useful in the rest of the paper.
The (j, l) entry of Z is given by
1
Zjl = p e
s

Let z : Rd ! Cn be defined by
z(⌘)j = e

⇤

k(x, z) = E' ['(x) '(z)] .
The idea of the Random Fourier features method is then to
define
s

1X
k̃(x, z) ⌘ '(x) '(z) =
e
s
⇤

ZZ⇤ =

(4)

Now suppose that Z 2 Cn⇥s is the matrix whose j th row
is '(xj )⇤ , and let K̃ = ZZ⇤ . K̃ is the kernel matrix corresponding to k̃(·, ·). The resulting
random Fourier features
Pn
KRR estimator is f˜(x) ⌘ j=1 k̃(xj , x)↵
˜ j where ↵
˜ is the
solution of (K̃ + In )↵
˜ = y. Typically, s < n and we can
represent f˜(·) more efficiently as:
f˜(x) = '(x)⇤ w
1

p(⌘ l )/q(⌘ l ).

2⇡ixjT ⌘

(5)

.

s

2⇡i⌘ lT (x z)

as a substitute kernel.

w = (Z⇤ Z + Is )

p

Note that
p column l of Z from the previous section is exactly
z(⌘ l ) p(⌘ l )/[s · q(⌘ l )]. So we have:

Z⇤ y

We can compute w in O(ns2 ) time, making random
Fourier features computationally attractive if s < n.
2.2.2. M ODIFIED R ANDOM F OURIER F EATURES
While it seems to be a natural choice, there is no fundamental reason that we must sample the frequencies ⌘ 1 , . . . , ⌘ s
using the Fourier transform density function p(·). In fact,
our results show that it is advantageous to use a different
sampling distribution based on the kernel leverage function
(defined later).

1 X p(⌘ l )
z(⌘ l )z(⌘ l )⇤ .
s
q(⌘ l )
l=1

l=1

where

2⇡ixjT ⌘ l

Finally, by (3) we have E [ZZ⇤ ] = K since
Z
Z
K=
z(⌘)z(⌘)⇤ dµ(⌘) =
z(⌘)z(⌘)⇤ p(⌘)d⌘ .
Rd

Rd

2.3. Related Work
Rahimi & Recht (2007)’s original analysis of random
Fourier features bounded the point-wise distance between
k(·, ·) and k̃(·, ·) . In follow-up work, they give learning
rate bounds for a broad class of estimators using random
Fourier features. However, their results do not apply to
classic KRR (Rahimi & Recht, 2008). Their main bound
becomes relevant only when the number of sampled features is on order of the training set size.
Rudi et al. (2016) prove generalization properties for KRR
with random features, under somewhat difficult to verify
technical assumptions, some of which can be seen as constraining the leverage function distribution that we study.
They leave open improving their bounds via a more refined sampling approach. Bach (2017) analyzes random
Fourier features from a function approximation point of
view. He defines a similar leverage function distribution
to the one that we consider, but leaves open establishing

Random Fourier Features for Kernel Ridge Regression

bounds on and effectively sampling from this distribution,
both of which we address in this work. Finally, Tropp
(2015) analyzes the distance between the kernel matrix and
its approximation in terms of the spectral norm, kK K̃k2 ,
which can be a significantly weaker error metric than (2).
Outside of work on random Fourier features, risk inflation bounds for approximate KRR and leverage score sampling have been used to analyze and improve the Nyström
method for kernel approximation (Bach, 2013; Alaoui &
Mahoney, 2015; Rudi et al., 2015; Musco & Musco, 2016).
We apply a number of techniques from this line of work.
Spectral approximation bounds, such as (2), are quite popular in the sketching literature; see Woodruff (2014). Most
closely related to our work is analysis of spectral approximation bounds without regularization (i.e. = 0) for the
polynomial kernel (Avron et al., 2014). Improved bounds
with regularization (still for the polynomial kernel) were
recently proved by Avron et al. (2016).

3. Spectral Bounds and Statistical Guarantees
Given a feature transformation, like random Fourier features, how do we analyze it and relate its use to nonapproximate methods? A common approach, taken for
example in the original paper on random Fourier features (Rahimi & Recht, 2007), is to bound the difference
between the true kernel k(·, ·) and the approximate kernel
k̃(·, ·). However, it is unclear how such bounds translate
to downstream guarantees on statistical learning methods,
such as KRR. In this paper we advocate and focus on spectral approximation bounds on the regularized kernel matrix,
specifically, bounds of the form
(1

)(K+ In )

ZZ⇤ + In

(1+ )(K+ In ) (6)

for some < 1.
Definition 1. We say that a matrix A is a -spectral approximation of another matrix B, if (1
)B
A
(1 + )B.
Remark 1. When
= 0, bounds of the form of (6)
can be viewed as a low-distortion subspace embedding
bounds. Indeed, when
= 0 it follows from (6) that
Sp (k(x1 , ·), . . . , k(xn , ·)) ✓ Hk can be embedded with
-distortion in Sp ('(x1 ), . . . , '(xn )) ✓ Rs .
The main mathematical question we seek to address in this
paper is: when using random Fourier features, how large
should s be in order to guarantee that ZZ⇤ + In is a spectral approximation of K + In ? To motivate this question, in the following two subsections we show that such
bounds can be used to derive risk inflation bounds for approximate kernel ridge regression. We also show that such
bounds can be used to analyze the use of ZZ⇤ + In as a
preconditioner for K + In .

While this paper focuses on KRR for conciseness, we remark that in the sketching literature, spectral approximation bounds also form the basis for analyzing sketching
based methods for tasks like low-rank approximation, kmeans and more. In the kernel setting, such bounds where
analyzed, without regularization, for the polynomial kernel (Avron et al., 2014). Cohen et al. (2017) recently
showed that (6) along with a trace condition on ZZ⇤ (which
holds for all sampling approaches we consider) yields a so
called “projection-cost preservation” condition for the kernel approximation. With chosen appropriately, this condition ensures that ZZ⇤ can be used in place of K for approximately solving kernel k-means clustering and for certain versions of kernel PCA and kernel CCA. See Musco &
Musco (2016) for details, where this analysis is carried out
for the Nyström method.
3.1. Risk Bounds
One way to analyze estimators is via risk bounds; several recent papers on approximate KRR employ such an
analysis (Bach, 2013; Alaoui & Mahoney, 2015; Musco &
Musco, 2016). In particular, these papers consider the fixed
design setting and seek to bound the expected in-sample
predication error of the KRR estimator f¯, viewing it as an
empirical estimate of the statistical risk. More specifically,
the underlying assumption is that yi satisfies
yi = f ? (xi ) + ⌫i

(7)

for some f ? : X ! R. The {⌫i }’s are i.i.d noise terms,
distributed as normal variables with variance ⌫2 . The empirical risk of an estimator f , which can be viewed as a
measure of the quality of the estimator, is
2
3
n
X
1
R(f ) ⌘ E{⌫i } 4
(f (xi ) f ? (xi ))2 5
n j=1
(note that f itself might be a function of {⌫i }).

Let f 2 Rn be the vector whose j th entry is f ? (xj ). It is
quite straightforward to show that for the KRR estimator f¯
we have (Bach, 2013; Alaoui & Mahoney, 2015):
R(f¯) = n

1 2 T

f (K + In )

+n

1 2
⌫ Tr

2

f

2

K (K + In )

2

.

Since 2 f T (K + In ) 2 f  f T (K + In ) 1 f and
Tr K2 (K + In ) 2  Tr K(K + In ) 1 = s (K),
we define
b K (f ) ⌘ n
R

1

f T (K + In )

1

f +n

1 2
⌫s

(K)

b K (f ). The first term in the above
and note that R(f¯)  R
b K (f ) is frequently referred to
expressions for R(f¯) and R
as the bias term, while the second is the variance term.

Random Fourier Features for Kernel Ridge Regression

Lemma 2. Suppose that (7) holds, and let f 2 Rn be the
vector whose j th entry is f ? (xj ). Let f¯ be the KRR estimator, and let f˜ be KRR estimator obtained using some
other kernel k̃(·, ·) whose kernel matrix is K̃. Suppose that
K̃ + In is a -spectral approximation to K + In for
some
< 1, and that kKk2
1. The following bound
holds:
R(f˜)  (1

)

1

b K (f ) +
R

(1 +

rank(K̃)
·
)
n
·

2
⌫

(8)

The proof appears in the supplementary material (Appendix B).
In short, Lemma 2 bounds the risk of the approximate
KRR estimator as a function of both the risk upper bound
b K (f ) (8) and an additive term which is small if the rank of
R
rank(K̃) and/or is small. In particular, it is instructive
to compare the additive term ( /(1+ ))n 1 ⌫2 ·rank(K̃)
to the variance term n 1 ⌫2 · s (K). Since approximation
K̃ is only useful computationally if rank(K̃) ⌧ n we
should expect the additive term in (8) to also approach 0 an
generally be small when n is large.
Remark 2. An approximation K̃ is only useful computationally if rank(K̃) ⌧ n so K̃ gives a significantly compressed approximation to the original kernel matrix. Ideally we should have rank(K̃)/n ! 0 as n ! 1 and so
the additive term in (8) will also approach 0 and generally
be small when n is large.
3.2. Random Features Preconditioning
Suppose we choose to solve (K + In )↵ = y using
an iterative method (e.g. CG). In this case, we can apply ZZ⇤ + In as a preconditioner. Using standard analysis of Krylov-subspace iterative methods it is immediate that if ZZ⇤ + In is a -spectral approximation of
K + pIn then the number of iterations until convergence
is O( (1 + )/(1
))). Thus, if ZZ⇤ + In is, say, a
1/2-spectral approximation of K+ In , then the number of
iterations is bounded by a constant. The preconditioner can
be efficiently applied (after preprocessing) via the Woodbury formula, giving cost per iteration (if s  n) of O(n2 ).
The overall cost of computing the KRR estimator is therefore O(ns2 + n2 ). Thus, as long as s = o(n) this approach
gives an advantage over direct methods which cost O(n3 ).
For small s it also
p beats non-preconditioned iterative methods cost O(n2 (K)). We reach again the question that
was poised earlier: how big should s be so that ZZ⇤ + In
is a 1/2-spectral approximation of K + In ?
See Cutajar et al. (2016) and Avron et al. (2016) for more
details and discussion on random features preconditioning.

4. Ridge Leverage Function Sampling and
Random Fourier Features
In this section we present upper bounds on the number of random Fourier features needed to guarantee that
ZZ⇤ + In is a -spectral approximation to K + In . Our
bounds are applicable to any shift-invariant kernel, and a
wide range of feature sampling distributions (and, in particular, for classical random Fourier features).
Our analysis is based on relating the sampling density to an
appropriately defined ridge leverage function. This function is a continuous generalization of the popular leverage scores (Mahoney & Drineas, 2009) and ridge leverage
scores (Alaoui & Mahoney, 2015; Cohen et al., 2017) used
in the analysis of linear methods. Bach (2017) defined the
leverage function of the integral operator given by the kernel function and the data distribution. For our purposes, a
more appropriate definition is with respect to a fixed input
dataset:
Definition 3. For given x1 , . . . , xn and shift-invariant kernel k(·, ·), define the ridge leverage function as
⌧ (⌘) ⌘ p(⌘)z(⌘)⇤ (K + I)

1

z(⌘) .

In the above, K is the kernel matrix and p(·) is the distribution associated with k(·, ·).
Proposition 4.

p(⌘)n/(n + )  ⌧ (⌘)  p(⌘)n/
Z

Rd

⌧ (⌘)d⌘ = s (K)

The (simple) proof of the proposition is given in the supplementary material (Appendix C).
Recall that we denote the ratio n/ , which appears frequently in our analysis, by n = n/ . As discussed, theoretical bounds generally set = !(1) (as a function of n)
so n = o(n). However we remark that in practice, it may
frequently be the case that is very small and n
n.
Corollary 5. For any K, s (K)  n .
For any shift-invariant kernel with k(x, x) = 1 and
k(x, z) ! 0 as kx zk2 ! 1 (e.g., the Gaussian kernel) if we allow points to be arbitrarily spread out, the kernel matrix converges to the identity matrix, and s (In ) =
n/(1+ ) = ⌦(n ) if = ⌦(1) so the above bound is tight.
However, this requires datasets of increasingly large diameter (as n grows). In contrast, the usual assumption in statistical learning is that the data is sampled from a bounded
domain X . In §7.2 we show via a leverage function upper
bound that for the important Gaussian kernel, for bounded
datasets we have s (K) = o(n ).

Random Fourier Features for Kernel Ridge Regression

In the matrix sketching literature it is well known that spectral approximation bounds similar to (6) can be constructed
by sampling columns relative to upper bounds on the leverage scores. In the following, we generalize this for the case
of sampling Fourier features from a continuous domain.
Lemma 6. Let ⌧˜ : Rd ! R be a measurable function such
that ⌧˜(⌘) ⌧ (⌘) for all ⌘ 2 Rd , and furthermore assume
that
Z
s⌧˜ ⌘
⌧˜(⌘)d⌘
Rd

is finite. Denote p⌧˜ (⌘) = ⌧˜(⌘)/s⌧˜ . Let
 1/2 and
⇢ 2 (0, 1). Assume that kKk2
. Suppose we take
2
s 83
s⌧˜ ln(16s (K)/⇢) samples ⌘ 1 , . . . , ⌘ s from the
distribution associated with the density p⌧˜ (·) and the construct the matrix Z according to (5) with q = p⌧˜ . Then
ZZ⇤ + In is -spectral approximation of K + In with
probability of at least 1 ⇢.
The proof is based on matrix concentration inequalities,
and appears in the supplementary material (Appendix D).
Lemma 6 shows that if we could sample using the ridge
leverage function, then O(s (K) log(s (K))) samples
suffice for spectral approximation of K (for a fixed and
failure probability). While there is no straightforward way
to perform this sampling, we can consider how well the
classic random Fourier features sampling distribution approximates the leverage function, obtaining a bound on its
performance (the proof is in Appendix D as well):
Theorem 7. Let
 1/2 and 2 (0, 1). Assume that
8
2
kKk2
. If we use s
n ln(16s (K)/⇢) ran3
dom Fourier features (i.e., sampled according to p(·)), then
ZZ⇤ + In is -spectral approximation of K + In with
probability of at least 1 ⇢.
Theorem 7 establishes that if = !(log(n)) and is fixed,
o(n) random Fourier features suffice for spectral approximation, and so the method can provably speed up KRR.
Nevertheless, the bound depends on n instead of s (K),
as is possible with true leverage function sampling (see
Lemma 6). This gap arises from our use of the simple,
often loose, ridge leverage function upper bound given by
Proposition 4.
Unfortunately, as the next section shows, the bound in
Theorem 7 cannot be improved since the classic random
Fourier features sampling distribution can be far enough
from the ridge leverage distribution that ⌦(n ) features
may be needed even when s (K) = o(n ).

5. Lower Bound
Our lower bound shows that the upper bound of Theorem
7 on the number of samples required by classic random
Fourier features to obtain a spectral approximation to K +

In is essentially best possible. The full proof is given in
the supplementary material (Appendix I).
Theorem 8. Consider the Gaussian kernel with
=
2
(2⇡) 1 (so p(⌘) = p12⇡ e ⌘ /2 ). Suppose n
17 is
an odd integer, satisfies 10
 n2 , and R satisfies
n <
pn
3000 log1.5 (n )  R 
. Then, there exists a
500

log(n )

dataset of n points {xj }nj=1 ✓ [ R, R] such that if s random Fourier features (i.e., sampled according to p(·)) are
n
used for some s  400
, then with probability at least 1/2,
there exists a vector ↵ 2 Rn such that
↵T (K + In )↵ <

2 T
↵ (ZZ⇤ + In )↵.
3

(9)

Furthermore, for the said dataset we have s (K) = O(R ·
poly (log n )).
Thus, the number of samples s required for ZZ⇤ + In to
be a 1/2-spectral approximation to K + In for a bounded
dataset of points must either depend exponentially on the
radius of the point set, or at least linearly on n , and there is
an asymptotic gap between what is achieved with classical
random Fourier features and what is achieved by modified
random Fourier features using leverage function sampling.
We note that the above lower bound is proven for a onedimensional point set, which makes it only stronger: even
at low dimensions, and for the common Gaussian kernel,
there is a large gap between the performance of classic random Fourier features and leverage function sampling.
The bound applies for datasets bounded on the range
[ R, R] for R = ⌦ log1.5 n . As we will see in §7,
the key idea behind the proof is to show that for such a
dataset, the ridge leverage function is large on a range of
low frequencies. In contrast, the classic random Fourier
features distribution is very small at the edges of this frequency range, and so significantly undersamples some frequencies and does not achieve spectral approximation.
We remark that it would be preferable if Theorem 8 applied to bounded datasets (i.e. with R fixed), as the usual
assumption in statistical learning theory is that data is sampled from a bounded domain. However, our current techniques are unable to address this scenario. Nevertheless,
our analysis allows R to grow very slowly with n and we
conjecture that the upper bound is tight even for bounded
domains.

6. Improved Sampling (Gaussian Kernel)
Contrasting with the lower bound of Theorem 8, we now
give a modified Fourier feature sampling distribution that
does perform well for the Gaussian kernel on bounded input sets. Furthermore, unlike the true ridge leverage function, this distribution is simple and efficient to sample from.

Random Fourier Features for Kernel Ridge Regression

To reduce clutter, we state the result for a fixed bandwidth
= (2⇡) 1 . This is without loss of generality since we
can rescale the points and adjust the bounding interval.
Our modified distribution essentially corrects the classic
distribution by “capping” the probability of sampling low
frequencies near the origin. This allows it to allocate more
samples to higher frequencies, which are undersampled by
classical random Fourier features. For simplicity, we focus on the one-dimensional setting. Our results extend to
higher dimensions, albeit with an exponential in the dimension loss.

7.1. Primal-Dual Characterization
In this section we prove two alternative characterizations of
the ridge leverage function: one as a minimization, and the
other as a maximization. These characterization are useful
for bounding the leverage function, as we exhibit in the next
subsection for the Gaussian kernel.
Define the operator

: L2 (dµ) ! Cn by
Z
y⌘
z(⇠)y(⇠)dµ(⇠).
Rd

(10)

Definition 9 (Improved Fourier Feature Distribution for the
The following two lemmas constitute the main result of this
Gaussian Kernel). Define the function
subsection. The proofs can be found in the supplementary
p
⇢
material (Appendix E).
1.5
25 max(R, 3000 log n ) |⌘|  10 log(n )
⌧¯R (⌘) ⌘
Lemma 11. The ridge leverage function can alternatively
p(⌘)n
o.w.
be defined as follows:
R
Let s⌧¯R = R ⌧¯R (⌘)d⌘ and define the probability density
p
1
⌧ (⌘) = min
k y
p(⌘)z(⌘)k22 + kyk2L2 (dµ)
function p̄R (⌘) = ⌧¯R (⌘)/s⌧¯R .
y2L2 (dµ)

Note that p̄R (⌘) is just the uniform
distribution for low
p
frequencies with |⌘|  10 log(n ), and the classic
Fourier features distribution, appropriately scaled, outside
this range. As we show in §7, ⌧¯R (⌘) upper bounds the true
ridge leverage function ⌧ (⌘) for all ⌘. Hence, simply applying Lemma 6:

Theorem 10. For any integer n and parameter 0 <  n2 ,
consider the one dimensional Gaussian kernel with =
2
(2⇡) 1 (so p(⌘) = p12⇡ e ⌘ /2 ) and any dataset of n points
{xj }nj=1 ✓ [ R, R] with any radius R > 0. If we sample
8
2
s
s⌧¯R ln(16s⌧¯R /⇢) random Fourier features ac3
cording to p̄R (·) and construct Z according to (5), then
with probability at least 1 ⇢, ZZ⇤ + In is -spectral
approximation of K+ Inp
for any  1/2 and ⇢ 2 (0, 1).
Furthermore, s⌧¯R = O(R log(n ) + log2 n ) and p̄R (·)
can be sampled from in O(1) time.
Theorem 10 represents a possibly exponential improvement over the bound obtainable by classic random Fourier
features. ForpR log1.5 (n ) our modified distribution requires O(R log(n )) samples, as compared to the lower
n
bound of 400
given by Theorem 8.

7. Bounding the Ridge Leverage Function
We conclude by discussing our approach to bounding the
ridge leverage function of the Gaussian kernel, which leads
to Theorems 8 and 10. The key idea is to reformulate the
leverage function as the solution of two dual optimization
problems. By exhibiting suitable test functions for these
optimization problems, we are able to give both upper and
lower bounds on the ridge leverage function, and correspondingly on the sampling performance of classic and
modified Fourier feature sampling.

(11)

Lemma 12. The ridge leverage function can alternatively
be defined as follows:
⌧ (⌘) = maxn
↵2C

p(⌘) · |↵⇤ z(⌘)|2
k ⇤ ↵k2L2 (dµ) + k↵k22

(12)

Similar results are well known for the finite dimensional
case. Here we extend these results to an infinite dimensional case. Lemma 11 allows us to upper bound the leverage function at any point ⌘ 2 Rd by exhibiting a carefully constructed function y(·) and upper bounding the ratio in (11), while Lemma 12 allows us to lower bound it in
a similar fashion.
7.2. Leverage Function: the Gaussian Case
In this section we prove nearly matching bounds on the
leverage score function for the one-dimensional Gaussian
kernel on bounded datasets. For simplicity of presentation
we focus on the one-dimensional setting. Our results extend to higher dimensions, albeit with an exponential in the
dimension loss in the gap between upper and lower bounds.
Our bounds are parameterized by the width of the point
set, which we denote by R. To reduce clutter, we present
all results for fixed = (2⇡) 1 . This is without loss of
generality since we can rescale the points. All the proofs
appear in the supplementary material (Appendices F–H).
Theorem 13. Consider the one dimensional Gaussian kernel with = (2⇡) 1 . For any integer n and parameter
0 <  n2 , and any
p radius R > 0, if x1 , ..., xn 2 [ R, R],
for every |⌘|  10 log n :
⌧ (⌘)  25 max(R, 3000 log1.5 n ) .

Random Fourier Features for Kernel Ridge Regression

The last two theorems lead to a tight bound on the statistical
dimension matrices corresponding to bounded points sets:
Corollary 15. Consider the Gaussian kernel with
=
(2⇡) 1 . For any integer n and parameter 0 <
 n2 ,
and any R > 0, if x1 , ..., xn 2 [ R, R] then we have:
p
s (K)  500 · max(R, 3000 log1.5 n ) log n + 1
p
= O(R log n + log2 n )
Furthermore, if 1000 log1.5 n  R 

500

pn

log(n )

there

exists a set of points x1 , . . . , xn ✓ [ R, R] such that:
⇣ p
⌘
s (K) = ⌦ R log(n /R) .

The bounds above match up to constant factors if
1.5
1000 log1.5 n  R  n0.99 . For any
p 1000 log n 
n
p
R
they match up to a log n factor.
500

log(n )

7.3. Theorems 13 and 14: Proof Outline
Lemma 11 allows us to bound ⌧ (⌘) simply by exhibiting any y(·) which makes the cost function small. One
(s)
simple attempt might be y⌘ (⇠) = (⌘ ⇠) where (·)
is the Dirac delta function. This choice zeros out the first
term. However the delta function is not square integrable,
(s)
y⌘ 62 L2 (dµ), so the lemma cannot be used. Another
trivial attempt is y (0) (⇠) = 0, which zeros out the second
term and recovers the trivial bound ⌧ (⌘)  p(⌘)n . Nevertheless, a smarter test functions y(·) can yield improved
bounds, yielding results on the leverage score function that
are parameterized by the diameter of the point set.
At a high level, our approach is to replace the spike function at ⌘ with a ‘soft spike’ whose Fourier transform still
looks approximately like a cosine wave on [ R, R], yet is
still square integrable. The smaller R is, the more spread
out this function will be able to be, and hence the smaller
its `2 norm, and the better the leverage score bound. A
natural candidate for a ‘soft spike’ is a Gaussian of appropriate variance, but this choice does not suffice to obtain
tight bounds, due to two difficulties. First, for the upper
bound a simple Gaussian does not result in a function that
is close enough to a pure frequency in time domain (first

1.5

smoothed approximation
to target
frequency
smoothed
approximation
to target
function

sinc and gaussian components
dampened test function with lower energy

1
real component

1
g η(ξ) = yη(ξ)p(ξ)

Theorem 14. Consider the one dimensional Gaussian kernel with = (2⇡) 1 . For any integer n 17, any paramn
eter 10
 16
, and every radius 1000 log1.5 n  R 
n 
pn
, there exist x1 , ..., xn 2 [ R, R] such that for
500 log(n )
p
p
every ⌘ 2 [ 100 log n , +100 log n ] we have:
✓
◆
R
p(⌘)
⌧ (⌘)
.
150 p(⌘) + 2Rn 1

0.5

0

0

-1

-0.5
η

-3δR

-2δR

frequency, ξ

-δR

0
x

R

R
δ

2δR

Figure 1. ‘Soft spike’ function y and its Fourier transform
which is approximately a pure cosine wave on [ R, R].

3R
3δ

y,

term of the objective function in Lemma 11) unless we settle for an upper bound of O(R · poly (n )) as opposed
to the tight O(R) on the leverage score density function.
Second, the lower bound on the leverage score function
resulting from using
a Gaussian pulse would only be of
p
the form ⌦(R/ log n ), leading to a weak lower bound
on the p
statistical dimension, namely ⌦(R) as opposed to
⌦(R · log n ), thereby missing entirely the effect of the
regularization parameter on the statistical dimension!
The remedy to the issues above turns out to be the convolution of a (modulated) Gaussian with a rectangular pulse
in time domain (product of a shifted Gaussian with the sinc
function in frequency domain). Specifically, our bounds
are based on variants of a flattened Gaussian spike function
y⌘,b,v (⇠) ⌘ e

(⇠ ⌘)2 b2 /4

· v · sinc (v(⇠

⌘)) .

(13)

for some b > 0, v > 0 and ⌘ 2 R.

It turns out that with a proper setting of parameters (where
one should think of b as large, i.e. the spike y is rather
narrow) the function y⌘,b,v satisfies
( y⌘,b,v )(x)

⇡ p(⌘) · exp(2⇡i⌘x)

R x+ v2
x

v
2

2
2
p 1 e t /2b dt.
2⇡b

An illustration of this function in y is given in Fig. 1, (left)
and the function y in Fig. 1, (right). Note that if the parameter v is chosen to be large, then for x not too large we
R x+ v 1
R +1 1
2
2
2
2
p
have x v2 p2⇡b
e t /2b dt ⇡
e t /2b dt, i.e.
1
2⇡b
2
the second multiplier is essentially constant, i.e. flat as a
function of x (hence the term ‘flattened Gaussian spike’).
This means that y⌘,b,v is essentially the kernel density
evaluated at ⌘ times a pure harmonic term exp(2⇡i⌘x),
which is exactly what one needs to minimize the first
p term
on the rhs of (11) in Lemma 11, up to a factor of p(⌘) –
see Appendix F. One can also see that setting v to be not too
large results in a good function to use in the maximization
problem in (12) in Lemma 12 – see Appendix G. Obtaining tight bounds
p and in particular achieving the right dependence on log n requires several modifications to the
function y above, but the intuition we just described works!

Random Fourier Features for Kernel Ridge Regression

Acknowledgements
The authors thank Arturs Backurs helpful discussions at
early stages of this project. Haim Avron acknowledges
the support from the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered
through Air Force Research Laboratory contract FA875012-C-0323 and an IBM Faculty Award. Cameron Musco
acknowledges the support by NSF Graduate Research Fellowship, AFOSR grant FA9550-13-1-0042 and the NSF
Center for Science of Information.

References
Alaoui, Ahmed El and Mahoney, Michael W. Fast randomized kernel ridge regression with statistical guarantees.
In Neural Information Processing Systems (NIPS), 2015.

2016. URL http://jmlr.org/proceedings/
papers/v48/cutajar16.html.
Feller, William.
An introduction to probability theory and its applications. Volume 1. Wiley series in
probability and mathematical statistics. John Wiley &
sons, New York, Chichester, Brisbane, 1968. ISBN
0-471-25711-7. URL http://opac.inria.fr/
record=b1122219.
Mahoney, Michael W. and Drineas, Petros.
CUR
matrix decompositions for improved data analysis.
Proceedings of the National Academy of
Sciences, 106(3):697–702, 2009.
doi: 10.1073/
pnas.0803205106. URL http://www.pnas.org/
content/106/3/697.abstract.

Avron, Haim, Nguyen, Huy, and Woodruff, David. Subspace embeddings for the polynomial kernel. In Neural
Information Processing Systems (NIPS), 2014.

Musco, Cameron and Musco, Christopher. Recursive sampling for the Nyström method. CoRR, abs/1605.07583,
2016.
URL http://arxiv.org/abs/1605.
07583.

Avron, Haim, Clarkson, Kenneth L., and Woodruff,
David P. Faster kernel ridge regression using sketching and preconditioning. CoRR, abs/1611.03220, 2016.
URL http://arxiv.org/abs/1611.03220.

Ogawa, Hidemitsu. An operator pseudo-inversion lemma.
SIAM Journal on Applied Mathematics, 48(6):1527–
1531, 1988. doi: 10.1137/0148095. URL http:
//dx.doi.org/10.1137/0148095.

Bach, Francis.
On the equivalence between kernel quadrature rules and random feature expansions.
Journal of Machine Learning Research, 18(21):1–38,
2017. URL http://jmlr.org/papers/v18/
15-178.html.

Rahimi, A. and Recht, B. Random features for large-scale
kernel machines. In Neural Information Processing Systems (NIPS), 2007.

Bach, Francis R. Sharp analysis of low-rank kernel
matrix approximations. In Conference on Learning
Theory (COLT), 2013. URL http://jmlr.org/
proceedings/papers/v30/Bach13.html.
Caponnetto, A. and De Vito, E.
Optimal rates
for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331–
368, 2007.
ISSN 1615-3383.
doi: 10.1007/
s10208-006-0196-8. URL http://dx.doi.org/
10.1007/s10208-006-0196-8.
Cohen, Michael B., Musco, Cameron, and Musco, Christopher. Input sparsity time low-rank approximation via
ridge leverage score sampling. In Proceedings of the
Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’17, pp. 1758–1777, Philadelphia, PA, USA, 2017. Society for Industrial and Applied Mathematics. URL http://dl.acm.org/
citation.cfm?id=3039686.3039801.
Cutajar, Kurt, Osborne, Michael, Cunningham, John, and
Filippone, Maurizio. Preconditioning kernel matrices. In
International Conference on Machine Learning (ICML),

Rahimi, Ali and Recht, Benjamin. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Neural Information Processing Systems (NIPS), 2008.
Rudi, Alessandro, Camoriano, Raffaello, and Rosasco,
Lorenzo. Less is more: Nyström computational regularization. In Neural Information Processing Systems
(NIPS), 2015.
Rudi, Alessandro, Camoriano, Raffaello, and Rosasco,
Lorenzo. Generalization properties of learning with random features. ArXiv e-prints, feb 2016.
Tropp, Joel A. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning, 8(1-2):1–230, 2015. ISSN 1935-8237. doi:
10.1561/2200000048. URL http://dx.doi.org/
10.1561/2200000048.
Woodruff, David P. Sketching as a tool for numerical linear
algebra. Found. Trends Theor. Comput. Sci., 10(1–2):
1–157, October 2014. URL http://dx.doi.org/
10.1561/0400000060.
Zhang, Yuchen, Duchi, John, and Wainwright, Martin. Divide and conquer kernel ridge regression: A distributed

Random Fourier Features for Kernel Ridge Regression

algorithm with minimax optimal rates. J. Mach. Learn.
Res., 16(1):3299–3340, January 2015. ISSN 15324435. URL http://dl.acm.org/citation.
cfm?id=2789272.2912104.

