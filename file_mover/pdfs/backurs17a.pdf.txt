Improving Viterbi is Hard:
Better Runtimes Imply Faster Clique Algorithms

Arturs Backurs 1 Christos Tzamos 1

Abstract
The classic algorithm of Viterbi computes the
most likely path in a Hidden Markov Model
(HMM) that results in a given sequence of observations. It runs in time O(T n2 ) given a sequence of T observations from a HMM with n
states. Despite significant interest in the problem and prolonged effort by different communities, no known algorithm achieves more than a
polylogarithmic speedup. In this paper, we explain this difficulty by providing matching conditional lower bounds. Our lower bounds are based
on assumptions that the best known algorithms
for the All-Pairs Shortest Paths problem (APSP)
and for the Max-Weight k-Clique problem in
edge-weighted graphs are essentially tight. Finally, using a recent algorithm by Green Larsen
and Williams for online Boolean
matrix-vector
√
multiplication, we get a 2Ω( log n) speedup for
the Viterbi algorithm when there are few distinct
transition probabilities in the HMM.

1. Introduction
A Hidden Markov Model (HMM) is a simple model that
describes a random process for generating a sequence of
observations. A random walk is performed on an underlying graph (Markov Chain) and, at each step, an observation
is drawn from a probability distribution that depends only
on the current state (the node in the graph).
HMMs are a fundamental statistical tool and one of the
most important questions in the applications of HMMs is
computing the most likely sequence of states visited by the
random walk in the HMM given the sequence of observations. Andrew Viterbi proposed an algorithm (Viterbi,
1967) for this problem that computes the solution in
1
Authors ordered alphabetically.
MIT, US. Correspondence to: Arturs Backurs <backurs@mit.edu>, Christos Tzamos
<tzamos@mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

O(T n2 ) time for any HMM with n states and an observation sequence of length T . This algorithm is known as the
Viterbi algorithm and the problem of computing the most
likely sequence of states is also known as the Viterbi Path
problem.
The Viterbi algorithm has found wide applicability in machine learning. It is an important tool for structured prediction, used e.g., for structured perceptrons (Collins, 2002).
Other applications include speech recognition (Rabiner,
1989; Nefian et al., 2002; Bengio, 2003), part-of-speech
tagging (Collins, 2002), action planning (Attias, 2003),
emotion recognition (Cohen et al., 2000), human activity
classification (Mannini & Sabatini, 2010), and waveform
classification (Kim & Smyth, 2006). Furthermore, it is often combined with other methods. For example, a combination of the Viterbi algorithm and neural networks is
used for speech recognition (Mohamed et al., 2012; AbdelHamid et al., 2012; Bourlard & Morgan, 2012), handwriting recognition and protein secondary structure prediction (Lin et al., 2005; Peng et al., 2009). It also can be combined with Support Vector Machines (Altun et al., 2003).
Finally, the Viterbi algorithm is used as a module in Graph
Transformer Networks, with applications to speech recognition (LeCun et al., 1998; Collobert, 2011).
The quadratic dependence of the algorithm’s runtime on
the number of states is a long-standing bottleneck that limits its applicability to problems with large state spaces,
particularly when the number of observations is large. A
lot of effort has been put into improving the Viterbi algorithm to lower either the time or space complexity. Many
works achieve speedups by requiring structure in the input, either explicitly by considering restricted classes of
HMMs (Felzenszwalb et al., 2004; Siddiqi & Moore, 2005)
or implicitly by using heuristics that improve runtime in
certain cases (Esposito & Radicioni, 2009; Kaji et al.,
2010). For the general case, in (Lifshits et al., 2009; Mahmud & Schliep, 2011) it is shown how to speed up the
Viterbi algorithm by O(log n) when the number of distinct
observations is constant using the Four Russians method
or similar ideas. More recently, in (Cairo et al., 2016),
the same logarithmic speed-up was shown to be possible
for the general case. Despite significant effort, only log-

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

Lower Bound
Theorem 1
Theorem 2

Complexity
2

Tn

Alphabet size

Assumption

T

APSP Conjecture

2

ε

n (for any constant ε > 0)

k-Clique Conjecture

2

Tn

Theorem 4

T n when T ≤ n

1 (unary)

APSP Conjecture

Upper Bound

Complexity

Alphabet size

Assumption

Any

2δ log n distinct transition
probabilities in HMM

Theorem 3

2

T n /2

√
Ω( log n)

√

Table 1: Summary of our upper and lower bounds for the Viterbi Path problem. n is the number of states in the underlying
HMM and T is the number of observations. For a sparse HMM with m non-zero transition probabilities, we show a tight
lower bound of T m (see Theorems 6, 7 and 8 in Section 6).
arithmic improvements are known other than in very special cases. In contrast, the memory complexity can be reduced to almost linear in the number of states without significant overhead in the runtime (Grice et al., 1997; Tarnas
& Hughey, 1998; Churbanov & Winters-Hilt, 2008).
In this work, we attempt to explain this apparent barrier for
faster runtimes by giving evidence of the inherent hardness
of the Viterbi Path problem. In particular, we show that getting a polynomial speedup1 would imply a breakthrough
for fundamental graph problems. Our lower bounds are
based on standard hardness assumptions for the All-Pairs
Shortest Paths and the Min-Weight k-Clique problems and
apply even in cases where the number of distinct observations is small.
Before formally stating our results, let us give some background on the Min-Weight k-Clique problem. This fundamental graph problem asks to find the minimum weight
k-clique in the given undirected weighted graph on n nodes
and O(n2 ) weighted edges. This is the parameterized version of the NP-hard Min-Weight Clique problem (Karp,
1972). The Min-Weight k-Clique is amongst the most wellstudied problems in theoretical computer science, and it is
the canonical intractable problem in parameterized complexity.
A naive algorithm solves the Min-Weight k-Clique in
O(nk ) time and the best known algorithm still runs in
O(nk−o(1) ) for any constant k. Obtaining a significantly
faster algorithm for this problem is a longstanding open
question.
A conjecture in graph algorithms and parameterized complexity is that it there is no O(nk−ε ) algorithm for any
constant ε > 0. The special case of the conjecture with
k = 3 says that finding the minimum weight triangle
in a weighted graph cannot be solved in O(n3−δ ) time
for any constant δ > 0. There are many negative results that intuitively support this conjecture: a truly sub1

Getting an algorithm running in time, say O(T n1.99 ).

cubic algorithm for Min-Weight 3-Clique implies such algorithm for the All-Pairs Shortest Paths as well (Williams
& Williams, 2010). The latter is a well studied problem
and no truly subcubic algorithm is known for it despite
significant effort (Williams, 2014). Unconditional lower
bounds for k-Clique are known for various computational
models, such as Ω(nk ) for monotone circuits (Alon & Boppana, 1987). The planted Clique problem has also proven
to be very challenging (e.g. (Alon et al., 2007; 1998; Hazan
& Krauthgamer, 2011; Jerrum, 1992)). Max-Clique is also
known to be hard to efficiently approximate within nontrivial factors (Håstad, 1999).
We complement our lower bounds with√ an algorithm for
Viterbi Path that achieves speedup 2Ω( log n) when there
are few distinct transition probabilities in the underlying
HMM. We summarize our results in Table 1.
Our results and techniques Our first lower bound shows
that the Viterbi Path problem cannot be computed in time
O(T n2 )1−ε for a constant ε > 0 unless the APSP conjecture is false. The APSP conjecture states that there is
no algorithm for the All-Pairs Shortest Paths problem that
runs in truly subcubic2 time in the number of vertices of the
graph. We obtain the following theorem:
Theorem 1. The V ITERBI PATH problem requires
Ω(T n2 )1−o(1) time assuming the APSP Conjecture.
The proof of the theorem gives a reduction from All-Pairs
Shortest Paths to the Viterbi Path problem. This is done
by encoding the weights of the graph of the APSP instance
as transition probabilities of the HMM or as probabilities
of seeing observations from different states. The proof requires a large alphabet size, i.e. a large number of distinct
observations, which can be as large as the number of total
steps T .
A natural question question to ask is whether there is a
faster algorithm that solves the Viterbi Path problem when
2

Truly subcubic means O(n3−δ ) for constant δ > 0.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

the alphabet size is much smaller than T , say when T = n2
and the alphabet size is n. We observe that in such a case,
the input size to the Viterbi Path problem is only O(n2 ):
we only need to specify the transition probabilities of the
HMM, the probabilities of each observation in each state
and the sequence of observations. The Viterbi algorithm
in this setting runs in Θ(T n2 ) = Θ(n4 ) time. Showing
a matching APSP based lower bound seems difficult because the runtime in this setting is quadratic in the input
size while the APSP conjecture gives only N 1.5 hardness
for input size N . To our best knowledge, all existing reduction techniques based on the APSP conjecture do not
achieve such an amplification of hardness. In order to get
a lower bound for smaller alphabet sizes, we need to use a
different hardness assumption.
For this purpose, we consider the k-Clique conjecture. It
is a popular hardness assumption which states that it is
not possible to compute a minimum weight k-clique on an
edge-weighted graph with n vertices in time O(nk−ε ) for
constant k and ε > 0. With this assumption, we are able
to extend Theorem 1 and get the following lower bound for
the Viterbi Path problem on very small alphabets:
Theorem 2. For any C, ε > 0, the V ITERBI PATH problem on T = Θ(nC ) observations from an alphabet of size
Θ(nε ) requires Ω(T n2 )1−o(1) time assuming the k-Clique
Conjecture for k = d Cε e + 2.
To show the theorem, we perform a reduction from the
Min-Weight k-Clique problem. Given a Min-Weight kClique instance, we create an HMM with two special
nodes, a start node and an end node, and enforce the following behavior of the optimal Viterbi path: Most of the
time it stays in the start or end node, except for a small
number of steps, during which it traverses the rest of the
graph to move from the start to the end node. The time at
which the traversal happens corresponds to a clique in the
original graph of the Min-Weight k-Clique instance. We
penalize the traversal according to the weight of the corresponding k-clique and thus the optimal path will find the
minimum weight k-clique. Transition probabilities of the
HMM and probabilities of seeing observations from different states encode edge-weights of the Min-Weight k-Clique
instance. Further, we encode the weights of smaller cliques
into the sequence of observations according to the binary
expansion of the weights.
Our results of Theorems 1 and 2 imply that the Viterbi algorithm is essentially optimal even for small alphabets. We
also study the extreme case of the Viterbi Path problem
with unary alphabet where the only information available
is the total number of steps T . We show a surprising behavior: when T ≤ n the Viterbi algorithm is essentially optimal, while there is a simple much faster algorithm when
T > n. See Section 5 for more details.

We complement our lower bounds with√ an algorithm for
Viterbi Path that achieves speedup 2Ω( log n) when there
are few distinct transition probabilities in the underlying
HMM. Such a restriction is mild in applications where one
can round the transition probabilities to a small number of
distinct values.
√

Theorem 3. When there are fewer than 2ε log n distinct transition probabilities
for a constant ε > 0,
√
there is a T n2 /2Ω( log n) randomized algorithm for the
V ITERBI PATH problem that succeeds whp.
We achieve this result by developing an algorithm for
online (min, +) matrix-vector multiplication for the case
when the matrix has few distinct values. Our algorithm is
presented in Section 7 and is based on a recent result for online Boolean matrix-vector multiplication by Green Larsen
and Williams (Larsen & Williams, 2017).
The results we presented above hold for dense HMMs. For
sparse HMMs that have at most m edges out of the n2 possible ones, i.e. the transition matrix has at most m nonzero probabilities, the V ITERBI PATH problem can be easily solved in O(T m) time. The lower bounds that we presented above can be adapted directly for this case to show
that no faster algorithm exists that runs in time O(T m)1−ε .
See the corresponding discussion in Section 6.

2. Preliminaries
Notation For an integer m,
{1, 2, . . . , m} by [m].

we denote the set

Definition 1 (Hidden Markov Model). A Hidden Markov
Model (HMM) consists of a directed graph with n distinct
hidden states [n] with transition probabilities Ã(u, v) of
going from state u to state v. In any given state, there is
a probability distribution of symbols that can be observed
and B̃(u, s) gives the probability of seeing symbol s on
state u. The symbols come from an alphabet [σ] of size
σ. An HMM can thus be represented by a tuple (Ã, B̃).
2.1. The Viterbi Path Problem
Given an HMM and a sequence of T observations, the
Viterbi algorithm (Viterbi, 1967) outputs a sequence of T
states that is most likely given the T observations. More
precisely, let S = (s1 , . . . , sT ) be the given sequence of
T observations where symbol st ∈ [σ] is observed at time
t = 1, . . . , T . Let ut ∈ [n] be the state of the HMM at time
t = 1, . . . , T . The Viterbi algorithm finds a state sequence
U = (u0 , u1 , . . . , uT ) starting at u0 = 1 that maximizes
Pr[U |S]. The problem of finding the sequence U is known
as the Viterbi Path problem. In particular, the Viterbi Path

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

problem solves the optimization problem
arg max
u0 =1,u1 ,...,uT

T h
Y

i
Ã(ut−1 , ut ) · B̃(ut , st ) .

t=1

The Viterbi algorithm solves this problem in O(T n2 ) by
computing for t = 1 . . . T the best sequence of length
t that ends in a given state in a dynamic programming
fashion. When run in a word RAM model with O(log n)
bit words, this algorithm is numerically unstable because
even representing the probability of reaching a state requires linear number of bits. Therefore, log probabilities
are used for numerical stability since that allows to avoid
underflows (Young et al., 1997; Amengual & Vidal, 1998;
Li & Tang, 2009; Lee et al., 2007; Huang et al., 2001).
To maintain numerical stability and understand the underlying combinatorial structure of the problem, we assume
that the input is given in the form of log-probabilities, i.e.
the input to the problem is A(u, v) = − log Ã(u, v) and
B(u, s) = − log B̃(u, s) and focus our attention on the
Viterbi Path problem defined by matrices A and B.
Definition 2 (Viterbi Path Problem). The V ITERBI PATH
problem is specified by a tuple (A, B, S) where A and B
are n × n and n × σ matrices, respectively, and S =
(s1 , . . . , sT ) is a sequence of T = nΘ(1) observations
s1 , . . . , sT ∈ [σ] over an alphabet of size σ. Given an instance (A, B, S) of the V ITERBI PATH problem, our goal
is to output a sequence of vertices u0 , u1 , . . . , uT ∈ [n]
with u0 = 1 that solves
arg min
u0 =1,u1 ,...,uT

T
X

[A(ut−1 , ut ) + B(ut , st )] .

t=1

We can assume that log probabilities in matrices A and B
are arbitrary positive numbers without the restriction that
the corresponding probabilities must sum to 1. See Appendix C for a discussion.
A simpler special case of the V ITERBI PATH problem asks
to compute the most likely path of length T without any
observations.
Definition 3 (Shortest Walk Problem). Given an integer
T and a weighted directed graph (with possible self-loops)
on n vertices with edge weights specified by a matrix A, the
S HORTEST WALK problem asks to compute a sequence of
vertices u0 = 1, u1 , . . . , uT ∈ [n] that solves
arg min
u0 =1,u1 ,...,uT

T
X

A(ut−1 , ut ).

t=1

It is easy to see that the S HORTEST WALK problem corresponds to the V ITERBI PATH problem when σ = 1 and
B(u, 1) = 0 for all u ∈ [n].

2.2. Hardness assumptions
We use the hardness assumptions of the following problems.
Definition 4 (A LL -PAIRS S HORTEST PATHS (APSP)
problem). Given an undirected graph G = (V, E) with n
vertices and positive integer weights on the edges, find the
shortest path between u and v for every u, v ∈ V .
The
APSP
conjecture
A LL -PAIRS S HORTEST PATHS
Ω(n3 )1−o(1) time in expectation.

states
that
the
problem
requires

Conjecture
1
(APSP
conjecture).
The
A LL -PAIRS S HORTEST PATHS problem on a graph
with n vertices and positive integer edge-weights bounded
by nO(1) requires Ω(n3 )1−o(1) time in expectation.
There is a long list of works showing conditional hardness
for various problems based on the All-Pairs Shortest Paths
conjecture (Roditty & Zwick, 2004; Williams & Williams,
2010; Abboud & Williams, 2014; Abboud et al., 2015b;c).
Definition 5 (M IN -W EIGHT k-C LIQUE problem). Given a
complete graph G = (V, E) with n vertices and positive integer edge-weights, output the minimum total edge-weight
of a k-clique in the graph.
This is a very well studied computational problem and despite serious efforts, the best known algorithm for this problem still runs in time O(nk−o(1) ), which matches the runtime of the trivial algorithm up to subpolynomial factors.
The k-Clique conjecture states that this problem requires
Ω(nk )1−o(1) time and it has served as a basis for showing conditional hardness results for several problems on
sequences (Abboud et al., 2015a; 2014; Bringmann et al.,
2016) and computational geometry (Backurs et al., 2016).
Conjecture
2
(k-Clique
conjecture).
The
M IN -W EIGHT k-C LIQUE problem on a graph with n
vertices and positive integer edge-weights bounded by
nO(k) requires Ω(nk )1−o(1) time in expectation.
For k = 3, the M IN -W EIGHT 3-C LIQUE problem asks to
find the minimum weight triangle in a graph. This problem is also known as the M INIMUM T RIANGLE problem
and under the 3-Clique conjecture it requires Ω(n3 )1−o(1)
time. The latter conjecture is equivalent to the APSP conjecture (Williams & Williams, 2010).
We often use the following
M IN -W EIGHT k-C LIQUE problem:

variant

of

the

Definition 6 (M IN -W EIGHT k-C LIQUE problem for
k-partite graphs). Given a complete k-partite graph G =
(V1 ∪ . . . ∪ Vk , E) with |Vi | = ni and positive integer weights on the edges, output the minimum total edgeweight of a k-clique in the graph.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms
Θ(1)

If for all i, j we have that ni = nj , it can be shown that
the M IN -W EIGHT k-C LIQUE problem for k-partite graphs
Q
1−o(1)
k
requires Ω
n
time assuming the k-Clique
i=1 i
conjecture. We provide a simple proof of this statement in
the appendix.

3. Hardness of V ITERBI PATH
We begin by presenting our main hardness result for the
V ITERBI PATH problem.

Proof. The optimal path for the V ITERBI PATH instance
begins at node 1. It must end in node 2 since otherwise
when observation ⊥F arrives we collect cost +∞. Similarly, whenever an observation ⊥ arrives the path must be
either on node 1 or 2. Thus, the path first loops in node 1
and then goes from node 1 to node 2 during three consecutive observations u, u and ⊥ for some u ∈ U and stays in
node 2 until the end. Let v1 ∈ V1 and v2 ∈ V2 be the two
nodes visited when moving from node 1 to node 2. The
only two steps of non-zero cost are:

Theorem 1. The V ITERBI PATH problem requires
Ω(T n2 )1−o(1) time assuming the APSP Conjecture.

1. Moving from node 1 to node v1 at the first observation
u. This costs A(1, v1 ) + B(v1 , u) = B(v1 , u).

To show APSP hardness, we will perform a reduction
from the M INIMUM T RIANGLE problem (described in Section 2.2) to the V ITERBI PATH problem. In the instance
of the M INIMUM T RIANGLE problem, we are given a 3partite graph G = (V1 ∪ V2 ∪ U, E) such that |V1 | =
|V2 | = n, |U | = m. We want to find a triangle of minimum
weight in the graph G. To perform the reduction, we define
a weighted directed graph G0 = ({1, 2} ∪ V1 ∪ V2 , E 0 ).
E 0 contains all the edges of G between V1 and V2 , directed
from V1 towards V2 , edges from 1 towards all nodes of V1
of weight 0 and edges from all nodes of V2 towards 2 of
weight 0. We also add a self-loops at nodes 1 and 2 of
weight 0.

2. Moving from node v1 to node v2 at the second observation u. This costs A(v1 , v2 ) + B(v2 , u).

We create an instance of the V ITERBI PATH problem
(A, B, S) as described below. Figure 1 illustrates the construction of the instance.
• Matrix A is the weighted adjacency matrix of G0 that
takes value +∞ (or a sufficiently large integer) for
non-existent edges and non-existent self-loops.
• The alphabet of the HMM is U ∪ {⊥, ⊥F } and thus
matrix B has 2n + 2 rows and σ = m + 2 columns.
For all v ∈ V1 ∪ V2 and u ∈ U , B(v, u) is equal to the
weight of the edge (v, u) in graph G. Moreover, for all
v ∈ V1 ∪ V2 , B(v, ⊥) = +∞ (or a sufficiently large
number) and for all v ∈ V1 ∪ V2 ∪ {1}, B(v, ⊥F ) =
+∞. Finally, all remaining entries corresponding to
nodes 1 and 2 are 0.
• Sequence S of length T = 3m + 1 is generated by
appending the observations u, u and ⊥ for all u ∈ U
and adding a ⊥F observation at the end.

Thus, the overall cost of the path is equal to B(v1 , u) +
A(v1 , v2 ) + B(v2 , u), which is equal to the weight of the
triangle (v1 , v2 , u) in G. Minimizing the cost of the path in
this instance is therefore the same as finding the minimum
weight triangle in G.

4. Hardness of V ITERBI PATH with small
alphabet
The proof of Theorem 1 requires a large alphabet size,
which can be as large as the number of total steps T . In
the appendix, we show how to get a lower bound for the
V ITERBI PATH problem on alphabets of small size by using a different hardness assumption.
Theorem 2. For any C, ε > 0, the V ITERBI PATH problem on T = Θ(nC ) observations from an alphabet of size
Θ(nε ) requires Ω(T n2 )1−o(1) time assuming the k-Clique
Conjecture for k = d Cε e + 2.

5. Complexity of V ITERBI PATH for unary
alphabet
In this section, we focus on the extreme case of
V ITERBI PATH with unary alphabet.
Theorem 4. The V ITERBI PATH problem requires
Ω(T n2 )1−o(1) time when T ≤ n even if the size of the
alphabet is σ = 1, assuming the APSP Conjecture.
The above theorem follows from APSP-hardness of the
S HORTEST WALK problem that we present next.

Given the above construction, the theorem statement follows directly from the following claim.

Theorem 5. The S HORTEST WALK problem requires
Ω(T n2 )1−o(1) time when T ≤ n, assuming the APSP Conjecture.

Claim 1. The weight of the solution to the V ITERBI PATH
instance is equal to the weight of the minimum triangle in
the graph G.

Proof. We will perform a reduction from the
M INIMUM T RIANGLE problem to the V ITERBI PATH

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

wv1 ,v2

0
1

0

0
0
0
0

0
0
0

V1

0
2

Node
1
v ∈ V1 ∪ V2
2

u∈U
0
wv,u
0

⊥
0
∞
0

⊥F
∞
∞
0

V2

(a) The graph specified by transition matrix A. Every edge (v1 , v2 ) in V1 × V2 has
the original edge-weight as in graph G.

(b) The cost of seeing a symbol at every
node given by matrix B.

Figure 1: The construction of matrices A and B for the reduction in the proof of Theorem 1. The notation wv,u denotes
the weight of the edge (v, u) in the original graph G.
problem. In the instance of the M INIMUM T RIANGLE
problem, we are given a 3-partite undirected graph
G = (V1 ∪ V2 ∪ U, E) with positive edge weights such
that |V1 | = |V2 | = n, |U | = m. We want to find a
triangle of minimum weight in the graph G. To perform
the reduction, we define a weighted directed and acyclic
graph G0 = ({1, 2} ∪ V1 ∪ V2 ∪ U ∪ U 0 , E 0 ). Nodes
in U 0 are in one-to-one correspondence with nodes in
U and |U 0 | = m. E 0 is defined as follows. We add all
edges of G between nodes in U and V1 directed from U
towards V1 and similarly, we add all edges of G between
nodes in V1 and V2 directed from V1 towards V2 . Instead
of having edges between nodes in V2 and U , we add the
corresponding edges of G between nodes in V2 and U 0
directed from V2 towards U 0 . Moreover, we add additional
edges of weight 0 to create a path P of m + 1 nodes,
starting from node 1 and going through all nodes in U in
some order. Finally, we create another path P 0 of m + 1
nodes going through all nodes in U 0 in the same order as
their counterparts on path P and ending at node 2. These
edges have weight 0 apart from the last one, entering node
2, which has weight −C (a sufficiently large negative
constant)3 .
We create an instance of the S HORTEST WALK problem
by setting T = m + 4 and A to be the weighted adjacency
matrix of G0 that takes value +∞ (or a sufficiently large
integer) for non-existent edges and self-loops.
The optimal walk of the S HORTEST WALK instance must
include the edge of weight −C entering node 2 since otherwise the cost will be non-negative. Moreover, the walk
3
Since the definition of S HORTEST WALK doesn’t allow negative weights, we can equivalently set its weight to be 0 and add
C to all the other edge weights.

must reach node 2 exactly at the last step since otherwise
the cost will be +∞ as there are no outgoing edges from
node 2. By the choice of T , the walk leaves path P at some
node u ∈ U , then visits nodes v1 and v2 in V1 and V2 , respectively, and subsequently moves to node u0 ∈ U 0 where
u0 is the counterpart of u on path P 0 . The total cost of the
walk is thus the weight of the triangle (u, v1 , v2 ) in G, minus C. Therefore, the optimal walk has cost equal to the
weight of the minimum triangle up to the additive constant
C.

Notice that when T > n, the runtime of the Viterbi algorithm is no longer optimal. We now present a√faster algorithm with a total running time log T · n3 /2Ω( log n) .
As we show in Section 7, the general V ITERBI PATH
problem reduces, according to Equation 2, to computing
(min, +) matrix-vector products. In the case of unary
alphabet, it corresponds to computing (min, +) matrixvector product T times as follows: A ⊕ A ⊕ ... ⊕ A ⊕ z.
This can be equivalently performed by first computing all
(min, +) matrix-matrix products A⊕T = A ⊕ A ⊕ ... ⊕ A
using exponentiation with repeated squaring and then multiplying the resulting matrix with the vector z. This requires only O(log T ) matrix (min, +)-multiplications. Using the currently best algorithm for (min, +) matrix product (Williams, 2014), we√get an algorithm with total running time log T · n3 /2Ω( log n) .

6. Hardness for sparse HMMs
The V ITERBI PATH lower-bounds we have provided apply
to the case where the HMM has all n2 possible edges.
For sparse HMMs that have at most m edges out of the

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

n2 possible ones, i.e. the transition matrix has at most m
non-zero probabilities, the V ITERBI PATH problem can be
easily solved in O(T m) time. The lower bounds that we
presented in the paper can be adapted directly for this case
to show that no faster algorithm exists that runs in time
O(T m)1−ε . This can be easily seen via a padding argument. Consider a√hard instance for V ITERBI PATH on
√ a
dense HMM with m states and m edges. Adding n− m
additional states with self-loops, we
√ obtain a sparse instance with n states and m + n − m = O(m) edges.
Thus, any algorithm that computes the optimal Viterbi Path
in O(T m)1−ε time for the resulting instance would solve
1−ε
√
√
the original instance with m states in O T ( m)2
time contradicting the corresponding lower bound.
This observation directly gives the following lower bounds
for V ITERBI PATH problem, parametrized by the number
m of edges in an HMM with n states.
Theorem 6. The V ITERBI PATH problem requires
Ω(T m)1−o(1) time for an HMM with m edges and n
states, assuming the APSP Conjecture.
Theorem 7. For any C, ε > 0, the V ITERBI PATH problem on T = Θ(mC ) observations from an alphabet of size
Θ(mε ) requires Ω(T m)1−o(1) time assuming the k-Clique
Conjecture for k = d Cε e + 2.
Theorem 8. The V ITERBI √
PATH problem requires
Ω(T m)1−o(1) time when T ≤ m even if the size of the
alphabet is σ = 1, assuming the APSP Conjecture.

7. A faster V ITERBI PATH algorithm
In this section, we present a faster algorithm for the
V ITERBI PATH problem, when there are only few distinct
transition probabilities in the underlying HMM.
√

Theorem 3. When there are fewer than 2ε log n distinct transition probabilities
for a constant ε > 0,
√
there is a T n2 /2Ω( log n) randomized algorithm for the
V ITERBI PATH problem that succeeds whp.
The number of distinct transition probabilities is equal to
the number of distinct entries in matrix Ã in Definition 1.
The same is true for matrix A in the additive version of
V ITERBI PATH, in Definition 2. So, from the theorem
√
statement we can assume that matrix A has at most 2ε log n
different entries for some constant ε > 0.
To present our algorithm, we revisit the definition of
V ITERBI PATH. We want to compute a path u0 =
1, u1 , . . . , uT that minimizes the quantity:
min

u0 =1,u1 ,...,uT

T
X

[A(ut−1 , ut ) + B(ut , st )] .

(1)

t=1

Defining the vectors bt = B(·, st ), we note that (1) is equal

to the minimum entry in the vector obtained by a sequence
of T (min, +) matrix-vector products4 as follows:
A ⊕ (. . . (A ⊕ (A ⊕ (A ⊕ z + b1 ) + b2 ) + b3 ) . . .) + bT (2)
where z is a vector with entries z1 = 0 and zi = ∞ for
all i 6= 1. Vector z represents the cost of being at node i at
time 0. Vector (A ⊕ z + b1 ) represents the minimum cost
of reaching each node at time 1 after seeing observation
s1 . After T steps, every entry i of vector (2) represents
the minimum minimum cost of a path that starts at u0 =
1 and ends at uT = i after T observations. Taking the
minimum of all entries gives the cost of the solution to the
V ITERBI PATH instance.
To evaluate (2), we design an online (min, +) matrixvector multiplication algorithm. In the online matrix-vector
multiplication problem, we are given a matrix and a sequence of vectors in online fashion. We are required to
output the result of every matrix-vector product before receiving the next vector. Our algorithm for online (min, +)
matrix-vector multiplication is based on a recent algorithm
for online Boolean matrix-vector multiplication by Green
Larsen and Williams (Larsen & Williams, 2017):
Theorem 9 (Green Larsen and Williams (Larsen &
Williams, 2017)). For√any matrix M ∈ {0, 1}n×n and any
sequence of T = 2ω( log n) vectors v1 , . . . , vT ∈ {0, 1}n ,
online Boolean matrix-vector
√ multiplication of M and vi
can be performed in n2 /2Ω( log n) amortized time whp. No
preprocessing is required.
We show the following theorem for online (min, +)
matrix-vector multiplication, which gives the promised
runtime for the V ITERBI PATH problem5 since we are interested in the case where T and n are polynomially related,
i.e. T = nΘ(1) .
Theorem
10. Let A ∈ Rn×n be a matrix with at most
√
ε log n
2
distinct entries
for a constant ε > 0. For any se√
quence of T = 2ω( log n) vectors v1 , . . . , vT ∈ Rn , online
(min, +) matrix-vector
multiplication of A and vi can be
√
2 Ω( log n)
performed in n /2
amortized time whp. No preprocessing is required.
Proof. We will show the theorem for the case where A ∈
{0,√+∞}n×n . The general case where matrix A has d ≤
2ε log n distinct values a1 , ..., ad can be handled by creating d matrices A1 , ..., Ad , where each matrix Ak has entries Akij = 0 if Aij = ak and +∞ otherwise. Then, vector
4

A (min, +) product between a matrix M and a vector v
is denoted by M ⊕ v and is equal to a vector u where ui =
minj (Mi,j + vj ).
5
Even though computing all (min, +) products does not directly give a path for the V ITERBI PATH problem, we can obtain
one at no additional cost by storing back pointers. This is standard
and we omit the details.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

r = A ⊕ v can be computed by computing rk = Ak ⊕ v for
every k and√setting ri = mink (rik + ak ). This introduces a
factor of 2ε log n in amortized√ runtime but the final amortized runtime remains n2 /2Ω( log n) if ε > 0 is sufficiently
small. From now on we assume that A ∈ {0, +∞}n×n and
define the matrix Ā ∈ {0, 1}n×n whose every entry is 1 if
the corresponding entry at matrix A is 0 and 0 otherwise.
For every query vector v, we perform the following:
– Sort indices i1 , ..., in such that vi1 ≤ ... ≤ vin in
O(n log n) time.
√

– Partition the indices into p = 2α log n sets, where set
Sk contains indices i(k−1)d np e+1 , ..., ikd np e .
– Set r = (⊥, ..., ⊥)T , where ⊥ indicates an undefined
value.
– For k = 1...p fill the entries of r as follows:
- Let ISk be the indicator vector of Sk that takes
value 1 at index i if i ∈ Sk and 0 otherwise.
- Compute the Boolean matrix-vector product
π k = Ā  ISk using the algorithm from Theorem 9.
- Set rj = mini∈Sk (Aj,i + vi ) for all j ∈ [n] such
that rj = ⊥ and πjk = 1.
– Return vector r.
Runtime of the algorithm
per query The algorithm per√
forms p = 2α log n Boolean matrix-vector multiplica√
tions, for
a total amortized cost of p · n2 /2Ω( log n) =
√
n2 /2Ω( log n) for a small enough constant α > 0. Moreover, to fill an entry rj the algorithm requires going through
all elements
in some set Sk for a total runtime of O(|Sk |) =
√
Ω( log n)
n/2
. Thus,
for all entries pj the total time re√
quired is n2 /2Ω( log n) . The runtime of the other steps is
dominated
√ by these two operations so the algorithm takes
n2 /2Ω( log n) amortized time per query.
Correctness of the algorithm To see that the algorithm
correctly computes the (min, +) product A ⊕ v, observe
that the algorithm fills in the entries of vector r from smallest to largest. Thus, when we set a value to entry rj we
never have to change it again. Moreover, if the value rj
0
gets filled at step k, it must be the case that πjk = 0 for all
0
k < k. This means that for all indices i ∈ S1 ∪ ... ∪ Sk−1
the corresponding entry Aj,i was always +∞.

Acknowledgments
We thank Piotr Indyk for many helpful discussions, for
comments on an earlier version of the writeup and for suggestion on how to improve the presentation. We also thank

the anonymous reviewers for their careful reviews. This
work was supported in part by an IBM PhD Fellowship,
the NSF and the Simons Foundation.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

References
Abboud, Amir and Williams, Virginia Vassilevska. Popular conjectures imply strong lower bounds for dynamic
problems. In Foundations of Computer Science (FOCS),
2014 IEEE 55th Annual Symposium on, pp. 434–443.
IEEE, 2014.

Amengual, Juan C and Vidal, Enrique. Efficient errorcorrecting viterbi parsing. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 20(10):1109–1116,
1998.
Attias, Hagai. Planning by probabilistic inference. In AISTATS, 2003.

Abboud, Amir, Williams, Virginia Vassilevska, and
Weimann, Oren. Consequences of faster alignment of
sequences. In Automata, Languages, and Programming,
pp. 39–51. Springer, 2014.

Backurs, Arturs, Dikkala, Nishanth, and Tzamos, Christos.
Tight Hardness Results for Maximum Weight Rectangles. In International Colloquium on Automata, Languages, and Programming, 2016.

Abboud, Amir, Backurs, Arturs, and Williams, Virginia Vassilevska. If the Current Clique Algorithms are
Optimal, so is Valiant’s Parser. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pp. 98–117. IEEE, 2015a.

Bengio, Samy. An asynchronous hidden markov model for
audio-visual speech recognition. Advances in Neural Information Processing Systems, pp. 1237–1244, 2003.

Abboud, Amir, Grandoni, Fabrizio, and Williams, Virginia Vassilevska. Subcubic equivalences between graph
centrality problems, APSP and diameter. In Proceedings
of the Twenty-Sixth Annual ACM-SIAM Symposium on
Discrete Algorithms, pp. 1681–1697. SIAM, 2015b.
Abboud, Amir, Williams, Virginia Vassilevska, and Yu,
Huacheng. Matching triangles and basing hardness on
an extremely popular conjecture. In Proceedings of the
Forty-Seventh Annual ACM on Symposium on Theory of
Computing, pp. 41–50. ACM, 2015c.
Abdel-Hamid, Ossama, Mohamed, Abdel-rahman, Jiang,
Hui, and Penn, Gerald. Applying convolutional neural
networks concepts to hybrid nn-hmm model for speech
recognition. In Acoustics, Speech and Signal Processing
(ICASSP), 2012 IEEE International Conference on, pp.
4277–4280. IEEE, 2012.
Alon, Noga and Boppana, Ravi B. The monotone circuit
complexity of boolean functions. Combinatorica, 7(1):
1–22, 1987.
Alon, Noga, Krivelevich, Michael, and Sudakov, Benny.
Finding a large hidden clique in a random graph. Random Struct. Algorithms, 13(3-4):457–466, 1998.
Alon, Noga, Andoni, Alexandr, Kaufman, Tali, Matulef,
Kevin, Rubinfeld, Ronitt, and Xie, Ning. Testing k-wise
and almost k-wise independence. In Proceedings of the
39th Annual ACM Symposium on Theory of Computing,
San Diego, California, USA, June 11-13, 2007, pp. 496–
505, 2007.
Altun, Yasemin, Tsochantaridis, Ioannis, Hofmann,
Thomas, et al. Hidden markov support vector machines.
In ICML, volume 3, pp. 3–10, 2003.

Bourlard, Herve A and Morgan, Nelson. Connectionist
speech recognition: a hybrid approach, volume 247.
Springer Science & Business Media, 2012.
Bringmann, Karl, Grønlund, Allan, and Larsen,
Kasper Green. A dichotomy for regular expression
membership testing. arXiv preprint arXiv:1611.00918,
2016.
Cairo, Massimo, Farina, Gabriele, and Rizzi, Romeo. Decoding Hidden Markov Models Faster Than Viterbi Via
Online Matrix-Vector (max,+)-Multiplication. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Churbanov, Alexander and Winters-Hilt, Stephen. Implementing EM and Viterbi algorithms for Hidden Markov
Model in linear memory. BMC bioinformatics, 9(1):1,
2008.
Cohen, Ira, Garg, Ashutosh, Huang, Thomas S, et al. Emotion recognition from facial expressions using multilevel
hmm. In Neural information processing systems, volume 2. Citeseer, 2000.
Collins, Michael. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pp. 1–8. Association for Computational Linguistics, 2002.
Collobert, Ronan. Deep learning for efficient discriminative parsing. In AISTATS, volume 15, pp. 224–232, 2011.
Esposito, Roberto and Radicioni, Daniele P. Carpediem:
Optimizing the viterbi algorithm and applications to supervised sequential learning. Journal of Machine Learning Research, 10(Aug):1851–1880, 2009.
Felzenszwalb, Pedro F, Huttenlocher, Daniel P, and Kleinberg, Jon M. Fast algorithms for large-state-space

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

HMMs with applications to web usage analysis. Advances in NIPS, 16:409–416, 2004.
Grice, J Alicia, Hughey, Richard, and Speck, Don. Reduced space sequence alignment. Computer applications
in the biosciences: CABIOS, 13(1):45–53, 1997.
Håstad, Johan. Clique is hard to approximate within n1−ε .
Acta Mathematica, 182(1):105–142, 1999.
Hazan, Elad and Krauthgamer, Robert. How hard is it to
approximate the best nash equilibrium? SIAM J. Comput., 40(1):79–91, 2011.
Huang, Xuedong, Acero, Alex, Hon, Hsiao-Wuen, and
Foreword By-Reddy, Raj. Spoken language processing:
A guide to theory, algorithm, and system development.
Prentice Hall PTR, 2001.
Jerrum, Mark. Large cliques elude the metropolis process.
Random Struct. Algorithms, 3(4):347–360, 1992.
Kaji, Nobuhiro, Fujiwara, Yasuhiro, Yoshinaga, Naoki, and
Kitsuregawa, Masaru. Efficient staggered decoding for
sequence labeling. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pp. 485–494. Association for Computational Linguistics, 2010.
Karp, Richard M. Reducibility among combinatorial problems. In Complexity of computer computations, pp. 85–
103. Springer, 1972.
Kim, Seyoung and Smyth, Padhraic. Segmental hidden
markov models with random effects for waveform modeling. Journal of Machine Learning Research, 7(Jun):
945–969, 2006.
Larsen, Kasper Green and Williams, Ryan. Faster online matrix-vector multiplication. In Proceedings of the
Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 2182–2189. SIAM, 2017.
LeCun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Lifshits, Yury, Mozes, Shay, Weimann, Oren, and ZivUkelson, Michal. Speeding up HMM decoding and
training by exploiting sequence repetitions. Algorithmica, 54(3):379–399, 2009.
Lin, Kuang, Simossis, Victor A, Taylor, Willam R, and
Heringa, Jaap. A simple and fast secondary structure
prediction method using hidden neural networks. Bioinformatics, 21(2):152–159, 2005.
Mahmud, Md Pavel and Schliep, Alexander. Speeding up
Bayesian HMM by the four Russians method. In International Workshop on Algorithms in Bioinformatics, pp.
188–200. Springer, 2011.
Mannini, Andrea and Sabatini, Angelo Maria. Machine
learning methods for classifying human physical activity from on-body accelerometers. Sensors, 10(2):1154–
1175, 2010.
Mohamed, Abdel-rahman, Dahl, George E, and Hinton,
Geoffrey. Acoustic modeling using deep belief networks. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):14–22, 2012.
Nefian, Ara V, Liang, Luhong, Pi, Xiaobo, Xiaoxiang, Liu,
Mao, Crusoe, and Murphy, Kevin. A coupled hmm for
audio-visual speech recognition. In Acoustics, Speech,
and Signal Processing (ICASSP), 2002 IEEE International Conference on, volume 2, pp. II–2013. IEEE,
2002.
Peng, Jian, Bo, Liefeng, and Xu, Jinbo. Conditional neural fields. In Advances in neural information processing
systems, pp. 1419–1427, 2009.
Rabiner, Lawrence R. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989.
Roditty, Liam and Zwick, Uri. On dynamic shortest
paths problems. In Algorithms–ESA 2004, pp. 580–591.
Springer, 2004.
Siddiqi, Sajid M and Moore, Andrew W. Fast inference and
learning in large-state-space hmms. In Proceedings of
the 22nd international conference on Machine learning,
pp. 800–807. ACM, 2005.

Lee, Peng, Dong, Ming, Liang, Weiqian, and Liu, Runsheng. Design of Speech Recognition Co-Processor for
the Embedded Implementation. In Electron Devices and
Solid-State Circuits, 2007. EDSSC 2007. IEEE Conference on, pp. 1163–1166. IEEE, 2007.

Tarnas, Christopher and Hughey, Richard. Reduced space
hidden Markov model training. Bioinformatics, 14(5):
401–406, 1998.

Li, Peng and Tang, Hua. Design a co-processor for Output Probability Calculation in speech recognition. In
Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium on, pp. 369–372. IEEE, 2009.

Viterbi, Andrew J. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. Information Theory, IEEE Transactions on, 13(2):260–
269, 1967.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms

Williams, Ryan. Faster all-pairs shortest paths via circuit complexity. In Proceedings of the 46th Annual
ACM Symposium on Theory of Computing, pp. 664–673.
ACM, 2014.
Williams, Virginia Vassilevska and Williams, Ryan. Subcubic equivalences between path, matrix and triangle
problems. In Foundations of Computer Science (FOCS),
2010 51st Annual IEEE Symposium on, pp. 645–654.
IEEE, 2010.
Young, Steve, Evermann, Gunnar, Gales, Mark, Hain,
Thomas, Kershaw, Dan, Liu, Xunying, Moore, Gareth,
Odell, Julian, Ollason, Dave, Povey, Dan, et al. The HTK
book, volume 2. Entropic Cambridge Research Laboratory Cambridge, 1997.

