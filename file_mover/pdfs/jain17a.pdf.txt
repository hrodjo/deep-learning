Scalable Generative Models for Multi-label Learning with Missing Labels

Vikas Jain 1 * Nirbhay Modhe 1 * Piyush Rai 1

Abstract
We present a scalable, generative framework for
multi-label learning with missing labels. Our
framework consists of a latent factor model for
the binary label matrix, which is coupled with
an exposure model to account for label missingness (i.e., whether a zero in the label matrix is
indeed a zero or denotes a missing observation).
The underlying latent factor model also assumes
that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted leastsquare regression problems, each of which can be
solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for
large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on
several benchmark data sets, comparing it with a
number of state-of-the-art methods.

1. Introduction
Multi-label learning (Gibaja & Ventura, 2015; 2014) is the
problem of assigning to an object a subset of labels from a
potentially very large label vocabulary (Prabhu & Varma,
2014; Jain et al., 2016; Babbar & SchoÃàlkopf, 2017). In
contrast to binary or multi-class classification, in multilabel learning, each example is associated with a binary
label vector (potentially very large), denoting the presence/absence (relevance/irrelevance) of each label. Multilabel learning has applications in several domains such as
computer vision (Wang et al., 2016), computational adver*

Equal contribution 1 Department of Computer Science and
Enginerring, IIT Kanpur, Kanpur 208016, UP, India. Correspondence to: Vikas Jain <vikasj@iitk.ac.in>, Nirbhay Modhe
<nirbhaym@iitk.ac.in>, Piyush Rai <piyush@cse.iitk.ac.in>
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tising and recommender systems (Prabhu & Varma, 2014;
Jain et al., 2016), etc.
Several state-of-the-art methods for multi-label learning are
based on certain structural assumptions on the binary label matrix. Some of the key structural assumptions that
have been used in prior work include low-rank assumption (Yu et al., 2014), locally low-rank assumption (Bhatia et al., 2015), and low-rank plus sparse assumption (Xu
et al., 2016), and clusters/topics of labels assumption (CisseÃÅ
et al., 2016; Rai et al., 2015). Models based on these assumptions are broadly dubbed as embedding based methods for multi-label learning and offer two key advantages:
(1) The relatedness/correlation among labels can be easily
modeled/captured, and (2) the label vector for each example can be represented as a low-dimensional embedding,
which faciliates developing computationally scalable models for multi-label learning. A more detailed discussion of
prior work is provided in the Related Work section.
Despite the considerable recent interest and progress on
the problem of multi-label learning (Yu et al., 2014; Bhatia
et al., 2015; Wang et al., 2016; CisseÃÅ et al., 2016), a number
of important issues still remain. One of such issues, especially for the embdding based methods, is the ambiguity
regarding the zeros vs unobserved (missing) entries in the
binary label vector of each example. Since, in practice, the
true value (0/1) for only a small subset of all the labels can
be obtained, the zeros in the label vector do not necessarily
represent negative labels. A typical heuristic employed by
multi-label learning algorithms is to simply treat all such
the zeros in the label vector as are true negatives (Yu et al.,
2014). Another heuristic is to assign different weights to
the zeros and ones in the binary label matrix (Yu et al.,
2017), which is inspired by matrix factorization based collaborative filtering models that learn from implicit (binary)
feedback data (Hu et al., 2008). However, a more principled strategy to address this issue is highly desirable.
Another important desideratum is scalability, especially in
the case of extreme multi-label learning problems (Prabhu
& Varma, 2014; Jain et al., 2016; Babbar & SchoÃàlkopf,
2017), which are characterized by a massive number of labels, features, and examples. Although a number of recent
multi-label learning models have been proposed that can
scale to large-scale problems, these models usually require

Scalable Generative Models for Multi-label Learning with Missing Labels

large computational resources to truly scale to massive data
sets (Babbar & SchoÃàlkopf, 2017; Bhatia et al., 2015; Jain
et al., 2016). Moreover, most of the scalable multi-label
learning algorithms only operate in batch setting and are
usually not designed to work (Prabhu & Varma, 2014; Bhatia et al., 2015; Jain et al., 2016) in online settings with
continuous stream of training examples.
In this paper, we present a scalable, generative framework
for multi-label learning, that not only bring to bear the
modeling flexibility of probabilistic, generative models for
the multi-label learning problem (Kapoor et al., 2012; Rai
et al., 2015), but is also designed to handle the abovementioned challenges in a principled way. Our framework
is based on a latent factor model for the binary label matrix,
and has the following distinguishing aspects: (1) It naturally handles the issue of missing vs negative labels via a
principled generative model with a exposure model (Liang
et al., 2016) for the label matrix; (2) It is accompanied by
a simple and scalable inference procedure (both via Gibbs
sampling and via fast point estimation); and (3) Inference
can also be easily performed in an online fashion, enabling
us to apply it on large-scale problems, even when using
moderate computational resources.

2. The Model
In the multi-label learning problem, we assume that we are
given N training examples {(x1 , y 1 ), . . . , (xN , y N )} with
xn ‚àà RD and y n ‚àà {0, 1}L , n = 1, . . . , N . We will denote X = {x1 , . . . , xN } ‚àà RN √óD to be the feature matrix
and Y = {y 1 , . . . , y N } ‚àà {0, 1}N √óL to be the label matrix. Given training data {X, Y}, the goal in multi-label
learning is to learn a model that can predict the label vector
y ‚àó ‚àà {0, 1}L for a new test input x‚àó ‚àà RD .
Note that an entry yn` = 0 in the label matrix Y may not
necessarily mean a negative label but could simply mean
that this label is missing (and its true value could be 0 or 1).
As we shall show, our generative model can infer the missingness of a label yn` = 0 by associating another binary
latent variable Œæn` (called exposure variable). These exposure variables will be incorporated in a latent factor model
(Sec. 2.1) for the label matrix Y and are jointly learned
along with the rest of the model parameters.
2.1. An Exposure-based Latent Factor Model for the
Binary Label Matrix
We model the binary label matrix Y using a latent factor model. Specifically, we assume that each training example n = 1, . . . , N is associated with a latent factor
un ‚àà RK and each label ` = 1, . . . , L is associated
with a latent factor v ` ‚àà RK . We further condition un
on the feature vector xn ‚àà RD of example n by as-

suming that the prior distribution of un is conditioned on
xn , as p(un |xn ) = N (un |Wxn , Œª‚àí1
u IK ). Here, W =
[w1 , . . . , wK ]> ‚àà RK√óD which denotes the matrix of regression weights that map the feature vector xn to the mean
of the Gaussian prior on un . We further assume a zeromean Gaussian prior p(v ` ) = N (v ` |0, Œª‚àí1
v IK ) on label
latent factors v ` , ` = 1, . . . , L. Note that, although we do
not consider it here, our model can also be easily extended
to incorporate label features (if available) by conditioning
Gaussian prior on v ` on those label features, in the same
manner we condition the prior on un on input features.

Figure 1. Our generative model in plate notation. Note: Hyperparameters not shown for brevity.

The complete generative story for each label yn` of the binary label matrix Y is given by
un |xn
v`
Œæn`
yn`

‚àº

N (un |Wxn , Œª‚àí1
u IK )

(1)

‚àº

N (v ` |0, Œª‚àí1
v IK )

(2)

‚àº Bernoulli(¬µn` )
(3)
(

>
Bernoulli yn` |œÉ(un v ` ) , if Œæn` = 1
‚àº
(4)
Œ¥0 , if Œæn` = 0

where œÉ(z) = 1/(1 + exp(‚àíz)) denotes the logistic function. Note that we have associated a binary exposure latent variable Œæn` with each label yn` such that Œæn` = 0
implies that yn` is 0 because it is missing (not exposed),
and Œæn` = 1 implies that yn` is exposed (and could be 0
or 1 depending on the outcome of the Bernoulli draw). In
Eq. 4, Œ¥0 denotes a point-mass at zero, which means that, if
Œæn` = 0, then yn` is zero with probability 1. Otherwise, we
draw the observed label yn` from a Bernoulli distribution
as yn` ‚àº Bernoulli(yn` |œÉ(u>
n v ` )). Note that, effectively,
each yn` is being modeled using a mixture of two distributions - a Bernoulli with probability given by the sigmoid
œÉ(u>
n v ` )) and a point-mass at 0.

yn` ‚àº Œæn` Bern yn` |œÉ(u>
n v ` ) + (1 ‚àí Œæn` )I[yn` = 0] (5)
Note that the latent variable Œæn` decides which of the two
distributions from this mixture generates yn` . Figure 1
shows our model in the plate notation. Also note that if
yn` = 1 then Œæn` = 1 with probability 1 and therefore Œæn`
only needs to be inferred for entries for which yn` = 0.

Scalable Generative Models for Multi-label Learning with Missing Labels

The generative model specified in Eq (1)-(4) has two additional parameters: W = [w1 , . . . , wK ]> ‚àà RK√óD which
denotes the matrix of regression weights that map each
input feature vector xn to the corresponding latent factor
un ‚àà RK , and a probability parameter ¬µn` ‚àà (0, 1) which
denotes the probability of the label yn` being exposed (but
note that yn` can be 0 or 1, depending on the outcome of
Bernoulli(yn` |œÉ(u>
n v ` ))). We refer to ¬µn` as the exposure
probability of label ` for example n.

from Eq. 4 as a Gaussian when conditioned on œân` ‚àº
>
PG(1, u>
n v ` ). In particular, œàn` = un v ` , conditioned on

œân` , becomes a Gaussian 
1
2
p(œàn` |œân` ) ‚àù exp Œ∫n` œàn` ‚àí œân` œàn`
(6)
2
where Œ∫n` = yn` ‚àí 0.5. This likelihood with the Gaussian
priors on the latent factors un and v ` results in Gaussian
posteriors on un and v ` . When doing EM, this also leads to
subproblems that are like least square regression problems.

We assume each regression weight vector wk to have a
Gaussian prior, i.e., wk ‚àº N (wk |0, Œª‚àí1
w ID ). Note that
the spherical covariance of this prior can also be replaced
by a more flexible diagonal covariance, which will give the
model ability to perform feature selection.

3.1. Gibbs Sampling

For the exposure probability ¬µn` , we consider two types of
priors. In the first case, we simply assume ¬µn` = ¬µ` , ‚àÄn,
which means that the probability that a label ` is observed
is the same for all the examples (i.e., the label exposure for
the label ` is global, not example specific). In this case,
we assume a Beta prior on ¬µ` , i.e., ¬µ` ‚àº Beta(Œ±1 , Œ±2 ).
In the second case, we assume access to some contexual
information (often available in applications such as recommender systems) that we may have for each example-label
pair (n, `), in form of some given covariates œÜn` ‚àà RM .
Given these covariates, we model the label exposure probability as ¬µn` = œÉ(Œ≤ > œÜn` ), where Œ≤ ‚àà RM is a vector of
regression coefficients. We assume a Gaussian prior on Œ≤,
i.e., Œ≤ ‚àº N (Œ≤|0, Œª‚àí1
Œ≤ IM )

3. Inference
Although the generative model specified in Eq. 1-4 is not
readily conjugate because the logistic-Bernoulli likelihood
is not conjugate to the Gaussian prior on the latent factors, we can leverage data-augmentation techniques (Polson et al., 2013) to make the model locally conjugate. This
enables us to develop a simple Gibbs sampling algorithm
for doing inference in our model. The conjugacy also allows us to design an online expectation maximization (EM)
algorithm (CappeÃÅ & Moulines, 2009), which enables us to
apply our model on large-scale problems.
We handle the non-conjugate logistic-Bernoulli likelihood
using the PoÃÅlya-gamma augmentation technique (Polson
et al., 2013), which is based on the following identity
(exp(œà)a
= 2‚àíb exp (Œ∫œà)
(1 + exp(œà))b

‚àû

Z


exp ‚àíœâœà 2 /2 p(œâ)dœâ
0

where Œ∫ = a ‚àí b/2 and p(œâ) = PG(b, 0) denotes
the PoÃÅlya-gamma distribution (Polson et al., 2013). This
identity allows us to write any likelihood of the form
(exp(œà)a
(e.g., Bernoulli, binomial, negative-binomial)
(1+exp(œà))b
as a Gaussian distribution, when conditioned on a PG random variable œâ|œà ‚àº PG(b, œà). Specifically, using PG augmentation, we can write the logistic-Bernoulli likelihood

Using the PG augmentation, we can derive the posterior
distributions of all the latent variables in our model, and
perform Gibbs sampling for doing inference in our model.
Due to conjugacy, the inference updates are straightforward
to derive as are summarized below.
Sampling Œæn` : Note that if yn` = 1 then Œæn` = 1 with
probability one, and therefore need not be inferred. For
yn` = 0, we sample Œæn` from the posterior
p(Œæn` = 1|.) ‚àù ¬µn` œÉ(‚àíu>
n v` )

(7)

p(Œæn` = 0|.) ‚àù (1 ‚àí ¬µn` ) √ó 1

(8)

Sampling ¬µn` : For the case when ¬µn` = ¬µ` , ‚àÄn, with
Beta(Œ±1 , Œ±2 ) prior on each ¬µ` , the posterior will be
N
N
X
X
p(¬µn` |.) = Beta(Œ±1 +
Œæn` , Œ±2 + N ‚àí
Œæn` ) (9)
n=1

n=1

Note that, if we parameterize each ¬µn` as ¬µn` = œÉ(Œ≤ > œÜn` )
where œÜn` is the interaction feature vector for the examplelabel pair, and the regresssion weight Œ≤ is assumed to have
a Gaussian prior, the model is not conjugate. However,
using the PG augmentation allows us to easily derive a
closed-form Gaussian posterior for Œ≤.
Sampling un : Given the PG variables ‚Ñ¶n,: = {œân` }L
`=1
and the other latent variables, the posterior of un will be
un ‚àº N (un |¬µun , Œ£un ) where the covariance is given by
PL
Œ£un = ( `=1 Œæn` œân` v ` v >
+ Œªu IK )‚àí1 and the mean is
PL`
given by ¬µun = Œ£un ( `=1 Œæn` Œ∫n` v ` + Œªu Wxn ). Note
that if a label ` is inferred as not exposed for example n,
i.e., Œæn` = 0, it does not contribute to the update of un .
Sampling v ` : Given ‚Ñ¶:,` = {œân` }N
n=1 and the other latent
variables, the posterior v ` will be v ` ‚àº N (v ` |¬µv` , Œ£v` )
PN
‚àí1
where covariance Œ£v` = ( n=1 Œæn` œân` un u>
n + Œª v IK )
PN
and the mean ¬µv` = Œ£v` ( n=1 Œæn` Œ∫n` un ). Note that
if an example n is inferred as not exposed to label `, i.e.,
Œæn` = 0, it does not contribute to the update of v ` .
Sampling W: Each row {wk }K
k=1 of the regression
weights matrix W = [w1 , . . . , wK ]> ‚àà RK√óD will have
a Gaussian posterior given by wk ‚àº N (wk |¬µwk , Œ£wk )
where covariance Œ£wk = (X> X + Œªw ID )‚àí1 , the mean
¬µwk = Œ£wk (X> U), and U = [u1 , . . . , uN ] ‚àà RK√óN .

Scalable Generative Models for Multi-label Learning with Missing Labels

3.2. Scalable Inference via EM and Online EM
Although the Gibbs sampler (Sec. 3.1) is easy to derive and
implement in practice, sampling tends to be slow in practice and convergence may be slow. We therefore present
an online expectation maximization algorithm (CappeÃÅ &
Moulines, 2009) for doing efficient inference in our model.
We first show the batch EM updates for our model parameters and then describe the online EM algorithm which can
process the training data in small mini-batches of examples,
and results in faster convergence in practice.

associated with a weight pn` = E[Œæn` |œàn` ]. Intuitively, in
the first term, the contribution of each label yn` to the loglikelihood gets modulated based on its expected exposure.
Maximizing Q(U, V, W, ¬µ) w.r.t. each of the model parameters U, V, W, ¬µ, fixing the rest, yields closed-form
updates for each of these. The updates are as follows:
‚Ä¢ Estimating each of the latent factors {un }N
n=1 is a
weighted ridge-regression problem with solution
!
L
X
un = Œ£un
pn` Œ∫n` v ` + Œªu Wxn
(13)
`=1

3.2.1. T HE EM A LGORITHM
The EM algorithm for our model alternates between computing the expectations of the local latent variables, namely
the PoÃÅlya-gamma variables {œân` } and the binary exposure
latent variables {Œæn` } in the E step, and then using these expectations to estimate the other model parameters un , v ` ,
W, and exposure probabilities {¬µn` } in the M step.
The E Step: The E step involves computing the expectations of the latent variables {œân` } and {Œæn` }, given the current values of the other model parameters un , v ` , W, and
¬µn` estimated in the previous M step. The E step update
equations are given below:
‚Ä¢ Expectations of PoÃÅlya-gamma variables {œân` }, ‚àÄn, `
are known to be available in closed form (Scott & Sun,
2013), and are given by


œàn`
1
(10)
tanh
Œ∑n` = E[œân` |œàn` ] =
2œàn`
2
where œàn` = u>
n v ` is computed using the estimates
of un and v m from the previous M step.
‚Ä¢ Expectations of each of the binary exposure variables
Œæn` , ‚àÄn, `, are given by
pn` = E[Œæn` |œàn` ] =

¬µn` œÉ(‚àíœàn` )
¬µn` œÉ(‚àíœàn` ) + (1 ‚àí ¬µn` )

(11)

The M Step: Given the expectations of the latent variables computed in the E step, the M step maximizes the
following expected complete data log-likelihood plus logprior terms, which we denote as Q(U, V, W, ¬µ), where
L
U = {un }N
n=1 , V = {v ` }`=1 , W, and ¬µ = {¬µn` }, ‚àÄn, `
Q(U, V, W, ¬µ) = ‚àí

+

X

2
1X
(Œ∫n` ‚àí Œ∑n` u>
n v` )
pn`
2 n,`
Œ∑n`

log Bernoulli(pn` |¬µn` ) ‚àí Œªu

L
X
`=1

2

||un ‚àí Wxn ||

n=1

n,`

‚àí Œªv

N
X

2

2

||v ` || ‚àí Œªw ||W|| +

X

log Beta(¬µn` |Œ±1 , Œ±2 )

(12)

n,`

Note that the first term in the objective function given in
Eq. 12 is due to the logistic likelihood transformed into a
Gaussian (using PG augmentation). This term is akin to
a weighted least squares objective where each label being

PL
‚àí1
where Œ£un = ( `=1 pn` Œ∑n` v ` v >
. Note
` + Œª u IK )
N
that the updates for {un }n=1 are all independent of
each other and are easily parallelizable.
‚Ä¢ Estimating each of the label latent factors {v ` }L
`=1 is
a weighted ridge-regression problem with solution
!
N
X
pn` Œ∫n` un
v ` = Œ£v`
(14)
n=1
P
‚àí1
N
>
where Œ£v` =
.
n=1 pn` Œ∑n` un un + Œªv IK
Again, note that the updates for {v n }L
`=1 are all independent of each other and are easily parallelizable.
‚Ä¢ Estimating the regression weight matrix W is equivalent to solving a vector-valued linear regression problem un ‚âà Wxn , ‚àÄn, with the following updates
W> = (X> X + Œªw ID )‚àí1 (X> U)

(15)

Note that solving Eq. (15) exactly requires inverting a
D √ó D matrix which will be expensive for large D.
However, the EM algorithm does not require solving
for W exactly in each M step. We therefore solve
for W efficient using gradient based methods, such
as conjugate-gradient (CG) method (Bertsekas, 1999),
which allows us to also leverage the sparsity in the
feature matrix X. Typically, a small number of CG
iterations are sufficient in practice.
‚Ä¢ Given pn` from the E step, the updates for ¬µn` for the
case when ¬µn` = ¬µ` , ‚àÄn, is simply the MAP solution
PN
Œ±1 + n=1 pn` ‚àí 1
(16)
¬µ` =
Œ±1 + Œ±2 + N ‚àí 2
For the other case when each ¬µn` in modeled as ¬µn` =
œÉ(Œ≤ > œÜn` ) with a Gaussian prior on Œ≤, estimating Œ≤
reduces to solving a regression problem with the training data being {œÜn` , pn` }, ‚àÄn, `, where œÜn` is the
given feature vector for the input-label pair n, ` and
pn` is estimated in the E step. Ignoring the prior term
(equivalent to `2 regularizer on Œ≤), we can estimate Œ≤
iteratively using gradient-descent updates
œÑ X
Œ≤=Œ≤‚àí
(œÉ(Œ≤ > œÜn` ) ‚àí pn` )œÜn`
(17)
NL
n,`

where œÑ denotes the learning rate.

Scalable Generative Models for Multi-label Learning with Missing Labels

3.2.2. O NLINE EM

4. Related Work

The EM algorithm described in Section 3.2.1 is more efficient than the Gibbs sampler described in Section 3.1. It is
also highly parallelizable since the updates for {u}N
n=1 and
{v ` }L
can
be
easily
parallelized,
and
solve
for
W effi`=1
ciently using CG updates. However, it is a batch procedure
and requires going over the entire training data in every iteration. For large-scale multi-label learning problems, which
are characterized by large N , D, and L, the batch setting
may not be feasible in practice, especially when having access to moderate computational resources and storage.

A prominent line of work on multi-label learning has been
based on models that learn a low-dimensional embedding
of the label vectors (Chen & Lin, 2012; Yu et al., 2014; Rai
et al., 2015; Bhatia et al., 2015). Note that this amounts to
assuming that the label matrix is low-rank.

We therefore present an efficient online version of the EM
algorithm for our model which allows it to scale up to
massive-sized data sets even on machines with moderate
hardware. As we show in our experiments, this enables us
to apply our model to be run efficiently on massive data
sets (e.g., one of the data sets we experiment with has more
than 600k examples with about 50k features per example)
even on a standard laptop with very moderate hardware.
The online EM algorithm works by maintaining sufficient
statistics of all the model parameters and updates these
sufficient statistics with every mini-batch of data. For
each mini-batch of training examples, the E step computes
the relevant expectations associated with these observations and then uses the expectations to update the sufficient
statistics of the parameters to be estimated in the M step.
For example, noting that the sufficient statistics for updat‚àí1
ing
b are given by A =
PNthe label latent>factors v ` = A P
N
n=1 pn` Œ∑n` un un +Œªv IK and b =
n=1 pn` Œ∫n` un , we
can update A and b using a small mini-batch containing Nb
b
examples as {(xn , y n )}N
n=1 as follows
A(t+1) = (1 ‚àí Œ≥t )A(t) + Œ≥t A(new)
(t+1)

(t)

(18)

(new)

= (1 ‚àí Œ≥t )b + Œ≥t b
(19)
P
Nb
where A(new) = ( n=1
pn` Œ∑n` un u>
n + Œªv IK ), and
PNb
(new)
b
= n=1 pn` Œ∫n` un are computing using only the
current mini-batch. The sufficient statistics of the other
model parameters can also be updated in the same manner. Here Œ≥t is a decaying learning rate (or a forgetting
factor), which also acts as a trade-off between the contribution from the old sufficient statistics computed thus far
and the sufficient statistics contribution from the new minibatch of data. We set Œ≥t = (a0 + t)‚àí with a0 = 1 and  to
be close to 0.5 (CappeÃÅ & Moulines, 2009).
3.2.3. P REDICTION
b

Given a new test input x‚àó , we first predict its latent factor
u‚àó ‚àà RK as Wx‚àó and then predict each entry of its label
vector y ‚àó as E[y‚àó` |u‚àó , v ` ] = œÉ(u>
‚àó v ` ). If we are only
interested in the top few labels, fast search methods such as
maximum inner product search (Fraccaro et al., 2016) can
be used to reduce the computational cost at test time.

Since many real-world data sets have a large number of
rare labels, sometimes the low-rank assumption may not
be appropriate. To address this issue, (Bhatia et al., 2015)
proposed a method which assumes the label matrix to be
locally low-rank. One way to impose this assumption is to
learn embeddings that only try to preserve distances in a
small neighborhood of each example. Another approach to
handle the rare labels is to assume that the label matrix is a
sum of a low-rank and a sparse matrix (Xu et al., 2016).
Note that our latent factor model is equivalent to imposing
a low-rank assumption on the label matrix, and is therefore similar in spirit to the label-embedding approaches.
However, unlike the existing label-embedding based approaches, our generative framework has a principled mechanism to handle/infer the unobserved labels. Moreover,
none of the existing label-embedding methods can work
in online fashion, and scaling up these methods to largescale problems requires large computational resources. In
addition, our model readily allows incorporating the label
features (if available) by a simple modification to the prior
on the label latent factors.
Apart from the label-embedding based multi-label learning methods, tree-based methods for multi-label learning (Agrawal et al., 2013; Prabhu & Varma, 2014; Jain
et al., 2016) are also popular due to being fast at test time,
especially when the number of labels is large. However,
these models usually have high training costs and cannot
be trained easily in an online fashion, unlike our model.
On the other hand, for faster predictions at test time, our
framework model can easily be adapting by replacing the
Gaussian prior on the label latent factors v ` by a von MisesFisher prior (Fraccaro et al., 2016), which naturally facilitates using maximum inner-product search techniques,
without the requirement of any post-processing.
Among other models to address the missing labels problem in multi-label learning, recently, (Kanehira & Harada,
2016) proposed a ranking based framework for learning
from positive and unlabeled data in the context of multilabel learning. Although this is similar in spirit to our
model in terms of not treating the unobserved labels as zeros, the approach in (Kanehira & Harada, 2016) is fundamentally different than ours. Moreover, their setting is not
amenable to online learning, nor does it leverage the lowrank structure of label matrices with a huge number of labels. Other approaches that try to handle missing labels in-

Scalable Generative Models for Multi-label Learning with Missing Labels

clude (Bucak et al., 2011) which uses group LASSO adaptation of a multi-label ranking objective, and (Kong et al.,
2014), which learns a model using a positive and unlabeled
(PU) stochastic gradient descent procedure. However, it
works in batch setting, uses stacking to leverage label correlations, and does not scale to large number of labels.
One-class matrix factorization (OCMF) is also an approach (Yu et al., 2017) to solve the missing labels problem by assigning different (but fixed) weights to the ones
and zeros. In contrast to this method, our generative framework can learn the weight for each label by modeling these
weights as latent variables. In another recent work, (Liang
et al., 2016) proposed an exposure model for recommender
system problems posed as matrix factorization of implicit
feedback data. Their approach of modeling the exposure
similar in spirit to our framework.
Some of the early works on generative models for multilabel learning problems include models specifically designed for image annotation problems (Barnard et al., 2003;
Feng et al., 2004). Other recent attempts on doing multilabel learning in more general problem settings include
models such as Bayesian compressive sensing (Kapoor
et al., 2012) and multi-label learning using Bayesian nonnegative matrix factorization (Rai et al., 2015). However,
these models do not have a mechanism to distinguish between unobserved and negative labels, have complicated
inference, and do not scale to large-scale problems.
Our generative framework is also amenable for various interesting extensions. For example, it can be be extended
to a mixture of latent factor models, which can handle the
situation when the label matrix is not low-rank but a mixture of several low-rank matrices. Note that such an extension would be a fully generative counter-part of the model
in (Bhatia et al., 2015) which learns a locally low-rank
model but has to rely on an ad-hoc clustering step beforehand, which is known to be unstable in practice (Bhatia
et al., 2015). Another nice aspect of our framework is that
is naturally allows active learning (Kapoor et al., 2012; Vasisht et al., 2014) where we can selectively ask for most
informative labels for an unannotated example. Moreover,
our framework is flexible and inference in our model can
be performed in a fully Bayesian manner (e.g., MCMC or
variational inference) as well as fast point estimation methods such as (online) EM, that we used in this work.
To summarize, our generative framework offers a flexible way to model the label generation mechanism for
real-world multi-label data sets, which most of the existing models currently lack. We can model label missingness/observability rigorously under our framework and infer the model parameters easily using a simple inference
procedure. Moreover, the simplicity of the inference procedure makes it easy to design scalable inference algorithms,

such as online EM for our model, which enables updating
the model whenever fresh training data is available. This is
in contrast to some of the other state-of-the-art multi-label
learning methods, which although scalable (Bhatia et al.,
2015; Prabhu & Varma, 2014; Jain et al., 2016), are not
suitable to be applied in such online settings.

5. Experiments
We evaluate our framework on a number of benchmark
data sets and compare it with several state-of-the-art methods. Our baselines include both label-embedding methods
as well as tree-based methods. The statistics of data sets
we use in our experiments are summarized in Table 1.
Dataset
Bibtex
Mediamill
Eurlex-4K
Movielens
RCV
Wikipedia

N
4880
30993
15539
4000
623847
14146

Ntest
2515
12914
3809
2040
155962
6616

D
1836
120
5000
29
47236
101938

L
159
101
3993
3952
2456
30938

Table 1. Dataset used for the experiments with their properties.
D: number of features, L: number of labels, N : number of training examples, Ntest : number of test examples

We report both quantitative results (in terms of label prediction accuracies) as well as some qualitative results, namely
looking at the relationship of empirical label frequencies
and label exposure. Note that the label frequency for a
given label denotes how many examples had this label as
1, while label exposure ¬µ` ‚àà (0, 1) in general refers to
how popular the label ` is.
In our experiments, we compare with the following stateof-the-art baselines.
‚Ä¢ LEML: This is a low-rank embedding based multilabel learning model (Yu et al., 2014). LEML assumes the label matrix Y to be modeled as Y ‚âà UV
where U = XW. LEML considers various types of
loss functions such as squared loss, logistic loss, hinge
loss, etc. Interestingly, note that LEML with logistic
loss can be seen as a special non-probabilistic case of
our model when also considering Œªu ‚Üí ‚àû, and the
label exposure model turned off.
‚Ä¢ BCS: Bayesian Compressive Sensing (BCS) is a generative model (Kapoor et al., 2012) for the label vector. It assumes a compressive sensing model for the
label vectors and is essentially a low-rank model.
‚Ä¢ FastXML: This is a fast tree-based multi-label learning model which uses an ensemble of trees (Prabhu &
Varma, 2014).
‚Ä¢ PfasterXML: This is an extension of FastXML and
uses propensity-weighted scores to improve performance on rare labels (Jain et al., 2016).

Scalable Generative Models for Multi-label Learning with Missing Labels

‚Ä¢ PD-Sparse: This model takes a different approach
as compared to label-embedding methods and uses a
margin-maximizing loss for the multi-label learning
problem (Yen et al., 2016).
For the baselines, the reported results are either obtained
using publicly available implementations (with the recommended hyperparameter settings), or the publicly known
best results. We refer to our model as GenEML (for Generative Exposure-based model for Multi-label Learning)
Hyperparameter Settings: For our model, we set the hyperparameters Œªu and Œªv to 0.001, which works well on
all the data sets we experimented with. We select the other
two hyperparameters Œªw and K (number of latent factors)
using cross-validation. On small-/medium-scale data, both
EM and online EM perform comparably and we only report
the results using online EM. On large data sets, we only use
online EM. On the small and medium-scale data, we however also show a separate experiment comparing EM and
online EM for our model in terms of convergence speed
versus accuracy. For the conjugate gradient (CG) method
used by the M step of our inference algorithm, we run 5 iterations, which was found to be sufficient. For online EM,
for each data set, we use mini-batch sizes of 1024 and 4096
and report the one which gives better results.
5.1. Quantitative Results
5.1.1. B ENEFIT OF E XPOSURE M ODEL
In our first experiment, we assess the benefit of using the
exposure model. For this, we apply our model with and
without exposure on a synthetic data set. For this experiment, we generate a synthetic data set with N =500,
D=100, and L=20 and use varying degrees of exposure
probabilities ¬µ` ‚àà {0.01, 0.05, 0.1, 0.3, 0.5, 0.9} for the
different labels ` = 1, . . . , 20. We also create a test set
with 500 test examples.
The results are shown in Table 2. As the results show,
our model with exposure turned on outperforms the model
when the exposure is turned off. This clearly demonstrate
the benefit of the exposure model when a significant fraction of labels are missing (i.e., not exposed). Our model
also outperforms LEML which does not have a mechanism
to model label exposure.
GenEML
P@1
P@2
P@3

87.8
75.2
65.2

GenEML w/o
Exposure
79.2
68.9
59.0

LEML
78.8
69.1
58.9

Table 2. Precision@k values on synthetic data obtained by our
model with and without using the label exposure

5.1.2. P REDICTION ACCURACIES
In our next set of experiments, in Table 3 we compare
our model (with exposure on) with the other baselines,
in terms of Precision@1, Precision@3, and Precision@5
scores. As Table 3 shows, our model outperforms the other
baselines in most of the cases, except for the RCV and
Wikipedia data, on which our model is outperformed by
LEML and/or PfasterXML. Note, however, that these stateof-the-art baselines use batch inference methods whereas
we only ran our model in the online setting on a moderate 4
core processor with 8GB RAM. Moreover, our results may
further improve with a more careful hyperparameter tuning
(including selection of minibatch size). The point of the
large-scale data experiment was to mainly show that the our
model can be feasibly run on such large-scale data sets, on
standard machines with moderate computational resources.
Most of the other existing models for multi-label learning
are infeasible to run under such restrictive settings.
5.1.3. BATCH EM VS O NLINE EM
The online version of our EM algorithm is scalable and
faster than its batch counterpart. Fig 2 shows that online
EM converges faster and to a precision score which is very
similar to the batch EM on Bibtex and Mediamill datasets.
Furthermore, online inference is also more effecient,
storage-wise, due the need of maintaining just the sufficient
statistics as in Eq 19 for the updates of each latent factor
un and v ` . For very large datasets, the size of the the sufficient statistics (a D √ó D covariance matrix) for updating
the regression weight matrix W might not be feasible to
store and update. Therefore, we use cheap, first-order gradient based updates for finding an approximate solution to
the update equation of W in each iteration of the EM algorithm (note that we need not solve for W exactly; the EM
algorithm just requires a few steps of updates for W in the
M step). This further reduces the memory requirement of
our model, while also speeding up inference due to faster
computation of gradients as compared to CG updates.

Figure 2. Convergence time comparison of the batch and online
EM algorithm for inference in our model

5.2. Qualitative Results
Finally, we do some qualitative analyses of our model‚Äôs behavior. We investigate whether the global frequency of
a label necessarily correlates to its exposure probability.

Scalable Generative Models for Multi-label Learning with Missing Labels
Dataset
Bibtex
Mediamill
Eurlex-4K
Movielens
RCV-2K
Wiki10-31K

P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5
P@1
P@3
P@5

BCS
60.24
34.87
24.48
-

WSABIE
54.78
32.29
23.98
81.29
64.74
49.83
68.55
55.11
45.12
-

FastXML
63.42
39.23
28.86
84.22
67.33
53.04
71.36
59.90
50.39
83.03
67.47
57.76

PfasterXML
63.46
39.22
29.14
83.98
67.37
53.02
75.45
62.70
52.51
83.57
68.61
59.10

PD-Sparse
61.29
35.82
25.74
81.86
62.52
45.11
76.43
60.37
49.72
-

LEML
62.54
38.41
28.21
84.01
67.20
52.80
63.40
50.35
41.28
54.22
50.44
48.63
89.09
70.96
50.73
73.47
62.43
54.35

GenEML
64.09
40.14
29.47
87.15
69.98
55.21
77.75
63.98
53.24
55.78
50.62
48.86
87.21
69.22
49.53
76.38
44.49
32.06

Table 3. Performance Comparison using Prec@k of the model with other baselines. The - denotes that either these results were not
available or the method was infeasible to run on that data set. On the large-scale data sets (RCV and Wiki), our model was run using
online EM based inference.

While it may be the case for some data sets where high label frequency implies a high inferred label exposure probability (e.g., see Fig. 3 for Bibtex and Mediamill data), it
need not be the case with other data sets. For example,
for Movielens data, each user-movie (example-label) pair
has an some context information (user and movie features)
available for it. As we show in Fig 4, the inferred exposure probability (which depends on the context features) of
the same movie (label) indeed turns out to be different for
different users (examples).
Fig. 4 shows the plot of inferred exposure probabilities ¬µnl
for two users (one female, one male) plotted against the
label frequencies (movie popularities).

Figure 3. Inferred label exposure probabilities for Bibtex and Mediamill data sets

As Fig. 4 shows, our model infers that, a popular movie
(shown in red dot in Fig 4) has a high exposure probability for the left user (Female, 25, Healthcare/Doctor) while
it has a low exposure probability for the right user (Male,
35, artist). This example illustrates that a high label frequency does not necessarily imply a high exposure probability, which can be context (user in this case) dependent.

6. Conclusion
We presented a flexible and scalable generative framework
for multi-label learning. Our framework is based on a latent

Figure 4. Inferred user-specific label exposure probabilties for
two users on Movielens data set: Female, 25, Healthcare/Doctor
(left) and Male, 35, artist (right), with label frequency for each
label. The red circle shows the most frequent movie (Hairspray(1988) Comedy, Drama) and its exposure probability for
both the users.

factor model for the label matrix and does not assume that
the zeros in the label matrix are necessarily negative labels.
We use a set of label exposure latent variables to model this,
and infer these exposure probabilities from data. Incorporating these latent variables leads to improve multi-label
classification accuracies, and also enables doing interesting
qualitative analyses. Our model admits a simple inference
procedure which can be implemeted using Gibbs sampling
or EM. We further develop a highly scalable online EM
algorithm for performing inference in our model, which allows our model to be applied on large-scale data sets, even
on standard machines with moderate hardware. The generative framework makes it easy to extend our model in many
interesting ways. For example, it can be extended to a mixture of latent factor models, which will allow handling the
cases where a single low-rank model does not adequately
capture the structure of the label matrix.
Acknowledgements: PR acknowledges support from Extreme
Classification research grant from Microsoft Research India,
DST-SERB Early Career Research Award, Dr. Deep Singh and
Daljeet Kaur Fellowship, and Research-I Foundation, IIT Kanpur.

Scalable Generative Models for Multi-label Learning with Missing Labels

References
Agrawal, Rahul, Gupta, Archit, Prabhu, Yashoteja, and Varma,
Manik. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In WWW, 2013.

Liang, Dawen, Charlin, Laurent, McInerney, James, and Blei,
David M. Modeling user exposure in recommendation. In
WWW, 2016.

Babbar, R. and SchoÃàlkopf, B. DiSMEC- distributed sparse machines for extreme multi-label classification. In WSDM, 2017.

Polson, Nicholas G, Scott, James G, and Windle, Jesse. Bayesian
inference for logistic models using poÃÅlya‚Äìgamma latent variables. Journal of the American Statistical Association, 108
(504):1339‚Äì1349, 2013.

Barnard, Kobus, Duygulu, Pinar, Forsyth, David, Freitas,
Nando de, Blei, David M, and Jordan, Michael I. Matching
words and pictures. JMLR, 2003.

Prabhu, Yashoteja and Varma, Manik. FastXML: a fast, accurate
and stable tree-classifier for extreme multi-label learning. In
KDD, 2014.

Bertsekas, Dimitri P. Nonlinear programming. Athena scientific
Belmont, 1999.

Rai, Piyush, Hu, Changwei, Henao, Ricardo, and Carin,
Lawrence. Large-scale bayesian multi-label learning via topicbased label embeddings. In NIPS, 2015.

Bhatia, Kush, Jain, Himanshu, Kar, Purushottam, Varma, Manik,
and Jain, Prateek. Sparse local embeddings for extreme multilabel classification. In NIPS, 2015.
Bucak, Serhat Selcuk, Jin, Rong, and Jain, Anil K. Multi-label
learning with incomplete class assignments. In CVPR, 2011.
CappeÃÅ, Olivier and Moulines, Eric.
On-line expectation‚Äì
maximization algorithm for latent data models. Journal of the
Royal Statistical Society: Series B (Statistical Methodology),
2009.

Scott, James G and Sun, Liang. Expectation-maximization for
logistic regression. arXiv preprint arXiv:1306.0040, 2013.
Vasisht, Deepak, Damianou, Andreas, Varma, Manik, and
Kapoor, Ashish. Active learning for sparse bayesian multilabel
classification. In KDD, 2014.
Wang, Jiang, Yang, Yi, Mao, Junhua, Huang, Zhiheng, Huang,
Chang, and Xu, Wei. CNN-RNN: A unified framework for
multi-label image classification. In CVPR, 2016.

Chen, Yao-Nan and Lin, Hsuan-Tien. Feature-aware label space
dimension reduction for multi-label classification. In NIPS,
2012.

Xu, Chang, Tao, Dacheng, and Xu, Chao. Robust extreme multilabel learning. In KDD, 2016.

CisseÃÅ, Moustapha, Al-Shedivat, COM Maruan, and Bengio,
Samy. Adios: Architectures deep in output space. In ICML,
2016.

Yen, Ian EH, Huang, Xiangru, Zhong, Kai, Ravikumar, Pradeep,
and Dhillon, Inderjit S. PD-sparse: A primal and dual sparse
approach to extreme multiclass and multilabel classification. In
ICML, 2016.

Feng, SL, Manmatha, Raghavan, and Lavrenko, Victor. Multiple
bernoulli relevance models for image and video annotation. In
CVPR, 2004.
Fraccaro, Marco, Paquet, Ulrich, and Winther, Ole. Indexable
probabilistic matrix factorization for maximum inner product
search. In AAAI, 2016.
Gibaja, Eva and Ventura, SebastiaÃÅn. Multilabel learning: A review of the state of the art and ongoing research. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
2014.
Gibaja, Eva and Ventura, SebastiaÃÅn. A tutorial on multilabel
learning. ACM Comput. Surv., 2015.
Hu, Yifan, Koren, Yehuda, and Volinsky, Chris. Collaborative
filtering for implicit feedback datasets. In ICDM, 2008.
Jain, Himanshu, Prabhu, Yashoteja, and Varma, Manik. Extreme
multi-label loss functions for recommendation, tagging, ranking & other missing label applications. In KDD, 2016.
Kanehira, Atsushi and Harada, Tatsuya. Multi-label ranking from
positive and unlabeled data. In CVPR, 2016.
Kapoor, Ashish, Viswanathan, Raajay, and Jain, Prateek. Multilabel classification using bayesian compressed sensing. In
NIPS, 2012.
Kong, Xiangnan, Wu, Zhaoming, Li, Li-Jia, Zhang, Ruofei, Yu,
Philip S, Wu, Hang, and Fan, Wei. Large-scale multi-label
learning with incomplete label assignments. In SDM, 2014.

Yu, Hsiang-Fu, Jain, Prateek, Kar, Purushottam, and Dhillon, Inderjit S. Large-scale multi-label learning with missing labels.
In ICML, 2014.
Yu, Hsiang-Fu, Huang, Hsin-Yuan, Dhillon, Inderjit S, and Lin,
Chih-Jen. A unified algorithm for one-class structured matrix
factorization with side information. In AAAI, 2017.

