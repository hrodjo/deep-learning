Contextual Decision Processes with low Bellman rank are PAC-Learnable
Nan Jiang 1 Akshay Krishnamurthy 2 Alekh Agarwal 3 John Langford 3 Robert E. Schapire 3

Abstract
This paper studies systematic exploration for reinforcement learning (RL) with rich observations
and function approximation. We introduce contextual decision processes (CDPs), that unify
most prior RL settings. Our first contribution is
a complexity measure, the Bellman rank, that we
show enables tractable learning of near-optimal
behavior in CDPs and is naturally small for many
well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in
CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial
in all relevant parameters but independent of the
number of unique contexts. Our approach uses
Bellman error minimization with optimistic exploration and provides new insights into efficient
exploration for RL with function approximation.

1. Introduction
In this paper, we study reinforcement learning (RL) problems where the agent receives rich sensory observations
from the environment, forms complex contexts from sensorimotor streams, uses function approximation to generalize to unseen contexts, and must perform systematic exploration to learn efficiently. Such problems are at the core
of empirical RL research (e.g., Mnih et al., 2015; Bellemare et al., 2016), yet no existing theory provides rigorous
and satisfactory guarantees in a general setting. This situation motivates an important question: how can we solve
RL problems where exploration is critical and the agent receives rich observations, in a sample-efficient manner?
To answer the question, we propose a new formulation,
Contextual Decision Processes (CDPs), to capture a large
class of sequential decision-making problems: CDPs gen1

University of Michigan, Ann Arbor 2 University of Massachusetts, Amherst 3 Microsoft Research, New York. Correspondence to: Nan Jiang <nanjiang@umich.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

eralize MDPs where the state forms the context (Ex. 1) and
POMDPs where the history forms the context (Ex. 2), and
can be much more concise than alternative formulations
based on sufficient statistics (e.g., Hutter, 2005). We define
CDPs in Section 2, and the learning goal is to find a nearoptimal policy for a CDP with the help of a value-function
approximator in a sample-efficient manner.1
A structural assumption: When the context space is very
large or infinite, as is common in practice, lower bounds
that are exponential in the problem horizon preclude efficient learning in CDPs, even when simple function approximators are used. However, RL problems arising in applications are often far more benign than the pathological lower
bound instances, and we identify a structural assumption
capturing this intuition. As our first major contribution, we
define a notion of Bellman factorization (Definition 5) in
Section 3, and focus on problems with low Bellman rank.
At a high level, Bellman rank is an algebraic dimension
capturing the interplay between the CDP and the valuefunction approximator that we show is small for many
previously-studied settings. For example, every MDP with
a tabular value-function has Bellman rank bounded by the
rank of its transition matrix, which is at most the number
of states but can be considerably smaller. For a POMDP
with reactive value-functions, the Bellman rank is at most
the number of hidden states and has no dependence on the
observation space. We provide other instances of low Bellman rank including Linear Quadratic Regulators and Predictive State Representations. Overall, CDPs with a small
Bellman rank yield a unified framework for a large class of
sequential decision making problems.
A new algorithm: Our second contribution is a new algorithm for episodic RL called O LIVE (Optimism Led Iterative Value-function Elimination), detailed in Section 4.1.
O LIVE iteratively refines a space of candidate Q-value
functions F. At each iteration, it chooses a value function f using an optimistic criterion and collects trajectories
from the corresponding greedy policy œÄf . If œÄf attains a
high-value, the algorithm terminates and outputs f . Other1
Throughout the paper, by sample-efficient we mean a number
of trajectories that is polynomial in the problem horizon, number
of actions, Bellman rank (to be introduced), and polylogarithmic
in the number of candidate value-functions.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Model
Bellman rank
PAC Learning

Tabular MDP
# states
known

Low-rank MDP
rank
new

Reactive POMDP
# hidden states
extended

Reactive PSR
PSR rank
new

LQR
# state variables
known3

Table 1. Summary of settings having low Bellman rank, with formal statements in Section 3 (Proposition 1 to 5, from left to right in the
table). The 2nd row gives the parameters that bound the Bellman rank. In the 3rd row, ‚Äúknown‚Äù means that sample-efficient algorithms
already exist (e.g., tabular MDPs), ‚Äúextended‚Äù means our results substantially extend previous work (e.g., (Krishnamurthy et al., 2016)
for reactive POMDPs), and ‚Äúnew‚Äù means our result gives the first sample-efficient algorithm (e.g., MDPs with low-rank transitions).

wise, it eliminates all g ‚àà F which violate certain Bellman
equations under trajectories generated by œÄf and performs
the next iteration with this refined class of functions.
A PAC guarantee: We prove that O LIVE performs
sample-efficient learning in CDPs with a small Bellman rank (Section 4.2). Concretely, when the Q? function for the CDP is contained in F, O LIVE requires OÃÉ(M 2 H 3 K log(N/Œ¥)/2 ) trajectories to find an suboptimal policy,2 where M is the Bellman rank, H is the
length of a trajectory, K is the number of actions, N is the
cardinality of F, and Œ¥ is the failure probability.
Importantly, the sample complexity bound has a logarithmic dependence on N , enabling powerful function approximation, and no direct dependence on the size of the context
space, which can be very large or infinite. As many existing models, including the ones highlighted in Table 1, have
low Bellman rank, the result immediately implies sampleefficient learning in all of these settings.3
The main PAC-guarantee can be extended in several ways,
discussed in Appendix A. Specifically, O LIVE is robust to
the failure of our assumptions, can adapt to unknown Bellman rank, and can handle infinite function classes with
bounded statistical complexity. These extensions demonstrate that the Bellman rank robustly captures the difficulty
of exploration in sequential-decision making problems.
To summarize, this work advances our understanding of RL
with complex observations where long-term planning and
exploration are critical. While O LIVE represents an exponential advance in statistical efficiency, its computational
complexity, which is polynomial in N , is intractable for
the powerful function classes of interest. This computational issue must be addressed before we can empirically
evaluate the effectiveness of the proposed algorithm. We
discuss this and other future directions in Section 6.
Related work. There is rich theoretical literature on RL
in tabular settings, including MDPs (Kearns & Singh, 2002;
Brafman & Tennenholtz, 2003; Strehl et al., 2006) and
POMDPs (Azizzadenesheli et al., 2016) with small state
2

A logarithmic dependence on a norm parameter Œ∂ is omitted
here, as Œ∂ is polynomial in most cases.
3
Our algorithm requires discrete action spaces and does not
immediately apply to LQRs; see more discussion in Section 3.

and observation spaces, with an emphasis on sophisticated exploration to find near-optimal policies in a sampleefficient manner. While there have been extensions to large
state spaces (Kakade et al., 2003; Jong & Stone, 2007;
Pazis & Parr, 2016), these approaches fail to be a good
fit for practical scenarios where the environment is typically perceived through complex observations such as image, text, or audio signals. Alternatively, Monte Carlo Tree
Search (MCTS) methods can handle large state spaces, but
only at the cost of exponential dependence on the planning
horizon (Kearns et al., 2002; Kocsis & SzepesvaÃÅri, 2006).
Closest to our work are the results of Wen & Van Roy
(2013) and Krishnamurthy et al. (2016), which also obtain
sample complexity independent of the number of unique
contexts, but only under deterministic dynamics and other
special structures. In contrast, we study a much broader
class of problems with relatively mild conditions.
On the empirical side, recent successes on both the Atari
platform (Mnih et al., 2015; Wang et al., 2015) and Go (Silver et al., 2016) have sparked a flurry of research interest.
These approaches leverage advances in deep learning for
powerful function approximation, but typically use simple
strategies, such as -greedy, for exploration. Better exploration strategies, such as pseudo-counts in Bellemare et al.
(2016), and combining MCTS with function approximation
(e.g., Silver et al. (2016)), typically require strong domain
knowledge and large amounts of data to be successful.
Hallak et al. (2015) have proposed Contextual MDPs,
where each context parametrizes an MDP. In contrast, our
use of contexts is in analogy with contextual bandits (Langford & Zhang, 2008), and is similar to state features in RL.

2. Contextual Decision Processes (CDPs)
In this section, we introduce a new model, called a Contextual Decision Process, as a unified framework for reinforcement learning with rich observations.
2.1. Model and Examples
CDPs make minimal assumptions to capture a general class
of RL problems and are defined as follows.
Definition 1 (Contextual Decision Process (CDP)). A

Contextual Decision Processes with low Bellman rank are PAC-Learnable

(finite-horizon) CDP is defined as a tuple (X , A, H, P ),
where X is the context space, A is the action space, and
H is the horizon of the problem. P = (P‚àÖ , P+ ) is the system descriptor, where P‚àÖ ‚àà ‚àÜ(X ) is a distribution over
initial contexts, that is x1 ‚àº P‚àÖ , and P+ : (X √ó A √ó
R)‚àó √ó X √ó A ‚Üí ‚àÜ(R √ó X ) elicits the next reward and
context from the interactions so far x1 , a1 , r1 , . . . , xh , ah :
(rh , xh+1 ) ‚àº P+ (x1 , a1 , r1 , . . . , xh , ah ).
In a CDP, the agent interacts with the environment in
episodes. In an episode, the agent observes a context
x1 , takes action a1 , receives reward r1 and observes x2 ,
repeating H times. A policy œÄ : X ‚Üí A specifies the agent‚Äôs decision-making strategy, i.e. ah =
œÄ(xh ), ‚àÄh ‚àà [H], and induces a distribution over trajectories (x1 , a1 , r1 , . . . , xH , aH , rH , xH+1 ) via the system descriptor P . The value of a policy, V œÄ , is defined as

i
hP

H
V œÄ = EP
(1)
h=1 rH  a1:H ‚àº œÄ ,

a CDP (X , A, H, P ) by letting X = (O √óA√óR)‚àó √óO and
xh = (o1 , a1 , r1 , . . . , oh ) is the observed history at level h.
Our next example considers a POMDP where the context
can be substantially more concise than the full history. As
will be formalized in Section 2.2, all we need is that the
context can express a good value function, which is significantly weaker than requiring it be a sufficient statistic
(unlike e.g., Hutter 2005). Therefore, it is important to separate the context in a CDP from any precise notion of state
in the process, and instead keep it as a modeling choice.
Example 3 (POMDPs with sliding windows of observations as contexts). Sometimes partial observability can be
resolved by using a small history: for example, in Atari
games, it is common to keep track of the last 4 images
(Mnih et al., 2015). In this case, we can represent the problem as a CDP by letting xh = (oh‚àí3 , oh‚àí2 , oh‚àí1 , oh ).
We hope the above examples demonstrate the generality
and flexibility of the CDP framework. Finally, we introduce a regularity assumption on the rewards.

where a1:H ‚àº œÄ abbreviates for a1 = œÄ(x1 ), . . . , aH =
œÄ(xH ). Throughout, the expectation is always taken over
contexts and rewards drawn according to the system descriptor P , so we suppress the subscript P . The goal of the
agent is to find a policy œÄ that attains the largest value.

Assumption 1 (Boundedness of rewards). We assume
that regardless of howPactions are chosen, for any h =
H
1, . . . , H, rh ‚â• 0 and h=1 rh ‚â§ 1 almost surely.4

CDPs capture classical RL models, like MDPs and
POMDPs, with appropriately chosen contexts:

A CDP makes no assumptions on the cardinality of the context space, which makes it critical to generalize across contexts, since an agent might not observe the same context
twice. Hence, we consider value-based RL with function
approximation. That is, the agent is given a set of functions F ‚äÜ X √ó A ‚Üí [0, 1] and uses it to approximate
an action-value function (or Q-function). To avoid imposing boundary-conditions, we set f (xH+1 ) ‚â° 0 w.l.o.g. For
ease of presentation, we assume that F is finite with |F| =
N < ‚àû throughout the paper. In Appendix A.3 we allow
infinite function classes with bounded complexity.

Example 1 (MDPs with states as contexts). Consider a
finite-horizon MDP (S, A, H, Œì1 , Œì, R), where S is the
state space, A is the action space, H is the horizon, Œì1 ‚àà
‚àÜ(S) is the initial state distribution, Œì : S √ó A ‚Üí ‚àÜ(S)
is the state transition function, R : S √ó A ‚Üí ‚àÜ([0, 1])
is the reward function, and an episode takes the form of
(s1 , a1 , r1 , . . . , sH , aH , rH ). We can convert the MDP
to a CDP (X , A, H, P ) by letting X = S √ó [H] and
xh = (sh , h), which allows the set of policies {X ‚Üí
A} to contain the optimal policy (Puterman, 1994). The
system descriptor is P = (P‚àÖ , P+ ), where P‚àÖ (x1 ) =
Œì1 (s1 ), and P+ (rh , xh+1 | x1 , a1 , r1 , . . . , xh , ah ) =
R(rh |sh , ah ) Œì(sh+1 |sh , ah ).
As above, the system descriptor for a model is usually obvious and we omit its specification in the following examples.
Turning to POMDPs, it might seem that a CDP limits the
agent‚Äôs decision-making strategies to memoryless (or reactive) policies, as we only consider policies in {X ‚Üí A}.
This is not true. We clarify this issue by showing that we
can use the history as context, and the induced CDP suffers
no loss in the ability to represent optimal policies.
Example 2 (POMDPs with histories as contexts). Consider a finite-horizon POMDP with a hidden state space S,
an observation space O, and an emission process Ds specifying a distribution over O. We can convert the POMDP to

2.2. Value-based RL and Function Approximation

As in typical value-based RL, the goal is to identify f ‚àà F
which respects a particular set of Bellman equations and
achieves a high value with its greedy policy œÄf (x) =
argmaxa‚ààA f (x, a). We next set up the appropriate extensions of Bellman equations to CDPs and the optimal value
VF? through a series of definitions. Unlike MDPs, these involve both the CDP and function approximator F.
Definition 2 (Average Bellman error). Given a policy œÄ :
X ‚Üí A and a function f : X √ó A ‚Üí [0, 1], the average
Bellman error of f under œÄ at level h is defined as

E(f, œÄ, h) = E f (xh ,ah ) ‚àí rh ‚àí f (xh+1 , ah+1 )


 a1:h‚àí1 ‚àº œÄ, ah:h+1 ‚àº œÄf . (2)
4

The bound of 1 is w.l.o.g. More generally, we may simply
replace  with /R in all the sample complexity results when the
bound is R. See more discussion in Kakade (2003, Section 2.2.3).

Contextual Decision Processes with low Bellman rank are PAC-Learnable

The average Bellman error measures the self-consistency
of f between its predictions at levels h and h + 1, when all
the previous actions are taken according to some policy œÄ.
We now define a set of Bellman equations.
Definition 3 (Bellman equations and validity of f ). Given
an (f, œÄ, h) triple, a Bellman equation posits E(f, œÄ, h) =
0. We say f ‚àà F is valid if the Bellman equation on
(f, œÄf 0 , h) holds for every f 0 ‚àà F, h ‚àà [H].
Note that the validity assumption only considers roll-ins
according to the greedy policies œÄf , which is the natural
policy class given F. In MDPs, each Bellman equation can
be viewed as the linear combination of the standard Bellman optimality equations for Q? ,5 where the coefficients
are the probabilities with which the roll-in policy œÄ visits
each state. This leads to the following consequence.
Fact 1 (Q? is always valid). Given an MDP and a space of
functions F : S √ó [H] √ó A ‚Üí [0, 1], if Q? ‚àà F, then in the
corresponding CDP with X = S √ó [H], Q? is valid.
While Q? satisfies the Bellman equations and yields the
optimal policy œÄ ? = œÄQ? , there can be other functions
which also satisfy the equations while yielding suboptimal
policies. For instance, if f (x, œÄf (x)) correctly predicts the
long-term reward of œÄf , then f is always valid. Since validity alone does not imply that we get a good policy, it
is natural to search for a valid value function which also
induces a high-value policy. We formalize this goal next.
Definition 4 (Optimal value). Define
f ? = argmax V œÄf , and VF? = V œÄf ? .
f ‚ààF : f is valid

Fact 2. In the setting of Fact 1, we have f ? = Q? ‚àà F,
and VF? = V ? , which is the optimal long-term value.
Definition 4 implicitly assumes that there is at least one
valid f ‚àà F. This is weaker than the realizability assumption made in the value-based RL literature, that F contains
Q? of an MDP (e.g., Krishnamurthy et al., 2016) (see
Facts 1 and 2). While some works only require Q? to be
approximately captured (e.g., Antos et al., 2008), our algorithm can also be adapted to work with an approximate
notion of validity as discussed in Appendix A.4.

POMDPs with arbitrarily large state/observation spaces.
Formally, the sample complexity of learning CDPs in the
worst-case is ‚Ñ¶(K H ) when K = |A|, even when the complexity of the function class, measured by log |F|, is small.
The result is due to Krishnamurthy et al. (2016) and is included in Appendix F.1 for completeness.
Of course the lower bound instances are quite pathological and devoid of any structure that is often present in real
problems. To capture these realistic scenarios, we propose
a new complexity measure and restrict our attention to settings where this measure is low. As we will see, this measure is naturally small for many existing models, and, when
it is small, efficient reinforcement learning is possible.
The complexity measure we propose is a structural characterization of the set of Bellman equations induced by the
CDP and the class F (recall Definitions 2 and 3), that we
need to check to find valid functions. Checking validity by
enumeration is statistically intractable for large F, since it
requires ‚Ñ¶(|F|) samples to perform all roll-ins. However,
observe that the Bellman equations are structured in tabular
MDPs: the average Bellman error under any roll-in policy
is a stochastic combination of the single-state errors, and
checking the single-state errors (which is tractable) is sufficient to guarantee validity. This observation hints toward
a more general phenomenon: whenever the collection of
Bellman errors across all roll-in policies can be concisely
represented, we may be able to check the validity of all
functions in a tractable way.
This intuition motivates a new complexity measure that we
call the Bellman rank. Define the Bellman error matrices,
one for each h, to be |F| √ó |F | matrices where the (f, f 0 )th
entry is the Bellman error E(f, œÄf 0 , h). Informally, the
Bellman rank for a CDP and a given value-function class
F is a uniform upper bound on the rank of these H Bellman error matrices.
Definition 5 (Bellman factorization and Bellman rank). We
say that a CDP (X , A, H, P ) and F ‚äÇ X √ó A ‚Üí [0, 1] admit Bellman factorization with Bellman rank M and norm
parameter Œ∂, if there exists ŒΩh : F ‚Üí RM , Œæh : F ‚Üí RM
for each h ‚àà [H], such that for any f, f 0 ‚àà F, h ‚àà [H],
E(f, œÄf 0 , h) = hŒΩh (f 0 ), Œæh (f )i,

(3)

and kŒΩh (f 0 )k2 ¬∑ kŒæh (f )k2 ‚â§ Œ∂ < ‚àû.

3. Bellman Factorization and Bellman Rank
CDPs are general models for sequential decision making,
but are there efficient RL algorithms for them?
Unfortunately, without further assumptions, learning in
CDPs is generally hard, since they subsume MDPs and
5
We refer the readers who are not familiar with the definition
of Q? to standard texts, such as (Sutton & Barto, 1998).

The exact factorization in Eq. (3) can be relaxed to an approximate version as is discussed in Appendix A.4. Unlike
rank-based notions in PSRs (Littman et al., 2001) and multiplicity automata (SchuÃàtzenberger, 1961), Bellman rank
depends both on the process and the class F. In the remainder of this section we showcase the generality of Definition 5 by describing a number of common RL settings
that have a small Bellman rank. Throughout, we see how

Contextual Decision Processes with low Bellman rank are PAC-Learnable

the Bellman rank captures the process-specific structures
that allow for efficient exploration. Proofs of all claims in
this section are deferred to Appendix B.
We start with the tabular MDP setting, and show that the
Bellman rank is at most the number of states.
Proposition 1 (Bellman rank bounded by number of states
in MDPs). Consider the setting of Example 1 with the corresponding CDP. With any class F, this model
‚àö admits a
Bellman factorization with M = |S| and Œ∂ = 2 M .
The MDP example is particularly simple as each coordinate
of the M -dimensional space corresponds to a state, which
is observable. Our next few examples show that this is not
necessary, and that the Bellman factorization can be based
on latent properties of the process. We next consider large
MDPs whose transition dynamics have a low-rank structure. A closely related setting has been considered by Barreto et al. (2011; 2014) where the low-rank structure is exploited to speed up MDP planning, but no sample-efficient
RL algorithms were previously known for this setting.
Proposition 2 (Bellman rank in low-rank MDPs, informally). Consider the setting of Example 1 with a transition matrix Œì having rank at most M . The induced CDP
along with any F ‚äÇ X √ó A ‚Üí [0, 1] admits a Bellman
factorization with Bellman rank M .
The next example considers POMDPs with large observations spaces and reactive value functions, where the Bellman rank is at most the number of hidden states.
Proposition 3 (Bellman rank bounded by hidden states in
reactive POMDPs). Consider the setting of Example 3 with
|S| < ‚àû and a sliding window of size 1. Given any F ‚äÇ
X √ó A ‚Üí [0, 1], this model
‚àö admits a Bellman factorization
with M = |S| and Œ∂ = 2 M .
Propositions 2 and 3 can be proved under a unified model
that generalizes POMDPs by allowing the transition and
reward functions to depend on the observation (Figure 1).
This model captures the experimental settings considered
in state-of-the-art empirical RL work, where agents act in a
grid-world (|S| is small) and receives complex and rich observations such as raw pixel images (|O| is large); see e.g.,
Johnson et al. (2016). The model also subsumes and generalizes the setting of Krishnamurthy et al. (2016) which
requires deterministic transitions in the underlying MDP.

s0

s
...

...
o

r

o0

a

Figure 1. A unified model that subsumes MDPs and reactive
POMDPs and has a low Bellman rank. Gray nodes represent unobservable quantities while diamonds are actions controlled by
the agent. The dashed arrow indicates that the action is a function
only of the current observation, so value functions are reactive.

Proposition 4 (Bellman rank in PSRs, informally). Consider a partially observable system with observation space
O and the induced CDP (X , A, H, P ) with xh = (oh , h).
If the linear dimension of the system (i.e., rank of its PSR
model) is at most L, then given any F : X √ó A ‚Üí [0, 1],
the Bellman rank is bounded by LK.
The last example considers a class of linear control problems called Linear Quadratic Regulators (LQRs). We show
that the Bellman rank in LQRs is bounded by the dimension
of the state space. Unlike previous examples, here we crucially use structure of the quadratic value functions, which
is the form Q? takes. Exploration in this class of problems has been previously considered by Osband & Van Roy
(2014). Note that the algorithm to be introduced in the next
section does not directly apply to LQRs due to the continuous action space, and adaptations that exploit the structure
of the action space may be needed.
Proposition 5 (Bellman rank in LQRs, informally). An
LQR can be viewed as an MDP with continuous state space
Rd and action space RK , where the dynamics are described by some linear equations. Given the function class
F which consists of non-stationary quadratic functions of
the state, the Bellman rank is bounded by d2 + 1.

4. Algorithm and Main Results

Next, we consider Predictive State Representations (PSRs),
which are models of partially observable systems with parameters grounded in observable quantities (Littman et al.,
2001). Similar to the case of POMDPs, we can bound the
Bellman rank in terms of the rank of the PSR6 when the
candidate value functions are reactive.

In this section we present our algorithm for learning CDPs
that have a Bellman factorization with small Bellman rank,
along with the main sample complexity guarantee. To aid
presentation and help convey the main ideas, we make three
simplifying assumptions. We assume that (1) the agent
knows the Bellman rank M and the corresponding norm
bound, (2) the function class F is finite with cardinality N ,
and (3) the validity and Bellman factorization conditions
(Definitions 3 and 5) hold exactly. We relax these assumptions in Section 5 and Appendix A.

6
Every POMDP has an equivalent PSR whose rank is bounded
by the number of hidden states (Singh et al., 2004).

We are interested in designing an algorithm for PAC
Learning CDPs. We say that an algorithm PAC learns a

Contextual Decision Processes with low Bellman rank are PAC-Learnable

CDP if given F, two parameters , Œ¥ ‚àà (0, 1), and access to the CDP, the algorithm outputs a policy œÄÃÇ with
V œÄÃÇ ‚â• VF? ‚àí  with probability at least 1 ‚àí Œ¥. The
sample complexity is the number of episodes needed to
achieve such a guarantee, and is typically expressed in
terms of , Œ¥, and other relevant parameters. The goal
is to design an algorithm with sample complexity that
is Poly(M, K, H, 1/, log(N ), log(1/Œ¥)) where M is the
Bellman rank, K is the number of actions, and H is the
time horizon. Importantly, the bound allows no dependence
on the number of unique contexts |X |.

The second fact is that if a function f is valid, then its predicted value is exactly the value achieved by the greedy
policy œÄf , that is Vf = E[f (x1 , œÄf (x1 ))] = V œÄf . This is
based on the following lemma.
Lemma 1 (Value-function error decomposition). Define
Vf = E[f (x1 , œÄf (x1 ))]. Then ‚àÄf : X √ó A ‚Üí [0, 1],

4.1. Algorithm
Pseudocode for our algorithm, O LIVE (Optimism Led Iterative Value-function Elimination), is displayed in Algorithm 1. Theorem 1 describes how to set the parameters
nest , neval , n, and œÜ. For brevity, we introduce a shorthand
for empirical Bellman errors given a tuple (x, a, r, x0 ):
œÉ(f, x, a, r, x0 ) := f (x, a) ‚àí r ‚àí f (x0 , œÄf (x0 )).

mates with population values and set  to 0. The first important fact is that the algorithm never eliminates a valid
function, since the learning step in Line 14 only eliminates a function f if we can find a distribution on which
it has a large average Bellman error. If f is valid, then
E(f, œÄ, h) = 0 for all œÄ, h, so f is never eliminated.

(4)

At a high level, the algorithm aims to eliminate functions
f ‚àà F that fail to satisfy the validity condition in Definition 3. This is done by Lines 13 and 14 inside the loop of
the algorithm. Line 13 uses importance weighting to get
an unbiased estimate of E(f, œÄt , ht ), the average Bellman
error for function f on roll-in policy œÄt at time ht . Thus,
Line 14 eliminates functions that have high average Bellman error under œÄt and hence are not valid.
The other major component of the algorithm involves
choosing the roll-in policy œÄt and level ht on which to
do the learning step. At iteration t, we choose the roll-in
policy œÄt optimistically, by choosing ft that predicts the
highest value at the starting context distribution and setting
œÄt = œÄft . To pick ht , we compute ft ‚Äôs average Bellman
error on its own roll-in distribution (Line 7), and set ht to
be any level for which this average Bellman error is high
(See Line 11). As we will show, these choices ensure that
substantial learning happens on each iteration, guaranteeing that the algorithm uses polynomially many episodes.
The last component is the termination criterion. The algorithm terminates if ft has small average Bellman error on
its own roll-in distribution at all levels. This criteria guarantees that œÄt is near optimal.
Computationally, the algorithm requires enumeration of the
value-function class, which we expect to be extremely large
or infinite in practice. A computationally efficient implementation is essential for a practical algorithm, which remains an open question. We focus on the sample efficiency
of the algorithm in this paper.
Intuition for OLIVE. To convey intuition, it is helpful to
ignore any sampling effects by replacing all empirical esti-

Vf ‚àí V

œÄf

=

H
X

E(f, œÄf , h).

(5)

h=1

Therefore, since ft is chosen optimistically as the maximizer of the value prediction among the surviving functions, and since we never eliminate valid functions, if
O LIVE terminates, it must output a policy with value VF? .
In the analysis, we incorporate sampling effects to derive
robust versions of these facts so the algorithm always outputs a policy that is at most -suboptimal.
The more challenging component is bounding the number
of iterations of the algorithm, which is critical for obtaining a polynomial sample complexity bound. This argument crucially relies on the Bellman factorization (Definition 5), which enables us to embed the distributions over
contexts for any roll-in policy into M dimensions and measure progress in this low-dimensional space.
For now, fix some h and focus on the iterations where
ht = h. If we ignore sampling effects we can set œÜ = 0. By
using the Bellman factorization to write E(f, œÄft , h) as an
inner product, we can think of the learning step in Line 14
as introducing a homogeneous linear constraint on the set
of Œæh (f ) vectors: hŒΩh (ft ), Œæh (f )i = 0. Now, if we execute the learning step at h again in a later iteration t0 , we
have hŒΩh (ft0 ), Œæh (ft0 )i =
6 0 from Line 11. Importantly, this
means that ŒΩh (ft0 ) must be linearly independent from previous ŒΩh (ft ) since hŒΩh (ft ), Œæh (ft0 )i = 0. Since every time
ht = h, the number of linearly independent constraints increases by 1, the number of iterations where ht = h is at
most M , the dimension of the space. Thus the Bellman
rank (times H) upper-bounds the number of iterations.
The above heuristic reasoning, despite relying on the brittle
notion of linear independence, can be made robust. With
sampling effects, rather than homogeneous linear equalities, the learning step for level h introduces linear inequality constraints to the Œæh (f ) vectors. But if f 0 is a surviving function that forces us to train at h, it means that
hŒΩh (f 0 ), Œæh (f 0 )i is very large, while hŒΩh (¬∑), Œæh (f 0 )i is very
small for all previous ŒΩh (¬∑) vectors used in the learning

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Algorithm 1 O LIVE (F, M, Œ∂, , Œ¥) ‚Äì Optimism Led Iterative Value-function Elimination
(i) n

est
.
1: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x1 }i=1

2: Estimate the predicted value for each f ‚àà F: VÃÇf = n1est
3: F0 ‚Üê F.
4: for t = 1, 2, . . . do
5:
Choose policy ft = argmaxf ‚ààFt‚àí1 VÃÇf , œÄt = œÄft .
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

(i)

(i)
(i)
i=1 f (x1 , œÄf (x1 )).

Pnest

(i)

Collect neval trajectories by following œÄt (i.e. ah = œÄt (xh ) for all h and i = 1, . . . , neval ).
Pneval
(i) (i) (i)
(i)
œÉ(f, xh , ah , rh , xh+1 ).
Estimate ‚àÄh ‚àà [H], EÃÉ(ft , œÄt , h) := n1eval i=1
PH
if h=1 EÃÉ(ft , œÄt , h) ‚â§ 5/8 then
Terminate and output œÄt .
end if
Pick ht ‚àà [H] such that EÃÉ(ft , œÄt , ht ) ‚â• 5/(8H).
(i)
(i)
(i)
Collect n trajectories where ah = œÄt (xh ) for all h 6= ht and aht is drawn uniformly at random.
(i)
(i)
b œÄt , ht ) := 1 Pn 1[aht =œÄf (xht )] œÉ(f, x(i) , a(i) , r(i) , x(i) ). (see Eq. (4))
Estimate ‚àÄf ‚àà F, E(f,
i=1
ht
ht
ht
ht +1
n
1/K


o
n

b
Learn Ft = f ‚àà Ft‚àí1 : E(f, œÄt , ht ) ‚â§ œÜ .
end for

step. Intuitively this means that the new ŒΩh (f 0 ) vector is
quite different from all of the previous ones. Our proof uses
a volumetric argument to show that this suffices to guarantee substantial learning takes place. In more detail, we
track the volume of an enclosing ellipsoid of the surviving
Œæh (f ) functions and show that each time we learn at level
h this volume shrinks multiplicatively, which results in an
iteration complexity that is linear in M H.
The optimistic choice for ft is critical for driving the
agent‚Äôs exploration. With this choice, if ft is valid, then the
algorithm terminates correctly, and if ft is not valid, then
substantial progress is made. Thus the agent does not get
stuck exploring with many valid but suboptimal functions,
which could result in exponential sample complexity.
4.2. Sample Complexity
We now turn to the main result, which guarantees that
O LIVE PAC-learns Contextual Decision Processes with
polynomial sample complexity.
Theorem 1. For any , Œ¥ ‚àà (0, 1), any CDP and function
class F that admit a Bellman factorization with parameters
M and Œ∂, run O LIVE with the following parameters:

‚àö ,
12H M

32
log(6N/Œ¥),
2
!
‚àö
288H 2
12H 2 M log(6H M Œ∂/)
neval =
log
,
2
Œ¥
!
‚àö
4608H 2 M K
12N HM log(6H M Œ∂/)
n=
log
.
2
Œ¥
œÜ=

nest =

Then, with probability at least 1‚àíŒ¥, O LIVE returns a policy

œÄÃÇ that satisfies V œÄÃÇ ‚â• VF? ‚àí  (recall Definition 3 for VF? ),
and the number of episodes required is at most7
 2 3

M H K
OÃÉ
log(N
Œ∂/Œ¥)
.
(6)
2
Thus, if a CDP and function class F admit a Bellman factorization with small Bellman rank and F contains valid
functions, O LIVE is guaranteed to find a near optimal valid
function using only polynomially many episodes. To our
knowledge, this is the most general polynomial sample
complexity bound for RL with rich observations and function approximation, as many popular models are shown to
admit small Bellman rank (see Section 3, Table 1). The result also certifies that the notion of Bellman factorization,
which is quite general, is sufficient for efficient exploration
and learning in sequential decision making problems.
It is worth briefly comparing this result with prior work.
1. The most closely related result is the recent work
of Krishnamurthy et al. (2016), who also consider
episodic RL with infinite observation spaces and function approximation. The model studied there is a CDP
with Bellman rank M , so our result applies as is to that
setting. Importantly, we eliminate the need for deterministic transitions in that work, while improving the
dependence on H and , although with worse scaling
in M . We emphasize that our result applies to a much
more general class of models.
2. Several works provide sample complexity bounds
for fitted value/policy iteration methods (e.g., Munos
7
We use OÃÉ(¬∑) notation to suppress poly-logarithmic dependence on everything except N and Œ¥.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

(2003); Antos et al. (2008); Munos & SzepesvaÃÅri
(2008)). While these results are relevant, they do not
address the exploration issue, which is our main focus, and circumvent it by impliciting assuming an exploratory policy for data collection.
3. Ng & Jordan (2000) proposed a policy search method
for POMDPs called PEGASUS, with a sample complexity that scales polynomially with the statistical
complexity of the policy class and the horizon. Despite the powerful result, the algorithm requires careful control over the random numbers that determine
the state transitions. While the assumption can hold
for certain simulated environments, the scope of applications is relatively limited.
4. Since CDPs include small-state MDPs (Kearns &
Singh, 2002; Brafman & Tennenholtz, 2003; Strehl
et al., 2006), the algorithm can be applied as is to
these problems. Unfortunately, our sample complexity is polynomially worse than the state of the
log(1/Œ¥)) bounds for PAC-learning
art OÃÉ( M poly(H)K
2
MDPs (Dann & Brunskill, 2015). On the other hand,
the algorithm also applies to MDPs with infinite state
spaces with Bellman factorizations, which cannot be
handled by tabular approaches.
5. Finally, Contextual Decision Processes also encompass contextual bandits, where the sample complexity
is Œò(K log(N )/2 ) (Agarwal et al., 2014). As contextual bandits have M = H = 1, O LIVE achieves
the optimal sample complexity in this case.
Turning briefly to lower bounds, since the CDP setting with
Bellman factorization is new, general lower bounds for the
broad class do not exist. However, we can use MDP lower
bounds for guidance on the question of optimality, since the
small-state MDPs in Example 1 are a special case. While
no existing MDP lower bounds apply as is (because formulations vary), in Appendix F.2 we adapt ideas from Auer
et al. (2002) to obtain a ‚Ñ¶(M KH/2 ) sample complexity
lower bound for learning the MDPs in Example 1.
In comparison, the sample complexity in Theorem 1 is
worse in M, H, and log(N ) factors, but of course the
small-state MDP is a significantly simpler special case. We
leave as future work the question of optimal sample complexity for learning CDPs with low Bellman rank.

5. Extensions
The basic result presented here is quite robust and admits
many extensions, some of which we briefly describe here;
the details are deferred to Appendix A.
1. Handling infinite function classes with dependence
on VC-dimension like quantities. This result uses a

context-value function class G ‚äÇ X ‚Üí [0, 1] and a
policy class Œ† ‚äÇ X ‚Üí A instead of a context-action
value class as in O LIVE, with sample complexity depending on the pseudo-dimension of G and the Natarajan dimension of Œ†. These are standard measures for
regression and multi-class classification, and several
natural classes have known bounds.
2. Competing with approximately valid value-functions
with inexact Bellman factorization. For this result, we
extend the definition of validity and VF? (Defs. 3 and 4)
to allow small but non-zero Bellman errors, and also
only require that the Bellman error matrices have a
low rank approximation with small `‚àû error.
3. Adapting to unknown Bellman rank. Here we run
O LIVE with choices of M growing at a doubling
schedule and show that the PAC-guarantee is preserved without loss in sample complexity.

6. Discussion
In this paper, we presented a new model for RL with rich
observations, called Contextual Decision Processes, and
a structural property, the Bellman factorization, of these
models that enables sample-efficient learning. The unified
approach allows us to address several settings of practical
interest that have largely eluded RL theory to date. Our
work also elicits several further questions:
1. Can we obtain a computationally efficient algorithm
for some form of this setting? Prior related work
(for instance in contextual bandits (Dudik et al., 2011;
Agarwal et al., 2014)) used supervised learning oracles for computationally efficient approaches. Is there
a suitable oracle for this setting?
2. The sample complexity depends polynomially on the
cardinality of the action space. Can we extend the results to handle large or continuous action spaces (e.g.,
by incorporating concepts such as Eluder dimension
(Russo & Van Roy, 2013))?
3. Can we address sample-efficient RL given only a policy class rather than a value function class? Empirical
approaches often rely on policy gradients, which are
subject to local optima. Are there parallel results to
this work, without access to value functions?
Resolutions to these questions are important for further
connecting RL theory with practice.

Acknowledgements
Part of this work was completed while NJ and AK were at
Microsoft Research. NJ was partially supported by Rackham Predoctoral Fellowship in University of Michigan.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

References
Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford, John, Li,
Lihong, and Schapire, Robert E. Taming the monster: A fast
and simple algorithm for contextual bandits. In International
Conference on Machine Learning, 2014.

Haussler, David. Sphere packing numbers for subsets of the
Boolean n-cube with bounded Vapnik-Chervonenkis dimension. Journal of Combinatorial Theory, Series A, 1995.
Haussler, David and Long, Philip M. A generalization of Sauer‚Äôs
lemma. Journal of Combinatorial Theory, Series A, 1995.

Anderson, Brian D.O. and Moore, John B. Optimal Control: Linear Quadratic Methods. Courier Corporation, 2007.

Hutter, Marcus. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, 2005.

Antos, AndraÃÅs, SzepesvaÃÅri, Csaba, and Munos, ReÃÅmi. Learning near-optimal policies with bellman-residual minimization
based fitted policy iteration and a single sample path. Machine
Learning, 2008.

Johnson, Matthew, Hofmann, Katja, Hutton, Tim, and Bignell,
David. The Malmo Platform for artificial intelligence experimentation. In International Joint Conference on Artificial Intelligence, 2016.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and Schapire,
Robert E. The nonstochastic multiarmed bandit problem. SIAM
Journal on Computing, 2002.

Jong, Nicholas K. and Stone, Peter. Model-based exploration in
continuous state spaces. In Abstraction, Reformulation, and
Approximation, 2007.

Azizzadenesheli, Kamyar, Lazaric, Alessandro, and Anandkumar, Animashree. Reinforcement learning of POMDPs using
spectral methods. Conference on Learning Theory, 2016.

Kakade, Sham. On the sample complexity of reinforcement learning. PhD thesis, University College London, 2003.

Barreto, AndreÃÅ da Motta Salles, Pineau, Joelle, and Precup,
Doina. Policy iteration based on stochastic factorization. Journal of Artificial Intelligence Research, 2014.
Barreto, Andre S, Precup, Doina, and Pineau, Joelle. Reinforcement learning using kernel-based stochastic factorization. In
Advances in Neural Information Processing Systems, 2011.
Bellemare, Marc G., Srinivasan, Sriram, Ostrovski, Georg,
Schaul, Tom, Saxton, David, and Munos, Remi. Unifying
count-based exploration and intrinsic motivation. In Advances
in Neural Information Processing Systems, 2016.
Ben-David, Shai, Cesa-Bianchi, Nicolo, and Long, Philip M.
Characterizations of learnability for classes of {0,. . . , n}valued functions. In Conference on Learning Theory, 1992.
Bland, Robert G, Goldfarb, Donald, and Todd, Michael J. The
ellipsoid method: A survey. Operations research, 1981.
Boots, Byron, Siddiqi, Sajid M., and Gordon, Geoffrey J. Closing the learning-planning loop with predictive state representations. International Journal of Robotics Research, 2011.
Brafman, Ronen I. and Tennenholtz, Moshe. R-max ‚Äì a general polynomial time algorithm for near-optimal reinforcement
learning. Journal of Machine Learning Research, 2003.
Dann, Christoph and Brunskill, Emma. Sample complexity of
episodic fixed-horizon reinforcement learning. In Advances in
Neural Information Processing Systems, 2015.
Devroye, Luc, GyoÃàrfi, LaÃÅszloÃÅ, and Lugosi, GaÃÅbor. A Probabilistic
Theory of Pattern Recognition. Springer-Verlag, 1996.
Dudik, Miroslav, Hsu, Daniel, Kale, Satyen, Karampatziakis,
Nikos, Langford, John, Reyzin, Lev, and Zhang, Tong. Efficient optimal learning for contextual bandits. In Uncertainty in
Artificial Intelligence, 2011.
Hallak, Assaf, Di Castro, Dotan, and Mannor, Shie. Contextual
Markov decision processes. arXiv:1502.02259, 2015.
Haussler, David. Decision theoretic generalizations of the PAC
model for neural net and other learning applications. Information and computation, 1992.

Kakade, Sham, Kearns, Michael, and Langford, John. Exploration in metric state spaces. In International Conference on
Machine Learning, 2003.
Kearns, Michael and Singh, Satinder. Near-optimal reinforcement
learning in polynomial time. Machine Learning, 2002.
Kearns, Michael, Mansour, Yishay, and Ng, Andrew Y. A sparse
sampling algorithm for near-optimal planning in large Markov
decision processes. Machine Learning, 2002.
Kocsis, Levente and SzepesvaÃÅri, Csaba. Bandit based MonteCarlo planning. In European Conference on Machine Learning, 2006.
Krishnamurthy, Akshay, Agarwal, Alekh, and Langford, John.
PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, 2016.
Langford, John and Zhang, Tong. The epoch-greedy algorithm
for multi-armed bandits with side information. In Advances in
Neural Information Processing Systems, 2008.
Li, Lihong. A unifying framework for computational reinforcement learning theory. PhD thesis, Rutgers, The State University of New Jersey, 2009.
Littman, Michael L., Sutton, Richard S., and Singh, Satinder. Predictive representations of state. In Advances in Neural Information Processing Systems, 2001.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg,
Shane, and Hassabis, Demis. Human-level control through
deep reinforcement learning. Nature, 2015.
Munos, ReÃÅmi. Error bounds for approximate policy iteration. In
International Conference on Machine Learning, 2003.
Munos, ReÃÅmi and SzepesvaÃÅri, Csaba. Finite-time bounds for fitted
value iteration. Journal of Machine Learning Research, 2008.
Natarajan, Balas K. On learning sets and functions. Machine
Learning, 1989.

Contextual Decision Processes with low Bellman rank are PAC-Learnable
Ng, Andrew Y and Jordan, Michael. Pegasus: A policy search
method for large MDPs and POMDPs. In Uncertainty in Artificial Intelligence, 2000.
Osband, Ian and Van Roy, Benjamin. Model-based reinforcement
learning and the eluder dimension. In Advances in Neural Information Processing Systems, 2014.
Panchenko, Dmitriy. Some extensions of an inequality of Vapnik
and Chervonenkis. Electronic Communications in Probability,
2002.
Pazis, Jason and Parr, Ronald. Efficient PAC-optimal exploration
in concurrent, continuous state MDPs with delayed updates. In
Conference on Artificial Intelligence, 2016.
Pollard, David. Convergence of Stochastic Processes. Springer
Science & Business Media, 2012.
Puterman, Martin. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience, 1994.
Russo, Dan and Van Roy, Benjamin. Eluder dimension and the
sample complexity of optimistic exploration. In Advances in
Neural Information Processing Systems, 2013.
SchuÃàtzenberger, M.P. On the definition of a family of automata.
Information and Control, 1961.
Silver, David, Huang, Aja, Maddison, Chris J., Guez, Arther,
Sifre, Laurent, van den Driessche, George, Schrittwieser,
Julian, Antonoglou, Ioannis, Penneershelvam, Veda, Lanctot, Marc, Dieleman, Sander, Grewe, Dominik, Nham, John,
Kalchbrenner, Nal, Sutskever, Ilya, Lillicrap, Timothy, Leach,
Madeleine, Kavukcuoglu, Koray, Graepel, Thore, and Hassabis, Demis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016.
Singh, Satinder and Yee, Richard C. An upper bound on the loss
from approximate optimal-value functions. Machine Learning,
1994.
Singh, Satinder, James, Michael R., and Rudary, Matthew R.
Predictive state representations: A new theory for modeling
dynamical systems. In Uncertainty in Artificial Intelligence,
2004.
Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Langford, John,
and Littman, Michael L. PAC model-free reinforcement learning. In International Conference on Machine Learning, 2006.
Sutton, Richard S and Barto, Andrew G. Reinforcement Learning:
An Introduction. MIT Press, 1998.
Todd, Michael J. On minimum volume ellipsoids containing part
of a given ellipsoid. Mathematics of Operations Research,
1982.
Todd, Michael J and Yƒ±ldƒ±rƒ±m, E Alper. On Khachiyan‚Äôs algorithm
for the computation of minimum-volume enclosing ellipsoids.
Discrete Applied Mathematics, 2007.
Wang, Ziyu, de Freitas, Nando, and Lanctot, Marc. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, 2015.
Wen, Zheng and Van Roy, Benjamin. Efficient exploration and
value function generalization in deterministic systems. In Advances in Neural Information Processing Systems, 2013.

