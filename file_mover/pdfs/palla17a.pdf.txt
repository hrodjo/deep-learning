A Birth-Death Process for Feature Allocation
Konstantina Palla 1 David Knowles 2 Zoubin Ghahramani 3 4

Abstract
We propose a Bayesian nonparametric prior over
feature allocations for sequential data, the birthdeath feature allocation process (BDFP). The
BDFP models the evolution of the feature allocation of a set of N objects across a covariate (e.g. time) by creating and deleting features.
A BDFP is exchangeable, projective, stationary
and reversible, and its equilibrium distribution
is given by the Indian buffet process (IBP). We
show that the Beta process on an extended space
is the de Finetti mixing distribution underlying
the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process
(BEP), that permits simplified inference. The
utility of the BDFP as a prior is demonstrated on
real world dynamic genomics and social network
data.

1. Introduction - Problem Statement
We are interested in time series settings where we observe
data {Yt âˆˆ Y : t = 1, . . . , L}. We consider problems
where the observations are explained by a latent structure
which assigns objects to features and this feature allocation changes over time. For instance, consider the topics covered by a number of newspapers over time; some
topics â€œdieâ€ while new ones are â€œbornâ€. The topic coverage of each paper is its latent feature allocation which
could be modelled with an Indian buffet process (Griffiths
& Ghahramani, 2011, IBP). While static feature allocation
models are well studied, these are not able to handle the
time series nature of many datasets. We propose a process
that extends the IBP by allowing the feature allocation to
evolve over the covariate as a result of â€œbirthâ€ and â€œdeathâ€
of features.
1
University of Oxford, Oxford, UK 2 Stanford University, California, USA 3 University of Cambridge, Cambridge, UK 4 Uber AI
Labs, SF, California, USA. Correspondence to: Konstantina Palla
<konstantina.palla@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2. Related Work
We target problems where the data depends on a covariate,
such as time or space, and is explained by a latent structure, in particular a (multi-membership) clustering of the
data points. The observations are result of the underlying
partitioning and its evolution over the covariate. Typical
models fall in two main categories: clustering and feature
allocation. The former allow each data point to belong to
one and only one class (cluster), while the latter let each
data point belong to multiple groups (features). Bayesian
nonparametric approaches are primarily based on the Chinese restaurant process (CRP, Aldous, 1983) or the Indian
buffet process (IBP, Griffiths & Ghahramani, 2005) corresponding to the two categories. In particular, a sample from a CRP is an assignment of data points to disjoint classes (a clustering), while a sample from an IBP is
an allocation of the data points to (possibly) overlapping
classes (a feature allocation). Dependent nonparametric
processes extend distributions over partitions to distributions over collections of partitions indexed by locations in
some covariate space, such as R+ (e.g. continuous time),
Z (e.g. discrete time), or Rd (e.g. geographical location).
Teh et al. (2013) define such a process based on the duality between Kingmanâ€™s coalescent (Kingman, 1982) and
the Dirichlet diffusion tree (Neal, 2003). In the resulting
â€œFragmentation-Coagulationâ€ process (FCP) a partitioning
of the data points evolves over the covariate undergoing
fragmentation and coagulation events while maintaining
CRP marginals. More recently, Palla et al. (2013) derived
a dependent partition-valued process (DPVP) on an arbitrary covariate space which, like the FCP, is exchangeable
and has CRP distributed marginals. In the setting of feature allocations, Williamson et al. (2010) propose a nonparametric process, the dependent IBP (dIBP), with IBP
distributed marginals and in which the feature allocations
are coupled over the covariate space using a Gaussian process (GP, Rasmussen & Williams, 2006). In a similar vein,
Van Gael et al. (iFHMM, 2009) define the Markov Indian
Buffet process (mIBP), a probability distribution over a potentially infinite number of binary Markov chains evolving
in discrete time. They use the mIBP to extend the factorial hidden Markov model (FHMM, Ghahramani & Jordan,
1997) to the infinite FHMM (iFHMM).
In this paper, we address the problem of dependence for

A Birth-Death Process for Feature Allocation

binary latent feature models. We propose a process that extends the IBP by allowing features to be â€œbornâ€ and â€œdieâ€
at times learnt by the model, while maintaining the essential mathematical properties of the IBP. The process is a
Markov Jump process (MJP) where the events are the birth
or the death of a feature. The idea is closely related to the
FCP where the events are either a fragmentation of a cluster or a coagulation of two clusters. The partitions at each
location in the FCP are marginally a sample from a Chinese restaurant process, while the feature allocations in the
BDFP are marginally samples from an IBP. Compared to
the dIBP, both processes model feature allocations evolving over the covariate. However, while in the dIBP the
assignment of data points to a feature might change over
the covariate, in our process, it remains the same until the
feature dies. In the case of the iFHMM, the authors model
the dependence of a feature allocation on a discrete time
variable as opposed to our process where continuous covariate space is assumed. Moreover, in the iFHMM, the
marginal distribution of a feature allocation is analogous
but not equal to an IBP. We call the proposed process the
birth-death feature allocation process (BDFP). The BDFP
is exchangeable, projective, stationary and reversible, and
its equilibrium distribution is given by the Indian buffet
process.

3. Feature Allocations and the Indian Buffet
Process
Consider a dataset with N data points indexed by integers
[N ] := {1, 2, . . . , N } (allowing N â†’ âˆ). Each datapoint n is associated with a binary vector Zn of length
K that defines its feature allocation; Znk = 1 if datapoint n has feature k and Znk = 0 otherwise. The potential total number of features K may be infinite. The
binary matrix Z[N ] = [ZT1 , ZT2 , . . . , ZTN ]T specifies a random feature allocation of [N ], while ZN denotes the space
of all feature allocations of [N ], i.e. Z[N ] âˆˆ ZN . We define mk as the number of datapoints that possess feature
P2N âˆ’1
k, K+ = h=1 Kh as the number of features for which
mk > 0 and Kh as the multiplicity of feature h, that is
the number of times the same binary column h appears in
Z[N ] . Under the IBP (Griffiths & Ghahramani, 2011), the
probability of a matrix Z[N ] is
K+
Y
Î±K+
(N âˆ’ mk )!(mk âˆ’ 1)
g([Z[N ] ]; Î±) = QH
exp(âˆ’Î±HN )
N!
K
h!
h=1
k=1

(1)
where
PN 1Î± > 0 is the concentration parameter, HNN =
âˆ’1
j=1 j is the N th harmonic number and H â‰¤ 2
is the number of distinct nonzero features in the allocation.
Thibaux & Jordan (2007) showed one can construct the Indian buffet process from a Beta-Bernoulli process using the

following two stage sampling process for n = 1, . . . , N :
B|c, Âµ0 âˆ¼BP(c, Âµ0 ) Zn |B âˆ¼ BeP(B)
(2)
Pâˆ
Pâˆ
where B =
k=1 Ï‰k Î´Î¸k and Z =
k=1 fk Î´Î¸k . First
a draw B is sampled from the Beta process BP(cÂµ0 )
(Hjort, 1990) with Âµ0 as the base distribution. B is a set
of pairs (Ï‰k , Î¸k ) sampled from a Poisson process on the
product space [0, 1] Ã— Î˜ with LeÌvy intensity Î½(dÏ‰, dÎ¸) =
cÏ‰ âˆ’1 (1 âˆ’ Ï‰)câˆ’1 dÏ‰Âµ0 (dÎ¸). Then, B is used as the atomic
hazard measure for a Bernoulli process BeP(B). Each
Zn is a draw from the Bernoulli process and constitutes
a collection of atoms of unit mass on Î˜. Then, Zn is
a binary vector containing the {fk }âˆ
k=1 values resulting
from tossing a countably infinite sequence of (conditionally independent) coins with success probabilities Ï‰k , i.e.
fk |Ï‰k âˆ¼ Bernoulli(Ï‰k ). This construction allows the use
of de Finettiâ€™s theorem (de Finetti, 1931) that lets the joint
distribution of the rows to be written as
Z hY
N
i
P (Zn |B) dP (B) (3)
P (Z1 , . . . , ZN ) =
n=1

where B is the random measure that renders the variables
Zn conditionally independent. Equation (3) shows the exchangeability of the rows of Zn , since they can be described as a mixture of Bernoulli processes.

4. Birth-Death Process for Feature Allocation
We consider a continuous-time Markov process (Z(t))tâ‰¥0
in which each Z(t) is a random feature allocation taking values in the discrete space ZN . The state space is
countably infinite; it is determined by all the possible feature allocations defined by N datapoints and K features,
where K â†’ âˆ. The Markov process (Z(t)) evolves over
time jumping to different states (feature allocations). Let
{t1 , . . . , tJ âˆˆ R : J âˆˆ N} denote the times when the chain
jumps such that tj = inf{Ï„ â‰¥ tjâˆ’1 : Z(Ï„ ) 6= Z(tjâˆ’1 )}
and Z(tj ) âˆˆ ZN . These jumps are a result of a birth or
a death of a feature. The process (Z(t)) can only jump to
neighbouring states, i.e. if the chain is currently at state
Z(tj ) = s, then at time tj+1 it transitions to Z(tj+1 ) = s0
where a new feature is created or an existing feature is
deleted after a birth or a death event respectively. Let
s
ZN
âŠ‚ ZN be the discrete space of neighboring states to
state s. The process is time homogeneous with transition
probabilities P(Z(t + y) = s0 |Z(y) = s) = P(Z(t) =
s0 |Z(0) = s) = pss0 (t) for all t, y, where s, s0 âˆˆ Z N . At
time tj+1 the process jumps to the next state Z(tj+1 ) = s0
with rate determined by the current state Z(t) = s and the
corresponding event, i.e birth or death. More specifically,
â€¢ Birth: Suppose s âˆˆ ZN is a feature allocation with
s
Ks nonzero features and s0 âˆˆ ZN
is another feature

A Birth-Death Process for Feature Allocation

allocation that differs from s in having one additional
feature of size |a| so that Ks0 = Ks + 1. We choose
the transition rate from s to s0 as
qss0

(|a| âˆ’ 1)!(N âˆ’ |a|)!
=R
N!

(4)

where R > 0 is a parameter governing the birth
rate. The new feature
 a is a binary column of length
N
N . There are |a|
binary formulations for this fea
PN
N
N
ture and 2 âˆ’ 1 =
n=1 n for all possible feature births
 and thus, the total birth rate from s is
PN
PN
N
âˆ’n)!
R (nâˆ’1)!(N
= R n=1 n1 = R Â· HN
n=1
N!
n
PN
where HN = n=1 1/n is the N -th harmonic number and n = |a| .
â€¢ Death: The rate of transitioning from s0 to s is
qs0 s =

Rr
Î±

(5)

where D = R
a is a parameter governing the death rate
and r is the multiplicity of the feature in s0 that dies.
The multiplicity r is the combinatorial factor that accounts for all the possible ways of obtaining the same
equivalence class as defined in Griffiths & Ghahramani (2011) . There are Ks0 features (including repetitions of the same feature) in s0 that might â€œdieâ€, thus
s0
the total death rate from s0 is RK
Î± .
The total rate of transition out of state s âˆˆ ZN is the sum
RKs
ofthe total birth
 and death rates, qs = RHN + Î± =
R HN + KÎ±s . We call (Z(t))t>=0 a birth-death feature
allocation process with birth rate R and death rate
write BDFP(Î±, R).

R
Î±

and

Theorem 1. The Markov process (Z(t))tâ‰¥0 is irreducible
and has stationary distribution IBP(Î±). Furthermore, it is
reversible.
Proof. A continuous time Markov chain is irreducible if it
is possible to eventually get from every state to every other
state with positive probability. It is reversible if detailed
balance holds, i.e. there is a probability distribution Ï€ on
ZN such that Ï€s qss0 = Ï€s0 qs0 s for all s, s0 âˆˆ ZN . Then Ï€ is
also the invariant (equilibrium) distribution of the Markov
chain. The chain in BDFP is irreducible, because for any
T > 0 and any two distinct feature allocations Î³, Ï âˆˆ ZN ,
there is a positive probability that if it starts at Î³ âˆˆ ZN ,
it will end at Ï âˆˆ ZN . Reversibility and the equilibrium
distribution can be demonstrated by detailed balance. Suppose Î³, Ï are feature allocations such that Î³, Ï âˆˆ ZN and Ï
differs from Î³ in that it has one additional feature a of size
|a|. The number of (nonzero) features in Ï is KÏ = KÎ³ +1.

Then,
g(Î³; Î±)qÎ³Ï =

Î± KÎ³
H

Î³
Kh !
Î h=1

KÎ³
Y
(N âˆ’ mk )!(mk âˆ’ 1)! (|a| âˆ’ 1)!(N âˆ’ |a|)!
R
N!
N!

k=1

KÎ³ +1

Î±KÎ³ +1

mKÎ³ +1 =|Î±|

=

exp (âˆ’Î±HN )

H

Î³
Î±Î h=1
Kh !

exp (âˆ’Î±HN )

Y (N âˆ’ mk )!(mk âˆ’ 1)!
R
N!

k=1

KÏ
Y
(N âˆ’ mk )!(mk âˆ’ 1)! R
exp (âˆ’Î±HN )
=
ra
HÎ³
N!
Î±
Kh !
rÎ± Î h=1
k=1

Î± KÏ

rÎ± =KÎ±

=

Î± KÏ
H

Ï
Î h=1
Kh !

exp (âˆ’Î±HN )

KÏ
Y
(N âˆ’ mk )!(mk âˆ’ 1)! R
ra
N!
Î±

k=1

= g(Ï; Î±)qÏÎ³

(6)

where g(Î³; Î±) is the probability of a feature allocation Î³
under the IBP as defined in Equation (1), qÎ³Ï is the transition rate from state Î³ to state Ï, HÎ³ , HÏ are the number of
distinct features in states Î³ and Ï respectively and ra is the
multiplicity (the times the feature is present at the current
feature allocation) of feature a that dies. Detailed balance
holds, and as such the process is reversible and the equilibrium distribution is IBP[N ] (Î±).
Assume that (z(t)) is a realization of the BDFP (Z(t))
over the finite interval [0, T ], T > 0 and we write
(z(t))0â‰¤tâ‰¤T . With probability one the sample path
(z(t))0â‰¤tâ‰¤T will only contain a finite number of jump
events, each of which is either a birth or a death event. We
write B and Q to denote the set of the features created or
turned off by birth or death events respectively.
Proposition 1. Writing q(t) = qz(t) to denote the total
transition rate out of state z(t), the probability of a realization (z(t)) under the law of the BDFP is:
 Z T

Î±Aâˆ’|B|âˆ’|Q|
R|B|+|Q| QAâˆ— âˆ’|B âˆ— |
exp (âˆ’Î±HN ) exp âˆ’
q(t)dt Ã— . . .
0
Kh !
h=1
Y
(|b| âˆ’ 1)!(N âˆ’ |b|)! Y
rd
N!
dâˆˆD

bâˆˆBâˆª{z(t=0)}

(7)

where A = K0 + |B| = KT + |Q|, Aâˆ— = H0 + |B âˆ— | =
HT + |Qâˆ— |. B âˆ— , Qâˆ— are the sets of features with zero multiplicity at their creation time or with multiplicity of one at
their death time respectively, and {z(t)} denotes the set of
features at time t.
4.1. Dependent Beta Process Construction
The BDFP process can be constructed using a nonhomogenous Poisson process Î . Consider the LeÌvy measure Î½(dÏ‰dxdtb dtÏ‰ ) on a product space [0, 1] âŠ— X âŠ— R âŠ—
[0, âˆ). A sample corresponds to set of points Î  =
{Ï‰k , xk , tkb , tkÏ‰ }k where the range of k is countably infinite.
Each atom corresponds to a feature and is associated with
a weight Ï‰k âˆˆ [0, 1], a location xk , a birth time tkb âˆˆ R
and a life-span tkÏ‰ âˆˆ [0, âˆ) (Figure 1). The LeÌvy measure
is of the form Î½(dÏ‰dxdtb dtÏ‰ ) = Ï(dÏ‰)Âµ(dxdtb dtÏ‰ ) and

A Birth-Death Process for Feature Allocation

Ï‰
â€“1

and the feature potential matrix S, i.e. Znk (t) = 1 iff
Snk = 1 and tkb < t < tkb + tkÏ‰ .

tb

x
Figure 1. Cartoon for the dependent Beta process construction of
the BDFP: a realisation of a Poisson process Î  over the product
space [0, 1] âŠ— X âŠ— R âŠ— [0, âˆ) is drawn. The tÏ‰ dimension over
the space [0, âˆ) is omitted in the axis representation. However,
each tÏ‰ corresponding to each point (feature) is drawn as a blue
line of length tÏ‰ starting at the associated birth time point.

corresponds to a Beta process on the combined space Î˜ =
X âŠ— R âŠ— [0, âˆ) with Ï(dÏ‰) = Î±Ï‰ âˆ’1 (1 âˆ’ Ï‰)Î±âˆ’1 and base
measure Âµ(dÎ¸) = Âµ(dxdtb dtw ). Setting g(dtb ) = dtb and
Î²(dtÏ‰ ) = D expâˆ’DtÏ‰ dtÏ‰ , the base measure is Âµ(dÎ¸) =
Âµ0 (dx)g(dtb )Î²(dtÏ‰ ) = Âµ0 (dx)dtb D expâˆ’DtÏ‰ dtÏ‰ , where
D is the death rate. The constant measure g(dtb ) over the
real line R is infinite but Ïƒ-finite, that is the total measure
g(R) = âˆ, but there is a measurable partition (Ek ) of R
with each g(Ek )R < âˆ.
R Since Î½(dÏ‰dÎ¸) integrates to infinity but satisfies [0,1] Î˜ (1 âˆ§ |Ï‰|)Î½(dÏ‰dÎ¸) < âˆ, a countably infinite number of i.i.d. random points P
{(Ï‰k , Î¸ k )}âˆ
k=1
âˆ
are obtained from the Poisson process and k=1 Ï‰k is finite with probability one. A Beta process is a completely
random measure (Kingman,P1967) and, as such, a sample
âˆ
can be expressed as B =
k=1 Ï‰k Î´Î¸ k |Î±, Âµ âˆ¼ BP(Î±Âµ),
where the atoms Î¸ k = {xk , tkb , tkÏ‰ } âˆˆ Î˜ and weights
Ï‰k âˆˆ [0, 1].
Having drawn a sample B we can construct the feature allocations over an index space R as follows:
B=

âˆ
X

Ï‰k Î´ Î¸ k

k=1
âˆ
X

Sn: =

bnk Î´Î¸

Proposition 2. The BDFP is exchangeable and the Beta
process BP(Î±Âµ) on XâŠ—RâŠ—[0, âˆ) describes its underlying
mixing measure.
Proof. Consider a sequence of variables (zn (t))T with
n = 1, 2, . . . , N such that each (zn (t))T is the feature allocation evolution of object n over the index space T. These
variables are not independent since each (zn (t))T depends
on the Z|[nâˆ’1] (t) = (z1:(nâˆ’1) (t))T . However, given a sample from the B âˆ¼ BP(Î±Âµ) described in Section 4.1, each
variable (zn (t))T becomes conditionally independent and
the following holds
Z N
P ((z1 (t))T , (z2 (t))T , . . . , (zN (t))T ) =

|B âˆ¼ BeP(Ï‰k )

k=1

(8)

with bnk |Ï‰k âˆ¼ Bernoulli(Ï‰k ) and n = 1, . . . , N . The binary matrix S of dimension N Ã— K, is a feature potential
matrix. Each binary element Snk indicates whether object
n possesses feature fk . S is a global variable and doesnâ€™t
depend on time t. At any time t, the feature allocation matrix Z(t) is a deterministic function of the current features
present at t, that is {fk : tkb < t < tkb + tkw , k = 1, . . . , âˆ}

Y

P ((zn (t))T |B)Ï†(dB)

n=1

(9)
where Ï† = BP(Î±Âµ).
Equation (9) is the de Finetti representation of the BDFP
and as such the BDFP is exchangeable and the BP on
Î˜ = X âŠ— R âŠ— [0, âˆ) is its underlying mixing measure.
Restricting our focus on each index t, the overall Beta process BP(Î±Âµ) on X âŠ— R âŠ— [0, âˆ) results in a set of dependent random measures over X, one Bt for each t âˆˆ T, such
that each Bt is marginally a Beta process. Consider a fixed
time point t âˆˆ T and the space [0, 1] âŠ— X (the red vertical
plane in Figure 1). The point process on this plane (where
blue lines intersect the plane) corresponds to features alive
at time t, i.e. t âˆˆ [tb , tb + tÏ‰ ]. The LeÌvy measure on this
plane, is calculated by projecting the overall LeÌvy measure
onto the plane,
Z âˆZ t
Î½t (dÏ‰dx) =
Î½(dÏ‰dxdtb dtÏ‰ )
0

= Î±Ï‰

|Î±, Âµ âˆ¼ BP(Î±Âµ)

Znk (t) = Snk I(tkb < t < tkb + tkÏ‰ )

The resulting feature allocation process (zn (t))T is equivalent to the following: every time a new feature fk is created,
each object n joins with probability Ï‰k , i.e. znk (tkb )|Ï‰k âˆ¼
Bernoulli(Ï‰k ). If znk (tkb ) = 1, object n will possess feature fk until tkb + tkÏ‰ . Repeat this process for all objects.

tâˆ’tÏ‰
âˆ’1

(1 âˆ’ Ï‰)Î±âˆ’1

Âµ0 (dx)
D

(10)

where Î½t is a measure over [0, 1] âŠ— X for a specific t âˆˆ T.
More specifically, it is the LeÌvy measure of a Beta process
on X with Ï(dÏ‰) = Î±Ï‰ âˆ’1 (1 âˆ’ Ï‰)Î±âˆ’1 and base measure
(dx)
Âµt (dx) = Âµ0 D
. Thus we have that marginally
Bt |Î±, Âµt âˆ¼ BP(Î±Âµt ), âˆ€t âˆˆ T.
(11)
The restricted and projected measure at any index t âˆˆ T defines a Beta process. Two draws, Bt and Bs , with t, s âˆˆ T,
will be dependent with the amount of dependence decreasing as |s âˆ’ t| increases.
Proposition 3. The dependent Beta process construction
presented has IBP marginals at any t.

A Birth-Death Process for Feature Allocation

Proof. At any t âˆˆ T, Bt |Î±, Âµt âˆ¼ BP(Î±Âµt ). It is straightforward to see that, marginally, the feature allocation matrix Zt obtained using the generative process in Equation (8) is equivalent to Zt |Bt âˆ¼ BeP(Bt ) and therefore
Zt âˆ¼ IBP(Î±), âˆ€t âˆˆ T.
Corollary 1. At any t âˆˆ T, the feature allocation matrix
Zt can be generated by the following generative model as
K â†’ âˆ:
 
R
, Znk |Ï‰k âˆ¼ Bernoulli(Ï‰k ) (12)
Ï‰k |Î± âˆ¼ Beta
K
for k = 1, . . . , K and n = 1, . . . , N
The proof of the corollary in included in the supplementary
material. Note that the above is true only marginally, i.e.
at time t âˆˆ T and it doesnâ€™t generste dependence structure
between Zt â€™s.
We underline the dependence of Zs and Zt when |s âˆ’ t| â†’
0, âˆ€s, t âˆˆ T. The closer s, t are, the more the atoms
(features) Bs and Bt share. If we independently sampled
Zs |Bs âˆ¼ BeP(Bs ) and Zt |Bt âˆ¼ BeP(Bt ) then Zs , Zt
would be dependent, but not equal, even as |s âˆ’ t| â†’ 0.
However, in the BDFP the presence of the same features results in the same (not just similar) allocation as |s âˆ’ t| â†’ 0.
In both cases, the marginal distribution of the feature allocation matrix at any t âˆˆ T is Zt |Bt âˆ¼ BeP(Bt ) and
Zt |Î± âˆ¼ IBP(Î±). The BDFP results in a continuous evolud

tion of the Z(t) over T: formally Zt â†’ Zs as t â†’ s.
This construction of the BDFP resembles the spatial normalised Gamma process (SNÎ“P) by (Rao & Teh, 2009).
The main difference lies in the marginal distribution; the
SNÎ“P admits DP marginals as opposed to the Beta process
marginals of the dependent Beta process as shown in Equation (11).
Proposition 4. The feature allocation process described
by Equation (8) with B âˆ¼ BP(Î±Âµ), has the same birth and
death rates as the BDF process.

5. Finite Model
For the BDFP, the inference simplifies considerably if we
consider a finite approximation which gives the countably infinite model in the limit. Consider the space
S = [0, 1] âŠ— X âŠ— [0, T ] âŠ— [0, âˆ), where we restrict the
space of tb to be [0, T ] instead of the whole real line
R. This accounts for typical applications of the model
where we observe data at distinct times over a finite
time range. Consider the LeÌvy measure Î½(dÏ‰dxdtb dtÏ‰ )
on the space S. Then, under the dependent Beta process representation (see section 4.1),
R the expected number of atoms present in S is S Î½(dÏ‰dxdtb dtÏ‰ ) =
R1
R
RT
Râˆ
Ï(dÏ‰) X Âµ0 (dx) 0 g(dtb ) 0 Î²(dtÏ‰ ) = KT , where
0

F EATURES P RESENT
fk
0

tfb k tfdk

t t0

T

Figure 2. Cartoon for the Beta event construction of the BDFP:
A Poisson(KT ) number of features are uniformly distributed
across the time range [0, T ] (blue lines). Each feature is assigned
R
a weight sampled from Beta( K
, 1). The leftmost point of each
line corresponds to the time of birth of that feature, while the
length of each line indicates the life span of each feature sampled from Exponential(D). To sample feature allocations from
the process, we consider random time points across time, e.g. t, t0
and draw imaginary red lines. The feature allocation matrix at t
involves the features that are crossing the red line at t. The membership of the objects n = 1, . . . , N to those features is defined
by the values of the corresponding elemnts in the potential matrix
S.

R1
K â†’ âˆ since 0 Ï(dÏ‰) = âˆ. By considering finite K
we allow inference on a finite model which approximates
the infinite case with increasing fidelity as K â†’ âˆ.
The process is depicted in Figure 2 and the infinite case can
be derived as the limit K â†’ âˆ of the following:
â€¢ Consider a time range [0, T ] and a set of features F,
such that |F| âˆ¼ Poisson(KT ). Assign to each feature
fk âˆˆ F, k = 1, . . . |F| a weight Ï‰, such that Ï‰k âˆ¼
R
Beta K
, 1 and â„¦ = [Ï‰1 , Ï‰2 . . . Ï‰|F | ].
â€¢ Associate each feature fk âˆˆ F, k = 1, . . . |F| with
a birth time tkb uniformly sampled in [0, T ]; tkb âˆ¼
|F |
U(0, T ) and tb = [t1b . . . tb ].
â€¢ For each fk âˆˆ F, sample its life span tkw âˆ¼
Exponential(D), where D is the death rate. Define
the time of death tkd as tkd = tkb + tkw and tw =
|F |
[t1w . . . tw ].
We call the sequence of the above steps Beta Event Process (BEP). Putting everything together, generate a sample
B = {F, â„¦, tb , tw } âˆ¼ BEP(Î±, R, K, T ) as follows:
|F| âˆ¼ Poisson(KT )

R
Ï‰k âˆ¼ Beta
, 1 , tkb âˆ¼ U(0, T ), tkÏ‰ âˆ¼ Exponential(D)
K
(13)


for k = 1, . . . , |F|. Having drawn a sample B from the
BEP, we can construct the feature allocations over time as
follows
Snk |Ï‰k âˆ¼ Bernoulli(Ï‰k )
Znk (t) = Snk I(tkb < t < tkb + tkÏ‰ )

(14)

A Birth-Death Process for Feature Allocation

where n = 1, . . . , N . The feature potential matrix (as defined in section 4.1) has now N Ã— |F | dimensions. Moreover, each Z(t) for t âˆˆ T is a matrix of dimensions
N Ã— F (t) and F (t) â‰¤ |F |. Figure 3(a) show the graphical model for the BEP.
Î±
tw

tb

Ïƒ

A

R
Ï‰

Yt

S

Zt

t

(a)
tb
Î±
tw

s

Wt

In the case of dynamic binary network data we extend
the latent feature relational model (LFRM) proposed by
(Miller et al., 2009). Let Yt be the N Ã— N binary matrix
that contains links, i.e. ytij = Yt (i, j) = 1 iff we observe
a link from entity i to entity j at time t. We assume that the
matrices Yt are symmetric and ignore diagonal elements
(self-links). The probability of a link from one entity to another is determined by the combined effect of all pairwise
feature interactions. Let Wt be a |F| Ã— |F| real-valued
weight matrix where Wt (k, k 0 ) is the weight that affects
the probability of there being a link from entity i to entity
j if entity i has feature k on, i.e. Ztik = Zt (i, k) = 1 and
entity j has feature k 0 on, i.e. Ztjk0 = Zt (j, k 0 ) = 1. The
links are independent conditioned on Zt and Wt , and only
the features that are on for the entities i and j at time t influence the probability of a link between those entities at
that time (see Figure 3(b)). Formally,
P (ytij = 1|Zt , Wt ) = Ïƒ

R

X


Ztik Ztjl Wtkl + s (16)

kl

Ï‰

for k, l = 1, . . . , |F|, where s is a bias term and Ïƒ(x) =
(1 + eâˆ’x )âˆ’1 is the sigmoid function. For completeness,

2
and s âˆ¼
we assume
the
priors
w
(k,
l)
âˆ¼
N
Âµ
,
Ïƒ
t
w
w

2
N Âµs , Ïƒs .

Yt

S

Zt

t

(b)
Figure 3. Graphical representation of the BEP for a time point t
and for (a) a linear-Gaussian likelihood and (b) a sigmoid likelihood. The time series Z and Y are represented as single nodes
indexed by the time location t. The birth and life span times of
the total KT features are depicted using vector notation tb and
tw . The black (Zt ) and grey (Yt ) nodes indicate deterministic
and observed parameters respectively.

Proposition 5. In the finite model, the expected number
of features present at any t âˆˆ T is E[Nf ] = K
D and for
KÎ±
D= R
we
have
E[N
]
=
.
f
Î±
R
Hyperpriors. We put gamma priors on Î± and R.
Likelihood models. We consider two different likelihood
models: linear-Gaussian for real data and logistic for binary network data.
For the linear-Gaussian likelihood model, consider a sequence of observations {Yt âˆˆ Y : t = 1, . . . , L} generated
as
Y t = Z t A + t

(15)

where Yt is a N Ã— M observation matrix at each time t =
1, . . . , L, A is a factor loading matrix of dimension |F| Ã—
M shared across time and t âˆ¼ N 0, Ïƒ2 is Gaussian white
noise. We choose a Gaussian prior over A, i.e Af m âˆ¼
N (0, 1).

5.1. Inference
As with many other Bayesian models, exact inference
is intractable so we employ Markov Chain Monte Carlo
(MCMC) for posterior inference over the latent variables
of the finite model. A detailed description is provided in
the supplementary material.

6. Experiments
We experimentally evaluate the BEP model on real-world
genomics and social network data. To evaluate the model
fit, we compared the BEP model to independent models at
each time point.
6.1. Circadian Rhythm Dataset
Here we used a subset of the gene expression data from
Piechota et al. (2010), including N = 500 genes in D = 4
different conditions (exposure to different drugs) over L =
24 time intervals. The measurements indicate how active
a gene is at different times. We created 7 train-test splits
holding out 20% of the data, and ran 700 MCMC iterations. We see that in terms of predictive performance the
BEP outperforms independent IBP models (Table 1). The
genes belonging to each factor show enrichment for different known biological pathways (Figure 4). Of particular
note are the tryptophan metabolism genes enriched in factor 2, given tryptophanâ€™s suspected effects on drowsiness;

A Birth-Death Process for Feature Allocation

the vasopressin regulated water reabsorption, given this
hormoneâ€™s known circadian regulation (Earnest & Sladek,
1986; Yamaguchi et al., 2013); and the regulation of insulin producing beta cells, another hormone with circadian
variation (Shi et al., 2013).
Table 1. Circadian dataset results using 20% held out data, a truncation level of K = 10, |F| = 24, 700 iterations and a burnin of
500. Results are the average over 7 MCMC chains.

BEP
T RAIN ERROR
T EST ERROR
T RAIN LOG LIKELIHOOD
T EST LOG LIKELIHOOD

INDEPENDENT

0.0917 Â± 0.0368
0.0948 Â± 0.0343
6.508 Â± 0.7715
1.5661 Â± 0.1583

IBP

0.0983 Â± 0.0012
1.3380 Â± 0.5155
6.6871 Â± 0.0217
âˆ’8.6861 Â± 4.0670

Kegg Tryptophan Metabolism
Kegg Ribosome
Kegg Ppar Signaling Pathway
Kegg Vascular Smooth Muscle Contraction
Kegg Vasopressin Regulated Water Reabsorption
Reactome Formation Of A Pool Of Free 40s Subunits
Reactome Formation Of The Ternary Complex And Subsequently The 43s Complex
Reactome Influenza Viral Rna Transcription And Replication
Reactome Muscle Contraction
Reactome Nuclear Receptor Transcription Pathway
Reactome Regulation Of Beta Cell Development
Reactome Regulation Of Gene Expression In Beta Cells
Reactome Regulation Of Lipid Metabolism By Peroxisome Proliferator Activated Receptor Alpha
Reactome Translation Initiation Complex Formation
Reactome Viral Mrna Translation
Reactome Smooth Muscle Contraction

3
2.5
2
1.5
1
0.5

5

10

15

20

0

feature index

Figure 4. Circadian dataset: many of the features uncovered show
enrichment in known biological pathways from Reactome and
KEGG. Values here are âˆ’ log10 p from a hypergeometric test for
enrichment of the genes in each factor against the 500 background
genes.

6.2. ChIP-seq Epigenetic Marks
For this experiment we used ChIP-seq (chromatin immunoprecipitation sequencing) data downloaded from the ENCODE project (Consortium, 2007), representing histone
modifications and transcription factor binding in human
neural crest cell lines (see (Park, 2009) for a nice review).
The observations involve counts associated with N = 14
(human) cell lines and D = 10 proteins. The counts indicate what proteins, with what chemical modifications, are
bound to DNA along the genome. The measurements are
stored in N Ã— D matrix of counts Yt : for each cell line,
how many reads for each of the 10 proteins mapped to bin t
(100 base pair (bp) region of the genome). t = 1, . . . , 500
bins were considered at the start of chromosome 1 (50K
bp in total). In Figure 5(a) each subfigure corresponds to
one of the 10 proteins and in each subfigure the counts for
the N = 14 cell lines are plotted over the genome section of length 50Kbp. Before inference, the raw counts
were square-root transformed (a standard variance stabilizing transform for Poisson data) to make the Gaussian
likelihood appropriate. We ran 7 different held-out tests,
holding out a different 20% of the data each time. Results,
using 700 MCMC iterations, are presented in Table 2. The

BEP outperforms the independent IBP model in both test
likelihood and error with a statistically significant difference. The independent IBP appears to have better results
in train error and likelihood, again suggesting overfitting.
Comparing the plots of the true measurements to the learnt
ones by the BEP and independent IBP model in Figure 5
we see that both models successfully reproduce the data
but the BEP reconstructions provide a cleaned up picture
of the meaningful signal.
The features found by the model in the different genome
locations correspond to different states associated with the
specific genome location. Genes and regulatory DNA elements such as enhancers, silencers and insulators are embedded in genomes. These genomic elements on the DNA
have footprints for the transacting proteins involved in transcription, either for the positioning or regulation of the transcriptional machinery. For instance, promoters are regions
of DNA which recruit proteins required to initiate transcription of a particular gene and located near the transcription
start sites. Enhancers are regions of DNA that can be bound
by proteins which activate transcription of a distal gene.
So a cell line, at specific genome location (recall that here
each location corresponds to 100 base pairs), will have underlying feature membership (some promoters and some
enhancer for example) that determines whether particular
protein are found there using ChIP-seq.
Genomic annotations, from ChromHMM (Ernst et al.,
2011), are shown in Figure 8 in the supplementary document for the region we model. Different levels of the marks
in these different regions are much easier to see in the reconstructed signal using BEP in Figure 5(b).
Table 2. Quantitative results for the ChIP-seq dataset . 20% held
out data, a truncation level of K = 3, |F| = 21, 700 iterations
and a burnin of 500. Results are the average over 7 held out sets.

BEP
T RAIN ERROR
T EST ERROR
T RAIN LOG LIKELIHOOD
T EST LOG LIKELIHOOD

0.4459 Â± 0.0229
0.4574 Â± 0.018
âˆ’12.4979 Â± 0.1439
âˆ’3.1666 Â± 0.0318

INDEPENDENT

IBP

0.032 Â± 0.0089
0.7746 Â± 0.013
âˆ’0.5916 Â± 0.0979
âˆ’175.7968 Â± 4.49

7. van de Buntâ€™s Dataset
In van de Bunt et al. (1999), 32 university freshman students in a given discipline at a Dutch university were surveyed at seven time points about who in their class they
considered as friends. Initially, i.e. t1 , most of the students
were unknown to each other. The first four time points are
three weeks apart, whereas the last three time points are six
weeks apart as showin in Figure 11 in the supplementary
matrial. We symmetrise the matrix by assuming friendship
if either individual reported it. We test the performance
of BEP using the sigmoid likelihood model as in Equation

A Birth-Death Process for Feature Allocation
H3K27acâˆ’human

H3K27me3âˆ’human

60

15

40

10

20

5

0

0
0

100

200

300

400

500

0

100

200

H3K36me3âˆ’human

300

400

500

5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

30

30

30

H3K4me1âˆ’human

20

30
20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

25

30

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

10
10
0

0
0

100

200

300

400

500

0

100

200

H3K4me2âˆ’human

300

400

500

H3K4me3âˆ’human

60

60

40

40

20

20

5

5

5

10

10

10

15

15

15

20

20

20

25

25

30

30

2

0

4

6

8

10

12

14

16

18

25

30

20

2

4

6

8

10

12

14

16

18

20

0
0

100

200

300

400

500

0

100

200

H3K79me2âˆ’human

300

400

500

H3K9acâˆ’human

30

60

20

40

10

20

0

0
0

100

200

300

400

500

0

100

200

H3K9me3âˆ’human

300

400

5

5

5

10

10

10

15

15

15

15

20

20

20

20

25

25

25

30

30

30

500

H4K20me1âˆ’human

15

5

10

15
0.5

10

10

5

5

0

1

1.5

2

100

200

300

400

500

0

100

200

300

400

H3K27me3âˆ’human

H3K27acâˆ’human

10

10

4

5

6

7

8

9

10

25

30

0.5

5

5

10

10

10

15

15

15

20

20

20

25

25

30

30

1

H3K27acâˆ’human

3

1

1.5

2

2.5

3

3.5

1

2

3

4

5

6

7

8

500

(a)
20

2

5

0
0

1

2.5

2

3

4

5

6

0.5

25

30

1

1.5

2

2.5

3

3.5

0.5

1

1.5

2

2.5

3

3.5

4

4.5

H3K27me3âˆ’human

40

20

20

10

5
0

0

âˆ’10

0
0

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

0

10

10

10

20

5

0

0
0

100

200

300

400

500

100

200

300

400

500

20

20

10

400

500

400

500

Figure 6. Inferred feature allocation matrices for the seven time
points (from left to right) in the van De Bunt friendship dataset.
First two rows: Feature allocation matrices inferred by BEP.
Last two rows: Feature allocation matrices inferred by independent LFRM.

400

500

8. Discussion

400

500

âˆ’20
0

100

H3K4me3âˆ’human
40

300

0

âˆ’10
0

H3K4me2âˆ’human

200

H3K4me1âˆ’human
40

0

100

H3K36me3âˆ’human
20

5

500

âˆ’10
0

H3K4me1âˆ’human
15

30

400

0

âˆ’20
0

H3K36me3âˆ’human
15

200

300

400

500

0

100

H3K4me2âˆ’human

200

300

H3K4me3âˆ’human

50

50

0

0

0

0

âˆ’20
0

100

200

300

400

500

âˆ’50
0

100

H3K79me2âˆ’human

200

300

400

500

âˆ’50
0

100

H3K9acâˆ’human

200

300

400

500

0

100

H3K79me2âˆ’human

15

30

40

10

20

20

5

10

0

200

300

H3K9acâˆ’human
50

0

0

0
0

100

200

300

400

500

âˆ’20
0

100

H3K9me3âˆ’human

200

300

400

500

âˆ’50
0

100

H4K20me1âˆ’human

8

200

300

400

500

0

100

H3K9me3âˆ’human

10

6

200

300

H4K20me1âˆ’human

20

20

10

10

5
4

0

2

0
0

100

200

300

400

500

0

âˆ’10
0

100

200

300

400

500

âˆ’10
0

100

200

300

(b)

400

500

0

100

200

300

(c)

Figure 5. ChIP-seq data: The observed (a) and reconstructed observations (b), (c). The BEP reconstructions smooth out the noise
making the meaning signal much easier to visualize. In both models, the noise signal was removed from the reconstructions.

(16) by holding out 10% of all links across all time points.
We ran each model for 1000 MCMC iterations. The results are shown in Table 3. The independent network LFR
models outperform BEP in the train setting and the test error while BEP outperforms in the test likelihood. However, here the results are comparable. Looking at Figure
6, both models provide the same picture of the allocation.
It is possible the stationary assumption hurts the BEP: in
the VDB dataset the number of links almost exclusively increases over time.

Table 3. van de Buntâ€™s dataset results using 10% held out data, a
truncation level of K = 4, |F| = 20, 1000 iterations and a burnin
of 200. Results are the average over 7 MCMC chains.
BEP
T RAIN ERROR
T EST ERROR
T RAIN LOG LIKELIHOOD
T EST LOG LIKELIHOOD

1.7009 Â± 0.0850
1.9107 Â± 0.1321
âˆ’1044.4943 Â± 41.6363
âˆ’345.7038 Â± 49.9882

INDEPENDENT

LFRM

1.3413 Â± 0.1147
1.7891 Â± 0.1131
âˆ’839.4544 Â± 56.9877
âˆ’438.5848 Â± 74.6396

Many modern machine learning and statistics tasks involve
multidimensional data positioned along some linear covariate: we have shown functional genomics data where the covariate is position in the genome, and network data where
links change over time. To model such data we need priors that utilize the dependencies through time, while handling high dimensionality. The BDFP is an expressive new
Bayesian non-parametric prior that fulfills these criteria. It
outputs time-evolving feature allocations, which can then
be effectively used to model high-dimensional time-series
data. Since the number of latent features is unbounded,
like other Bayesian non-parametric methods, the model
can adapt its complexity to the data. While the combinatorial BDFP may seem like a complex object to handle
computationally, our theoretical results showing that the de
Finetti measure underlying the BDFP is a specific beta process, which can be well approximated by a finite K model,
the BEP. Our experimental results, compared to independent feature allocations, provides evidence that effectively
modeling dependency in the feature allocation through the
birth-death mechanism is appropriate for a wide range of
statistical applications. Moreover, the BEP provides an interpretable structure using parameters not found, to the best
of our knowledge, in existing models, i.e. birth and death
rate of features. We are interested in scaling inference under the BEP to larger datasets, for example using (stochastic) variational inference methods that have been successful
for the IBP (Doshi et al., 2009).
Acknowledgements Konstantinaâ€™s research leading to these
results has received funding from the European Research Council under the European Unionâ€™s Seventh Framework Programme
(FP7/2007-2013) ERC grant agreement no. 617411.

A Birth-Death Process for Feature Allocation

References
Aldous, D J. Exchangeability and related topics. In Ecole dâ€™Ete de
Probabilities de Saint-Flour, volume XIII, pp. 1â€“198. Springer,
1983.
Consortium, The ENCODE Project. Identification and analysis
of functional elements in 1% of the human genome by the ENCODE pilot project. Nature, 447(7146):799â€“816, 06 2007.
de Finetti, B. Funzione Caratteristica Di un Fenomeno Aleatorio,
pp. 251â€“299. 6. Memorie. Academia Nazionale del Linceo,
1931.
Doshi, F., Miller, K. T., Van Gael, J., and Teh, Y. W. Variational
inference for the Indian buffet process. In Proceedings of the
International Conference on Artificial Intelligence and Statistics, volume 12, 2009.
Earnest, David J and Sladek, Celia D. Circadian rhythms of vasopressin release from individual rat suprachiasmatic explants in
vitro. Brain research, 382(1):129â€“133, 1986.
Ernst, Jason, Kheradpour, Pouya, Mikkelsen, Tarjei S, Shoresh,
Noam, Ward, Lucas D, Epstein, Charles B, Zhang, Xiaolan,
Wang, Li, Issner, Robbyn, Coyne, Michael, et al. Mapping and
analysis of chromatin state dynamics in nine human cell types.
Nature, 473(7345):43â€“49, 2011.
Ghahramani, Zoubin and Jordan, Michael. Factorial hidden
markov models. Machine Learning, 29(2-3):245â€“273, 1997.
Griffiths, Thomas L. and Ghahramani, Zoubin. Infinite latent feature models and the indian buffet process. In In NIPS, pp. 475â€“
482. MIT Press, 2005.
Griffiths, Thomas L. and Ghahramani, Zoubin. The indian buffet process: An introduction and review. Journal of Machine
Learning Research, 12:1185â€“1224, July 2011.
Hjort, N. L. Nonparametric Bayes estimators based on Beta processes in models for life history data. Annals of Statistics, 18:
1259â€“1294, 1990.
Kingman, J.F.C. The coalescent. Stochastic Processes and their
Applications, 13(3):235 â€“ 248, 1982.
Kingman, John F. C. Completely Random Measures. Pacific Journal of Mathematics, 21(1):59â€“78, 1967.
Miller, Kurt, Griffiths, Thomas, and Jordan, Michael. Nonparametric latent feature models for link prediction. In Advances in
Neural Information Processing Systems 22, 2009.
Neal, Radford M. Density Modeling and Clustering Using Dirichlet Diffusion Trees. In Bayesian Statistics 7, pp. 619â€“629,
2003.
Palla, Konstantina, Knowles, David A., and Ghahramani, Zoubin.
A dependent partition-valued process for multitask clustering
and time evolving network modelling, 2013.
Park, Peter J. Chipâ€“seq: advantages and challenges of a maturing
technology. Nature Reviews Genetics, 10(10):669â€“680, 2009.
Piechota, Marcin, Korostynski, Michal, Solecki, Wojciech,
Gieryk, Agnieszka, Slezak, Michal, Bilecki, Wiktor, Ziolkowska, Barbara, Kostrzewa, Elzbieta, Cymerman, Iwona,
Swiech, Lukasz, et al. The dissection of transcriptional modules regulated by various drugs of abuse in the mouse striatum.
Genome Biology, 11(5):R48, 2010.

Rao, Vinayak and Teh, Yee Whye. Spatial normalized gamma
processes. 2009.
Rasmussen, Carl Edward and Williams, Christopher K I. Gaussian processes for machine learning. MIT Press, 2006.
Shi, Shu-qun, Ansari, Tasneem S, McGuinness, Owen P, Wasserman, David H, and Johnson, Carl Hirschie. Circadian disruption leads to insulin resistance and obesity. Current Biology,
23(5):372â€“381, 2013.
Teh, Y. W., Elliott, L. T., and Blundell, C. Bayesian nonparametric modelling of genetic variations using fragmentationcoagulation processes. Submitted, 2013.
Thibaux, Romain and Jordan, Michael I. Hierarchical beta processes and the indian buffet process. In Proceedings of the
Eleventh International Conference on Artificial Intelligence
and Statistics (AISTATS), volume 2, pp. 564â€“571, 2007.
van de Bunt, Gerhard G, Van Duijn, Marijtje AJ, and Snijders,
Tom AB. Friendship networks through time: An actor-oriented
dynamic statistical network model. Computational & Mathematical Organization Theory, 5:167â€“192, 1999.
Van Gael, J., Teh, Y. W., and Ghahramani, Z. The infinite factorial hidden Markov model. In Advances in Neural Information
Processing Systems, volume 21, 2009.
Williamson, Sinead, Orbanz, Peter, and Ghahramani, Zoubin. Dependent Indian buffet processes. In Proceedings of the Thirteenth International Workshop on Artificial Intelligence and
Statistics, AISTATS, 2010.
Yamaguchi, Yoshiaki, Suzuki, Toru, Mizoro, Yasutaka, Kori, Hiroshi, Okada, Kazuki, Chen, Yulin, Fustin, Jean-Michel, Yamazaki, Fumiyoshi, Mizuguchi, Naoki, Zhang, Jing, et al.
Mice genetically deficient in vasopressin v1a and v1b receptors are resistant to jet lag. Science, 342(6154):85â€“90, 2013.

