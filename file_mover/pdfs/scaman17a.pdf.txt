Optimal Algorithms for Smooth and Strongly Convex
Distributed Optimization in Networks

Kevin Scaman 1 Francis Bach 2 Sébastien Bubeck 3 Yin Tat Lee 3 Laurent Massoulié 1

Abstract
In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a
network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov’s
accelerated gradient descent is optimal and
√
achieves a precision ε > 0 in time O( κg (1 +
∆τ ) ln(1/ε)), where κg is the condition number of the (global) function to optimize, ∆ is
the diameter of the network, and τ (resp. 1)
is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based
on gossip, we provide the first optimal algorithm,
called the multi-step dual accelerated (MSDA)
method, that achieves a precision ε > 0 in time
√
O( κl (1 + √τγ ) ln(1/ε)), where κl is the condition number of the local functions and γ is the
(normalized) eigengap of the gossip matrix used
for communication between nodes. We then verify the efficiency of MSDA against state-of-theart methods for two problems: least-squares regression and classification by logistic regression.

1. Introduction
Given the numerous applications of distributed optimization in machine learning, many algorithms have recently
emerged, that allow the minimization
of objective functions
Pn
f defined as the average n1 i=1 fi of functions fi which
are respectively accessible by separate nodes in a network
(Nedic & Ozdaglar, 2009; Boyd et al., 2011; Duchi et al.,
2012; Shi et al., 2015). These algorithms typically alter1
MSR-INRIA Joint Center, Palaiseau, France 2 INRIA, Ecole
Normale Supérieure, Paris, France 3 Theory group, Microsoft Research, Redmond, United States. Correspondence to: Kevin Scaman <kevin.scaman@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

nate local incremental improvement steps (such as gradient
steps) with communication steps between nodes in the network, and come with a variety of convergence rates (see
for example Shi et al. (2014; 2015); Jakovetić et al. (2015);
Nedich et al. (2016)).
Two main regimes have been looked at: (a) centralized
where communications are precisely scheduled and (b) decentralized where communications may not exhibit a precise schedule. In this paper, we consider these two regimes
for objective functions which are smooth and stronglyconvex and for which algorithms are linearly (exponentially) convergent. The main contribution of this paper is
to propose new and matching upper and lower bounds of
complexity for this class of distributed problems.
The optimal complexity bounds depend on natural quantities in optimization and network theory. Indeed, (a) for
a single machine the optimal number of gradient steps to
optimize a function is proportional to the square root of
the condition number (Nesterov, 2004), and (b) for mean
estimation, the optimal number of communication steps is
proportional to the diameter of the network in centralized
problems or to the square root of the eigengap of the Laplacian matrix in decentralized problems (Boyd et al., 2006).
As shown in Section 3, our lower complexity bounds happen to be combinations of the two contributions above.
These lower complexity bounds are attained by two separate algorithms. In the centralized case, the trivial distribution of Nesterov’s accelerated gradient attains this rate,
while in the decentralized case, as shown in Section 4, the
rate is achieved by a dual algorithm. We compare favorably
our new optimal algorithms to existing work in Section 5.
Related Work. Decentralized optimization has been extensively studied and early methods such as decentralized gradient descent (Nedic & Ozdaglar, 2009; Jakovetić
et al., 2014) or decentralized dual averaging (Duchi et al.,
2012) exhibited sublinear convergence rates. More recently, a number of methods with provable linear convergence rates were developed, including EXTRA (Shi
et al., 2015; Mokhtari & Ribeiro, 2016), augmented Lagrangians (Jakovetić et al., 2015), and more recent approaches (Nedich et al., 2016). The most popular of

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

such approaches is the distributed alternating direction
method of multipliers (D-ADMM) (Boyd et al., 2011;
Wei & Ozdaglar, 2012; Shi et al., 2014) and has led to
a large number of variations and extensions. In a different direction, second order methods were also investigated (Mokhtari et al., 2016; Tutunov et al., 2016). However, to the best of our knowledge, the field still lacks
a coherent theoretical understanding of the optimal convergence rates and its dependency on the characteristics
of the communication network. In several related fields,
complexity lower bounds were recently investigated, including the sequential optimization of a sum of functions
(Arjevani & Shamir, 2016a;b), distributed optimization in
flat (i.e. totally connected) networks (Shamir, 2014; Arjevani & Shamir, 2015), or distributed stochastic optimization (Shamir & Srebro, 2014).

2.2. Decentralized Communication
A large body of literature considers a decentralized approach to distributed optimization based on the gossip algorithm (Boyd et al., 2006; Nedic & Ozdaglar, 2009; Duchi
et al., 2012; Wei & Ozdaglar, 2012). In such a case, communication is represented as a matrix multiplication with a
matrix W verifying the following constraints:
1. W is an n × n symmetric matrix,
2. W is positive semi-definite,
3. The kernel of W is the set of constant vectors:
Ker(W ) = Span(1), where 1 = (1, ..., 1)> ,
4. W is defined on the edges of the network: Wij 6= 0
only if i = j or (i, j) ∈ E.

2. Distributed Optimization Setting
2.1. Optimization Problem
Let G = (V, E) be a connected simple (i.e. undirected)
graph of n computing units and diameter ∆, each having
access to a function fi (θ) over θ ∈ Rd . We consider minimizing the average of the local functions
n

min f¯(θ) =

θ∈Rd

1X
fi (θ)
n i=1

(1)

in a distributed setting. More specifically, we assume that:
1. Each computing unit can compute first-order characteristics, such as the gradient of its own function or
its Fenchel conjugate. By renormalization of the time
axis, and without loss of generality, we assume that
this computation is performed in one unit of time.
2. Each computing unit can communicate values (i.e.
vectors in Rd ) to its neighbors. This communication
requires a time τ (which may be smaller or greater
than 1).

The third condition will ensure that the gossip step converges to the average of all the vectors shared between the
nodes. We will denote the matrix W as the gossip matrix,
since each communication step will be represented using
it. Note that a simple choice for the gossip matrix is the
Laplacian matrix L = D − A, whereP
A is the
 adjacency
matrix of the network and D = diag
i Aij . However,
in the presence of large degree nodes, weighted Laplacian
matrices are usually a better choice, and the problem of optimizing these weights is known as the fastest distributed
consensus averaging problem and is investigated by Xiao
& Boyd (2004); Boyd et al. (2009).
We will denote by λ1 (W ) ≥ · · · ≥ λn (W ) = 0 the spectrum of the gossip matrix W , and its (normalized) eigengap
the ratio γ(W ) = λn−1 (W )/λ1 (W ) between the second
smallest and the largest eigenvalue. Equivalently, this is
the inverse of the condition number of W projected on the
space orthogonal to the constant vector 1. This quantity
will be the main parameter describing the connectivity of
the communication network in Section 3.3 and Section 4.

3. Optimal Convergence Rates
These actions may be performed asynchronously and in
parallel, and each node i possesses a local version of the
parameter, which we refer to as θi . Moreover, we assume that each function fi is α-strongly convex and ββ
smooth, and we denote by κl = α
≥ 1 the local condition
number. We also denote by αg , βg and κg , respectively,
the strong convexity, smoothness and condition number
of the average (global) function f¯. Note that we always
have κg ≤ κl , while the opposite inequality is, in general, not true (take for example f1 (θ) = 1{θ < 0}θ2 and
f2 (θ) = 1{θ > 0}θ2 for which κl = +∞ and κg = 1).
However, the two quantities are close (resp. equal) when
the local functions are similar (resp. equal) to one another.

In this section, we prove oracle complexity lower bounds
for distributed optimization in two settings: strongly convex and smooth functions for centralized (i.e. master/slave)
and decentralized algorithms based on a gossip matrix W .
In the first setting, we show that distributing accelerated
gradient descent matches the optimal convergence rate,
while, in the second setting, the algorithm proposed in Section 4 is shown to be optimal. Note that we will use the
notation g(ε) = Ω(f (ε)) for ∃C > 0 s.t. ∀ε > 0, g(ε) ≥
Cf (ε), and will, for simplicity, omit the additive terms that
do not depend on the precision ε in Corollary 1 and Corollary 2.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

3.1. Black-box Optimization Procedures
The lower bounds provided hereafter depend on a new notion of black-box optimization procedures for the problem
in Eq. (1), where we consider distributed algorithms verifying the following constraints:
1. Local memory: each node i can store past values in
a (finite) internal memory Mi,t ⊂ Rd at time t ≥ 0.
These values can be accessed and used at time t by
the algorithm run by node i, and are updated either
by local computation or by communication (defined
below), that is, for all i ∈ {1, ..., n},
∪ Mcomm
Mi,t ⊂ Mcomp
.
i,t
i,t

(2)

2. Local computation: each node i can, at time t, compute the gradient of its local function ∇fi (θ) or its
Fenchel conjugate ∇fi∗ (θ) for a value θ ∈ Mi,t in the
node’s internal memory, that is, for all i ∈ {1, ..., n},

Arjevani & Shamir (2015), and are natural since at least
√
Ω( κg ln(1/ε)) steps are necessary to solve a strongly
convex and smooth problem up to a fixed precision, and
at least ∆ communication steps are required to transmit a
message between any given pair of nodes.
In order to simplify the proofs of the following theorems,
and following the approach of Bubeck (2015), we will
consider the limiting situation d → +∞. More specifically, we now
that we are working in `2 = {θ =
P assume
2
(θk )k∈N :
θ
<
+∞}
rather than Rd .
k k
Theorem 1. Let G be a graph of diameter ∆ > 0 and
size n > 0, and βg ≥ αg > 0. There exists n functions
fi : `2 → R such that f¯ is αg strongly convex and βg
smooth, and for any t ≥ 0 and any black-box procedure
one has, for all i ∈ {1, ..., n},
t

1+ 1+∆τ
4
αg
∗
¯
¯
kθi,0 − θ∗ k2 ,
1− √
f (θi,t ) − f (θ ) ≥
2
κg
(6)
where κ = β /α .
g

Mcomp
= Span ({θ, ∇fi (θ), ∇fi∗ (θ) : θ ∈ Mi,t−1 }) .
i,t
(3)
3. Local communication: each node i can, at time t,
share a value to all or part of its neighbors, that is, for
all i ∈ {1, ..., n},
 [

Mcomm
=
Span
M
(4)
j,t−τ .
i,t
(i,j)∈E

4. Output value: each node i must, at time t, specify one
vector in its memory as local output of the algorithm,
that is, for all i ∈ {1, ..., n},
θi,t ∈ Mi,t .

(5)

Hence, a black-box procedure will return n output values—
one for each node of the network—and our analysis will
focus on ensuring that all local output values are converging to the optimal parameter of Eq. (1). Moreover, we will
say that a black-box procedure uses a gossip matrix W if
the local communication is achieved by multiplication of a
vector with W . For simplicity, we assume that all nodes
start with the simple internal memory Mi,0 = {0}. Note
that communications and local computations may be performed in parallel and asynchronously.
3.2. Centralized Algorithms
In this section, we show that, for any black-box optimiza√
tion procedure, at least Ω( κg ln(1/ε)) gradient steps and
√
Ω(∆ κg ln(1/ε)) communication steps are necessary to
achieve a precision ε > 0, where κg is the global condition number and ∆ is the diameter of the network. These
lower bounds extend the communication complexity lower
bounds for totally connected communication networks of

g

g

The proof of Theorem 1 relies on splitting the function
used by Nesterov to prove oracle complexities for strongly
convex and smooth optimization (Nesterov, 2004; Bubeck,
2015) on two nodes at distance ∆. One can show that most
dimensions of the parameters θi,t will remain zero, and local gradient computations may only increase the number
of non-zero dimensions by one. Finally, at least ∆ communication rounds are necessary in-between every gradient computation, in order to share information between the
two nodes. The detailed proof is available as supplementary material.
Corollary 1. For any graph of diameter ∆ and any blackbox procedure, there exists functions fi such that the time
to reach a precision ε > 0 is lower bounded by

  1 
√ 
Ω
κg 1 + ∆τ ln
,
(7)
ε
This optimal convergence rate is achieved by distributing
Nesterov’s accelerated gradient descent on the global function. Computing the gradient of f¯ is performed by sending all the local gradients ∇fi to a single node (denoted
as master node) in ∆ communication steps (which may
involve several simultaneous messages), and then returning the new parameter θt+1 to every node in the network
(which requires another ∆ communication steps). In practice, summing the gradients can be distributed by computing a spanning tree (with the root as master node), and asking for each node to perform the sum of its children’s gradients before sending it to its parent. Standard methods as
described by Bertsekas & Tsitsiklis (1989) can be used for
performing this parallelization of gradient computations.
This algorithm has three limitations: first, the algorithm is
not robust to machine failures, and the central role played

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

by the master node also means that a failure of this particular machine may completely freeze the procedure. Second,
and more generally, the algorithm requires precomputing a
spanning tree, and is thus not suited to time-varying graphs,
in which the connectivity between the nodes may change
through time (e.g. in peer-to-peer networks). Finally, the
algorithm requires every node to complete its gradient computation before aggregating them on the master node, and
the efficiency of the algorithm thus depends on the slowest of all machines. Hence, in the presence of non-uniform
latency of the local computations, or the slow down of a
specific machine due to a hardware failure, the algorithm
will suffer a significant drop in performance.
3.3. Decentralized Algorithms
The gossip algorithm (Boyd et al., 2006) is a standard
method for averaging values across a network when its connectivity may vary through time. This approach was shown
to be robust against machine failures, non-uniform latencies and asynchronous or time-varying graphs, and a large
body of literature extended this algorithm to distributed optimization (Nedic & Ozdaglar, 2009; Duchi et al., 2012;
Wei & Ozdaglar, 2012; Shi et al., 2015; Jakovetić et al.,
2015; Nedich et al., 2016; Mokhtari et al., 2016).
The convergence analysis of decentralized algorithms usually relies on the spectrum of the gossip matrix W used
for communicating values in the network, and more specifically on the ratio between the second smallest and the
largest eigenvalue of W , denoted γ. In this section, we
show that, with respect to this quantity and κl , reaching a
√
precision
qε requires at least Ω( κl ln(1/ε)) gradient steps
κl
and Ω
γ ln(1/ε) communication steps, by exhibiting
a gossip matrix such that a corresponding lower bound exists.
Theorem 2. Let α, β > 0 and γ ∈ (0, 1]. There exists a
gossip matrix W of eigengap γ(W ) = γ, and α-strongly
convex and β-smooth functions fi : `2 → R such that, for
any t ≥ 0 and any black-box procedure using W one has,
for all i ∈ {1, ..., n},
3α
f¯(θi,t ) − f¯(θ∗ ) ≥
2



16
1− √
κl

1+ 1+ t√τ
5

γ

kθi,0 − θ∗ k2 ,
(8)

where κl = β/α is the local condition number.
The proof of Theorem 2 relies on the same technique as
that of Theorem 1, except that we now split the two functions on a subset of a linear graph. These networks have
√
the appreciable property that ∆ ≈ 1/ γ, and we can thus
use a slightly extended version of Theorem 1 to derive the
desired result. The complete proof is available as supplementary material.

Corollary 2. For any γ > 0, there exists a gossip matrix
W of eigengap γ and α-strongly convex, β-smooth functions such that, with κl = β/α, for any black-box procedure using W the time to reach a precision ε > 0 is lower
bounded by

  

√
τ
1
κl 1 + √
ln
.
(9)
Ω
γ
ε
We will see in the next section that this lower bound is met
for a novel decentralized algorithm called multi-step dual
accelerated (MSDA) and based on the dual formulation of
the optimization problem. Note that these results provide
optimal convergence rates with respect to κl and γ, but do
not imply that γ is the right quantity to consider on gen√
eral graphs. The quantity 1/ γ may indeed be very large
compared to ∆, for example
for star networks, for which
√
√
∆ = 2 and 1/ γ = n. However, on many simple networks, the diameter ∆ and the eigengap of the Laplacian
√
matrix are tightly connected, and ∆ ≈ 1/ γ. For exam√
ple, for linear graphs, ∆ = n − 1 and 1/ γ ≈ 2n/π, for
√
totally connected networks, ∆ = 1 and 1/ γ = 1, and
√
for regular networks, 1/ γ ≥ 2√2∆ln n (Alon & Milman,
2
1985). Finally, note that the case of totally connected networks corresponds to a previous complexity lower bound
on communications proven by Arjevani & Shamir (2015),
and is equivalent to our result for centralized algorithms
with ∆ = 1.

4. Optimal Decentralized Algorithms
In this section, we present a simple framework for solving
the optimization problem in Eq. (1) in a decentralized setting, from which we will derive several variants, including
a synchronized algorithm whose convergence rate matches
the lower bound in Corollary 2. Note that the naive approach of distributing each (accelerated) gradient step by
gossiping does not lead to a linear convergence rate, as the
number of gossip steps has to increase with the number of
iterations to ensure the linear rate is preserved. We begin
with the simplest form of the algorithm, before extending
it to more advanced scenarios.
4.1. Single-Step Dual Accelerated Method
A standard approach for solving Eq. (1) (see Boyd et al.
(2011); Jakovetić et al. (2015)) consists in rewriting the optimization problem as
n

min f¯(θ) =

θ∈Rd

min

θ1 =···=θn

1X
fi (θi ).
n i=1

(10)

Furthermore, the
√ equality constraint θ1 = · · · = θn is
equivalent to Θ W = 0, where Θ = (θ1 , . . . , θn ) and
W is a gossip matrix verifying the assumptions described

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Algorithm 1 Single-Step Dual Accelerated method
Input: number of iterations √T >√0, gossip matrix W ∈
κ − γ
α
√ l √
Rn×n , η = λ1 (W
), µ =
κl + γ
Output: θi,T , for i = 1, ..., n
1: x0 = 0, y0 = 0
2: for t = 0 to T − 1 do
3:
θi,t = ∇fi∗ (xi,t ), for all i = 1, ..., n
4:
yt+1 = xt − ηΘt W
5:
xt+1 = (1 + µ)yt+1 − µyt
6: end for
in
√ Section 2. Note that, since W
√ is positive semi-definite,
W exists and is defined as W = V > Σ1/2 V , where
W = V > ΣV √
is the singular value decomposition of W .
W = 0 implies that each row of Θ is conThe equality Θ √
stant (since Ker( W ) = Span(1)), and is thus equivalent
to θ1 = · · · = θn . This leads to the following primal version of the optimization problem:
min √
F (Θ),
Θ∈Rd×n : Θ W =0

(11)

Pn
where F (Θ) =
i=1 fi (θi ). Since Eq. (11) is a convex
problem, it is equivalent to its dual optimization problem:
√
max −F ∗ (λ W ),
(12)
λ∈Rd×n

where F ∗ (y) = supx∈Rd×n hy, xi − F (x) is the Fenchel
conjugate of F , and hy, xi = tr(y > x) is the standard scalar
product between matrices.
The optimization problem in Eq. (12) is unconstrained and
convex, and can thus be solved using a variety of convex
optimization techniques. The proposed single-step dual accelerated (SSDA) algorithm described in Alg. (1) uses Nesterov’s accelerated gradient descent, and can be thought of
as an accelerated version of the distributed augmented Lagrangian method of Jakovetić et al. (2015) for ρ = 0. The
algorithm is derived by noting that a gradient step of size
η > 0 for Eq. (12) is
√ √
λt+1 = λt − η∇F ∗ (λt W ) W ,
(13)
√
and the change of variable yt = λt W leads to
yt+1 = yt − η∇F ∗ (yt )W.

(14)

This equation can be interpreted as gossiping the gradients of the local conjugate functions ∇fi∗ (yi,t ), since
∇F ∗ (yt )ij = ∇fj∗ (yj,t )i .
Theorem 3. The iterative scheme in Alg. (1) converges to
Θ = θ∗ 1> where θ∗ is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given
precision ε > 0 is
 

r
κl
1
.
(15)
ln
O (1 + τ )
γ
ε

This theorem relies on proving that the condition number
of the dual objective function is upper bounded by κγl , and
noting that the convergence rate for accelerated gradient
descent depends on the square root of the condition number
(see, e.g., Bubeck (2015)). A detailed proof is available as
supplementary material.
4.2. Multi-Step Dual Accelerated Method
The main problem of Alg. (1) is that it always performs the
same number of gradient and gossip steps. When communication is cheap compared to local computations (τ  1),
it would be preferable to perform more gossip steps than
gradient steps in order to propagate the local gradients further than the local neighborhoods of each node. This can be
achieved by replacing W by PK (W ) in Alg. (1), where PK
is a polynomial of degree at most K. If PK (W ) is itself a
gossip matrix, then the analysis of the previous section can
be applied and the convergence rate of the resulting algorithm depends on the eigengap of PK (W ). Maximizing
this quantity for a fixed K leads to a common acceleration scheme known as Chebyshev acceleration (Auzinger,
2011; Arioli & Scott, 2014) and the choice
PK (x) = 1 −

TK (c2 (1 − x))
,
TK (c2 )

(16)

1+γ
and TK are the Chebyshev polynomials
where c2 = 1−γ
(Auzinger, 2011) defined as T0 (x) = 1, T1 (x) = x, and,
for all k ≥ 1,

Tk+1 (x) = 2xTk (x) − Tk−1 (x).

(17)

Finally, verifying that this particular choice of PK (W ) is
indeed a gossip matrix, and taking K = b √1γ c leads to
Alg. (2) with an optimal convergence rate with respect to γ
and κl .
Theorem 4. The iterative scheme in Alg. (2) converges to
Θ = θ∗ 1> where θ∗ is the solution of Eq. (1). Furthermore, the time needed for this algorithm to reach any given
precision ε > 0 is


  
√
τ
1
O
κl 1 + √
ln
.
(18)
γ
ε
The proof of Theorem 4 relies on standard properties of
Chebyshev polynomials that imply that, for the particular
choice of K = b √1γ c, we have √ 1
≤ 2. Hence,
γ(PK (W ))

Theorem 3 applied to the gossip matrix W 0 = PK (W )
gives the desired convergence rate. The complete proof is
available as supplementary material.
4.3. Discussion and Further Developments
We now discuss several extensions to the proposed algorithms.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Algorithm 2 Multi-Step Dual Accelerated method
Input: number of iterations
T > 0, gossip matrix W ∈
√
1− γ
1+γ
2
Rn×n , c1 = 1+√γ , c2 = 1−γ
, c3 = (1+γ)λ
,
1 (W )
j k
K √
K
2K
(1+c ) κ −1+c
α(1+c )
K = √1γ , η = (1+cK1)2 , µ = (1+c1K )√κll +1−c1K
1
1
1
Output: θi,T , for i = 1, ..., n
1: x0 = 0, y0 = 0
2: for t = 0 to T − 1 do
3:
θi,t = ∇fi∗ (xi,t ), for all i = 1, ..., n
4:
yt+1 = xt − η ACCELERATED G OSSIP(Θt ,W ,K)
5:
xt+1 = (1 + µ)yt+1 − µyt
6: end for
7:
8:
9:
10:
11:
12:
13:
14:
15:

procedure ACCELERATED G OSSIP(x,W ,K)
a0 = 1, a1 = c2
x0 = x, x1 = c2 x(I − c3 W )
for k = 1 to K − 1 do
ak+1 = 2c2 ak − ak−1
xk+1 = 2c2 xk (I − c3 W ) − xk−1
end for
K
return x0 − xaK
end procedure

Computation of ∇fi∗ (xi,t ). In practice, it may be hard
to apply the dual algorithm when conjugate functions are
hard to compute. We now provide three potential solutions
to this problem: (1) warm starts may be used for the optimization problem ∇fi∗ (xi,t ) = argminθ fi (θ) − x>
i,t θ
by starting from the previous iteration θi,t−1 . This will
drastically reduce the number of steps required for convergence. (2) SSDA and MSDA can be extended to composite functions of the form fi (θ) = gi (Bi θ) + ckθk22 for
Bi ∈ Rmi ×d and gi smooth, and for which we know how
to compute the proximal operator. This allows applications
in machine learning such as logistic regression. See supplementary material for details. (3) Beyond the composite
case, one can also add a small (well-chosen) quadratic term
to the dual, and by applying accelerated gradient descent on
the corresponding primal, get an algorithm that uses primal
gradient computations and achieves almost the same guarantee as SSDA and MSDA (off by a log(κl /γ) factor).
Local vs. Global Condition Number. MSDA and SSDA
depend on the worst strong convexity of the local functions
mini αi , which may be very small. A simple trick can be
used to depend on the average strong convexity. Using the
proxy functionsP
gi (θ) = fi (θ)−(αi − ᾱ)kθk22 instead of fi ,
1
where ᾱ = n i αi is the average strong convexity, will
i βi
improve the local condition number from κl = max
mini αi to
maxi βi − αi
− 1.
(19)
ᾱ
Several algorithms, including EXTRA (Shi et al., 2015)
and DIGing (Nedich et al., 2016), have convergence rates
that depend on the strong convexity of the global funcκ0l =

tion αg . However, their convergence rates are not optimal,
and it is still an open question
to know if a rate close to

√
O
κg (1 + √τγ ) ln(1/ε) can be achieved with a decentralized algorithm.
Asynchronous Setting. Accelerated stochastic gradient
descent such as SVRG (Johnson & Zhang, 2013) or SAGA
(Defazio et al., 2014) can be used on the dual problem in
Eq. (12) instead of accelerated gradient descent, in order
to obtain an asynchronous algorithm with a linear convergence rate. The details and exact convergence rate of such
an approach are left as future work.

5. Experiments
In this section, we compare our new algorithms, single-step
dual accelerated (SSDA) descent and multi-step dual accelerated (MSDA) descent, to standard distributed optimization algorithms in two settings: least-squares regression
and classification by logistic regression. Note that these experiments on simple generated datasets are made to assess
the differences between existing state-of-the-art algorithms
and the ones provided in Section 4, and do not address the
practical implementation details nor the efficiency of the
compared algorithms on real-world distributed platforms.
The effect of latency, machine failures or variable communication time is thus left for future work.
5.1. Competitors and Setup
We compare SSDA and MSDA to four state-of-the-art distributed algorithms that achieve linear convergence rates:
distributed ADMM (D-ADMM) (Shi et al., 2014), EXTRA
(Shi et al., 2015), a recent approach named DIGing (Nedich
et al., 2016), and the distributed version of accelerated gradient descent (DAGD) described in Section 3.2 and shown
to be optimal among centralized algorithms.
When available in the literature, we used the optimal parameters for each algorithm (see Theorem 2 by Shi et al.
(2014) for D-ADMM and Remark 3 by Shi et al. (2015)
for EXTRA). For the DIGing algorithm, the parameters
provided by Nedich et al. (2016) are very conservative,
and lead to a very slow convergence. We thus manually
optimized the parameter for this algorithm. The experiments are simulated using a generated dataset consisting of
10, 000 samples randomly distributed to the nodes of a network of size 100. In order to assess the effect of the connectivity of the network, we ran each experiment on two
networks: one 10 × 10 grid and an Erdös-Rényi random
network of average degree 6. The quality metric used in
this section is the maximum approximation error
et = max f¯(θi,t ) − f¯(θ∗ ),
i∈V

(20)

where θ∗ is the optimal parameter of the optimization prob-

max. approximation error (et)

max. approximation error (et)

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

D−ADMM
EXTRA
DIGing
DAGD
SSDA
MSDA

0

10

−10

10

0

500

1000
1500
time (t)

0

10

−5

10

−10

10

2000

0

0

10

−10

10

0

50

100
150
time (t)

4000 6000
time (t)

8000 10000

(a) high communication time: τ = 10
max. approximation error (et)

max. approximation error (et)

(a) high communication time: τ = 10

2000

0

10

−5

10

−10

10

200

0

200

400

600
time (t)

800

1000

(b) low communication time: τ = 0.1

(b) low communication time: τ = 0.1

Figure 1. Maximum approximation error for least-squares regression on an Erdös-Rényi random network of average degree 6
(n = 100).

Figure 2. Maximum approximation error for least-squares regression on a 10 × 10 grid graph (n = 100).

lem in Eq. (1). The computation time attributes a unit time
per local computation and a time τ per communication.
5.2. Least-squares Regression
The regularized least-squares regression problem consists
in solving the optimization problem
min

θ∈Rd

1
ky − X > θk22 + ckθk22 ,
m

(21)

where X ∈ Rd×m is a matrix containing the m data points,
and y ∈ Rm is a vector containing the m associated values. The task is thus to minimize the empirical quadratic
error between a function yi = g(Xi ) of d variables and its
linear regression ĝ(Xi ) = Xi> θ on the original dataset (for
i = 1, ..., m), while smoothing the resulting approximation
by adding a regularizer ckθk22 . For our experiments, we
fixed c = 0.1, d = 10, and sampled m = 10, 000 Gaussian
random variables Xi ∼ N (0, 1) of mean 0 and variance 1.
The function to regress is then yi = Xi> 1+cos(Xi> 1)+ξi
where ξi ∼ N (0, 1/4) is an i.i.d. Gaussian noise of variance 1/4. These data points are then distributed randomly
and evenly to the n = 100 nodes of the network. Note

that the choice of function to regress y does not impact the
Hessian of the objective function, and thus the convergence
rate of the optimization algorithms.
Figure 1 and Figure 2 show the performance of the compared algorithms on two networks: a 10 × 10 grid graph
and an Erdös-Rényi random graph of average degree 6. All
algorithms are linearly convergent, although their convergence rates scale on several orders of magnitude. In all
experiments, the centralized optimal algorithm DAGD has
the best convergence rate, while MSDA has the best convergence rate among decentralized methods. In all our experiments, MSDA outperforms SSDA, indicating that performing several communication rounds per gradient iteration never degrades the efficiency of the algorithm, while
significantly improving it when τ  1.
5.3. Logistic Classification
The logistic classification problem consists in solving the
optimization problem
m

min

θ∈Rd


>
1 X 
ln 1 + e−yi ·Xi θ + ckθk22 ,
m i=1

(22)

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks
0

D−ADMM
EXTRA
DIGing
DAGD
SSDA
MSDA

−10

10

0

2000

4000 6000
time (t)

max. approximation error (et)

max. approximation error (et)

0

10

10

−10

10

0

8000

(a) high communication time: τ = 10

4000 6000
time (t)

8000

(a) high communication time: τ = 10

0

0

10

max. approximation error (et)

max. approximation error (et)

2000

−10

10

0

200

400
600
time (t)

800

10

−10

10

0

200

400
600
time (t)

800

(b) low communication time: τ = 0.1

(b) low communication time: τ = 0.1

Figure 3. Maximum approximation error for logistic classification
on an Erdös-Rényi random network of average degree 6 (n =
100).

Figure 4. Maximum approximation error for logistic classification
on a 10 × 10 grid graph (n = 100).

where X ∈ Rd×m is a matrix containing m data points,
and y ∈ {−1, 1}m is a vector containing the m class assignments. The task is thus to classify a dataset by learning
a linear classifier mapping data points Xi to their associated
class yi ∈ {−1, 1}. For our experiments, we fixed c = 0.1,
d = 10, and sampled m = 10, 000 data points, 5, 000 for
the first class and 5, 000 for the second. Each data point
Xi ∼ N (yi 1, 1) is a Gaussian random variable of mean
yi 1 and variance 1, where yi = 21{i ≤ 5, 000} − 1 is
the true class of Xi . These data points are then distributed
randomly and evenly to the n = 100 nodes of the network.

improve the efficiency of the algorithm and MSDA substantially outperforms SSDA. Note that, in Figure 4(a), DADMM requires 383 iterations to reach the same error obtained after only 10 iterations of SSDA, demonstrating a
substantial improvement over state-of-the-art methods.

Figure 3 and Figure 4 show the performance of the compared algorithms for logistic classification on two networks: a 10 × 10 grid graph and an Erdös-Rényi random
graph of average degree 6. As for least-squares regression,
all algorithms are linearly convergent, and their convergence rates scale on several orders of magnitude. In this
case, the centralized optimal algorithm DAGD is outperformed by MSDA, although the two convergence rates are
relatively similar. Again, when the communication time is
smaller than the computation time (τ  1), performing
several communication rounds per gradient iteration will

6. Conclusion
In this paper, we derived optimal convergence rates for
strongly convex and smooth distributed optimization in two
settings: centralized and decentralized communications in
a network. For the decentralized setting, we introduced
the multi-step dual accelerated (MSDA) algorithm with a
provable optimal linear convergence rate, and showed its
high efficiency compared to other state-of-the-art methods,
including distributed ADMM and EXTRA. The simplicity of the approach makes the algorithm extremely flexible,
and allows for future extensions, including time-varying
networks and an analysis for non-strongly-convex functions. Finally, extending our complexity lower bounds to
time delays, variable computational speeds of local systems, or machine failures would be a notable addition to
this work.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

References
Alon, N. and Milman, V. D. λ1 , isoperimetric inequalities
for graphs, and superconcentrators. Journal of Combinatorial Theory, series B, 38:73–88, 1985.
Arioli, M. and Scott, J. Chebyshev acceleration of iterative refinement. Numerical Algorithms, 66(3):591–608,
2014.

Duchi, John C, Agarwal, Alekh, and Wainwright, Martin J. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3):592–606, 2012.
Jakovetić, Dušan, Xavier, Joao, and Moura, José MF. Fast
distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131–1146, 2014.

Arjevani, Yossi and Shamir, Ohad. Communication complexity of distributed convex learning and optimization.
In Advances in Neural Information Processing Systems
28, pp. 1756–1764, 2015.

Jakovetić, Dušan, Moura, José MF, and Xavier, Joao. Linear convergence rate of a class of distributed augmented
lagrangian algorithms. IEEE Transactions on Automatic
Control, 60(4):922–936, 2015.

Arjevani, Yossi and Shamir, Ohad. On the iteration complexity of oblivious first-order optimization algorithms.
In 33nd International Conference on Machine Learning,
pp. 908–916, 2016a.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems 26,
pp. 315–323, 2013.

Arjevani, Yossi and Shamir, Ohad. Dimension-free iteration complexity of finite sum optimization problems. In
Advances in Neural Information Processing Systems 29,
pp. 3540–3548, 2016b.

Mokhtari, A., Shi, W., Ling, Q., and Ribeiro, A. A decentralized second-order method with exact linear convergence rate for consensus optimization. IEEE Transactions on Signal and Information Processing over Networks, 2(4):507–522, 2016.

Auzinger, W. Iterative Solution of Large Linear Systems.
Lecture notes, TU Wien, 2011.
Bertsekas, Dimitri P. and Tsitsiklis, John N. Parallel and
distributed computation : numerical methods. PrenticeHall International, 1989.
Boyd, Stephen, Ghosh, Arpita, Prabhakar, Balaji, and
Shah, Devavrat.
Randomized gossip algorithms.
IEEE/ACM Transactions on Networking (TON), 14(SI):
2508–2530, 2006.
Boyd, Stephen, Diaconis, Persi, Parrilo, Pablo, and Xiao,
Lin. Fastest mixing markov chain on graphs with symmetries. SIAM Journal on Optimization, 20(2):792–819,
2009.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.
Bubeck, Sébastien. Convex optimization: Algorithms and
complexity. Foundations and Trends in Machine Learning, 8(3-4):231–357, 2015.
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
SAGA: A fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pp.
1646–1654, 2014.

Mokhtari, Aryan and Ribeiro, Alejandro. DSA: Decentralized double stochastic averaging gradient algorithm.
Journal of Machine Learning Research, 17(1):2165–
2199, 2016.
Nedic, Angelia and Ozdaglar, Asuman. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
Nedich, A., Olshevsky, A., and Shi, W. Achieving geometric convergence for distributed optimization over timevarying graphs. ArXiv e-prints, 2016.
Nesterov, Yurii. Introductory lectures on convex optimization : a basic course. Kluwer Academic Publishers,
2004.
Shamir, Ohad. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In Advances in Neural Information Processing Systems 27, pp.
163–171, 2014.
Shamir, Ohad and Srebro, Nathan. Distributed stochastic optimization and learning. In 52nd Annual Allerton
Conference on Communication, Control, and Computing
(Allerton), pp. 850–857. IEEE, 2014.
Shi, Wei, Ling, Qing, Yuan, Kun, Wu, Gang, and Yin,
Wotao. On the linear convergence of the ADMM in decentralized consensus optimization. IEEE Transactions
on Signal Processing, 62(7):1750–1761, 2014.

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

Shi, Wei, Ling, Qing, Wu, Gang, and Yin, Wotao. EXTRA:
An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):
944–966, 2015.
Tutunov, R., Ammar, H. B., and Jadbabaie, A. A distributed
newton method for large scale consensus optimization.
ArXiv e-prints, 2016.
Wei, Ermin and Ozdaglar, Asuman. Distributed alternating
direction method of multipliers. In 51st Annual Conference on Decision and Control (CDC), pp. 5445–5450.
IEEE, 2012.
Xiao, Lin and Boyd, Stephen. Fast linear iterations for distributed averaging. Systems & Control Letters, 53(1):
65–78, 2004.

