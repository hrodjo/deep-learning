Global optimization of Lipschitz functions

CeÌdric Malherbe 1 Nicolas Vayatis 1

Abstract
The goal of the paper is to design sequential
strategies which lead to efficient optimization of
an unknown function under the only assumption
that it has a finite Lipschitz constant. We first
identify sufficient conditions for the consistency
of generic sequential algorithms and formulate
the expected minimax rate for their performance.
We introduce and analyze a first algorithm called
LIPO which assumes the Lipschitz constant to
be known. Consistency, minimax rates for LIPO
are proved, as well as fast rates under an additional HoÌˆlder like condition. An adaptive version
of LIPO is also introduced for the more realistic
setup where the Lipschitz constant is unknown
and has to be estimated along with the optimization. Similar theoretical guarantees are shown
to hold for the adaptive algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with
respect to state-of-the-art methods over typical
benchmark problems for global optimization.

1. Introduction
In many applications such as complex system design or
hyperparameter calibration for learning systems, the goal
is to optimize the output value of an unknown function
with as few evaluations as possible. Indeed, in such contexts, evaluating the performance of a single set of parameters often requires numerical simulations or crossvalidations with significant computational cost and the operational constraints impose a sequential exploration of the
solution space with small samples. Moreover, it can generally not be assumed that the function has good properties such as linearity or convexity. This generic problem of sequentially optimizing the output of an unknown
and potentially nonconvex function is often referred to as
1

CMLA, ENS Cachan, CNRS, UniversiteÌ Paris-Saclay, 94235,
Cachan, France. Correspondence to:
<name@cmla.enscachan.fr>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

global optimization (PinteÌr, 1991), black-box optimization
(Jones et al., 1998) or derivative-free optimization (Rios &
Sahinidis, 2013). There is a large number of algorithms
based on various heuristics which have been introduced
in order to solve this problem such as genetic algorithms,
model-based methods or Bayesian optimization. We focus
here on the smoothness-based approach to global optimization. This approach is based on the simple observation that,
in many applications, the system presents some regularity
with respects to the input. In particular, the use of the Lipschitz constant, first proposed in the seminal works of (Shubert, 1972; Piyavskii, 1972), initiated an active line of research and played a major role in the development of many
efficient global optimization algorithms such as DIRECT
(Jones et al., 1993), MCS (Huyer & Neumaier, 1999) or
SOO (Preux et al., 2014). Convergence properties of global
optimization methods have been developed in the works of
(Valko et al., 2013; Munos, 2014) under local smoothness
assumptions, but, up to our knowledge, such properties
have not been considered in the case where only the global
smoothness of the function can be specified. An interesting question is how much global assumptions on regularity
which cover in some sense local assumptions may improve
the convergence of the latter. In this work, we address the
following questions: (i) find the limitations and the best
performance that can be achieved by any algorithm over the
class of Lipschitz functions and (ii) design efficient and optimal algorithms for this class of problems. Our contribution with regards to the above mentioned works is twofold.
First, we introduce two novel algorithms for global optimization which exploit the global smoothness of the function and display good performance in typical benchmarks
for optimization. Second, we show that these algorithms
can achieve faster rates of convergence on globally smooth
problems than the previously known methods which only
exploit the local smoothness of the function. The rest of the
paper is organized as follows. In Section 2, we introduce
the framework and give generic results about the convergence of sequential algorithms. In Section 3, we introduce
and analyze the LIPO algorithm which requires the knowledge of the Lipschitz constant. In Section 4, the algorithm
is extended to the case where the Lipschitz constant is unknown and the adaptive algorithm is compared to existing
methods in Section 5. All proofs can be found in the Supplementary Material provided as a separate document.

Global optimization of Lipschitz functions

2. Setup and preliminary results

2.2. Preliminary results

2.1. Setup and notations

In order to design efficient procedures, we first investigate
the best performance that can be achieved by any algorithm
over the class of Lipschitz functions.

Setup. Let X âŠ‚ Rd be a compact and convex set with nonempty interior and let f : X â†’ R be an unknown function
which is only supposed to admit a maximum over its input domain X . The goal in global optimization consists in
finding some point
x? âˆˆ arg max f (x)
xâˆˆX

with a minimal amount of function evaluations. The standard setup involves a sequential procedure which starts
by evaluating the function f (X1 ) at an initial point X1
and then selects at each step t â‰¥ 1 an evaluation
point Xt+1 âˆˆ X depending on the previous evaluations
(X1 , f (X1 )), . . . , (Xt , f (Xt )) and receives the evaluation
of the unknown function f (Xt+1 ) at this point. After n
iterations, we consider that the algorithm returns an evaluation point XÄ±Ì‚n with Ä±Ì‚n âˆˆ arg mini=1...n f (Xi ) which has
recorded the highest evaluation. The performance of the
algorithm over the function f is then measured after n iterations through the difference between the value of the true
maximum and the highest evaluation observed so far:

Sequential algorithms and optimization consistency. We
describe the sequential procedures that are considered here
and the corresponding concept of consistency in the sense
of global optimization.
Definition 1 (S EQUENTIAL ALGORITHM ) The class of
optimization algorithms we consider, denoted in the sequel
by A, contains all the algorithms A = {At }tâ‰¥1 completely
described by:
1. A distribution A1 taking values in X which allows to
generate the first evaluation point, i.e. X1 âˆ¼ A1 ;
2. An infinite collection of distributions {At }tâ‰¥2 taking values in X and based on the previous evaluations which define the iteration loop, i.e. Xt+1 âˆ¼
At+1 ((X1 , f (X1 )), . . . , (Xt , f (Xt ))).

max f (x) âˆ’ max f (Xi ).

Note that this class of algorithms also includes the deterministic methods in which case the distributions {At }tâ‰¥1
are degenerate. The next definition introduces the notion of
asymptotic convergence.

The analysis provided in the paper considers that the number n of evaluation points is not fixed and it is assumed that
function evaluations are noiseless. Moreover, the assumption made on the unknown function f throughout the paper
is that it has a finite Lipschitz constant k, i.e.

Definition 2 (O PTIMIZATION C ONSISTENCY ) A global
optimization algorithm A is said to be consistent over a
set F of real-valued functions admitting a maximum over
X if and only if

âˆƒk â‰¥ 0 s.t. |f (x) âˆ’ f (x0 )| â‰¤ k Â· kx âˆ’ x0 k2 âˆ€(x, x0 ) âˆˆ X 2.

âˆ€f âˆˆ F, max f (Xi ) âˆ’
â†’ max f (x)

Before starting the analysis, we point out that similar settings have also been studied in (Munos, 2014; Malherbe
et al., 2016) and that (Valko et al., 2013; Grill et al., 2015)
considered the noisy scenario.

where X1 , . . . , Xn denotes a sequence of n evaluations
points generated by the algorithm A over the function f .

xâˆˆX

i=1...n

d

Notations. For all x = (x1 , . . . , xd ) âˆˆ R , we dePd
note by kxk2 = ( i=1 x2i )1/2 the standard `2 -norm and
by B(x, r) = {x0 âˆˆ Rd : kx âˆ’ x0 k2 â‰¤ r} the ball
centered in x of radius r â‰¥ 0. For any bounded set
X âŠ‚ Rd , we define its inner-radius as rad(X ) = max{r >
0 : âˆƒx âˆˆ X such that B(x, r) âŠ† X }, its diameter as
diam(X ) = max(x,x0 )âˆˆX 2 kx âˆ’ x0 k2 and we denote by
Âµ(X ) its volume where Âµ(Â·) stands for the Lebesgue measure. Lip(k) = {f : X â†’ R s.t. |f (x) âˆ’ f (x0 )| â‰¤
k Â· kx âˆ’ x0 k2 , âˆ€(x, x0 ) âˆˆ X 2 } denotes
the class of kS
Lipschitz functions defined on X and kâ‰¥0 Lip(k) denotes
the set of Lipschitz continuous functions. U(X ) stands for
the uniform distribution over a bounded measurable domain X , B(p) for the Bernoulli distribution of parameter
p, I{Â·} for the standard indicator function taking values in
{0, 1} and the notation X âˆ¼ P means that the random variable X has the distribution P.

p

i=1...n

xâˆˆX

Asymptotic performance. We now investigate the minimal conditions for a sequential algorithm to achieve asymptotic convergence. Of course, it is expected that a global
optimization algorithm should be consistent at least for the
class of Lipschitz functions and the following result reveals
a necessary and sufficient condition (NSC) in this case.
Proposition 3 (C ONSISTENCY NSC) A global optimization algorithm A is consistent over the set of Lipschitz functions if and only if
S
p
âˆ€f âˆˆ kâ‰¥0 Lip(k), sup min kXi âˆ’ xk2 âˆ’
â†’ 0.
xâˆˆX i=1...n

A crucial consequence of the latter proposition is that the
design of any consistent method ends up to covering the
whole input space regardless of the function values. The
example below introduces the most popular space-filling
method which will play a central role in our analysis.

Global optimization of Lipschitz functions

Example 4 (P URE R ANDOM S EARCH ) The Pure Random
Search (PRS) consists in sequentially evaluating the function over a sequence of points X1 , X2 , X3 , . . . uniformly
and independently distributed over the input space X . For
this method, a simple union bound indicates that for all
n âˆˆ N? and Î´ âˆˆ (0, 1), we have with probability at least
1 âˆ’ Î´ and independently of the function values,

sup min kXi âˆ’xk2 â‰¤ diam(X )Â·

xâˆˆX i=1...n

ln(n/Î´) + d ln(d)
n

 d1
.

In addition to this result, we point out that the covering rate
of any method can easily be shown to be at best of order
â„¦(nâˆ’1/d ) and thus subject to to the curse of dimensionality by means of covering arguments. Keeping in mind
the equivalence of Proposition 3, we may now turn to the
nonasymptotic analysis.
Finite-time performance. We investigate here the best
performance that can be achieved by any algorithm with
a finite number of function evaluations. We start by casting a negative result stating that any algorithm can suffer, at
any time, an arbitrarily large loss over the class of Lipschitz
functions.
Proposition 5 Consider any global optimization algorithm A. Then, for any constant C > 0 arbitrarily large,
any S
n âˆˆ N? and Î´ âˆˆ (0, 1), there exists a function
Ëœ
f âˆˆ kâ‰¥0 Lip(k) only depending on (A, C, n, Î´) for which
we have with probability at least 1 âˆ’ Î´,
C â‰¤ max fËœ(x) âˆ’ max fËœ(Xi ).
i=1...n

xâˆˆX

This result might however not be very surprising since the
class of Lipschitz functions includes functions with finite,
but arbitrarily large variations. When considering the subclass of functions with fixed Lipschitz constant, it becomes
possible to derive finite-time bounds on the minimax rate
of convergence.
Proposition 6 (M INIMAX RATE ) adapted from (Bull,
2011). For any Lipschitz constant k â‰¥ 0 and any n âˆˆ N? ,
the following inequalities hold true:

We point out that this minimax rate of convergence of order Î˜(nâˆ’1/d ) can still be achieved by any method with
an optimal covering rate of order O(nâˆ’1/d ). Observe indeed that since E [maxxâˆˆX f (x) âˆ’ maxi=1...n f (Xi )] â‰¤ k
Ã— E [supxâˆˆX mini=1...n kx âˆ’ Xi k2 ] for all f âˆˆ Lip(k),
then an optimal covering rate necessarily implies minimax
efficiency. However, as it can be seen by examining the
proof of Proposition 6 provided in the Supplementary Material, the functions constructed to prove the limiting bound
of â„¦(nâˆ’1/d ) are spikes which are almost constant everywhere and do not present a large interest from a practical
perspective. In particular, we will see in the sequel that one
can design:
I) An algorithm with fixed constant k â‰¥ 0 which achieves
minimax efficiency and also presents exponentially
decreasing rates over a large subset of functions, as
opposed to space-filling methods (LIPO, Section 3).
II) A consistent algorithm which does not require the
knowledge of the Lipschitz constant and presents
comparable performance as when the constant k is assumed to be known (AdaLIPO, Section 4).

3. Optimization with fixed Lipschitz constant
In this section, we consider the problem of optimizing an
unknown function f given the knowledge that f âˆˆ Lip(k)
for a given k â‰¥ 0.
3.1. The LIPO Algorithm
The inputs of the LIPO algorithm (Algorithm 1) are a number n of function evaluations, a Lipschitz constant k â‰¥ 0,
the input space X and the unknown function f . At each
iteration t â‰¥ 1, a random variable Xt+1 is sampled uniformly over the input space X and the algorithm decides
whether or not to evaluate the function at this point. Indeed, it evaluates the function over Xt+1 if and only if
the value of the upper bound on possible values U B :
x 7â†’ mini=1...t f (Xi ) +k Â· kx âˆ’ Xi k2 evaluated at this
point and computed from the previous evaluations is at least
equal to the value of the best evaluation observed so far
maxi=1...t f (Xi ). As an example, the computation of the
decision rule of LIPO is illustrated in Figure 1.

1

c1 Â· k Â· nâˆ’ d â‰¤
inf



sup E max f (x) âˆ’ max f (Xi )

AâˆˆA f âˆˆLip(k)

xâˆˆX

i=1...n

1

âˆš

â‰¤ c2 Â· k Â· nâˆ’ d

where c1 = rad(X ) /(8 d), c2 = diam(X ) Ã— d! and the
expectation is taken over a sequence of n evaluation points
X1 , . . . , Xn generated by the algorithm A over f .

Algorithm 1 LIPO(n, k, X , f )
1. Initialization: Let X1 âˆ¼ U(X )
..... Evaluate f (X1 ), t â† 1
2. Iterations: Repeat while t < n
..... Let Xt+1 âˆ¼ U(X )
..... If min (f (Xi ) + k Â· kXt+1 âˆ’ Xi k2 ) â‰¥ max f (Xi )
i=1...t

i=1...t

........... Evaluate f (Xt+1 ), t â† t + 1

3. Output: Return XÄ±Ì‚n where Ä±Ì‚n âˆˆ arg maxi=1...n f (Xi )

Global optimization of Lipschitz functions

More formally, the mechanism behind this rule can be explained using the active subset of consistent functions previously considered in active learning (see, e.g., (Dasgupta,
2011) and (Hanneke, 2011)).
Definition 7 (C ONSISTENT FUNCTIONS ) The active subset of k-Lipschitz functions consistent with the unknown
function f over a sample (X1 , f (X1 )), . . . , (Xt , f (Xt )) of
t â‰¥ 1 evaluations is defined as follows:
Fk,t := {g âˆˆ Lip(k) : âˆ€i âˆˆ {1 . . . t}, g(Xi ) = f (Xi )}.

Indeed, one can recover from this definition the subset of
points which can actually maximize the function f .
Definition 8 (P OTENTIAL MAXIMIZERS ) Using the same
notations as in Definition 7, we define the subset of potential maximizers estimated over any sample t â‰¥ 1 evaluations with a constant k â‰¥ 0 as follows:


Xk,t := x âˆˆ X : âˆƒg âˆˆ Fk,t such that x âˆˆ arg max g(x) .
xâˆˆX

We may now provide an equivalence which makes the link
with the decision rule of the LIPO algorithm.
Lemma 9 If Xk,t denotes the set of potential maximizers
defined above, then we have the following equivalence:
x âˆˆ Xk,t â‡” min f (Xi ) + k Â· kx âˆ’ Xi k2 â‰¥ max f (Xi ).
i=1...t

i=1...t

Hence, we deduce from this lemma that the algorithm only
evaluates the function over points that still have a chance to
be maximizers of the unknown function.
Remark 10 (E XTENSION TO OTHER SMOOTHNESS AS SUMPTIONS ) It is important to note the proposed optimization scheme could easily be extended to a large number of sets of globally and locally smooth functions by
slightly adapting the decision rule. For instance, when
F` = {f : X â†’ R | x? is unique and âˆ€x âˆˆ
X , f (x? ) âˆ’ f (x) â‰¤ `(x? , x)} denotes the set of functions
locally smooth around their maxima with regards to any
semi-metric ` : X Ã— X â†’ R previously considered in
(Munos, 2014), a straightforward derivation of Lemma 9
directly gives that the decision rule applied in Xt+1 would

simply consists in testing whether maxi=1...t f (Xi ) â‰¤
mini=1...t f (Xi ) + `(Xt+1 , Xi ). However, since the purpose of this work is to design fast algorithms for Lipschitz
functions, we will only derive convergence results for the
version of the algorithm stated above.
3.2. Convergence analysis
We start with the consistency property of the algorithm.
Proposition 11 (C ONSISTENCY ) For any Lipschitz constant k â‰¥ 0, the LIPO algorithm tuned with a parameter
k is consistent over the set k-Lipschitz functions, i.e.,
p

âˆ€f âˆˆ Lip(k), max f (Xi ) âˆ’
â†’ max f (x).
i=1...n

xâˆˆX

The next result shows that the value of the highest evaluation observed by the algorithm is always superior or equal
in the usual stochastic ordering sense to the one of a PRS.
Proposition 12 (FASTER THAN PURE RANDOM SEARCH )
Consider the LIPO algorithm tuned with any constant k â‰¥
0. Then, for any f âˆˆ Lip(k) and n âˆˆ N? , we have that
âˆ€y âˆˆ R,




P max f (Xi ) â‰¥ y â‰¥ P max f (Xi0 ) â‰¥ y
i=1...n

i=1...n

where X1 , . . . , Xn is a sequence of n evaluation points
generated by LIPO and X10 , . . . , Xn0 is a sequence of n independent random variables uniformly distributed over X .
Based on this result, one can easily derive a first finite-time
bound on the difference between the value of the true maximum and its approximation.
Corollary 13 (U PPER BOUND ) For any f âˆˆ Lip(k), n âˆˆ
N? and Î´ âˆˆ (0, 1), we have with probability at least 1 âˆ’ Î´,
1

ln(1/Î´) d
.
max f (x) âˆ’ max f (Xi ) â‰¤ k Â· diam(X ) Â·
i=1...n
xâˆˆX
n
This bound which assesses the miminax optimality of LIPO
stated in Proposition 6 does however not show any improvement over PRS and it cannot be significantly improved without any additional assumption as shown below.
Proposition 14 For any n âˆˆ N? and Î´ âˆˆ (0, 1), there exists a function fËœ âˆˆ Lip(k) only depending on n and Î´ for
which we have with probability at least 1 âˆ’ Î´:
  d1
Î´
â‰¤ max fËœ(x) âˆ’ max fËœ(Xi ).
k Â· rad(X ) Â·
i=1...n
xâˆˆX
n

X

Xk,t

Figure 1. Left: A Lipschitz function, a sample of 4 evaluations and
the upper bound U B : x 7â†’ mini=1...t f (Xi ) +k Â· kx âˆ’ Xi k2
in grey. Right: the set of points Xk,t := {x âˆˆ X : U B(x) â‰¥
maxi=1...t f (Xi )} which satisfy the decision rule.

As announced in Section 2.2, one can nonetheless get
tighter polynomial bounds and even an exponential decay
by using the following condition which describes the behavior of the function around its maximum.

Global optimization of Lipschitz functions
Îº = 0.5

Îº=1

Îº=2

3.3. Comparison with previous works

Figure 2. Three one-dimensional functions satisfying Condition 1
with Îº = 1/2 (Left), Îº = 1 (Middle) and Îº = 2 (Right).

Condition 1 (D ECREASING RATE AROUND THE MAXI MUM ) A function f : X â†’ R is (Îº, cÎº )-decreasing around
its maximum for some Îº â‰¥ 0, cÎº â‰¥ 0 if:
1. The global optimizer x? âˆˆ X is unique;
2. For all x âˆˆ X , we have that:

Îº

f (x? ) âˆ’ f (x) â‰¥ cÎº Â· kx âˆ’ x? k2 .
This condition, already considered in the works of (Zhigljavsky & PinteÌr, 1991) and (Munos, 2014), captures how
fast the function decreases around its maximum. It can be
seen as a local one-sided HoÌˆlder condition which can only
be met for Îº â‰¥ 1 when f is assumed to be Lipschitz. As
an example, three functions satisfying this condition with
different values of Îº are displayed on Figure 3.2.
Theorem 15 (FAST RATES ) Let f âˆˆ Lip(k) be any Lipschitz function satisfying Condition 1 for some Îº â‰¥ 1, cÎº >
0. Then, for any n âˆˆ N? and Î´ âˆˆ (0, 1), we have with
probability at least 1 âˆ’ Î´,
max f (x) âˆ’ max f (Xi ) â‰¤ k Ã— diam(X ) Ã—
xâˆˆX

i=1...n



ï£±
n ln(2)
ï£´
ï£´
âˆš
exp âˆ’ Ck,Îº Â·
,
ï£´
ï£´
ï£´
ln(n/Î´) + 2(2 d)d
ï£´
ï£²

Îº=1

ï£´
Îº
ï£´

âˆ’ d(Îºâˆ’1)
ï£´
Îº
ï£´
n(2d(Îº 1) âˆ’ 1)
ï£´2
ï£´
âˆš
1 + Ck,Îº Â·
,
Îº>1
ï£³
2
ln(n/Î´) + 2(2 d)d

The Piyavskii algorithm (Piyavskii, 1972) is a Lipschitz method with fixed k â‰¥ 0 consisting in sequentially evaluating the function over a point Xt+1 âˆˆ
arg maxxâˆˆX mini=1...t f (Xi ) + k Â· kx âˆ’ Xi k maximizing
the upper bound displayed on Figure 1. (Munos, 2014)
also proposed a similar algorithm (DOO) which uses a hierarchical partitioning of the space in order to sequentially
expand and evaluate the function over the center of a partition which has the highest upper bound computed from
a semi-metric ` set as input. Up to our knowledge, only
the consistency of the Piyavskii algorithm was proven in
(Mladineo, 1986) and (Munos, 2014) derived finite-time
bounds for DOO with the use of weaker local assumptions. To compare our results, we thus considered DOO
tuned with `(x, x0 ) = k kx âˆ’ x0 k2 over X = [0, 1]d partitioned into a 2d -ary tree of hypercubes and with f belonging to the sets of globally smooth functions: (a) Lip(k), (b)
FÎº = {f âˆˆ Lip(k) satisfying Condition 1 with cÎº , Îº â‰¥ 0}
and (c) FÎº0 = {f âˆˆ FÎº : âˆƒc2 > 0, f (x? ) âˆ’ f (x) â‰¤
Îº
c2 kx âˆ’ x? k2 }. The results of the comparison can be found
in Table 1. In addition to the novel lower bounds and
the rate over Lip(k), we were able to obtain similar upper bounds as DOO over FÎº , uniformly better rates for the
Îº
functions in FÎº0 locally equivalent to kx? âˆ’ xk2 with Îº > 1
and a similar exponenital rate, up to a constant factor, when
Îº = 1. Hence, when f is only known to be k-Lipschitz,
one thus should expect the algorithm exploiting the global
smoothness (LIPO) to perform asymptotically better or at
least similarly to the one using the local smoothness (DOO)
or no information (PRS). However, keeping in mind that
the constants are not necessarily
âˆš optimal, it is also interesting to note that the term (k d/cÎº )d appearing in both the
exponential rates of LIPO and DOO tends to suggest that
if f is also known to be locally smooth for some k`  k,
then one should expect an algorithm exploiting the local
smoothness k` to be asymptotically faster than the one using the global smoothness k in the case where Îº = 1.
Algorithm

? Îºâˆ’1

where Ck,Îº = (cÎº maxxâˆˆX kx âˆ’ x k

d

/8k) .

The last result we provide states an exponentially decreasing lower bound.
Theorem 16 (L OWER B OUND ) For any f âˆˆ Lip(k) satisfying Condition 1 for some Îº â‰¥ 1, cÎº > 0 and any n âˆˆ N?
and Î´ âˆˆ (0, 1), we have with probability at least 1 âˆ’ Î´,


âˆš
âˆ’ Îº Â· n+ 2n ln(1/Î´)+ln(1/Î´)
Îº
cÎº rad(X ) Â· e d
â‰¤ max f (x) âˆ’ max f (Xi ).
xâˆˆX

i=1...n

A discussion on these results can be found in the next section where LIPO is compared with similar algorithms.

f âˆˆ Lip(k)
Consistency
Upper Bound

DOO

LIPO

Piyavskii

PRS

X
-

X
1
OP (nâˆ’ d )

X
-

X
1
OP (nâˆ’ d )

-

OP (nâˆ’ d )
Îº
â„¦P (nâˆ’ d )

-

OP (nâˆ’ d )
Îº
â„¦P (nâˆ’ d )

) -

OP (nâˆ’ d )

f âˆˆ FÎº , Îº > 1
Îº
âˆ’
Upper bound
O(n d(Îº 1) )
Lower bound
-

OPâˆ— (n d(Îº 1) )
âˆ’Îºn
d )
â„¦âˆ—
P (e

f âˆˆ FÎº0 , Îº > 1
Îº
âˆ’
Upper bound
O(n d(Îº 1) )
Lower bound
-

OPâˆ— (n d(Îº 1) )
âˆ’Îºn
d )
â„¦âˆ—
P (e

f âˆˆ FÎº0 , Îº = 1
Upper bound O(e
Lower bound

n ln(2)
âˆš
(2k d/cÎº )d

-

âˆ’

Îº

âˆ’ ÎºÃ—Îº

) OPâˆ— (e

n ln(2)
âˆš
2(16k d/cÎº )d

âˆ’n
d
â„¦âˆ—
P (e

)

-

1

Îº

1

1
â„¦P (nâˆ’ d

)

Table 1. Comparison of the results reported over the difference
maxxâˆˆX f (x) âˆ’ maxi=1...n f (Xi ) in Lipschitz optimization.
Dash symbols are used when no results could be found.

Global optimization of Lipschitz functions

4. Optimization with unknown Lipschitz
constant
In this section, we consider the problem
of optimizing any
S
unknown function f in the class kâ‰¥0 Lip(k).
4.1. The adaptive algorithm
The AdaLIPO algorithm (Algorithm 2) is an extension of
LIPO which involves an estimate of the Lipschitz constant
and takes as input a parameter p âˆˆ (0, 1) and a nondecreasing sequence of Lipschitz constant kiâˆˆZ defining a meshgrid of R+ (i.e. such that âˆ€x > 0, âˆƒi âˆˆ Z with ki â‰¤ x â‰¤
ki+1 ). The algorithm is initialized with a Lipschitz constant
kÌ‚1 set to 0 and alternates randomly between two distinct
phases: exploration and exploitation. Indeed, at step t < n,
a Bernoulli random variable Bt+1 of parameter p driving
this trade-off is sampled. If Bt+1 = 1, then the algorithm
explores the space by evaluating the function over a point
uniformly sampled over X . Otherwise, if Bt+1 = 0, the
algorithm exploits the previous evaluations by making an
iteration of the LIPO algorithm with the smallest Lipschitz
constant of the sequence kÌ‚t which is associated with a subset of Lipschitz functions that probably contains f (step abbreviated in the algorithm by Xt+1 âˆ¼ U(XkÌ‚t ,t )). Once an
evaluation has been made, the Lipschitz constant estimate
kÌ‚t is updated.
Remark 17 (E XAMPLES OF MESHGRIDS ) Several sequences of Lipschitz constants with various shapes such as
ki = |i|sgn(i) , ln(1 + |i|sgn(i) ) or (1 + Î±)i for some Î± > 0
could be considered to implement the algorithm. In particular, we point out that with these sequences the computation of the estimate is straightforward. For instance,
when ki = (1 + Î±)i , we have kÌ‚t = (1 + Î±)it where it =
dln(maxi6=j |f (Xj ) âˆ’ f (Xl )|/kXj âˆ’ Xl k2 )/ ln(1 + Î±)e.
4.2. Convergence analysis
Lipschitz constant estimate. Before starting the analysis
of AdaLIPO, we first provide a control on the Lipschitz
constant estimate based on a sample of random evaluations
that will be useful to analyse its performance. In particular, the next result illustrates the purpose of using a discretization of Lipschitz constant instead of a raw estimate
of the maximum slope by showing that, given this estimate,
a small subset of functions containing the unknown function can be recovered in a finite-time.
Proposition 18 Let f be any non-constant Lipschitz function. Then, if kÌ‚t denotes the Lipschitz constant estimate of Algorithm 2 computed with any increasing sequence kiâˆˆZ defining a meshgrid of R+ over a sample
(X1 , f (X1 )), . . . , (Xt , f (Xt )) of t â‰¥ 2 evaluations where
X1 , . . . , Xt are uniformly and independently distributed

Algorithm 2 A DA LIPO(n, p, kiâˆˆZ , X , f )
1. Initialization: Let X1 âˆ¼ U(X )
..... Evaluate f (X1 ), t â† 1, kÌ‚1 â† 0
2. Iterations: Repeat while t < n
..... Let Bt+1 âˆ¼ B(p)
..... If Bt+1 = 1 (Exploration)
........... Let Xt+1 âˆ¼ U(X )
..... If Bt+1 = 0 (Exploitation)
........... Let Xt+1 âˆ¼ U(XkÌ‚t ,t ) where XkÌ‚t ,t denotes the set
........... of potential maximizers introduced in Definition 8
........... computed with k set to kÌ‚t
..... Evaluate f (X
t+1 ), t â† t + 1

|f (Xi ) âˆ’ f (Xj )|
â‰¤ ki
..... Let kÌ‚t := inf kiâˆˆZ : max
i6=j
kXi âˆ’ Xj k2

3. Output: Return XÄ±Ì‚n where Ä±Ì‚n âˆˆ arg maxi=1...n f (Xi )
over X , we have that


P f âˆˆ Lip(kÌ‚t ) â‰¥ 1 âˆ’ (1 âˆ’ Î“(f, ki? 1 ))bt/2c
where the coefficient


|f (X1 ) âˆ’ f (X2 )|
?
?
> ki 1 > 0
Î“(f, ki 1 ) := P
kX1 âˆ’ X2 k2
with i? = min{i âˆˆ Z : f âˆˆ Lip(ki )}, is strictly positive.

Remark 19 (M EASURE OF GLOBAL SMOOTHNESS ) The
coefficient Î“(f, ki? 1 ) which appears in the lower bound
of Proposition 18 can be seen as a measure of the global
smoothness of the function f with regards to ki? 1 . Indeed,
Pbt/2c
observing that 1/bt/2c i=1 I{|f (Xi )âˆ’f (Xi+bt/2c )| >
p
ki? 1 kXi âˆ’ Xbt/2c+i k2 } âˆ’
â†’ Î“(f, ki? 1), it is easy to see
that Î“ records the ratio of volume the product space X Ã— X
where f is witnessed to be at least ki? 1 Lipschitz.
Remark 20 (D ENSITY OF THE SEQUENCE ) As a direct
consequence of the previous remark, we point out that
the density of the sequence kiâˆˆZ , captured here by Î± =
supiâˆˆZ (ki+1 âˆ’ ki )/ki has opposite impacts on the maximal deviation of the estimate and its convergence rate.
Indeed, since Î± is involved in both the following upper
bounds on the deviation (limtâ†’âˆ kÌ‚t âˆ’ k ? )/k ? â‰¤ Î± where
k ? = sup{k â‰¥ 0 : f âˆˆ
/ Lip(k)} and on the coefficient
Î“(f, ki? 1) â‰¤ Î“(f, k ? /(1 + Î±)), we deduce that using a
sequence with a small Î± reduces the bias but also the convergence rate through a small coefficient Î“(f, ki? âˆ’1 ).
Analysis of AdaLIPO. Given the consistency equivalence
of Proposition 3, one can directly obtain the following
asymptotic result.
Proposition 21 (C ONSISTENCY ) The AdaLIPO algorithm
tuned with any parameter p âˆˆ (0, 1) and any sequence of

Global optimization of Lipschitz functions

Lipschitz constant kiâˆˆZ covering R+ is consistent over the
set of Lipschitz functions, i.e.,
âˆ€f âˆˆ

S

p

kâ‰¥0

Lip(k), max f (Xi ) âˆ’
â†’ max f (x).
i=1...n

xâˆˆX

The next result provides a first finite-time bound on the difference between the maximum and its approximation.
Proposition 22 (U PPER B OUND ) Consider AdaLIPO
tuned with any p âˆˆ (0, 1) and any sequence kiâˆˆZ defin+
ing
for any non-constant f âˆˆ
S a meshgrid of R . Then,
?
kâ‰¥0 Lip(k), any n âˆˆ N and Î´ âˆˆ (0, 1), we have with
probability at least 1 âˆ’ Î´,
max f (x) âˆ’ max f (Xi ) â‰¤ diam(X ) Ã—
i=1...n

xâˆˆX


k Ã—
i?

2 ln(Î´/3)
5
+
p p ln(1 âˆ’ Î“(f, ki? -1 ))

 d1


Ã—

ln(3/Î´)
n

 d1

where Î“(f, ki? 1 ) and i? are defined as in Proposition 18.
This result might be misleading since it advocates that doing pure exploration gives the best rate (i.e., when p â†’ 1).
However, as Proposition 18 provides us with the guarantee that f âˆˆ Lip(kÌ‚t ) within a finite number of iterations
where kÌ‚t denotes the Lipschitz constant estimate, one can
recover faster convergence rates similar to the one reported
for LIPO where the constant k is assumed to be known.
Theorem 23 (FAST R ATES ) Consider the same assumptions as in Proposition 22 and assume in addition that the
function f satisfies Condition 1 for some Îº â‰¥ 1, cÎº â‰¥ 0.
Then, for any n âˆˆ N? and Î´ âˆˆ (0, 1), we have with probability at least 1 âˆ’ Î´,
max f (x) âˆ’ max f (Xi ) â‰¤ diam(X )Ã—
xâˆˆX

i=1...n

ki? Ã— exp



2 ln(Î´/4)
p ln(1âˆ’Î“(f,ki?

1 ))

+

7 ln(4/Î´)
p(1âˆ’p)2



Ã—



ï£±
n (1 âˆ’ p) ln(2)
ï£´
ï£´
âˆš
exp
âˆ’C
Â·
,
ï£´
ki? ,Îº
ï£´
ï£´
2 ln(n/Î´) + 4(2 d)d
ï£´
ï£²

Îº=1

ï£´
Îº
ï£´

âˆ’ d(Îºâˆ’1)
ï£´
d(Îºâˆ’1)
ï£´
n(1
âˆ’
p)(2
âˆ’
1)
ï£´
Îº
ï£´
âˆš
,
Îº>1
ï£³2 1 + Cki? ,Îº Â·
2 ln(n/Î´) + 4(2 d)d
Îºâˆ’1

where Cki? ,Îº = (cÎº , maxxâˆˆX kx âˆ’ x? k2

/8ki? )d .

This bound shows the precise impact of the parameters p
and kiâˆˆZ on the convergence of the algorithm. In particular,
it illustrates the complexity of the exploration/exploitation
trade-off through a constant term and a convergence rate
which are inversely correlated to the exploration parameter
and the density of the sequence of Lipschitz constants.

4.3. Comparison with previous works
The DIRECT algorithm (Jones et al., 1993) is a Lipschitz
algorithm with unknown constant which uses a deterministic splitting technique of the search space to evaluate the
function on subdivisions of the space that have recorded
the highest evaluation among all subdivisions of similar
size. Moreover, (Munos, 2014) generalized DIRECT in
a broader setting by extending DOO to any unknown and
arbitrary local semi-metric. With regards to these works,
we proposed an alternative stochastic strategy which directly relies on the estimation of the Lipschitz constant and
thus only presents guarantees for globally smooth functions. However, as far as we know, only the consistency
property of DIRECT was shown in (Finkel & Kelley, 2004)
and (Munos, 2014) derived convergence rates of the same
order as for DOO,
âˆš except that the best rate they derive is
of order O(eâˆ’c n ) to be compared with the fast rate of
AdaLIPO which is of order OPâˆ— (eâˆ’cn ). The conclusion of
the comparison thus remains the same as in Section 3: exploiting the global smoothness instead of just the local one
allows to derive faster algorithms in the some cases where
the unknown function is indeed globally smooth.

5. Experiments
We compare here the empirical performance of AdaLIPO
with five state-of-the-art global optimization methods.
Algorithms. BAYES O PTâˆ— (Martinez-Cantin, 2014) is a
Bayesian optimization algorithm which uses a distribution
over functions to build a surrogate model of the unknown
function. The parameters of the distribution are estimated
during the optimization process. CMA-ESâ€¡(Hansen, 2006)
is an evolutionary algorithm which samples the new evaluation points according to a multivariate normal distribution
with mean vector and covariance matrix computed from the
previous evaluations. CRSâ€ (Kaelo & Ali, 2006) is a variant of PRS including local mutations which starts with a
random population and evolves these points by an heuristic rule. MLSLâ€ (Kan & Timmer, 1987) is a multistart algorithm performing a series of local optimizations starting from points randomly chosen by a clustering heuristic that helps to avoid repeated searches of the same local optima. DIRECTâ€  (Jones et al., 1993) and PRS were
previously introduced. For a fair comparison, the tuning
parameters were all set to default and AdaLIPO was constantly used with a parameter p set to 0.1 and a sequence
ki = (1 + 0.01/d)i fixed by an arbitrary rule of thumb. 1
Data sets. Following the steps of (Malherbe & Vayatis,
2016), we first studied the task of estimating the regularization parameter Î» and the bandwidth Ïƒ of a gaussian kernel
ridge regression minimizing the empirical mean squared
â€¡

1
In Python 2.7 from âˆ— BayesOpt (Martinez-Cantin, 2014),
CMA 1.1.06 (Hansen, 2011) and â€  NLOpt (Johnson, 2014).

Global optimization of Lipschitz functions
Problem

Auto-MPG

BreastCancer

Concrete

Housing

Yacht

HolderTable

Rosenbrock

LinearSlope

Sphere

Deb N.1

AdaLIPO
BayesOpt
CMA-ES
CRS
DIRECT
MLSL
PRS

14.6 (Â±09)
10.8 (Â±03)
29.3 (Â±25)
28.7 (Â±14)
11.0 (Â±00)
13.1 (Â±15)
65.1 (Â±62)

05.4 (Â±03)
06.8 (Â±04)
11.1 (Â±09)
08.9 (Â±08)
06.0 (Â±00)
06.6 (Â±03)
10.6 (Â±10)

04.9 (Â±02)
06.4 (Â±03)
10.4 (Â±08)
10.0 (Â±09)
06.0 (Â±00)
06.1 (Â±04)
09.8 (Â±09)

05.4 (Â±04)
07.5 (Â±04)
12.4 (Â±12)
13.8 (Â±10)
06.0 (Â±00)
07.2 (Â±03)
11.5 (Â±10)

25.2 (Â±21)
13.8 (Â±20)
29.6 (Â±25)
32.6 (Â±15)
11.0 (Â±00)
14.4 (Â±13)
73.3 (Â±72)

077 (Â±058)
410 (Â±417)
080 (Â±115)
307 (Â±422)
080 (Â±000)
305 (Â±379)
210 (Â±202)

07.5 (Â±07)
07.6 (Â±05)
10.0 (Â±10)
09.0 (Â±09)
10.0 (Â±00)
06.9 (Â±05)
09.0 (Â±09)

029 (Â±13)
032 (Â±58)
100 (Â±76)
094 (Â±43)
092 (Â±00)
016 (Â±33)
831(Â±283)

036 (Â±12)
019 (Â±03)
171 (Â±68)
233 (Â±54)
031 (Â±00)
175(Â±302)
924(Â±210)

916(Â±225)
814(Â±276)
930(Â±166)
980(Â±166)
1000(Â±00)
198(Â±326)
977(Â±117)

target 90%

AdaLIPO
BayesOpt
CMA-ES
CRS
DIRECT
MLSL
PRS

17.7 (Â±09)
12.2 (Â±06)
42.9 (Â±31)
35.8 (Â±13)
11.0 (Â±00)
15.0 (Â±15)
139 (Â±131)

06.6 (Â±04)
08.4 (Â±03)
13.7 (Â±10)
13.6 (Â±10)
11.0 (Â±00)
07.6 (Â±03)
17.7 (Â±17)

06.4 (Â±04)
07.9 (Â±03)
13.5 (Â±10)
14.6 (Â±11)
11.0 (Â±00)
07.3 (Â±04)
14.0 (Â±12)

17.9 (Â±25)
13.9 (Â±22)
23.0 (Â±16)
22.8 (Â±12)
19.0 (Â±00)
16.3 (Â±10)
39.6 (Â±39)

33.3 (Â±26)
15.9 (Â±21)
40.5 (Â±30)
38.3 (Â±31)
27.0 (Â±00)
16.3 (Â±13)
247(Â±249)

102 (Â±065)
418 (Â±410)
136 (Â±184)
580 (Â±444)
080 (Â±000)
316 (Â±384)
349 (Â±290)

11.5 (Â±11)
12.0 (Â±08)
16.1 (Â±13)
15.8 (Â±14)
10.0 (Â±00)
08.8 (Â±05)
18.0 (Â±17)

053 (Â±22)
032 (Â±59)
151 (Â±94)
131 (Â±62)
116 (Â±00)
018 (Â±37)
985(Â±104)

042 (Â±11)
045 (Â±16)
223 (Â±57)
340 (Â±66)
098 (Â±00)
226(Â±336)
1000(Â±00)

986(Â±255)
949(Â±153)
952(Â±127)
997(Â±127)
1000(Â±00)
215(Â±328)
998(Â±025)

target 95%

AdaLIPO
BayesOpt
CMA-ES
CRS
DIRECT
MLSL
PRS

32.6 (Â±16)
14.0 (Â±07)
73.7 (Â±49)
48.5 (Â±16)
47.0 (Â±00)
20.6 (Â±17)
747(Â±330)

34.1 (Â±36)
31.0 (Â±51)
35.1 (Â±20)
34.8 (Â±12)
27.0 (Â±00)
12.8 (Â±03)
145(Â±124)

70.8 (Â±58)
28.2 (Â±34)
46.3 (Â±29)
36.6 (Â±15)
37.0 (Â±00)
14.7 (Â±10)
176(Â±148)

65.4 (Â±62)
17.9 (Â±22)
61.5 (Â±85)
43.7 (Â±14)
41.0 (Â±00)
16.3 (Â±10)
406(Â±312)

61.7 (Â±39)
18.5 (Â±22)
70.9 (Â±50)
52.9 (Â±18)
49.0 (Â±00)
21.4 (Â±14)
779(Â±334)

212 (Â±129)
422 (Â±407)
215 (Â±198)
599 (Â±427)
080 (Â±000)
322 (Â±382)
772 (Â±310)

44.6 (Â±39)
27.6 (Â±22)
43.5 (Â±37)
42.7 (Â±23)
24.0 (Â±00)
19.4 (Â±49)
100(Â±106)

122 (Â±31)
032 (Â±59)
211 (Â±92)
168 (Â±76)
226 (Â±00)
022 (Â±42)
1000(Â±00)

052 (Â±10)
222 (Â±77)
308 (Â±60)
607 (Â±81)
548 (Â±00)
304(Â±357)
1000(Â±00)

1000(Â±00)
1000(Â±00)
962(Â±106)
1000(Â±00)
1000(Â±00)
256(Â±334)
1000(Â±00)

target 99%

Table 2. Results of the numerical experiments. The table displays the number of evaluations required by each method to reach the
specified target (mean Â± standard deviation). In bold, the best result obtained in terms of average of function evaluations.

error of the predictions over a 10-fold cross validation
with real data sets. The optimization was performed over
(ln(Î»), ln(Ïƒ)) âˆˆ [âˆ’3, 5] Ã— [âˆ’2, 2] with five data sets from
the UCI Machine Learning Repository (Lichman, 2013):
Auto-MPG, Breast Cancer Wisconsin (Prognostic), Concrete slump test, Housing and Yacht Hydrodynamics. We
then compared the algorithms on a series of five synthetic
problems commonly met in standard optimization benchmark taken from (Jamil & Yang, 2013; Surjanovic & Bingham, 2013): HolderTable, Rosenbrock, Sphere, LinearSlope and Deb N.1. This series includes multimodal and
non-linear functions as well as ill-conditioned and wellshaped functions with a dimensionality ranging from 2 to 5.
A complete description of the test functions of the benchmark can be found in the Supplementary Material.
Protocol and performance metrics. For each problem
and each algorithm, we performed K = 100 distinct runs
with a budget of n =1000 function evaluations. For each
target parameter t = 90%, 95% and 99%, we have collected the stopping times corresponding to the number of
evaluations required by each method to reach the specified
(k)
target Ï„k := min{i = 1, . . . , n : f (Xi ) â‰¥ ftarget (t)}
(k)
where min{âˆ…} = 1000 by convention, {f (Xi )}ni=1 denotes the evaluations made by a given method on the k-th
run with k â‰¤ K and the target value
R is set to ftarget (t) :=
maxxâˆˆX f (x)âˆ’ maxxâˆˆX f (x) âˆ’ xâˆˆX f (x) dx/Âµ(X ) Ã—
(1 âˆ’ t). The normalization of the target to the average
value prevents the performance measures from being dependent of any constant term in the unknown function. In
practice, the average was estimated from a Monte Carlo
sampling of 106 evaluations and the maximum by taking
the best value observed over all the sets of experiments.
Based on these stopping times, we computed the average
and standard deviation of the number of
rePevaluations
K
quired to reach the target, i.e. Ï„Ì„K =
Ï„
/K
and
k=1 k

PK
ÏƒÌ‚Ï„ = ( k=1 (Ï„k âˆ’ Ï„Ì„K )2 /K)1/2 .
Results. Results are collected in Table 2. Due to space
constraints, we only make few comments. First, we point
out that the proposed method displays very competitive results over most of the problems of the benchmark (except
on the non-smooth DebN.1 where most methods fail). In
particular, AdaLIPO obtains several times the best performance for the target 90% and 95% (see, e.g., BreastCancer,
HolderTable, Sphere) and experiments Linear Slope and
Sphere also suggest that, in the case of smooth functions, it
can be robust against the dimensionality of the input space.
However, in some cases, the algorithm can be witnessed
to reach the 95% target with very few evaluations while
getting more slowly to the 99% target (see, e.g., Concrete,
Housing). This problem is due to the instability of the Lipschitz constant estimate around the maxima but could certainly be solved with the addition of a noise parameter that
would allow the algorithm be more robust against local perturbations. Additionally, investigating better values for p
and ki as well as alternative covering methods such as LHS
(Stein, 1987) could also be promising approaches to improve its performance. However, an empirical analysis of
the algorithm with these extensions is beyond the scope of
the paper and will be carried out in a future work.

6. Conclusion
We introduced two novel strategies for global optimization:
LIPO which requires the knowledge of the Lipschitz constant and its adaptive version AdaLIPO which estimates
the constant during the optimization process. A theoretical analysis is provided and empirical results based on synthetic and real problems have been obtained demonstrating
the performance of the adaptive algorithm with regards to
existing state-of-the-art global optimization methods.

Global optimization of Lipschitz functions

References
Bull, Adam D. Convergence rates of efficient global optimization algorithms. The Journal of Machine Learning
Research, 12:2879â€“2904, 2011.
Dasgupta, Sanjoy. Two faces of active learning. Theoretical
Computer Science, 412(19):1767â€“1781, 2011.

Lichman, Moshe. UCI machine learning repository, 2013.
URL http://archive.ics.uci.edu/ml.
Malherbe, CeÌdric and Vayatis, Nicolas.
A ranking
approach to global optimization.
arXiv preprint
arXiv:1603.04381, 2016.

Finkel, Daniel E and Kelley, CT. Convergence analysis of
the direct algorithm. Optimization On-line Digest, 2004.

Malherbe, CeÌdric, Contal, Emile, and Vayatis, Nicolas. A
ranking approach to global optimization. In In Proceedings of the 33st International Conference on Machine
Learning, pp. 1539â€“1547, 2016.

Grill, Jean-Bastien, Valko, Michal, and Munos, ReÌmi.
Black-box optimization of noisy functions with unknown smoothness. In Neural Information Processing
Systems, 2015.

Martinez-Cantin, Ruben. Bayesopt: A bayesian optimization library for nonlinear optimization, experimental design and bandits. The Journal of Machine Learning Research, 15(1):3735â€“3739, 2014.

Hanneke, Steve. Rates of convergence in active learning.
The Annals of Statistics, 39(1):333â€“361, 2011.

Mladineo, Regina Hunter. An algorithm for finding the
global maximum of a multimodal, multivariate function.
Mathematical Programming, 34(2):188â€“200, 1986.

Hansen, Nikolaus. The cma evolution strategy: a comparing review. In Towards a New Evolutionary Computation, pp. 75â€“102. Springer, 2006.
Hansen, Nikolaus. The cma evolution strategy: A tutorial. Retrieved May 15, 2016, from http://www.
lri.fr/hansen/cmaesintro.html, 2011.
Huyer, Waltraud and Neumaier, Arnold. Global optimization by multilevel coordinate search. Journal of Global
Optimization, 14(4):331â€“355, 1999.
Jamil, Momin and Yang, Xin-She. A literature survey of
benchmark functions for global optimization problems.
International Journal of Mathematical Modelling and
Numerical Optimisation, 4(2):150â€“194, 2013.
Johnson, Steven G. The NLopt nonlinear-optimization
package. Retrieved May 15, 2016, from http://
ab-initio.mit.edu/nlopt, 2014.
Jones, Donald R, Perttunen, Cary D, and Stuckman,
Bruce E. Lipschitzian optimization without the lipschitz
constant. Journal of Optimization Theory and Applications, 79(1):157â€“181, 1993.
Jones, Donald R., Schonlau, Matthias, and Welch,
William J. Efficient global optimization of expensive
black-box functions. Journal of Global Optimization,
13(4):455â€“492, 1998.
Kaelo, Professor and Ali, Montaz. Some variants of the
controlled random search algorithm for global optimization. Journal of Optimization Theory and Applications,
130(2):253â€“264, 2006.
Kan, AHG Rinnooy and Timmer, Gerrit T. Stochastic
global optimization methods part i: Clustering methods.
Mathematical Programming, 39(1):27â€“56, 1987.

Munos, ReÌmi. From bandits to monte-carlo tree search:
The optimistic principle applied to optimization and
R in Machine Learnplanning. Foundations and Trends
ing, 7(1):1â€“129, 2014.
PinteÌr, JaÌnos D. Global optimization in action. Scientific
American, 264:54â€“63, 1991.
Piyavskii, SA. An algorithm for finding the absolute extremum of a function. USSR Computational Mathematics and Mathematical Physics, 12(4):57â€“67, 1972.
Preux, Philippe, Munos, ReÌmi, and Valko, Michal. Bandits
attack function optimization. In Evolutionary Computation (CEC), 2014 IEEE Congress on, pp. 2245â€“2252.
IEEE, 2014.
Rios, Luis Miguel and Sahinidis, Nikolaos V. Derivativefree optimization: a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247â€“1293, 2013.
Shubert, Bruno O. A sequential method seeking the global
maximum of a function. SIAM Journal on Numerical
Analysis, 9(3):379â€“388, 1972.
Stein, Michael. Large sample properties of simulations
using latin hypercube sampling. Technometrics, 29(2):
143â€“151, 1987.
Surjanovic, Sonja and Bingham, Derek. Virtual library of
simulation experiments: Test functions and datasets. Retrieved May 15, 2016, from http://www.sfu.ca/
Ëœssurjano, 2013.
Valko, Michal, Carpentier, Alexandra, and Munos, ReÌmi.
Stochastic simultaneous optimistic optimization. In In
Proceedings of the 30th International Conference on
Machine Learning, pp. 19â€“27, 2013.

Global optimization of Lipschitz functions

Zhigljavsky, A.A. and PinteÌr, J.D. Theory of Global Random Search. Mathematics and its Applications. Springer
Netherlands, 1991. ISBN 9780792311225.

