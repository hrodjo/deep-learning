Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

Yichen Chen 1 Dongdong Ge 2 Mengdi Wang 1 Zizhuo Wang 3 Yinyu Ye 4 Hao Yin 4

Abstract
Consider the regularized sparse minimization
problem, which involves empirical sums of loss
functions for n data points (each of dimension
d) and a nonconvex sparsity penalty. We prove
that finding an O(nc1 dc2 )-optimal solution to
the regularized sparse optimization problem is
strongly NP-hard for any c1 , c2 ∈ [0, 1) such that
c1 + c2 < 1. The result applies to a broad class
of loss functions and sparse penalty functions.
It suggests that one cannot even approximately
solve the sparse optimization problem in polynomial time, unless P = NP.

Keywords: Nonconvex optimization · Computational
complexity · NP-hardness · Concave penalty · Sparsity

1

Introduction

We study the sparse minimization problem, where the objective is the sum of empirical losses over input data and a
sparse penalty function. Such problems commonly arise
from empirical risk minimization and variable selection.
The role of the penalty function is to induce sparsity in the
optimal solution, i.e., to minimize the empirical loss using
as few nonzero coefficients as possible.
Problem 1 Given the loss function ` : R × R 7→ R+ ,
penalty function p : R 7→ R+ , and regularization parameter
λ > 0, consider the problem
min

x∈Rd

n
X
i=1

` aTi x, bi + λ


d
X

p (|xj |) ,

j=1

where A = (a1 , . . . , an )T ∈ Rn×d , b = (b1 , . . . , bn )T
∈ Rn are input data.
1
Princeton University, NJ, USA 2 Shanghai University of Finance and Economics, Shanghai, China 3 University of Minnesota,
MN, USA 4 Stanford University, CA, USA. Correspondence to:
Mengdi Wang <mengdiw@princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

We are interested in the computational complexity of Problem 1 under general conditions of the loss function ` and
the sparse penalty p. In particular, we focus on the case
where ` is a convex loss function and p is a concave penalty
with a unique minimizer at 0. Optimization problems with
convex ` and concave p are common in sparse regression,
compressive sensing, and sparse approximation. A list of
applicable examples of ` and p is given in Section 3.
For certain special cases of Problem 1, it has been shown
that finding an exact solution is strongly NP-hard (Huo &
Chen, 2010; Chen et al., 2014). However, these results have
not excluded the possibility of the existence of polynomialtime algorithms with small approximation error. (Chen
& Wang, 2016) established the hardness of approximately
solving Problem 1 when p is the L0 norm.
In this paper, we prove that it is strongly NP-hard to approximately solve Problem 1 within certain optimality error. More precisely, we show that there exists a lower
bound on the suboptimality error of any polynomial-time
deterministic algorithm. Our results apply to a variety of
optimization problems in estimation and machine learning.
Examples include sparse classification, sparse logistic regression, and many more. The strong NP-hardness of approximation is one of the strongest forms of complexity
result for continuous optimization. To our best knowledge,
this paper gives the first and strongest set of hardness results for Problem 1 under very general assumptions regarding the loss and penalty functions.
Our main contributions are three-fold.
1. We prove the strong NP-hardness for Problem 1 with
general loss functions. This is the first results that
apply to the broad class of problems including but
not limited to: least squares regression, linear model
with Laplacian noise, robust regression, Poisson regression, logistic regression, inverse Gaussian models,
etc.
2. We present a general condition on the sparse penalty
function p such that Problem 1 is strongly NP-hard.
The condition is a slight weaker version of strict concavity. It is satisfied by typical penalty functions such
as the Lq norm (q ∈ [0, 1)), clipped L1 norm, SCAD,
etc. To the best of our knowledge, this is the most gen-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

eral condition on the penalty function in the literature.
3. We prove that finding an O (λnc1 dc2 )-optimal solution to Problem 1 is strongly NP-hard, for any c1 , c2 ∈
[0, 1) such that c1 + c2 < 1. Here the O(·) hides parameters that depend on the penalty function p, which
is to be specified later. It illustrates a gap between
the optimization error achieved by any tractable algorithm and the desired statistical precision. Our proof
provides a first unified analysis that deals with a broad
class of problems taking the form of Problem 1.
Section 2 summarizes related literatures from optimization,
machine learning and statistics. Section 3 presents the key
assumptions and illustrates examples of loss and penalty
functions that satisfy the assumptions. Section 4 gives the
main results. Section 5 discusses the implications of our
hardness results. Section 6 provides a proof of the main
results in a simplified setting. The full proofs are deferred
to the appendix.

2

Background and Related Works

Sparse optimization is a powerful machine learning tool for
extracting useful information for massive data. In Problem
1, the sparse penalty serves to select the most relevant variables from a large number of variables, in order to avoid
overfitting. In recent years, nonconvex choices of p have received much attention; see (Frank & Friedman, 1993; Fan
& Li, 2001; Chartrand, 2007; Candes et al., 2008; Fan &
Lv, 2010; Xue et al., 2012; Loh & Wainwright, 2013; Wang
et al., 2014; Fan et al., 2015).
Within the optimization and mathematical programming
community, the complexity of Problem 1 has been considered in a number of special cases. (Huo & Chen, 2010) first
proved the hardness result for a relaxed family of penalty
functions with L2 loss. They show that for the penalties in
L0 , hard-thresholded (Antoniadis & Fan, 2001) and SCAD
(Fan & Li, 2001), the above optimization problem is NPhard. (Chen et al., 2014) showed that the L2 -Lp minimization is strongly NP-hard when p ∈ (0, 1). At the same time,
(Bian & Chen, 2014) proved the strongly NP-hardness for
another class of penalty functions. The preceding existing
analyses mainly focused on finding an exact global optimum to Problem 1. For this purpose, they implicitly assumed that all the input and parameters involved in the reduction are rational numbers with a finite numerical representation, otherwise finding a global optimum to a continuous problem would be always intractable. A recent technical report (Chen & Wang, 2016) proves the hardness of
obtaining an -optimal solution when p is the L0 norm.
Within the theoretical computer science community, there
have been several early works on the complexity of sparse

recovery, beginning with (Arora et al., 1993). (Amaldi &
Kann, 1998) proved that the problem min{kxk0 | Ax = b}
1−
is not approximable within a factor 2log d for any  > 0.
(Natarajan, 1995) showed that, given  > 0, A and b, the
problem min{kxk0 | kAx − bk2 ≤ } is NP-hard. (Davis
et al., 1997) proved a similar result that for some given
 > 0 and M > 0, it is NP-complete to find a solution
x such that kxk0 ≤ M and kAx − bk ≤ . More recently,
(Foster et al., 2015) studied sparse recovery and sparse linear regression with subgaussian noises. Assuming that the
true solution is K-sparse, it showed that no polynomial1−δ
time (randomized) algorithm can find a K · 2log d -sparse
2
C1 1−C2
solution x with kAx−bk2 ≤ d n
with high probability, where δ, C1 , C2 are arbitrary positive scalars. Another
work (Zhang et al., 2014) showed that under the Gaussian
linear model, there exists a gap between the mean square
loss that can be achieved by polynomial-time algorithms
and the statistically optimal mean squared error. These two
works focus on estimation of linear models and impose distributional assumptions regarding the input data. These results on estimation are different in nature with our results
on optimization.
In contrast, we focus on the optimization problem itself.
Our results apply to a variety of loss functions and penalty
functions, not limited to linear regression. Moreover, we do
not make any distributional assumption regarding the input
data.
There remain several open questions. First, existing results
mainly considered least square problems or Lq minimization problems. Second, existing results focused mainly on
the L0 penalty function. The complexity of Problem 1 with
general loss function and penalty function is yet to be established. Things get complicated when p is a continuous
function instead of the discrete L0 norm function. The
complexity for finding an -optimal solution with general
` and p is not fully understood. We will address these questions in this paper.

3

Assumptions

In this section, we state the two critical assumptions that
lead to the strong NP-hardness results: one for the penalty
function p, the other one for the loss function `. We argue that these assumptions are essential and very general.
They apply to a broad class of loss functions and penalty
functions that are commonly used.

3.1

Assumption About Sparse Penalty

Throughout this paper, we make the following assumption
regarding the sparse penalty function p(·).
Assumption 1. The penalty function p(·) satisfies the fol-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

3.2

lowing conditions:
(i) (Monotonicity) p(·) is non-decreasing on [0, +∞).
(ii) (Concavity) There exists τ > 0 such that p(·) is concave but not linear on [0, τ ].
In words, condition (ii) means that the concave penalty p(·)
is nonlinear. Assumption 1 is the most general condition
on penalty functions in the existing literature of sparse optimization. Below we present a few such examples.
1. In variable selection problems, the L0 penalization
p(t) = I{t6=0} arises naturally as a penalty for the
number of factors selected.
2. A natural generalization of the L0 penalization is the
Lp penalization p(t) = tp where (0 < p < 1).
The corresponding minimization problem is called the
bridge regression problem (Frank & Friedman, 1993).
3. To obtain a hard-thresholding estimator, Antoniadis &
Fan (2001) use the penalty functions pγ (t) = γ 2 −
((γ − t)+ )2 with γ > 0, where (x)+ := max{x, 0}
denotes the positive part of x.
4. Any penalty function that belongs to the folded concave penalty family (Fan et al., 2014) satisfies the conditions in Theorem 1. Examples include the SCAD
(Fan & Li, 2001) and the MCP (Zhang, 2010a),
whose derivatives on (0, +∞) are p0γ (t) = γI{t≤γ} +
(aγ−t)+
a−1 I{t>γ}

and p0γ (t) = (γ − bt )+ , respectively, where γ > 0, a > 2 and b > 1.
5. The conditions in Theorem 1 are also satisfied by the
clipped L1 penalty function (Antoniadis & Fan, 2001;
Zhang, 2010b) pγ (t) = γ · min(t, γ) with γ > 0.
This is a special case of the piecewise linear penalty
function:

k1 t
if 0 ≤ t ≤ a
p(t) =
k2 t + (k1 − k2 )a if t > a

Assumption About Loss Function

We state our assumption about the loss function `.
Assumption 2. Let M be an arbitrary constant. For any
interval [τ1 , τ2 ] where 0 < τ1 < τ2 < M , there exists
Pk
k ∈ Z+ and b ∈ Qk such that h(y) = i=1 `(y, bi ) has
the following properties:
(i) h(y) is convex and Lipschitz continuous on [τ1 , τ2 ].
(ii) h(y) has a unique minimizer y ∗ in (τ1 , τ2 ).
(iii) There exists N ∈ Z+ , δ̄ ∈ Q+ and C ∈ Q+ such that
when δ ∈ (0, δ̄), we have
h(y ∗ ± δ) − h(y ∗ )
≥ C.
δN
(iv) h(y ∗ ), {bi }ki=1 can be represented in O(log
bits.

1
τ2 −τ1 )

Assumption 2 is a critical, but very general, assumption
regarding the loss function `(y, b). Condition (i) requires
convexity and Lipschitz continuity within a neighborhood.
Conditions (ii), (iii) essentially require that, given an interval [τ1 , τ2 ], one can artificially pick b1 , . . . , bk to conPk
struct a function h(y) = i=1 `(y, bi ) such that h has its
unique minimizer in [τ1 , τ2 ] and has enough curvature near
the minimizer. This property ensures that a bound on the
minimal value of h(y) can be translated to a meaningful
bound on the minimizer y ∗ . The conditions (i), (ii), (iii)
are typical properties that a loss function usually satisfies.
Condition (iv) is a technical condition that is used to avoid
dealing with infinitely-long irrational numbers. It can be
easily verified for almost all common loss functions.
We will show that Assumptions 2 is satisfied by a variety
of loss functions. An (incomplete) list is given below.
1. In the least squares regression, the loss function has
the form
n
X
2
aTi x − bi .
i=1

where 0 ≤ k2 < k1 and a > 0.
6. Another family of penalty functions which bridges the
L0 and L1 penalties are the fraction penalty functions
(γ + 1)t
with γ > 0 (Lv & Fan, 2009).
pγ (t) =
γ+t
7. The family of log-penalty functions:
pγ (t) =

1
log(1 + γt)
log(1 + γ)

with γ > 0, also bridges the L0 and L1 penalties (Candes et al., 2008).

Using our notation, the corresponding loss function
is `(y, b) = (y − b)2 . For all τ1 , τ2 , we choose an
arbitrary b0 ∈ [τ1 , τ2 ]. We can verify that h(y) =
`(y, b0 ) satisfies all the conditions in Assumption 2.
2. In the linear model with Laplacian noise, the negative
log-likelihood function is
n
X
 T

ai x − bi  .
i=1

So the loss function is `(y, b) = |y − b|. As in the case
of least squares regression, the loss function satisfy

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions
q/r
at y = ln 1−q/r
. To verify (iii), we consider the sec-

Assumption 2. Similar argument also holds when we
consider the Lq loss | · |q with q ≥ 1.

q/r
, which
ond order Taylor expansion at y = ln 1−q/r
is
q
h(y + δ) − h(y) =
δ 2 + o(δ 2 )
2(1 + ey )

3. In robust regression, we consider the Huber loss (Huber, 1964) which is a mixture of L1 and L2 norms.
The loss function takes the form
 1
2
for |y − b| ≤ δ,
2 |y − b|
Lδ (y, b) =
δ(|y − b| − 12 δ) otherwise.

where y ∈ [τ1 , τ2 ]. Note that ey is bounded by eM ,
which can be computed beforehand. As a result, (iii)
holds as well.

for some δ > 0 where y = aT x. We then verify that
Assumption 2 is satisfied. For any interval [τ1 , τ2 ], we
pick an arbitrary b ∈ [τ1 , τ2 ] and let h(y) = `(y, b).
We can see that h(y) satisfies all the conditions in Assumption 2.

6. In the mean estimation of inverse Gaussian models
(McCullagh, 1984), the negative log-likelihood function minimization is
p
n
X
(bi · aTi x − 1)2
.
min
bi
x∈Rd
i=1

4. In Poisson regression (Cameron & Trivedi, 2013), the
negative log-likelihood minimization is
min − log L(x; A, b) = min

x∈Rd

x∈Rd

n
X

Now we show that the loss function `(y, b) =
√
(b· y−1)2
b

satisfies Assumption 2. By setting the
derivative to be zero with regard to y, we can see that
y take its minimum at y = 1/b2 . Thus for any [τ1 , τ2 ],
√
√
we choose b0 = q/r ∈ [1/ τ2 , 1/ τ1 ]. We can see
that h(y) = `(y, b0 ) satisfies all the conditions in Assumption 2.

(exp(aTi x)−bi ·aTi x).

i=1

We now show that `(y, b) = ey − b · y satisfies Assumption 2. For any interval [τ1 , τ2 ], we choose q and
r such that q/r ∈ [eτ1 , eτ2 ]. Note that eτ2 − eτ1 =
eτ1 +τ2 −τ1 − eτ1 ≥ τ2 − τ1 . Also, eτ2 is bounded
by eM . Thus, q, r can be chosen to be polynomial
in d1/(τ2 − τ1 )e by letting r = d1/(τ2 − τ1 )e and q
be some number less than r · eM . Then,
Pk we choose
k = r and b ∈ Zk such that h(y) = i=1 `(y, bi ) =
r · ey − q · y. Let us verify Assumption 2. (i), (iv)
are straightforward by our construction. For (ii), note
that h(y) take its minimum at ln(q/r) which is inside
[τ1 , τ2 ] by our construction. To verify (iii), consider
the second order Taylor expansion of h(y) at ln(q/r),
h(y + δ) − h(y) =

δ2
r · ey 2
· δ + o(δ 2 ) ≥
+ o(δ 2 ),
2
2

We can see that (iii) is satisfied. Therefore, Assumption 2 is satisfied.
5. In logistic regression, the negative log-likelihood
function minimization is
min

x∈Rd

n
X
i=1

log(1 +

exp(aTi x))

−

n
X

bi ·

aTi x.

i=1

We claim that the loss function `(y, b) = log(1 +
exp(y)) − b · y satisfies Assumption 2. By a similar argument as thePone in Poisson regression, we can verify
r
that h(y) = i=1 `(y, bi ) = r log(1 + exp(y)) − qy
eτ1
eτ2
where q/r ∈ [ 1+e
τ1 , 1+eτ2 ] and q, r are polynomial
in d1/(τ2 −τ1 )e satisfies all the conditions in Assumption 2. For (ii), observe that `(y, b) take its minimum

7. In the estimation of generalized linear model under the
exponential distribution (McCullagh, 1984), the negative log-likelihood function minimization is
min − log L(x; A, b) = min

x∈Rd

x∈Rd

bi
+ log(aTi x).
aTi x

By setting the derivative to 0 with regard to y, we can
see that `(y, b) = yb + log y has a unique minimizer at
y = b. Thus by choosing b0 ∈ [τ1 , τ2 ] appropriately,
we can readily show that h(y) = `(y, b0 ) satisfies all
the conditions in Assumption 2.
To sum up, the combination of any loss function given in
Section 3.1 and any penalty function given in Section 3.2
results in a strongly NP-hard optimization problem.

4

Main Results

In this section, we state our main results on the strong NPhardness of Problem 1. We warm up with a preliminary
result for a special case of Problem 1.
Theorem 1 (A Preliminary Result). Let Assumption 1
hold, and let p(·) be twice continuously differentiable in
(0, ∞). Then the minimization problem
minn kAx − bkqq + λ

x∈R

is strongly NP-hard.

d
X
j=1

p(|xj |),

(1)

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

The result shows that many of the penalized least squares
problems, e.g., (Fan & Lv, 2010), while enjoying small
estimation errors, are hard to compute. It suggests that
there does not exist a fully polynomial-time approximation
scheme for Problem 1. It has not answered the question:
whether one can approximately solve Problem 1 within certain constant error.
Now we show that it is not even possible to efficiently approximate the global optimal solution of Problem 1, unless
P = N P . Given an optimization problem minx∈X f (x),
we say that a solution x̄ is -optimal if x̄ ∈ X and f (x̄) ≤
inf x∈X f (x) + .

Theorem 2 (Strong NP-Hardness of Problem 1). Let Assumptions 1 and 2 hold, and let c1 , c2 ∈ [0, 1) be arbitrary such that c1 + c2 < 1. Then it is strongly NPhard to find a λ · κ · nc1 dc2 -optimal solution of Problem 1, where d is the dimension of variable space and
}.
κ = mint∈[τ /2,τ ] { 2p(t/2)−p(t)
t

The non-approximable error in Theorem 2 involves the
constant κ which is determined by the sparse penalty function p. In the case where p is the L0 norm function, we can
take κ = 1. In the case of piecewise linear L1 penalty, we
have κ = (k1 − k2 )/4. In the case of SCAD penalty, we
have κ = Θ(γ 2 ).
According to Theorem 2, the non-approximable error λ ·
κ · nc1 dc2 is determined by three factors: (i) properties of
the regularization penalty λ · κ; (ii) data size n; and (iii) dimension or number of variables d. This result illustrates a
fundamental gap that can not be closed by any polynomialtime deterministic algorithm. This gap scales up when either the data size or the number of variables increases. In
Section 5.1, we will see that this gap is substantially larger
than the desired estimation precision in a special case of
sparse linear regression.
Theorems 1 and 2 validate the long-lasting belief that optimization involving nonconvex penalty is hard. More importantly, Theorem 2 provide lower bounds for the optimization error that can be achieved by any polynomial-time
algorithm. This is one of the strongest forms of hardness
result for continuous optimization.

5

An Application and Remarks

In this section, we analyze the strong NP-hardness results
in the special case of linear regression with SCAD penalty
(Problem 1). We give a few remarks on the implication of
our hardness results.

5.1

Hardness of Regression with SCAD
Penalty

Let us try to understand how significant is the nonapproximable error of Problem 1. We consider the special
case of linear models with SCAD penalty. Let the input
data (A, b) be generated by the linear model Ax̄ + ε = b,
where x̄ is the unknown true sparse coefficients and ε is a
zero-mean multivariate subgaussian noise. Given the data
size n and variable dimension d, we follow (Fan & Li,
2001) and obtain a special case of Problem 1, given by
d
X
1
pγ (|xj |),
min kAx − bk22 + n
x 2
j=1

(2)

p
where γ = log d/n. (Fan & Li, 2001) showed that the
optimal solution x∗ of problem (2) has a small statistical
error, i.e., kx̄ − x∗ k22 = O n−1/2 + an , where an =
max{p0λ (|x∗j |) : x∗j 6= 0}. (Fan et al., 2015) further showed
√
that we only need to find a n log d-optimal solution to (2)
to achieve such a small estimation error.
However, Theorem 2 tells us that it is not possible to compute an d,n -optimal solution for problem (2) in polynomial
time, where d,n = λκn1/2 d1/3 (by letting c1 = 1/2, c2 =
1/3). In the special case of problem (2), we can verify that
λ = n and κ = Ω(γ 2 ) = Ω(log d/n). As a result, we see
that
p
d,n = Ω(n1/2 d1/3 )  n log d,
for high values of the dimension d. According to Theorem
2, it is strongly NP-hard to approximately √
solve problem
(2) within the required statistical precision n log d. This
result illustrates a sharp contrast between statistical properties of sparse estimation and the worst-case computational
complexity.

5.2

Remarks on the NP-Hardness Results

As illustrated by the preceding analysis, the nonapproximibility of Problem 1 suggests that computing the
sparse estimator is hard. The results suggest a fundamental conflict between computation efficiency and estimation accuracy in sparse data analysis. Although the results seem negative, they should not discourage researchers
from studying computational perspectives of sparse optimization. We make the following remarks:
1. Theorems 1, 2 are worst-case complexity results.
They suggest that one cannot find a tractable solution
to the sparse optimization problems, without making
any additional assumption to rule out the worst-case
instances.
2. Our results do not exclude the possibility that, under
more stringent modeling and distributional assump-

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

tions, the problem would be tractable with high probability or on average.

2. gθ,µ (t) has a unique global minimizer t∗ (θ, µ) ∈
(τ0 , τ );

In short, the sparse optimization Problem 1 is fundamentally hard from a purely computational perspective. This
paper together with the prior related works provide a complete answer to the computational complexity of sparse optimization.

3. Let δ̄ = min{t∗ (θ, µ) − τ0 , τ − t∗ (θ, µ), 1}, then for
any δ ∈ (0, δ̄), we have gθ,µ (t) < h(θ, µ) + δ 2 only if
|t − t∗ (θ, µ)| < δ, where h(θ, µ) is the minimal value
of gθ,µ (t).

6

Proof of Theorem 1

In this section, we prove Theorem 1. The proof of Theorems 2 is deferred to the appendix which is based on
the idea of the proof in this section. We construct a
polynomial-time reduction from the 3-partition problem
(Garey & Johnson, 1978) to the sparse optimization problem. Given a set S of 3m integers s1 , ...s3m , the three partition problem is to determine whether S can be partitioned
into m triplets such that the sum of the numbers in each
subset is equal. This problem is known to be strongly NPhard (Garey & Johnson, 1978). The main proof idea bears
a similar spirit as the works by Huo & Chen (2010), Chen
et al. (2014) and Chen & Wang (2016). The proofs of all
the lemmas can be found in the appendix.
We first illustrate several properties of the penalty function
if it satisfies the conditions in Theorem 1.
Lemma 3. If p(t) satisfies the conditions in Theorem 1,
then for any l ≥ 2, and any t1 , t2 , . . . , tl ∈ R, we have
p(|t1 |) + · · · + p(|tl |) ≥ min{p(|t1 + · · · + tl |), p(τ )}.
Lemma 4. If p(t) satisfies the conditions in Theorem 1,
then there exists τ0 ∈ (0, τ ) such that p(·) is concave
but not linear on [0, τ0 ] and is twice continuously differentiable on [τ0 , τ ]. Furthermore, for any t̃ ∈ (τ0 , τ ), let
δ̄ = min{τ0 /3, t̃ − τ0 , τ − t̃}. Then for any δ ∈ (0, δ̄)
l ≥ 2, and any t1 , t2 , . . . , tl such that t1 + · · · + tl = t̃, we
have
p(|t1 |) + · · · + p(|tl |) < p(t̃) + C1 δ
only if |ti − t̃| < δ for some i while |tj | < δ for all j 6= i,
0 /3)−p(τ0 )
> 0.
where C1 = p(τ0 /3)+p(2τ
τ0 /3
In our proof of Theorem 1, we will consider the following
function
gθ,µ (t) := p(|t|) + θ · |t|q + µ · |t − τ̂ |q

Lemma 6. If p(t) satisfies the conditions in Theorem 1,
q = 1, and τ0 satisfies the properties in Lemma 4, then
there exist µ̂ > 0 such that for any µ ≥ µ̂, the following
properties are satisfied:
0
0
1. g0,µ
(t) < −1 for any t ∈ [τ0 , τ̂ ) and g0,µ
(t) > 1 for
any t ∈ (τ̂ , τ ];

2. g0,µ (t) has a unique global minimizer t∗ (0, µ) = τ̂ ∈
(τ0 , τ );
3. Let δ̄ = min{τ̂ − τ0 , τ − τ̂ , 1}, then for any δ ∈ (0, δ̄),
we have g0,µ (t) < h(0, µ) + δ 2 only if |t − τ̂ | < δ.
By combining the above results, we have the following
lemma, which is useful in our proof of Theorem 1.
Lemma 7. Suppose p(t) satisfies the conditions in Theorem 1 and τ0 satisfies the properties in Lemma 4. Let
h(θ, µ) and t∗ (θ, µ) be as defined in Lemma 5 and Lemma
6 respectively for the case q > 1 and q = 1. Then we can
find θ and µ such that for any l ≥ 2, t1 , . . . , tl ∈ R,
q

q


 l

 l

X
X 
tj − τ̂  ≥ h(θ, µ).
tj  + µ · 
p(|tj |) + θ · 

 j=1
 j=1 
j=1

l
X

Moreover, let δ̄ = min

n

∗
∗
τ0 t (θ,µ)−τ0 τ −t (θ,µ)
,
, 1, C1
3 ,
2
2

o

where C1 is defined in Lemma 4, then for any δ ∈ (0, δ̄),
we have

q

q
X

X

l
X

 l

 l



p(|tj |) + θ · 
tj  + µ · 
tj − τ̂  < h(θ, µ) + δ 2
 j=1 
 j=1

j=1
(3)
holds only if |ti − t∗ (θ, µ)| < 2δ for some i while |tj | ≤ δ
for all j 6= i.

with θ, µ > 0, where τ̂ is an arbitrary fixed rational number
in (τ0 , τ ). We have the following lemma about gθ,µ (t).
Lemma 5. If p(t) satisfies the conditions in Theorem 1,
q > 1, and τ0 satisfies the properties in Lemma 4, then
there exist θ > 0 and µ > 0 such that for any θ ≥ θ and
µ ≥ µ · θ, the following properties are satisfied:
00
1. gθ,µ
(t) ≥ 1 for any t ∈ [τ0 , τ ];

Proof of Theorem 1. We present a polynomial time reduction to problem (1) from the 3-partition problem.
For any given instance of the 3-partition problem with
b = (b1 , . . . , b3m ), we consider the minimization problem
minx f (x) in the form of (1) with x = {xij }, 1 ≤ i ≤

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

3m, 1 ≤ j ≤ m, where
q


q

3m
m
m X
3m
3m 

X
X
X
X


1



(λθ) q
f (x) :=
bi xij −
x
bi xi1  +

ij 




j=1
j=2 i=1
i=1
i=1 


q

3m 
m
3m X
m
X
X
X


1
(λµ) q 


+
+
λ
x
−
τ̂
p(|xij |).
ij



i=1 
j=1
i=1 j=1
Note that the lower bounds θ, µ, and µ̂ only depend on
the penalty function p(·), we can choose θ ≥ θ and µ ≥
µθ if q > 1, or θ = 0 and µ ≥ µ̂ if q = 1, such that
1/q

1/q

we are able to find a near-optimal solution x such that
f (x) < 3mλ · h(θ, µ) +  within a polynomial time of
log(1/) and the size of f (x), which is polynomial with
respect to the size of the given 3-partition instance. Now
we show that we can find an equitable partition based on
this near-optimal solution. By the definition of , f (x) <
3mλ · h(θ, µ) +  implies
q

q


m

m
m
X

X
X 
xij − τ 
xij  + µ · 
p(|xij |) + θ 
(5)

 j=1
 j=1 
j=1
<h(θ, µ) + δ 2 ,

∀i = 1, . . . , 3m.

(λθ)
and (λµ)
are both rational numbers. Since τ̂ is
also rational, all the coefficients of f (x) are of finite size
and independent of the input size of the given 3-partition
instance. Therefore, the minimization problem minx f (x)
has polynomial size with respect to the given 3-partition
instance.

According to Lemma 7, for each i = 1, . . . , 3m, (5) implies that there exists k such that |xik − t∗ (θ, µ)| < 2δ and
|xij | < δ for any j 6= k. Now let
 ∗
t (θ, µ) if |xik − t∗ (θ, µ)| < 2δ
yij =
.
0
if |xij | < δ

For any x, by Lemma 7,

We define a partition by assigning bi to the jth subset Sj if
yij = t∗ (θ, µ). Note that this partition is well-defined since
for each i, by the definition of δ, there exists one and only
one yik = t∗ (θ, µ) while the others equal 0. Now we show
that this is an equitable partition.

q


m
X 
xij 
f (x) ≥0 + λ ·
p(|xij |) + θ · 
 j=1 
i=1
j=1

q )
X

m

+ µ · 
xij − τ̂ 
≥ 3mλ · h(θ, µ).
 j=1

3m
X

(

m
X

(4)

Now we claim that there exists an equitable partition to the
3-partition problem if and only if the optimal value of f (x)
is smaller than 3mλ · h(θ, µ) +  where  is specified later.
On one hand, if S can be equally partitioned into m subsets,
then we define
 ∗
t (θ, µ) if bi belongs to the jth subset;
xij =
0
otherwise.
It can be easily verified that these xij ’s satisfy f (x) =
3mλ · h(θ, µ). Then due to (4), we know that these xij ’s
provide an optimal solution to f (x) with optimal value
3mλ · h(θ, µ).
On the other hand, suppose the optimal value of f (x) is
3mλ · h(θ, µ), and there is a polynomial-time algorithm
that solves (1). Then for
)
(
τ0
and  = min{λδ 2 , (τ0 /2)q }
δ = min
P3m , δ̄
8 i=1 bi
where

δ̄ = min

∗

∗

τ0 t (θ, µ) − τ0 τ − t (θ, µ)
,
,
,
3
2
2

p(τ0 /3) + p(2τ0 /3) − p(τ0 )
,1 ,
τ0 /3

Note that for any j = 1, . . . , m, the difference between the
sum of the j-th subset and the first subset is

 

X
3m
3m

X  X
X

yij
yi1



b
−
b
=
·
b
−
·
b


i
i
i
i

 
∗ (θ, µ)
∗ (θ, µ)

t
t
 Sj

i=1
i=1
S1

 3m
3m

X
X
1


bi yi1  .
bi yij −
= ∗


t (θ, µ) 
i=1

i=1

By triangle inequality, we have




3m
X 
X
X
1

≤
b
−
b
bi · |yij − xij |
i
i

 t∗ (θ, µ)
 Sj

i=1
S1
 3m
!
3m
3m
X

X
X


bi · |yi1 − xi1 | + 
bi xij −
bi xi1  .
+


i=1

i=1

i=1

By the definition of yij , we have |yij − xij | < 2δ for any
i, j. for the last term, since f (x) < 3mλ · h(θ, µ) + , we
know that


n
n
X

X


bi xij −
bi xi1  < 1/q ≤ τ0 /2.



i=1

i=1

Therefore, we have




X 
X
1


b
−
b
i
i < ∗

t
(θ,
µ)
 Sj

S1

n
X

τ0
4δ
bi +
2
i=1

!
≤ 1.

P
Now since bi ’s are all integers, we must have Sj bi =
P
S1 bi , which means that the partition is equitable.

Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions

References
Amaldi, E. and Kann, V. On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. Theoretical Computer Science, 209(1):237–
260, 1998.
Antoniadis, A. and Fan, J. Regularization of wavelet approximations. Journal of the American Statistical Association, 96(455):939–967, 2001.
Arora, S., Babai, L., Stern, J., and Sweedy, Z. The hardness
of approximate optima in lattices, codes, and systems
of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on,
pp. 724–733. IEEE, 1993.
Bian, W. and Chen, X. Optimality conditions and complexity for non-lipschitz constrained optimization problems.
Preprint, 2014.
Cameron, A. C. and Trivedi, P. K. Regression analysis
of count data, volume 53. Cambridge university press,
2013.
Candes, E., Wakin, M., and Boyd, S. Enhancing sparsity by
reweighted L1 minimization. Journal of Fourier Analysis and Applications, 14(5-6):877–905, 2008.
Chartrand, R. Exact reconstruction of sparse signals via
nonconvex minimization. Signal Processing Letters,
IEEE, 14(10):707–710, 2007.

Fan, J., Liu, H., Sun, Q., and Zhang, T.
TAC
for sparse learning: Simultaneous control of algorithmic complexity and statistical error. arXiv preprint
arXiv:1507.01037, 2015.
Foster, D., Karloff, H., and Thaler, J. Variable selection is
hard. In COLT, pp. 696–709, 2015.
Frank, L. E. and Friedman, J. H. A statistical view of some
chemometrics regression tools. Technometrics, 35(2):
109–135, 1993.
Garey, M. R. and Johnson, D. S. “Strong”NP-completeness
results: Motivation, examples, and implications. Journal
of the ACM (JACM), 25(3):499–508, 1978.
Huber, P. J. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964.
Huo, X. and Chen, J. Complexity of penalized likelihood
estimation. Journal of Statistical Computation and Simulation, 80(7):747–759, 2010.
Loh, P.-L. and Wainwright, M. J. Regularized M-estimators
with nonconvexity: Statistical and algorithmic theory for
local optima. In Advances in Neural Information Processing Systems, pp. 476–484, 2013.
Lv, J. and Fan, Y. A unified approach to model selection
and sparse recovery using regularized least squares. The
Annals of Statistics, 37(6A):3498–3528, 2009.
McCullagh, P. Generalized linear models. European Journal of Operational Research, 16(3):285–292, 1984.

Chen, X., Ge, D., Wang, Z., and Ye, Y. Complexity of unconstrained L2 − Lp minimization. Mathematical Programming, 143(1-2):371–383, 2014.

Natarajan, B. K. Sparse approximate solutions to linear
systems. SIAM journal on computing, 24(2):227–234,
1995.

Chen, Y. and Wang, M. Hardness of approximation for
sparse optimization with L0 norm. Technical Report,
2016.

Wang, Z., Liu, H., and Zhang, T. Optimal computational
and statistical rates of convergence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164,
2014.

Davis, G., Mallat, S., and Avellaneda, M. Adaptive greedy
approximations. Constructive approximation, 13(1):57–
98, 1997.
Fan, J. and Li, R. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal
of the American Statistical Association, 96(456):1348–
1360, 2001.

Xue, L., Zou, H., Cai, T., et al. Nonconcave penalized
composite conditional likelihood estimation of sparse
ising models. The Annals of Statistics, 40(3):1403–1429,
2012.
Zhang, C.-H. Nearly unbiased variable selection under
minimax concave penalty. The Annals of Statistics, 38
(2):894–942, 2010a.

Fan, J. and Lv, J. A selective overview of variable selection
in high dimensional feature space. Statistica Sinica, 20
(1):101–148, 2010.

Zhang, T. Analysis of multi-stage convex relaxation for
sparse regularization. Journal of Machine Learning Research, 11:1081–1107, 2010b.

Fan, J., Xue, L., and Zou, H. Strong oracle optimality
of folded concave penalized estimation. The Annals of
Statistics, 42(3):819–849, 2014.

Zhang, Y., Wainwright, M. J., and Jordan, M. I. Lower
bounds on the performance of polynomial-time algorithms for sparse linear regression. In COLT, 2014.

