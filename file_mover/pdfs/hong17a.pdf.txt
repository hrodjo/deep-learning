Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed
Nonconvex Optimization and Learning Over Networks

Mingyi Hong 1 Davood Hajinezhad 1 Ming-Min Zhao 2

Abstract

2016); 3) nonconvex utility functions used in applications
such as resource allocation (Bjornson & Jorswieck, 2013).
Recently, a number of works in machine learning community have been focused on designing fast algorithms for
solving problem (1) in centralized setting; e.g., SAG (Defazio et al., 2014), SAGA (Schmidt et al., 2013), and SVRG
(Johnson & Zhang, 2013) for convex problems, and (Reddi
et al., 2016; Allen-Zhu & Hazan, 2016; Hajinezhad et al.,
2016b; Rahimpour et al., 2016) for nonconvex problems.

In this paper we consider nonconvex optimization and learning over a network of distributed
nodes. We develop a Proximal Primal-Dual Algorithm (Prox-PDA), which enables the network
nodes to distributedly and collectively compute
the set of first-order stationary solutions in a
global sublinear manner [with a rate of O(1/r),
where r is the iteration counter]. To the best
of our knowledge, this is the first algorithm that
enables distributed nonconvex optimization with
global sublinear rate guarantees. Our numerical
experiments also demonstrate the effectiveness
of the proposed algorithm.

1. Introduction
We consider the following optimization problem
min g(z) :=

z∈RM

N
X

fi (z),

(1)

i=1

where each fi , i ∈ {1, · · · , N } := [N ] is a nonconvex cost
function, and we assume that it is smooth and has Lipschitz
continuous gradient.
Such a finite sum problem plays a central role in machine
learning and signal/information processing (Cevher et al.,
2014; Hong et al., 2016). In particular, in the class of empirical risk minimization (ERM) problem, z represents the
feature vectors to be learned, and each fi can represent: 1)
a mini-batch of (possibly nonconvex) loss functions modeling data fidelity (Antoniadis et al., 2009); 2) nonconvex activation functions of neural networks (Allen-Zhu & Hazan,
1

Department of Industrial and Manufacturing Systems Engineering, Iowa State University, Ames, IA, USA
2
College of Information Science and Electronic Engineering, Zhejiang University, China.
Correspondence
to:
Mingyi Hong <mingyi@iastate.edu>, Davood Hajinezhad <dhaji@iastate.edu>, Ming-Min Zhao <zmmblack@zju.edu.cn>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

In this work, we are interested in designing algorithms
that solve problem (1) in a distributed manner. In particular, we focus on the scenario where each fi (or equivalently, each subset of data points in the ERM problem) is
available locally at a given computing node i ∈ [N ], and
the nodes are connected via a network. Clearly, such distributed optimization and learning scenario is important for
machine learning, because in contemporary applications
such as document topic modeling and/or social network
data analysis, oftentimes data corporas are stored in geographically distributed locations without any central controller managing the entire network of nodes; see (Forero
et al., 2010; Yan et al., 2013; Rahmani & Atia, 2015; Aybat
& Hamedani, 2016).
Related Works. Distributed convex optimization and
learning has been thoroughly investigated in the literature.
In (Nedic & Ozdaglar, 2009b), the authors propose a distributed subgradient algorithm (DSG), which allows the
agents to jointly optimize problem (1). Subsequently, many
variants of DSG have been proposed, either with special assumptions on the underlying graph, or having additional
structures of the problem; see, e.g., (Lobel & Ozdaglar,
2011; Lobel et al., 2011; Nedic & Olshevsky,
√ 2015). The
rate of convergence for DSG is O(log(r)/ r) under certain diminishing stepsize rules. Recently, a number of algorithms such as the exact first-order algorithm (EXTRA)
(Shi et al., 2014) and DLM (Ling et al., 2015) have been
proposed, which use constant stepsize and achieve faster
O(1/r) rate for convex problems. Recent works that apply distributed optimization algorithms to machine learning applications include (Scardapane et al., 2016; Aybat &
Hamedani, 2016; Scardapane & Lorenzo, 2016).
On the other hand, there has been little work for dis-

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

tributed optimization and learning when the objective function involves nonconvex problems. A dual subgradient
method has been proposed in (Zhu & Martinez, 2010),
which relaxes the exact consensus constraint. In (Bianchi
& Jakubowicz, 2013) a stochastic projection algorithm using diminishing stepsizes has been proposed. An ADMM
based algorithm has been presented in (Hong et al., 2014;
Hajinezhad & Hong, 2015) for a special type of problem called global consensus, where all distributed nodes
are directly connected to a central controller. Utilizing
certain convexification decomposition technique the authors of (Lorenzo & Scutari, 2016) designed an algorithm
named NEXT, which converges to the set of stationary solutions when using diminishing stepsizes. To the best of
our knowledge, no multi agent distributed algorithm is able
to guarantee global sublinear convergence rate for problem
(1).
Our Contributions. In this work, we propose a proximal
primal-dual algorithm (Prox-PDA) for problem (1) over an
undirected connected network. We show that Prox-PDA
converges to the set of stationary solutions of problem (1)
(satisfying the first-order optimality condition) in a globally sublinear manner. We also show that Prox-PDA can
be extended in several directions to improve its practical
performance. To the best of our knowledge, this is the first
algorithm that is capable of achieving global sublinear convergence rate for distributed non-convex optimization.
Further, our work reveals an interesting connection between the primal-dual based algorithm Prox-PDA and the
primal-only fast distributed algorithms such as EXTRA
(Shi et al., 2014). Such new insight of the connection between primal-dual and primal-only algorithms could be of
independent interest for the optimization community. Finally, we generalize the theory for Prox-PDA based algorithms to a challenging distributed matrix factorization
problem.

2. System Model
Define a graph G := {V, E}, where V and E are the
node and edge sets; Let |V| = N and |E| = E. Each
node v ∈ V represents an agent in the network, and each
edge eij = (i, j) ∈ E indicates that node i and j are
neighbors; see Fig.1(Left). Assume that each node i can
only communicate with its immediate neighbors, defined
as Ni := {j | (i, j) ∈ V}, with |Ni | = di . The distributed
version of problem (1) is given as below
min f (x) :=

xi ∈RM

N
X

fi (xi ), s.t. xi = xj , ∀ (i, j) ∈ E. (2)

i=1

Clearly the above problem is equivalent to (1) as long as G
is connected. For notational simplicity, define x := {xi } ∈
RN M ×1 , and Q := N × M .

e12
N2

N1
e14

N3

e34

e24


1
1
Ã = 
0
0

−1
0
1
0


0 0
0 −1
.
0 −1
1 −1

N4

Figure 1. (Left) An undirected Connected Network,
(Right) Incidence Matrix.

To proceed, let us introduce a few useful quantities related
to graph G.
• The incidence matrix Ã ∈ RE×N is a matrix with entires
Ã(k, i) = 1 and Ã(k, j) = −1 if k = (i, j) ∈ E with j > i,
and all the rest of the entries being zero. For example, for
the network in Fig.1 (Left); the incidence matrix is given in
Fig.1 (Right). Define the extended incidence matrix as
A := Ã ⊗ IM ∈ REM ×Q ,

(3)

where ⊗ denotes the Kronecker product.
• The Degree matrix D̃ ∈ RN ×N is given by D̃ :=
diag[d1 , · · · , dN ]; Let D := D̃ ⊗ IM ∈ RQ×Q .
• The signed and the signless Laplacian matrices (denoted
as L− and L+ respectively), are given below
L− := A> A ∈ RQ×Q , L+ := 2D − A> A ∈ RQ×Q . (4)
Using the above notations, one can verify that problem (2)
can be written in the following compact form
min f (x),

x∈RQ

s.t. Ax = 0.

(5)

3. The Prox-PDA Algorithm
The proposed algorithm builds upon the classical augmented Lagrangian (AL) method (Bertsekas, 1982; Powell,
1969). Let us define the AL function for (5) as
β
(6)
Lβ (x, µ) = f (x) + hµ, Axi + kAxk2 ,
2
where µ ∈ RQ is the dual variable; β > 0 is a penalty
parameter. Let B ∈ RQ×Q be some arbitrary matrix to be
determined shortly. Then the proposed algorithm is given
in the table below (Algorithm 1).
In Prox-PDA, the primal iteration (7a) minimizes the augmented Lagrangian plus a proximal term β2 kx − xr k2B T B .
We emphasize that the proximal term is critical in both the
algorithm implementation and the analysis. It is used to ensure the following key properties:
(1). The primal problem is strongly convex;
(2). The primal problem is decomposable over different
network nodes, hence distributedly implementable.
To see the first point, suppose B T B is chosen such that
AT A + B T B  IQ , and that f (x) has Lipschitz gradient.
Then by a result in (Zlobec, 2005)[Theorem 2.1], we know

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

Algorithm 1 The Prox-PDA Algorithm

4.1. The Analysis Steps

0

0

Q

1: At iteration 0, initialize µ = 0 and x ∈ R .
2: At each iteration r + 1, update variables by:

xr+1 = arg min f (x) + hµr , Axi +
x∈RQ

β
kAxk2
2

β
kx − xr k2B T B ;
2
= µr + βAxr+1 .
+

µr+1

(7a)
(7b)

that there exists β > 0 large enough such that the objective function of (7a) is strongly convex. To see the second
point, Let B := |A|, where the absolute value is taken for
each component of A. It can be verified that B T B = L+ ,
and step (7a) becomes
xr+1 = arg min
x

N
X

fi (xi ) + hµr , Axi

i=1

β
β
+ xT L− x + (x − xr )T L+ (x − xr )
2
2
N
X
fi (xi ) + hµr , Axi + βxT Dx − βxT L+ xr .
= arg min
x

i=1

Clearly this problem is separable over the nodes, therefore
it can be solved completely distributedly.

4. The Convergence Analysis
In this section we provide convergence analysis for Algorithm 1. The key in the analysis is the construction of a
novel potential function, which decreases at every iteration
of the algorithm. In particular, the constructed potential
function is a conic combination of the AL function and the
size of the violation of the consensus constraint, therefore it
measures the progress of both the primal and dual updates.
We first state our main assumptions below.
[A1.] The function f (x) is differentiable and has Lipschitz
continuous gradient, i.e.,
k∇f (x) − ∇f (x)k ≤ Lkx − yk,

∀ x, y ∈ RQ .

Further assume that AT A + B T B  IQ .
[A2.] There exists a constant δ > 0 such that
∃ f > −∞,

δ
s.t. f (x) + kAxk2 ≥ f , ∀ x ∈ RQ .
2

Without loss of generality we will assume that f = 0.
Below we provide a few nonconvex smooth functions that
satisfy our assumptions, all of which are commonly used
as activation functions for neural networks.
• The sigmoid function sigmoid(x) =
• The arctan and tanh function.
x
• The logit function logit(x) = exe+1 .

1
1+e−x .

Below we provide the analysis of Prox-PDA. First we provide a bound on the size of the constraint violation using
a quantity related to the primal iterates. Let σmin denotes
the smallest non-zero eigenvalue of AT A, and we define
wr := (xr+1 − xr ) − (xr − xr−1 ) for notational simplicity.
We have the following result.
Lemma 1 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Prox-PDA
1 r+1
kµ
− µr k2
β

2L2 
xr − xr+1 2 + 2β kB T Bwr k2 . (8)
≤
βσmin
σmin
Then we bound the descent of the AL function.
Lemma 2 Suppose Assumptions [A1] and [A2] are satisfied. Then the following is true for Algorithm 1
2βkB T Bk
2
kwr kB T B
Lβ (xr+1 , µr+1 ) − Lβ (xr , µr ) ≤
σmin


β−L
2L2
−
−
kxr+1 − xr k2 .
(9)
2
βσmin
A key observation from Lemma 2 is that no matter how
large β is, the rhs of (9) cannot be made negative. This
observation suggests that in contrast to (Hong et al., 2014;
Hajinezhad et al., 2016a) the augmented Lagrangian alone
cannot serve as the potential function for Prox-PDA. In
search for an appropriate potential function, we need a new
2
object that is decreasing in the order of β kwr kB T B . The
following lemma shows that the descent of the sum of the
constraint violation and the proximal term has the desired
property.
Lemma 3 Suppose Assumption [A1] is satisfied. Then the
following is true

β
kAxr+1 k2 + kxr+1 − xr k2B T B
2

β
≤ Lkxr+1 − xr k2 +
kAxr k2 + kxr − xr−1 k2B T B
2

β
kwr k2B T B + kA(xr+1 − xr )k2 .
(10)
−
2

It is interesting to observe that the new object,
β/2 kAxr+1 k2 + kxr+1 − xr k2B T B ,
increases
in
Lkxr+1 − xr k2 and decreases in β/2kwr k2B T B , while the
AL behaves in an opposite manner (cf. Lemma 2). More
importantly, in our new object, the constant in front of
kxr+1 − xr k2 is independent of β. Although neither of
these two objects decreases by itself, quite surprisingly, a
proper conic combination of these two objects decreases at
every iteration of Prox-PDA. To precisely state the claim,
let us define the potential function for Algorithm 1 as
Pc,β (xr+1 , xr , µr+1 ) := Lβ (xr+1 , µr+1 )

cβ
+
kAxr+1 k2 + kxr+1 − xr k2B T B ,
2

(11)

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

where c > 0 is some constant to be determined later. We
have the following result.

Theorem 1 Suppose Assumption A and the conditions (13)
and (14) are satisfied. Then we have
• Eventual Consensus:

Lemma 4 Suppose the assumptions made in Lemmas 1 –
3 are satisfied. Then we have the following
r+1

r

r+1

r

r−1

Pc,β (x , x , µ ) ≤ Pc,β (x , x , µ )


2L2
β−L
−
− cL kxr+1 − xr k2
−
2
βσmin


cβ
2βkB T BkF
2
−
−
kwr kB T B .
2
σmin

Second, for any given c, we need β to satisfy
2L2
βσmin − cL > 0, which implies the following


s
2
L
16L
.
β>
2c + 1 + (2c + 1)2 +
2
σmin

(12)

β−L
2

−

(14)

We conclude that if both (13) and (14) are satisfied, then
the potential function Pc,β (xr+1 , xr , µr+1 ) decreases at
every iteration.
Our next step shows that by using the particular choices of
c and β in (13) and (14), the constructed potential function
is lower bounded.
Lemma 5 Suppose [A1] - [A2] are satisfied, and (c, β)
are chosen according to (13) and (14). Then the following statement holds true
∃ P > −∞ s.t. Pc,β (xr+1 , xr , µr+1 ) ≥ P , ∀ r > 0.
Now we are ready to present the main result of this section.
To this end, define Q(xr+1 , µr ) as the optimality gap of
problem (5), given by
Q(xr+1 , µr ) := k∇x Lβ (xr+1 , µr )k2 + kAxr+1 k2 . (15)

• Sublinear Convergence Rate: For any given ϕ > 0,
let us define T to be the first time that the optimality gap
reaches below ϕ, i.e.,
T := arg min Q(xr+1 , µr ) ≤ ϕ.
r

Then for some ν > 0, we have ϕ ≤ T ν−1 . That is, the
optimality gap Q(xr+1 , µr ) converges sublinearly.

5. Variants of Prox-PDA
In this section, we discuss two important extensions of the
Prox-PDA, one allows the x-problem (7a) to be solved
inexactly, while the second allows the use of increasing
penalty parameter β. In many practical applications, exactly minimizing the augmented Lagrangian may not be
easy. Therefore, we propose the proximal gradient primaldual algorithm (Prox-GPDA), whose main steps are given
below
xr+1 = arg min h∇f (xr ), x − xr i + hµr , Axi
x∈RQ

+
µr+1

β
β
kAxk2 + kx − xr k2B T B ;
2
2
= µr + βAxr+1 .

It is easy to see that Q(x , µ ) → 0 implies that any
limit point (x∗ , µ∗ ), if it exists, is a KKT point of (5) that
satisfies the following conditions
Ax∗ = 0.

(17)
(18)

The analysis of this algorithm follows similar steps as that
for Prox-PDA. For detailed discussion see (Hong, 2016).
Our second variant do not require to explicitly compute the
bound for β given in (14). Indeed, the bounds on β derived in the previous sections are the worst case bounds,
and algorithms that use stepsizes that strictly satisfy such
bounds may be slow at the beginning. In practice, one may
prefer to start with a small penalty parameter and gradually increase it. We denote this algorithm by Prox-PDA-IP,
whose main steps are given below
xr+1 = arg min f (x) + hµr , Axi

r

0 = ∇f (x∗ ) + AT µ∗ ,

r→∞

• Convergence to Stationary Points: Every limit point of
the iterates {xr , µr } generated by Algorithm 1 converges
to a KKT point of problem (5). Further, Q(xr+1 , µr ) → 0.

From the above analysis, it is easy to see that as long as c
and β are sufficiently large, the potential function decreases
at each iteration of Prox-PDA. Below we derive the precise
bounds for c and β. First, a sufficient condition for c is
given below (note, that δ > 0 is defined in Assumption
[A2])


δ 4kB T BkF
,
.
(13)
c ≥ max
L
σmin

r+1

lim µr+1 − µr → 0, lim Axr → 0.

r→∞

r

β r+1
kx − xr k2B T B ;
2
2
= µr + β r+1 Axr+1 .

+
r+1

µ

x∈RQ
r+1

β

kAxk2 +

(19)
(20)

(16)

In the following we show that the gap Q(·) not only decreases to zero, but does so in a sublinear manner.

Note that one can also replace f (x) in (19) by h∇f (xr ), x−
xr i to obtain a similar variant for Prox-GPDA denoted by
Prox-GPDA-IP. Throughout this section we will still as-

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

sume that Assumption A holds true. Further, we will assume that β r satisfies the following conditions
1
→ 0,
βr

∞
X
r=1

1
= ∞, β r+1 ≥ β r ,
βr

max (β r+1 − β r ) < κ, for some finite κ > 0.
r

− βL+ (xr − xr−1 ) = 0,
where we have used the fact that AT (µr − µr−1 ) =
βAT Axr = βL− xr . Rearranging terms, we have

(21)


1 −1
D
∇f (xr ) − ∇f (xr−1 )
2β
1
1
+ D−1 (L+ − L− )xr − D−1 L+ xr−1
2
2

1 −1
r
r
=x −
D
∇f (x ) − ∇f (xr−1 ) + W xr
2β
1
− (I + W )xr−1 ,
(23)
2

xr+1 = xr −

Also without loss of generality we will assume that
B T B  0,

and kB T BkF > 1.

(22)

Note that this is always possible, by adding an identity
matrix to B T B if necessary.
The analysis for Prox-PDA-IP is long and technical, therefore we refer the readers to (Hong, 2016). The key step is
to construct a new potential function, given below
Pβ r+1 ,c (xr+1 , xr , µr+1 ) = Lβ r+1 (xr+1 , µr+1 )
+

∇f (xr ) − ∇f (xr−1 ) + βL− xr + 2βD(xr+1 − xr )

cβ r+1 β r r
cβ r+1 β r
kAxr+1 k2 +
kx − xr+1 k2B T B .
2
2

The insight here is that in order to achieve the desired descent, in the potential function the coefficients for
kxr+1 − x r k2B T B and kAxr+1 k2 should be proportional to
O (β r )2 . We have the following theorem regarding to the
convergence of Prox-PDA-IP.
Theorem 2 Suppose Assumption A and (21) are satisfied.
Suppose that B is selected such that (22) holds true. Then
the following hold for Prox-PDA-IP
• Eventual Consensus:
lim µr+1 − µr → 0, lim Axr → 0.

r→∞

r→∞

where in the last equality we have defined the weight matrix W := 21 D−1 (L+ − L− ), which is a row stochastic
matrix.
Iteration (23) has the same form as the EXTRA algorithm
given in (Shi et al., 2014), therefore we can conclude that
EXTRA is a special case of Prox-GPDA. Moreover, by appealing to our analysis in Section 5, it readily follows that
EXTRA works for the nonconvex distributed optimization
problem as well.
We remark that each node i can distributedly implement
iteration (23) by performing the following

1
∇fi (xri ) − ∇fi (xr−1
)
i
2βdi


X 1 r−1
X 1 r 1
.
xj − 
x
+ xr−1
+
i
di
2
di j

= xri −
xr+1
i

j∈N (i)

(24)

j∈N (i)

• Convergence to KKT Points: Every limit point of the
iterates {xr , µr } generated by Prox-PDA-IP converges to
a KKT point of problem (5). Further, Q(xr+1 , µr ) → 0.

Clearly, at iteration r + 1, besides the local gradient information, node i only
P needs the aggregated information
from its neighbors, j∈N (i) xrj . Therefore the algorithm
is distributedly implementable.

6. Connections and Discussions

7. Distributed Matrix Factorization

In this section we present an interesting observation which
establishes links between the so-called EXTRA algorithm
(Shi et al., 2014) (developed for distributed, but convex optimization) and the Prox-GPDA.

In this section we study a variant of the Prox-PDA/ProxPDA-IP for the following distributed matrix factorization
problem (Ling et al., 2012)
1
min
kXY − Zk2F + ηkXk2F + h(Y )
(25)
X,Y
2
N
X
1
=
kXyi − zi k2 + γkXk2F + hi (yi ),
2
i=1

Specifically, the optimality condition of the x-update step
(17) is given by
∇f (xr ) + AT (µr + βAxr+1 ) + β(B T B(xr+1 − xr )) = 0.
Utilizing the fact that AT A = L− , B T B = L+ and L+ +
L− = 2D, we have
∇f (xr ) + AT µr + 2βDxr+1 − βL+ xr = 0.
Subtracting the same equation evaluated at the previous
iteration, we obtain

s.t. kyi k2 ≤ τ, ∀ i
where X ∈ RM ×K , Y ∈ RK×N ; for each i, yi ∈ RK
M ×N
consists of one column
is some known
PN of Y ; Z ∈ R
matrix; h(Y ) := i=1 hi (yi ) is some convex but possibly
nonsmooth penalization term; η > 0 is some given constant; for notation simplicity we have defined γ := 1/N η.

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

It is easy to extend the above formulation to the case where
Y and Z both have N P columns, and each yi and zi consists of P columns of Y and Z respectively.
We assume that h(Y ) is lower bounded over dom (h). One
application of problem (25) is the distributed sparse dictionary learning problem where X is the dictionary to be
learned, each zi is a training data sample, and each yi is the
sparse coefficient corresponding to the particular training
sample zi . The constraint kyi k2 ≤ τ simply says that the
size of the coefficient must be bounded.
Consider a distributed scenario where N agents form a
graph {V, E}, each having a column of Y . We reformulate problem (25) as

N 
X
1
kXi yi − zi k2 + hi (yi ) + γkXi k2F
min
2
{Xi },{yi }
i=1
2

s.t. kyi k ≤ τ, ∀ i

Xi = Xj , ∀ (i, j) ∈ E.

Let us stack all the variables Xi , and define X :=
[X1 ; X2 ; · · · ; XN ] ∈ RN M ×K . Define the block signed
incidence matrix as A = Ã ⊗ IM ∈ REM ×N M , where
A is the standard graph incidence matrix. Define the block
signless incidence matrix B ∈ REM ×N M similarly. If the
graph is connected, then the condition AX = 0 implies
network-wide consensus. We formulate the distributed matrix factorization problem as
min

f (X, Y ) + h(Y )

{Xi },{yi }

:=

N 
X
1

2

i=1

2

kXi yi − zi k +

s.t. kyi k2 ≤ τ, ∀ i

γkXi k2F


+ hi (yi )

AX = 0.

(26)

Algorithm 2 Prox-PDA for Distr. Matrix Factorization
1: At iteration 0, initialize Ω0 = 0, and X 0 , y 0
2: At each iteration r + 1, update variables by:
θir = kXir yir − zi k2 , ∀ i;
(29a)
1
r+1
r
2
= arg min
yi
kXi yi − zi k + hi (yi )
kyi k2 ≤τ 2
θr
+ i kyi − yir k2 , ∀ i;
(29b)
2
r+1
r+1
r
X
= arg min f (X, Y
) + hΩ , AXi
(29c)
X
β
β
+ hAX, AXi + hB(X − X r ), B(X − X r )i;
2
2
Ωr+1 = Ωr + βAX r+1 .
(29d)

of the local factorization error. We note that including the
θr
proximal term 2i kyi − yir k2 is the key to achieve convergence for Algorithm 2.
Let us comment on the distributed implementation of
the algorithm.
First note that the y subproblem
(29b) is naturally distributed to each node, that is,
only local information is needed to perform the update. Second, the X subproblem (29c) can also be
decomposed into N subproblems, one for each node.
To be more precise, let us examine the terms in
(29c) one by one. First, the term f (X, Y r+1 ) =

PN 1
r+1
− zi k2 + hi (yir+1 ) + γkXi k2F , hence
i=1 2 kXi yi
it is decomposable. Second, the term hΩr , AXi can be
expressed as
N
X
X
X
hΩre , Xi i −
hΩre , Xi i
hΩr , AXi =
i=1 e∈U (i)

Clearly the above problem does not satisfy Assumption
A, because the objective function is not smooth, and neither ∇X f (X, Y ) nor ∇Y f (X, Y ) is Lipschitz continuous.
The latter fact poses significant difficulty in algorithm development and analysis. Define the block-signed/signless
Laplacians as
−

T

+

T

L = A A, L = B B.

(27)

e∈H(i)

where the sets U (i) and H(i) are defined as
U (i) := {e | e = (i, j) ∈ E, i ≥ j},
H(i) := {e | e = (i, j) ∈ E, j ≥ i}.
Similarly, we have
r

hBX , BXi =

N
X

*
Xi , di Xir

+
+

i=1

The AL function for the above problem is given by
Lβ (X, Y, Ω) =

N 
X
1
i=1

+ hΩ, AXi +

2

2

kXi yi − zi k +

β
hAX, AXi,
2

γkXi k2F


+ hi (yi )
(28)

where Ω := {Ωe } ∈ REM ×K is the matrix of the dual
variable, with Ωe ∈ RM ×K being the dual variable for the
consensus constraint on link e, i.e, Xi = Xj , e = (i, j).
Let us generalize Algorithm 1 for distributed matrix factorization given in Algorithm 2. In Algorithm 2 we have
introduced a sequence {θir ≥ 0} which measures the size

X

Xjr

j∈N (i)

β
(hAX, AXi + hBX, BXi)
2
N
X
= βhDX, Xi = β
di kXi k2F ,
i=1

where D := D̃ ⊗ IM ∈ RN M ×N M with D̃ being the
degree matrix. It is easy to see that the X subproblem (29c)
is separable over the distributed agents. Finally, one can
verify that the Ω update step (29d) can be implemented by
each edge e ∈ E as follows

Ωr+1
= Ωre + β Xir+1 − Xjr+1 , e = (i, j), i ≥ j.
e

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

To show convergence rate of the algorithm, we need the
following definition
Q(X r+1 , Y r+1 , Ωr ) := βkAX r+1 k2 + k[Z r+1
; Z r+1
]k2 ,
1
2

where we have defined
Z r+1
:= ∇X Lβ (X r+1 , Y r+1 , Ωr );
1
Z r+1
:= Y r+1
2


− proxh+ι(Y) Y r+1 − ∇Y Lβ (X r+1 , Y r+1 , Ωr ) − h(Y ) .

	 expression, we have used Y :=
SIn the 2 above
ky
k
≤
τ
to denote the feasible set of Y , and used
i
i
ι(Y) to denote the indicator function of such set. Similarly
as in Section 4, we can show that Q(X r+1 , Y r+1 , Ωr ) →
0 implies that every limit point of (X r+1 , Y r+1 , Ωr ) is a
KKT point of problem (26).
Next we present the main convergence analysis for Algorithm 2. The proof is long and technical, therefore we refer
the readers to (Hong, 2016).
Theorem 3 Consider using Algorithm 2 to solve the distributed matrix factorization problem (26). Suppose that
h(Y ) is lower bounded over dom h(x), and that the penalty
parameter β, together with two positive constants c and d,
satisfies the following conditions
8(τ 2 + 4γ 2 ) cd
β + 2γ
−
> 0,
−
2
βσmin
2
1
8
c
1
8τ
cτ
−
− > 0, −
−
> 0,
(30)
2 σmin β
d
2 σmin β
d
2βkB T Bk
cβ
−
> 0.
2
σmin

gradually increase d to find an appropriate β that satisfies
the first three conditions.
We remark that Algorithm 2 can be extended to the case
with increasing penalty. Due to the space limitation we
omit the details here.

8. Numerical Results
In this section, we demonstrate the performance of the proposed algorithms. All experiments are performed in Matlab
(2016b) on a desktop with an Intel Core(TM) i5-4690 CPU
(3.50 GHz) and 8GB RAM running Windows 7.
8.1. Distributed Binary Classification
In this subsection, we study the problem of binary classification using nonconvex regularizers in the mini-bach setup
i.e. each node stores b (batch size) data points, and each
component function is given by
fi (xi ) =

 b

M
X
λαx2i,k
1 X
log(1 + exp(−yij xTi vij )) +
N b j=1
1 + αx2i,k
k=1

where vij ∈ RM and yij ∈ {1, −1} are the feature vector
and the label for the jth date point in ith agent (Antoniadis
et al., 2009). We use the parameter settings of λ = 0.001,
α = 1 and M = 10. We randomly generated 100, 000 data
points and distribute them into N = 20 nodes (i.e. b =
5000). We use the optimality gap (opt-gap) and constraint
violation (con-vio), displayed below, to measure the quality
of the solution generated by different algorithms
X
2
N

2
2

opt-gap := 
∇fi (zi )
 + kAxk , con-vio = kAxk .
i=1

Then in the limit, consensus will be achieved, i.e.,
lim kXir − Xjr k = 0,

r→∞

∀ (i, j) ∈ E.

Further, the sequences {X r+1 } and {Ωr+1 } are both
bounded, and every limit point generated by Algorithm 2
is a KKT point of problem (25).
Additionally, Algorithm 2 converges sublinearly. Specifically, for any given ϕ > 0, define T to be the first time that
the optimality gap reaches below ϕ, i.e.,
T := arg min Q(X r+1 , Y r+1 , Ωr ) ≤ ϕ.
r

Then for some constant ν > 0 we have ϕ ≤ T ν−1 .
We can see that it is always possible to find the tuple
{β, c, d > 0} that satisfies (30): c can be solely determined by the last inequality; for fixed c, the constant d
needs to be chosen large enough such that 1/2 − dc > 0
and 1/2 − cτ
d > 0 are satisfied. After c and d are fixed,
one can always choose β large enough to satisfy the first
three conditions. In practice, we typically prefer to choose
β as small as possible to improve the convergence speed.
Therefore empirically one can start with (for some small
T
Bk
ν > 0): c = 4kB
+ ν, d = max{4, 2cτ }, and then
σmin

We compare the the Prox-GPDA, and Prox-GPDA-IP
with the distributed subgradient (DSG) method (Nedic &
Ozdaglar, 2009a;b) (which is only known to work for convex cases) and the Push-sum algorithm (Tatarenko & Touri,
2015). The performance of all three algorithms in terms of
the consensus error and the optimality gap (averaged over
30 problem instances) are presented in Fig. 2. The penalty
parameter for Prox-GPDA is chosen such that satisfy (14),
and β r for Prox-GPDA-IP is set as 0.05 log(r), the stepsizes of the DSG algorithm and the Push-sum algorithm are
chosen as 1/0.05 log(r) and 1/r, respectively. Note that
these parameters are tuned for each algorithm to achieve
the best results. All Algorithms will stop after 1000 iterations. It can be observed that the Prox-GPDA with constant
stepsize outperforms other algorithms. The Push-sum algorithm does not seem to converge within 1000 iterations.
To see more results, we compare different algorithms with
different number of agents in the network (N ). The problem and the algorithms setup are set as the previous case.
We measure the optimality gap as well as the constraint
violation and the results are respectively reported in Table 1 and Table 2. In the tables Alg1, Alg2, Alg3, Alg4

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization
10 10

10 10
10 0

10 0

10 -10

10 -10

10 -20

10 -20

10 -30

10 -30

10 -40

10 -40
0

200

400

600

800

0

1000

200

400

600

800

1000

Figure 2. Results for the matrix factorization problem.
1010

Prox-PDA-IP

Optimality Gap

102
10

Prox-PDA-IP

EXTRA-AO

0

Consensus Error

10

4

10-2
10

-4

10-6
10-8

0

1000

2000

3000

4000

5000

Iteration number

105

EXTRA-AO

100

10-5

10-10

0

1000

2000

3000

4000

5000

Iteration Number

Figure 3. Results for the matrix factorization problem.
Table 1. Optimality Gap for different Algorithms

N
10
20
30
40
50

Alg1
5.1e-36
4.7e-32
2.3e-21
1.3e-12
5.5e-10

Alg2
2.4e-22
5.0e-9
5.1e-8
2.9e-7
4.2e-6

Alg3
1.34
0.04
0.008
0.007
0.005

Alg4
2.79
0.42
0.20
0.21
0.40

Table 2. Constraint Violation for different Algorithms

N
10
20
30
40
50

Alg1
1.3e-36
1.2e-34
2.3e-24
2.2e-16
2.2e-14

Alg2
3.4e-27
3.7e-16
7.8e-15
2.1e-14
2.2e-12

Alg3
0.35
0.02
0.01
0.03
0.01

Alg4
0.65
0.40
0.18
0.20
0.12

denote Prox-GPDA, Prox-GPDA-IP, DGS, and Push-sum
algorithms respectively. As seen, the performance of the
proposed algorithms (Alg1, Alg2) are significantly better
than DGS and Push-Sum.

by randomly extracting 300 overlapping patches from the
512 × 512 image of barbara.png, each with size 16 × 16
pixels. Each of the extracted patch is vectorized, resulting a training data set Z of size 256 × 300. We consider
a network of N = 10 agents, and the columns of Z are
evenly distributed among the agents (each having P = 30
columns). We compare Prox-PDA-IP with the EXTRAAO algorithm proposed in (H.-T. Wai & Scaglione, 2015).
Note that the EXTRA-AO is also designed for a similar
distributed matrix factorization problem and it works well
in practice. However, it does not have formal convergence
proof. We initialize both algorithms with X being the 2D
discrete cosine transform (DCT) matrix. We set γ = 0.05,
τ = 105 and β = 0.001r, and the results are averaged
over 30 problem instances. The stepsizes of the EXTRAAO is set as αAO = 0.03 and βAO = 0.002. In Fig. 3,
we compare the performance of the proposed Prox-PDAIP and the EXTRA-AO. It can be observed that our proposed algorithm converges faster than the EXTRA-AO. We
have observed that the EXTRA-AO does have reasonably
good practical performance, however it lacks formal convergence proof.

Acknowledgment
8.2. Distributed Matrix Factorization
In this section we consider the distributed matrix factorization problem (25). The training data is constructed

The authors supported by National Natural Science Foundation (Grant No. CCF-1526078).

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization

References
Allen-Zhu, Z. and Hazan, E. Variance Reduction for Faster NonConvex Optimization. In Proceedings of the 33rd International
Conference on Machine Learning, ICML, 2016.
Antoniadis, A., Gijbels, I., and Nikolova, M. Penalized likelihood
regression for generalized linear models with non-quadratic
penalties. Annals of the Institute of Statistical Mathematics,
63(3):585–615, 2009.
Aybat, N-S. and Hamedani, E-Y. A primal-dual method for conic
constrained distributed optimization problems. Advances in
Neural Information Processing Systems, 2016.
Bertsekas, D. P. Constrained Optimization and Lagrange Multiplier Method. Academic Press, 1982.
Bianchi, P. and Jakubowicz, J. Convergence of a multi-agent projected stochastic gradient algorithm for non-convex optimization. IEEE Transactions on Automatic Control, 58(2):391–405,
2013.

Hong, M., Razaviyayn, M., Luo, Z.-Q., and Pang, J.-S. A unified algorithmic framework for block-structured optimization
involving big data. IEEE Signal Processing Magazine, 33(1):
57–77, 2016.
Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance reduction. In the Proceedings
of the Neural Information Processing (NIPS). 2013.
Ling, Q., Xu, Y., Yin, W., and Wen, Z. Decentralized low-rank
matrix completion. In 2012 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 2925–
2928, March 2012.
Ling, Q., Shi, W., Wu, G., and Ribeiro, A. DLM: Decentralized
linearized alternating direction method of multipliers. IEEE
Transactions on Signal Processing, 63(15):4051–4064, Aug
2015.
Lobel, I. and Ozdaglar, A. Distributed subgradient methods for
convex optimization over random networks. Automatic Control, IEEE Transactions on, 56(6):1291–1306, 2011.

Bjornson, E. and Jorswieck, E. Optimal resource allocation in coordinated multi-cell systems. Foundations and Trends in Communications and Information Theory, 9, 2013.

Lobel, I., Ozdaglar, A., and Feijer, D. Distributed multi-agent optimization with state-dependent communication. Mathematical
Programming, 129(2):255–284, 2011.

Cevher, V., Becker, S., and Schmidt, M. Convex optimization
for big data: Scalable, randomized, and parallel algorithms for
big data analytics. IEEE Signal Processing Magazine, 31(5):
32–43, 2014.

Lorenzo, P. Di and Scutari, G. Next: In-network nonconvex optimization. IEEE Transactions on Signal and Information Processing over Networks, 2(2):120–136, 2016.

Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A fast incremental gradient method with support for non-strongly convex
composite objectives. In The Proceeding of NIPS, 2014.
Forero, P. A., Cano, A., and Giannakis, G. B. Consensusbased distributed support vector machines. Journal of Machine
Learning Research, 11(May):1663–1707, 2010.
H.-T. Wai, T.-H. Chang and Scaglione, A. A consensus-based
decentralized algorithm for non-convex optimization with application to dictionary learning. In the Proceedings of the IEEE
ICASSP, 2015.
Hajinezhad, D. and Hong, M. Nonconvex alternating direction
method of multipliers for distributed sparse principal component analysis. In IEEE Global Conference on Signal and Information Processing (GlobalSIP). IEEE, 2015.
Hajinezhad, D., Chang, T-H., Wang, X., Shi, Q., , and Hong, M.
Nonnegative matrix factorization using admm: Algorithm and
convergence analysis. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2016a.
Hajinezhad, D., Hong, M., Zhao, T., and Wang, Z. Nestt: A
nonconvex primal-dual splitting method for distributed and
stochastic optimization. In Advances in Neural Information
Processing Systems 29, pp. 3215–3223. 2016b.
Hong, M. Decomposing linearly constrained nonconvex problems
by a proximal primal dual approach: Algorithms, convergence,
and applications. arXiv preprint arXiv:1604.00543, 2016.
Hong, M., Luo, Z.-Q., and Razaviyayn, M. Convergence analysis
of alternating direction method of multipliers for a family of
nonconvex problems. 2014. technical report, University of
Minnesota.

Nedic, A. and Olshevsky, A. Distributed optimization over timevarying directed graphs. IEEE Transactions on Automatic Control, 60(3):601–615, 2015.
Nedic, A. and Ozdaglar, A. Cooperative distributed multi-agent
optimization. In Convex Optimization in Signal Processing and
Communications. Cambridge University Press, 2009a.
Nedic, A. and Ozdaglar, A. Distributed subgradient methods for
multi-agent optimization. IEEE Transactions on Automatic
Control, 54(1):48–61, 2009b.
Powell, M. M. D. An efficient method for nonlinear constraints
in minimization problems. In Optimization. Academic Press,
1969.
Rahimpour, Alireza, Taalimi, Ali, Luo, Jiajia, and Qi, Hairong.
Distributed object recognition in smart camera networks. In
IEEE International Conference on Image Processing, Phoenix,
Arizona, USA. IEEE, 2016.
Rahmani, M. and Atia, G. A decentralized approach to robust subspace recovery. In 2015 53rd Annual Allerton Conference on
Communication, Control, and Computing (Allerton), pp. 802–
807. IEEE, 2015.
Reddi, S., Sra, S., Poczos, B., and Smola, A. Fast incremental method for nonconvex optimization. arXiv preprint
arXiv:1603.06159, 2016.
Scardapane, S. and Lorenzo, P. Di. A framework for parallel
and distributed training of neural networks. arXiv preprint
arXiv:1610.07448, 2016.
Scardapane, S., Fierimonte, R., Lorenzo, P. Di, Panella, M., and
Uncini, A. Distributed semi-supervised support vector machines. Neural Networks, 80:43–52, 2016.

Proximal Primal Dual Algorithm for Distributed Nonconvex Optimization
Schmidt, M., Roux, N. Le, and Bach., F. Minimizing finite sums
with the stochastic average gradient. 2013. Technical report,
INRIA.
Shi, W., Ling, Q., Wu, G., and Yin, W. Extra: An exact first-order
algorithm for decentralized consensus optimization. SIAM
Journal on Optimization, 25(2):944–966, 2014.
Tatarenko, T. and Touri, B. Non-convex distributed optimization.
2015. arXiv Preprint: arXiv:1512.00895.
Yan, F., Sundaram, S., Vishwanathan, S. V. N., and Qi, Y. Distributed autonomous online learning: Regrets and intrinsic
privacy-preserving properties. IEEE Transactions on Knowledge and Data Engineering, 25(11):2483–2493, 2013.
Zhu, M. and Martinez, S. An approximate dual subgradient algorithm for multi-agent non-convex optimization. In 49th IEEE
Conference on Decision and Control (CDC), pp. 7487–7492,
2010.
Zlobec, S. On the liu-floudas convexification of smooth programs.
Journal of Global Optimization, 32(3):401–407, 2005.

