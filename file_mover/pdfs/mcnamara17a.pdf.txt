Risk Bounds for Transferring Representations With and Without Fine-Tuning

Daniel McNamara 1 Maria-Florina Balcan 2

Abstract
A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction
function) learned on a source task to a target task.
Examples include the re-use of neural network
weights or word embeddings. We develop sufficient conditions for the success of this approach.
If the representation learned from the source task
is fixed, we identify conditions on how the tasks
relate to obtain an upper bound on target task risk
via a VC dimension-based argument. We then
consider using the representation from the source
task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable
conditions. We show examples of our bounds
using feedforward neural networks. Our results
motivate a practical approach to weight transfer,
which we validate with experiments.

1. Introduction
A widely used machine learning technique is the transfer
of a representation learned from a source task, for which
labeled data is abundant, to a target task, for which labeled
data is scarce. This may be effective if the tasks approximately share an intermediate representation. For example:
• features learned from an image of a human face to predict age may also be useful for predicting gender
• word embeddings learned to predict word contexts
may also be useful for part of speech tagging
• features learned from financial data to predict loan default may also be useful for predicting insurance fraud.
Often a representation is learned by a different organization
that may have greater access to data, computational and human resources. Examples are the Google word2vec package (Mikolov et al., 2013), and downloadable pre-trained
1
The Australian National University and Data61, Canberra, ACT, Australia 2 Carnegie Mellon University, Pittsburgh, PA, USA. Correspondence to: Daniel McNamara
<daniel.mcnamara@anu.edu.au>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

neural networks.1 Under this ‘representation-as-a-service’
model, a user may expect to access the representation itself,
as well as information about its performance on the source
task data on which it was trained. We aim to convert this
into a guarantee of the usefulness of the representation on
other tasks, which is known in advance without the effort
or cost of testing the representation on the target task(s).
Our analysis also covers the case where the source task is
constructed from unlabeled data, as in neural network unsupervised pre-training.
We consider two approaches to transferring a representation learned from a source task to a target task, as shown in
Figure 1. We may either treat the representation as fixed, or
we may narrow the class of representations considered on
the target task, which we refer to as fine-tuning. The fixed
option may be attractive when very little labeled target task
data is available and hence overfitting is a strong concern,
while the advantage of fine-tuning is relatively greater hypothesis class expressiveness.
Let X, Y and Z be sets known as the input, output and
feature spaces respectively. Let F be a class of representations, where f : X → Z for f ∈ F . Let G be a class
of specialized classifiers, where g : Z → Y for g ∈ G.
Let the hypothesis class H := {h : ∃f ∈ F, g ∈ G
such that h = g ◦ f }. Let hS , hT : X → Y be the labeling functions and PS , PT be the input distributions for
source task S and target task T respectively. We consider
the setting Y = {−1, 1}. Let the risk of a hypothesis
h on S and T be RS (h) := Ex∼PS [hS (x) 6= h(x)] and
RT (h) := Ex∼PT [hT (x) 6= h(x)] respectively. Let R̂S (h)
and R̂T (h) be the corresponding empirical (i.e. training
set) risks. We have mS labelled points for S and mT labelled points for T . Let dH be the VC dimension of H.
The remainder of the paper is structured as follows. In Section 2 we introduce related work. In Sections 3 and 4 we
analyze the cases where the transferred representation is
fixed and fine-tuned respectively. In Section 5 we apply
the results and use them to motivate and test a practical approach to weight transfer in neural networks. We conclude
in Section 6 and defer more involved proofs to Section 7.
1
See
http://code.google.com/archive/p/
word2vec,
http://caffe.berkeleyvision.org/
model_zoo and http://vlfeat.org/matconvnet/
pretrained for examples.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

2. Background
Empirical studies have shown the success of transferring
representations between tasks (Donahue et al., 2014; Hoffman et al., 2014; Girshick et al., 2014; Socher et al., 2013;
Bansal et al., 2014). Word embeddings learned on a source
task have been shown (Qu et al., 2015) to perform better
than unigram features on target tasks such as part of speech
tagging, and comparably or better than embeddings finetuned on the target task. Yosinski et al. (2014) learned
neural network weights using half of the ImageNet classes,
and then learned the other classes with a neural network
initialized with these weights, finding a benefit compared
to random initialization only with target task fine-tuning.
The transfer of representations, both with and without finetuning, is widely and successfully used.
Previous work on domain adaptation (Ben-David et al.,
2010; Mansour et al., 2009; Germain et al., 2013) has considered learning a hypothesis h on S and re-using it on
T , bounding RT (h) using RS (h) (measured with labeled
source data) and some notion of similarity between PS and
PT (measured with additional unlabeled target data). Such
results motivate a joint optimization using labeled source
and unlabeled target data (Ganin et al., 2016; Long et al.,
2015) to learn separate mappings fS , fT : X → Z which
make the induced distributions in the feature space Z similar, and a hypothesis g : Z → Y learned from the source
labels which can be re-used on T . This approach assumes
the tasks become the same if their input distributions can be
aligned. We consider a relaxation where the tasks are more
weakly related but some representation step can be transferred. We consider learning f : X → Z on S, re-using it
on T , and then learning gT : Z → Y from a small amount
of labeled target data. Given the widespread use of ‘downloadable’ representations, where f and gT are learned separately and there is no joint optimization over source and
target data, this is a realistic setting.
Work on lifelong learning relates the past performance of
a representation over many tasks to its expected future
performance. For a representation f ∈ F we construct
G ◦ f := {g ◦ f : g ∈ G}. Suppose there is a distribution over tasks, known as an environment. Assume several
tasks from this environment have been sampled, and that
for each task some hypothesis in G ◦ f has been selected
and its empirical risk evaluated. Previous work has provided bounds on the difference between the average empirical risk and the expected risk of the best hypothesis in
G ◦ f for a new task drawn from the environment. Such
bounds have been given by measuring the complexity of
F and G using covering numbers (Baxter, 2000), a variant of the growth function (Galanti et al., 2016), and a
distribution-dependent measure known as Gaussian complexity (Maurer et al., 2016). All of these bounds rely on

Learn representation
from scratch
F

Transfer representation
without fine-tuning
F

Transfer representation
with fine-tuning
F
F̂

fˆ
f

fˆ
f

f

Figure 1. A comparison of approaches to learning a representation
on a target task, where the search space in each case is the shaded
area. Learning from scratch, we search a representation class F
for a good representation f ∈ F . Without fine-tuning, we fix a
representation fˆ learned from the source task. With fine-tuning,
we narrow the search to F̂ ⊆ F near fˆ, which still contains f .

known past performance on a large number of tasks.2 In
practice, however, representations such as neural network
weights or word embeddings are often learned using only a
single source task, which is the setting we consider.

3. Representation Fixed by Source Task
Suppose labeled source data is abundant, labeled target data
is scarce, and we believe the tasks share a representation.
A natural approach to leveraging the source data is to learn
ĝS ◦ fˆ ∈ H on S, from which we assume we may extract fˆ ∈ F ,3 then conduct empirical risk minimization
over G ◦ fˆ := {g ◦ fˆ : g ∈ G} on T yielding ĝT ◦ fˆ.
Theorem 1 upper-bounds RT (ĝT ◦ fˆ) using four terms: a
function ω measuring a transferrability property obtained
analytically from the problem setting, the empirical risk
R̂S (ĝS ◦ fˆ), the generalization error of a hypothesis in H
learned from mS samples, and the generalization error of
a hypothesis in G learned from mT samples. The value
of the theorem is that if ω(R) = O(R), R̂S (ĝS ◦ fˆ) is a
small constant, mS  mT and dH  dG ,4 we improve on
the VC dimension-based bound for learning T from scratch
by avoiding the generalization error of a hypothesis in H
learned from mT samples. Furthermore, we do not settle
for bounding RT (ĝT ◦ fˆ) in terms of R̂T (ĝT ◦ fˆ), which
may be large. The theorem can be used to select S given
2
Pentina & Lampert (2014) extend this analysis to stochastic hypotheses (i.e. distributions over deterministic hypotheses),
where for each task we learn a posterior given a prior and training
data. The quality of the prior affects the learner’s performance.
The study proposes using source tasks to learn a ‘hyperposterior’,
a distribution over priors which is sampled to give a prior for each
task. Such a hyperposterior may focus the learner on a representation shared across tasks. The study gives a PAC-Bayes bound
on the expected risk of using a hyperposterior to learn a new task
drawn from the environment, in terms of the average empirical
risk obtained using the hyperposterior to learn the source tasks.
3
This is not possible with knowledge of ĝS ◦ fˆ alone, but in
the case of feedforward neural networks which we focus on, fˆ is
known if the weights learned on S are known.
4
We have mS  mT if labeled source task data is abundant
while labeled target task data is scarce, and dH  dG if we simplify target task learning by substantially reducing the hypothesis
space to be searched.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

several options. While we refer to ω in a general form, we
give an example in Section 3.1 and expect that others exist.5

Learn T
from scratch

Learn ĝS ◦ fˆ Transfer fˆ from S,
learn ĝT on T
on S
fˆ

Theorem 1. Let ω : R → R be a non-decreasing function. Suppose PS , PT , hS , hT , fˆ, G have the property that
∀ĝS ∈ G, minRT (g ◦ fˆ) ≤ ω(RS (ĝS ◦ fˆ)). Let ĝT :=

fˆ
ĝS

ĝT

g∈G

arg minR̂T (g ◦ fˆ). Then with probability at least 1 − δ
g∈G

ˆ
over pairs of training sets
qfor tasks S and T , RT (ĝT ◦ f )
/dH )+2 log(8/δ)
) +
≤ ω(R̂S (ĝS ◦ fˆ) + 2 2dH log(2emSm
S
q
4 2dG log(2emTm/dTG )+2 log(8/δ) .
Proof. Let gT∗ := arg minRT (g ◦ fˆ). With probability at
g∈G

least 1 − δ,
RT (ĝT ◦ fˆ)
≤ R̂T (ĝT ◦ fˆ) + 2

q

2dG log(2emT /dG )+2 log(8/δ)
mT

≤ R̂T (gT∗ ◦ fˆ) + 2

q

2dG log(2emT /dG )+2 log(8/δ)
mT

≤ RT (gT∗ ◦ fˆ) + 4

q

2dG log(2emT /dG )+2 log(8/δ)
mT

≤ ω(RS (ĝS ◦ fˆ)) + 4

q

2dG log(2emT /dG )+2 log(8/δ)
mT

q

/dH )+2 log(8/δ)
) +
≤ ω(R̂S (ĝS ◦ fˆ) + 2 2dH log(2emSm
S
q
4 2dG log(2emTm/dTG )+2 log(8/δ) .

Using m training points and a hypothesis class of VC dimension d, with probability at least 1 − δ, for all hypotheses h simultaneously, the risk
q R(h) and empirical risk R̂(h)

log(4/δ)
satisfy |R(h)− R̂(h)| ≤ 2 2d log(2em/d)+2
(Mohri
m
et al., 2012). For G this yields the first and third inequalities
with probability at least 1 − 2δ . For H, because ω is nondecreasing, this yields the fifth inequality with probability
at least 1 − 2δ . Applying the union bound achieves the desired result. The second inequality is by the definition of ĝT
and the fourth inequality follows from our assumption.

3.1. Neural Network Example with Fixed
Representation
In Theorem 2, we give an example of the property required
by Theorem 1 which is specific to a particular problem setting. We consider a neural network with a single hidden
layer (see Figure 2). We propose transferring the lowerlevel weights (corresponding to fˆ) learned on S, so that
only the upper-level weights (corresponding to G) have to
be learned on T . We want to show fˆ is also useful for T .

Figure 2. Neural network example learning T from scratch (left)
and with weights transferred from S (right). Thin blue and thick
red lines show weights trained on S and T respectively. Under
certain assumptions using weight transfer yields low risk on T .

To do this, we assume that some lower-level weights perform well on both tasks, which is clearly a necessary condition for the specific fˆ we are transferring to perform well
on both tasks. We also assume PS and PT have the relative rotation invariance property and that the upper-level
weights have fixed magnitude. This is so that a point x for
which fˆ(x) contributes to the risk on T cannot be ‘hidden’
from the risk of using fˆ on S, either through low PS (x)
or low magnitude upper-level weights. Hence RS (ĝS ◦ fˆ)
reliably indicates the usefulness of fˆ on T .
Let X = Rn and Z = Rk . Let F be the function class such
that f (x) = [a(w1 · x), . . . , a(wk · x)], where wi ∈ Rn
for 1 ≤ i ≤ k, a : R → R is an odd function6 and
· is the dot product. Let G be the function class such that
g(z) = sign(v · z), where v ∈ {−1, 1}k . Suppose ∃f ∈
F, gS , gT ∈ G such that max[RS (gS ◦f ), RT (gT ◦f )] ≤ .
Let fˆ(x) := [a(ŵ1 · x), . . . , a(ŵk · x)]. Given wi and
ŵi , pick nonzero constants αi and βi such that ||wi || =
||αi ŵi − βi wi || and wi · (αi ŵi − βi wi ) = 0. Let M be a
2k×n matrix with rows w1 , α1 ŵ1 −β1 w1 , . . . , wk , αk ŵk −
βk wk . Suppose M is full rank.7 Suppose ∀x, x0 such that
||M x|| = ||M x0 ||, PT (x) ≤ cPS (x0 ) for some c ≥ 1,
which we call relative rotation invariance and implies PS
and PT have the same support. If M is an orthogonal matrix then ∀x, x0 such that ||x|| = ||x0 ||, PT (x) ≤ cPS (x0 ).8
Theorem 2. Let ω(R) := cR + (1 + c). Then ∀ĝS ∈ G,
minRT (g ◦ fˆ) ≤ ω(RS (ĝS ◦ fˆ)).
g∈G
6

i.e. a(−x) = −a(x). Examples are tanh, sign and identity.
To see that this condition is necessary, consider the following
example where M is not full rank. Let n = 4, k = 2, hS =
sign(x1 ) and hT = sign(x2 ). For f (x) = [x1 + x2 , x1 − x2 ],
gS (z) = sign(z1 + z2 ) and gT (z) = sign(z1 − z2 ), we have
RS (gS ◦ f ) = RT (gT ◦ f ) = 0. On S we learn fˆ(x) = [x1 +
x3 , x1 − x3 ] and ĝS (z) = sign(z1 + z2 ), so that RS (ĝS ◦ fˆ) = 0
but in general minRT (g ◦ fˆ) > 0 since fˆ ignores x2 .
7

g∈G

5

We define ω by relating RS (ĝS ◦ fˆ) to minRT (g ◦ fˆ), since
g∈G

we expect this may be feasible analytically as in our example in
Section 3.1. However, because we only observe R̂S (ĝS ◦ fˆ), in
Theorem 1 we use this to bound RS (ĝS ◦ fˆ) and then apply ω.

8

For example, PS and PT are spherical Gaussians. For a zeromean multivariate Gaussian distribution this is achieved by the
whitening transformation x → Λ−1/2 U T x, where the columns
of U and entries of the diagonal matrix Λ are the eigenvectors and
eigenvalues of the distribution’s covariance matrix respectively.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

4. Representation Fine-Tuned on Target Task

4.1. Neural Network Example with Fine-Tuning

Consider learning ĝS ◦ fˆ on S, and then using fˆ and
RS (ĝS ◦ fˆ) to find F̂ ⊆ F , as in Figure 1. Let h̃g◦f be
a stochastic hypothesis (i.e. a distribution over H) associated with g ◦ f (e.g. g ◦ f is the mode of h̃g◦f ). We
propose learning T with the hypothesis class H̃G◦F̂ :=
{h̃g◦f : f ∈ F̂ , g ∈ G} and the prior h̃ĝS ◦fˆ. Learning T from scratch we assume that we would instead use
H̃G◦F := {h̃g◦f : f ∈ F, g ∈ G} and some fixed prior
h̃0 ∈ H̃G◦F . Let RT (h̃) := Ex∼PT ,h∼h̃ [hT (x) 6= h(x)]
and compute R̂T (h̃) on the training set distribution of T .

We transfer and fine-tune weights in a feedforward neural network with one hidden layer to instantiate the property required by Theorem 3. We learn a deterministic hypothesis of this type on S and obtain k estimated lowerlevel weight vectors ŵi . Learning T we now consider only
lower-level weights near ŵi , corresponding to F̂ . On T we
learn a stochastic hypothesis formed by taking a deterministic network and adding independent sources of spherical
Gaussian noise to the lower-level weights and sign-flipping
noise to the upper-level weights. The KL divergence between two of the stochastic hypotheses is expressed using
the angles between their lower-level weights10 and a quantity computable from their upper-level weights.

In Theorem 3 we show that if F̂ is ‘small enough’ so that
all h̃ ∈ H̃G◦F̂ have a small KL divergence from h̃ĝS ◦fˆ,
we may apply a PAC-Bayes bound to the generalization
error of hypotheses in H̃G◦F̂ involving four terms: a function ω measuring a transferrability property, the empirical
risk R̂S (ĝS ◦ fˆ), the generalization error of a hypothesis
in H learned from mS points, and a weak dependence on
mT . The value of the theorem is that if ω(R) = O(R),
R̂S (ĝS ◦ fˆ) is a small constant, and mS  mT , we improve
on the PAC-Bayes bound for H̃G◦F and h̃0 .9 F̂ is useful if
it is also ‘large enough’ in the sense that ∃h̃gT ◦f ∈ H̃G◦F̂
such that RT (h̃gT ◦f ) ≤ . Here ω quantifies how large the
F̂ we search on T must be in order to be ‘large enough’,
in terms of RS (ĝS ◦ fˆ). While in general such an F̂ and ω
may not exist, we give an example in Section 4.1.
Theorem 3. Let ω : R → R be non-decreasing. Suppose given fˆ ∈ F and RS (ĝS ◦ fˆ) estimated from S,
it is possible to construct F̂ with the property ∀h̃ ∈
H̃G◦F̂ , KL(h̃||h̃ĝS ◦fˆ) ≤ ω(RS (ĝS ◦ fˆ)). Then with
probability at least 1 − δ over pairs of training sets for
tasks
S and T , ∀h̃ ∈ H̃G◦F̂ , RT (h̃) ≤ R̂T (h̃) +
r
ω(R̂S (ĝS ◦fˆ)+2

q

2dH log(2emS /dH )+2 log(8/δ)
)+log 2mT /δ
mS

2(mT −1)

.

Proof. With probability at least 1 − δ,
RT (h̃)
r
≤ R̂T (h̃) +
≤ R̂T (h̃) +

KL(h̃||h̃ĝ

ˆ)+log 2mT /δ
S ◦f

2(mT −1)

q

ω(RS (ĝS ◦fˆ))+log 2mT /δ
.
2(mT −1)

We want to prove that we can construct such an F̂ to successfully learn T . To do this, we assume some lower-level
weights wi perform well on both S and T . We make F̂
‘small enough’ by only including lower-level weights with
small angles to ŵi , and ‘large enough’ by using the risk
observed using ŵi on S to provide an upper bound on the
angle between each pair wi and ŵi . Our assumptions ensure that poor ŵi cannot be ‘hidden’ from the risk on S, either through low PS density in the region of disagreement
between wi and ŵi , or through low magnitude higher-level
weights. Hence we know that searching F̂ will include wi .
Let X = Rn and Z = Rk , where k is odd.
Let F be the function class such that f (x) =
[sign(w1 · x), . . . , sign(wk · x)], where wi ∈ Rn
for 1 ≤ i ≤ k.
Let G be the function class
such that g(z) = sign(v · z), where v ∈ {−1, 1}k .
Let Bv be a distribution on {−1, 1}k such that for
k
Q
0
0
v 0 ∼ Bv , P r(v 0 ) =
p1(vj =vj ) (1 − p)1(vj =−vj ) , where
j=1

p ∈ [0.5, 1]. Let h̃g◦f := g 0 ◦ f 0 such that v 0 , w10 , . . . , wk0 ∼
k
Q
Bv
N (wi , σ 2 I). Suppose ∃f ∈ F, gS , gT ∈ G such
i=1

that max[RS (gS ◦ f ), RT (h̃gT ◦f )] ≤ . Let fˆ(x) :=
[sign(ŵ1 · x), . . . , sign(ŵk · x)], θ(wi , ŵi ) be the angle
between wi and ŵi , and assume ∀i, ||ŵi || = 1. Define M
as in Section 3.1. Let PS have the rotation invariance property ∀x, x0 such that ||M x|| = ||M x0 ||, PS (x) ≤ cPS (x0 )
for some c ≥ 1.

The first inequality holds with probability at least 1 − 2δ
(Shalev-Shwartz & Ben-David, 2014). The second inequality holds by assumption.
Furthermore, RS (ĝS ◦ fˆ) ≤
q
2d
log(2em
H
S /dH )+2 log(8/δ)
R̂S (ĝS ◦ fˆ) + 2
with prob-

ˆ
ˆ
Theorem 4. Given
p f and RS (ĝS ◦ f ) estimated from S,
let θmax := π 2(k − 1)c(RS (ĝS ◦ fˆ) + ) and F̂ :=
{f ∈ F : ∀i, ||wi || = 1 ∧ |θ(wi , ŵi )| ≤ θmax }. Let
p
ω(R) := σk2 [1 − cos θmax ] + k[2p − 1 + (1 − p)k ] log2 1−p
.

ability at least 1 − 2δ (Mohri et al., 2012) and ω is nondecreasing. The result follows from the union bound.

Then ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤  and
∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS ◦fˆ) ≤ ω(RS (ĝS ◦ fˆ)).

mS

9

Using the restricted deterministic hypothesis class G ◦ F̂ :=
{h : ∃f ∈ F̂ , g ∈ G such that h = g ◦ f } and a VC dimensionbased bound may not improve on H, since possibly dG◦F̂ = dH .

10

Assuming that the lower-level weight vectors are of fixed
magnitude, which is no loss of model expressiveness since we
use the sign activation function at the hidden layer.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

5. Applications
We show the utility of the risk bounds, and present a novel
technique and experiments motivated by our theorems.
5.1. Using the Risk Bounds
The results described yield tighter bounds on risk when
transferring representations from S, compared to learning
T from scratch. Examples are shown in Figure 3.11
We set δ = 0.05. For the top part, we use the example
from Section 3.1 and set n = 10, k = 5. Learning T
from scratch with H, we use the bound from Mohri et al.
(2012) used previously. The VC dimension of a network
of |E| edges using the sign activation is O(|E| log |E|)
(Shalev-Shwartz & Ben-David, 2014), where in our case
|E| = nk+k. We use dH = |E| log |E| in the chart. Transferring a representation from S to T without fine-tuning,
we consider the limit  → 0, R̂S (ĝS ◦ fˆ) → 0, mS → ∞,
and hence ω(·) → 0 by Theorem 2. Furthermore, dG ≤ k
since G is finite and hence dG ≤ log2 |G| (Shalev-Shwartz
& Ben-David, 2014). We use the bound from Theorem 1.
For the bottom part, we use the example from Section 4.1
1
, k = 499, p = 32 . Learning T from
and set σ 2 = 10
scratch we use the stochastic hypothesis class {h̃g◦f : f ∈
F such that ∀i||wi || = 1, g ∈ G} and a prior h̃0 where
∀i wi = 0 and v ∈ {−1, 1}k is arbitrary.12 Hence we
have the bound KL(h̃||h̃0 ) ≤ 10k + k3 , which becomes
tight for large k. We apply the PAC-Bayes bound (ShalevShwartz & Ben-David, 2014) used previously. Transferring
a representation from S and fine-tuning on T , we consider
the limit  → 0, R̂S (ĝS ◦ fˆ) → 0, mS → ∞. We have
KL(h̃||h̃ĝS ◦fˆ) ≤ k3 by Theorem 4. We use the bound from
Theorem 3.
5.2. Fine-Tuning through Regularization
We relax the hard constraint on F̂ from Section 4.1 by using a modified loss function, which we find performs better
in practice. Let yi and ŷi be the label and prediction respectively for the ith training point. In a fully-connected
feedforward neural network with l layers of weights, let
W (j) be the jth weight matrix, Ŵ (j) be its estimate from
S (excluding weights for bias units in both cases), and ||·||2
be the entry-wise 2 norm. A typical loss function (1) used
for training is composed of the sum of training set log loss
and L2 regularization on the weights.
11
Note that VC dimension risk bounds are known for being
rather loose, while PAC-Bayesian bounds are tighter and hence
yield non-trivial results in higher dimensions with fewer samples.
12
This class is as expressive as H̃G◦F but by setting ||wi || = 1
the KL divergence of all hypotheses from any prior is bounded,
allowing a fair comparison to H̃G◦F̂ . The choice of h̃0 minimizes
worst case KL divergence to a hypothesis in the class.

Figure 3. A comparison of risk bounds compared to learning T
from scratch, without fine-tuning (top) and with fine-tuning (bottom). The two charts use different parameters (see Section 5.1).
m
X

l

[−yi log ŷi − (1 − yi ) log(1 − ŷi )] +

i=1

λX
(||W (j) ||22 )
2 j=1
(1)
13

We replace the regularization penalty with (2).

l
X
λ1 (j)
λ2 (j)
[
||W (j) − Ŵ (j) ||22 +
||W (j) ||22 ]
(2)
2
2
j=1
This penalizes estimates of W far from the representation
learned on S. Since we expect the tasks to share a lowlevel representation (e.g. edge detectors for vision, word
embeddings for text) but be distinct at higher levels (e.g.
image components for vision, topics for text), we set λ1 (·)
to be a decreasing function, while λ2 (·) controls standard
L2 regularization. The technique is novel to our knowledge, although other approaches to transferring regularization between tasks exist (Evgeniou & Pontil, 2004; Raina
et al., 2006; Argyriou et al., 2008; Ghifary et al., 2014).

5.3. Experiments
We experiment on basic image and text classification
tasks.14 We show that learning algorithms motivated by
our theoretical results can help to overcome a scarcity of
labeled target task data. Note that we do not replicate the
conditions specified in our theorems, nor do we attempt extensive tuning to achieve state-of-the-art performance.
13
Basing our approach on (1), we follow the convention that
weights connected to bias units are excluded from the regularization penalty. However, the inclusion of these weights in the
||W (j) − Ŵ (j) || term of (2) is a plausible variant.
14
The MNIST and 20 Newgroups datasets are available at
http://yann.lecun.com/exdb/mnist and http://
qwone.com/˜jason/20Newsgroups respectively.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

We randomly partition label classes into sets S+ and S− ,
where |S+ | = |S− |.15 We construct T+ by randomly pick+|
ing from S+ up to γ := |S+|S∩T
, then randomly picking
+|
from S− such that |T+ | = |T− |. We let S be the task of
distinguishing between S+ and S− and T be that of distinguishing T+ and T− . Constructing S+ and T+ as disjunctions of classes means that the class labels are a perfect
representation shared between S and T .
We compare the accuracy on T of four options:

Table 1. Evaluation of transferring representations. Entries are the
test set accuracy of the technique (row) for the task (column) averaged over 10 trials, with the best result for each task bolded.
T ECHNIQUE

MNIST, γ =
0.6
0.8
1

N EWSGROUPS, γ =
0.6
0.8
1

BASE
F INE - TUNE fˆ
F IX fˆ
F IX ĝS ◦ fˆ

88.4
91.9
87.5
67.4

62.6
62.3
52.2
55.5

87.9
93.9
92.3
85.6

87.9
95.4
97.3
98.1

63.2
72.3
69.6
70.7

66.1
83.3
83.3
83.6

• learn T from scratch (BASE)
• transfer fˆ from S, fine-tune f and train g on T using
(2) (F INE - TUNE fˆ)
• transfer fˆ from S and fix, train g on T (F IX fˆ)16
• transfer ĝS ◦ fˆ from S and fix (F IX ĝS ◦ fˆ).17
We use λ1 (1) = λ2 (2) = λ := 1,18 λ1 (2) = λ2 (1) = 0,
mT = 500 and the sigmoid activation function. For
MNIST we use raw pixel intensities, a 784 × 50 × 1 network and mS = 50000. For N EWSGROUPS we use TF-IDF
weighted counts of most frequent words, a 2000 × 50 × 1
network and mS = 15000. We use conjugate gradient optimization with 200 iterations.
The results are shown in Table 1.19 When the tasks are nonidentical, F INE - TUNE fˆ is mostly the strongest but performs better on MNIST. F IX fˆ outperforms BASE when
γ ≥ 0.8 and hence the tasks are similar. While F IX fˆ outperforms F IX ĝS ◦ fˆ when the tasks are non-identical on
MNIST, on N EWSGROUPS there is no evidence of benefit.
When the tasks are identical, F IX ĝS ◦ fˆ is the strongest.
It appears that learning an MNIST digit requires a dense
weight vector and so Ŵ (1) tends to encode single digits,
which helps transferrability. However, it appears that since
we may learn a newsgroup with a sparse weight vector,
Ŵ (1) tends to encode disjunctions of newsgroups which
somewhat reduces transferrability. When transferring representations does work, fine-tuning using the regularization
penalty proposed in (2) improves performance.
15

For MNIST there are 10 label classes and for 20 Newgroups
there are 20. In both cases the classes are approximately balanced. Note that we ignore the hierarchical structure of the 20
Newsgroups classes, which likely contributes to the lower accuracies reported for all methods for this dataset relative to MNIST.
16
i.e. logistic regression with L2 regularization and fˆ fixed.
17
Used to isolate the benefit of transferring fˆ rather than ĝS ◦ fˆ.
18
We explored tuning λ to lift the performance of BASE on
MNIST, but found that the results did not materially improve. Potentially λ1 (j) and λ2 (j) in (2) could be tuned with cross validation on the target task.
19
For γ = 1, hS = hT . We do not consider γ < 0.5, since that
is equivalent to 1 − γ with the definitions of T+ and T− swapped.

6. Conclusion
We developed sufficient conditions for the successful transfer of representations both with and without fine-tuning.
This is a step towards a principled explanation of the empirical success achieved by such techniques. A promising
direction for future work is generalizing the neural network
architectures considered (e.g. using multiple hidden layers)
and relaxing the distributional assumptions required. Furthermore, in the fine-tuning case it may be possible to upper
bound the target task generalization error of hypotheses in
G ◦ F̂ := {h : ∃f ∈ F̂ , g ∈ G such that h = g ◦ f } using another measure such as the Rademacher complexity of
G ◦ F̂ , eliminating the need for stochastic hypotheses.
We proposed a novel form of regularization for neural network training motivated by our theoretical results,
which penalizes divergence from source task weights and
is stricter for lower-level weights. We validated this technique through applications to image and text classification.
Future directions include experiments on more challenging
tasks using deeper and more tailored network architectures
(e.g. convolutional neural networks).

7. Additional Proofs
We provide complete proofs of Theorems 2 and 4. For
brevity, we drop the explicit dependence of f , fˆ, hS and
hT on x in our notation where the meaning is clear.
7.1. Proof of Theorem 2
Proof. Let gS (z) := sign(vS · z), gT (z) := sign(vT · z),
ĝS (z) := sign(v̂S · z), ĝT (z) := sign(d ∗ v̂S · z), where
d := vS ∗ vT ∈ {−1, 1}k and ∗ is the elementwise product.
It is sufficient to show RT (ĝT ◦ fˆ) ≤ cRS (ĝS ◦ fˆ)+(1+c).
RT (ĝT ◦ fˆ)
= P rx∼PT (hT d ∗ v̂S · fˆ ≤ 0)
≤ P rx∼PT (hT d ∗ vS · f ≤ 0, d ∗ vS · f d ∗ v̂S · fˆ ≥ 0) +
P rx∼PT (hT d ∗ vS · f ≥ 0, d ∗ vS · f d ∗ v̂S · fˆ ≤ 0)
≤ P rx∼PT (hT d ∗ vS · f ≤ 0)+
P rx∼PT (d ∗ vS · f d ∗ v̂S · fˆ ≤ 0)

Risk Bounds for Transferring Representations With and Without Fine-Tuning

≤  + P rx∼PT (d ∗ vS · f d ∗ v̂S · fˆ ≤ 0)

max|θ(wi ,ŵi )|
i

π

≤  + cP rx∼PS (vS · f v̂S · fˆ ≤ 0)

√

2(k−1)

≤ P rx∼P (vS · f vS · fˆ ≤ 0)

≤  + c[P rx∼PS (hS v̂S · fˆ ≤ 0, hS vS · f ≥ 0)+
P rx∼PS (hS v̂S · fˆ ≥ 0, hS vS · f ≤ 0)]

≤ P rx∼P (vS · f v̂S · fˆ ≤ 0)

≤  + c[P rx∼PS (hS v̂S · fˆ ≤ 0) + P rx∼PS (hS vS · f ≤ 0)]

≤ cP rx∼PS (vS · f v̂S · fˆ ≤ 0)

≤ cRS (ĝS ◦ fˆ) + (1 + c).

≤ c[P rx∼PS (hS vS · f ≤ 0, hS v̂S · fˆ ≥ 0)+
P rx∼PS (hS vS · f ≥ 0, hS v̂S · fˆ ≤ 0)]

The third and final inequalities are due to the shared representation assumption in the problem statement. The fourth
inequality holds by Lemma 1. The remaining lines apply
simple rules of probability.

≤ c[P rx∼PS (hS vS · f ≤ 0) + P rx∼PS (hS v̂S · fˆ ≤ 0)]
≤ c[ + RS (ĝS ◦ fˆ)].

Lemma 1. Suppose ∀x, x0 such that ||M x|| = ||M x0 ||,
PT (x) ≤ cPS (x0 ). Let f, fˆ ∈ F , v, v̂, d ∈ {−1, 1}k . Then
P rx∼PT (d ∗ v · f d ∗ v̂ · fˆ ≤ 0) ≤ cP rx∼PS (v · f v̂ · fˆ ≤ 0).

The first inequality holds by Lemma 2. The second inequality holds by Lemma 3, using the fact ∀i, wi · ŵi ≥ 0.
The third inequality uses the rotation invariance of PS . The
following two lines use basic laws of probability. The final
inequality uses the assumption RS (gS ◦ f ) ≤ .

Proof. Suppose there is an invertible map Rn → Rn yielding x0 on input x, such that ∀x, ||M x|| = ||M x0 || and
d ∗ v · f (x)d ∗ v̂ · fˆ(x) = v · f (x0 )v̂ · fˆ(x0 ). Then the result
follows since PT (x) ≤ cPS (x0 ) by assumption. Furthermore, if M is an orthogonal matrix, ||x|| = ||x0 ||.

Proof of ∀h̃ ∈ H̃G◦F̂ , KL(h̃||h̃ĝS ◦fˆ) ≤ ω(RS (ĝS ◦ fˆ)).
For any h̃g◦f ∈ H̃G◦F̂ , KL(h̃g◦f ||h̃ĝS ◦fˆ)
=

k
P

[KL(N (wi , σ 2 I)||N (ŵi , σ 2 I))] + KL(Bv ||Bv̂S ).

Such a map is x := (M M ) M d˜ ∗ (M x), where d˜ :=
[d1 , d1 , . . . , dk , dk ]. We have ∀i, wi · x0 = di wi · x and
(αi ŵi −βi wi )·x0 = di (αi ŵi −βi wi )·x, and hence ŵi ·x0 =
di ŵi · x for αi , βi 6= 0. Therefore:

The KL divergence of a product distribution is the sum of
the KL divergences of its component distributions. We upper bound both terms and apply the definition of ω.

d ∗ v · f (x)d ∗ v̂ · fˆ(x)

k
P

= v · d ∗ f (x)v̂ · d ∗ fˆ(x)

i=1

= v · f (x0 )v̂ · d ∗ fˆ(x)

=

0

T

−1

T

i=1

KL(N (wi , σ 2 I)||N (ŵi , σ 2 I))
1
2σ 2

= v · f (x0 )v̂ · fˆ(x0 ).
The first equality is a property of the elementwise and dot
products. For the second equality, a(wi ·x0 ) = a(di wi ·x) =
di a(wi · x) since a is an odd function. Similarly, for the
third equality a(ŵi · x0 ) = a(di ŵi · x) = di a(ŵi · x).
7.2. Proof of Theorem 4
Proof of ∃h̃gT ◦f ∈ H̃G◦F̂ such that RT (h̃gT ◦f ) ≤ .
Recall that wi are the weight vectors for f and ŵi are
those for fˆ. Observe that for any wi such that wi · ŵi < 0,
we have −wi · ŵi > 0 and −vi sign(−wi · x) =
vi sign(wi · x). Combining this with the assumption
min
max[RS (gS ◦ f ), RT (gT ◦ f )] ≤ , we con-

=
=
≤

Let gS (z) := sign(vS · z) and ĝS (z) := sign(v̂S · z). Let
P be a rotation invariant distribution for c = 1. To prove
h̃gT ◦f ∈ H̃G◦F̂ , by the definition of H̃G◦F̂ it is sufficient
p
to show ∀i, |θ(wi , ŵi )| ≤ π 2(k − 1)c(RS (ĝS ◦ fˆ) + ).

1
σ2

||wi − ŵi ||2

i=1
k
P

(||wi ||2 + ||ŵi ||2 − 2||wi ||||ŵi || cos |θ(wi , ŵi )|)

i=1
k
P

(1 − cos |θ(wi , ŵi )|)

i=1

k
σ 2 [1

− cos(π

p

2(k − 1)c(RS (ĝS ◦ fˆ) + ))].

The first equality uses the KL divergence of Gaussian distributions. The second equality uses the law of cosines. The
third equality is because ∀i, ||wi || = ||ŵi || = 1 by construction. The inequality follows by the definition of F̂ and
the fact that 1 − cos |θ| is non-decreasing for |θ| ∈ [0, π].
KL(Bv ||Bv̂S )

f ∈F,gS ,gT ∈G

clude ∃f ∈ F, gS , gT ∈ G such that ∀i, wi · ŵi ≥ 0 and
max[RS (gS ◦ f ), RT (h̃gT ◦f )] ≤ .

1
2σ 2

k
P

≤

k
P
i=1

k
i



pi (1 − p)k−i log2

pi (1−p)k−i
(1−p)i pk−i

= k[2p − 1 + (1 − p)k ] log2

p
1−p .

The first inequality uses the definition of Bv to express
KL(Bv ||Bv̂S ). The equality is a simplification.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

Lemma 2. Suppose k is odd, v ∈ {−1, 1}k , f, fˆ ∈ F such
that ∀i, wi · ŵi ≥ 0 and P is rotation invariant with c = 1.
max|θ(wi ,ŵi )|
≤ P rx∼P (v · f v · fˆ ≤ 0).
Then i √
π

2(k−1)

Proof. Let v−j := [v1 , . . . , vj−1 , vj+1 , . . . , vk ] and define
f−j and fˆ−j similarly. Let P r(·) := P rx∼P (·).

Let Φ(a) :=

1
2k−1

bk/2c
P

b
P

b=0 j=da/2+b/2−k/4e

a
j



≥ P r(v · f v · fˆ < 0)

= E[∆]
P
=
P r(f˜)E[∆|f˜] + P r(d ∗ f˜)E[∆|d ∗ f˜]

= P r(v−j · f−j = 0)
[P r(vj fj v−j · fˆ−j < −1, fj fˆj = 1|v−j · f−j = 0)+
P r(vj fj v−j · fˆ−j < 1, fj fˆj = −1|v−j · f−j = 0)]
≥ P r(v−j · f−j = 0)
[P r(vj fj v−j · fˆ−j < −1, fj fˆj = −1|v−j · f−j = 0)+
P r(vj fj v−j · fˆ−j < 1, fj fˆj = −1|v−j · f−j = 0)]
= P r(v−j · f−j = 0)
[P r(vj fj v−j · fˆ−j < −1, fj fˆj = −1|v−j · f−j = 0)+
P r(vj fj v−j · fˆ−j > −1, fj fˆj = −1|v−j · f−j = 0)]
= P r(v−j · f−j = 0)P r(fj fˆj = −1|v−j · f−j = 0)
= P r(v−j · f−j = 0)P r(fj fˆj = −1)
 1 k−1 |θ(wj ,ŵj )|
= k−1
k−1 ( )
2
π

f˜∈F̃

=

P

≥

=

P

i

π

√

2(k−1)

[P r(v · f v · fˆ ≤ 0|d ∗ f˜) − P r(v · f v · fˆ ≤ 0|f˜)]
P
=
[P r(f˜) − P r(d ∗ f˜)][Φ(a(d ∗ f˜)) − Φ(a(f˜))]
f˜∈F̃

≥ 0.
The second equality uses linearity of expectation. The third
equality uses the law of total expectation and the definition
of F̃ .
The fourth
since E[∆|d ∗ f˜]
P equality holds
=
P r(f |d ∗ f˜)E[∆|d ∗ f˜, f ]
=−

P

P r(f |d ∗ f˜)E[∆|f˜, f ]

f ∈{−1,1}k

=−

P

P r(f |f˜)E[∆|f˜, f ] = −E[∆|f˜] due to the

f ∈{−1,1}k

rotation invariance of P .
.

The third inequality follows since P is rotation invariant
and wj · ŵj ≥ 0. The third and fifth equalities use rotation
invariance. The final equality uses rotation invariance and
the fact that k is odd. The fourth inequality is a standard
lower bound for the central binomial coefficient. The other
lines use basic simplifications and laws of probability.
Lemma 3. Suppose k is odd, v, v̂ ∈ {−1, 1}k , f, fˆ ∈ F
such that ∀i, wi · ŵi ≥ 0 and P is rotation invariant with c = 1. Then P rx∼P (v · f v · fˆ ≤ 0) ≤
P rx∼P (v · f v̂ · fˆ ≤ 0).
Proof. Let P r(·) := P rx∼P (·) and E[·] := Ex∼P [·]. Let
P r(f˜) := P rx∼P ([f1 (x)fˆ1 (x), . . . , fk (x)fˆk (x)] = f˜).
Let d := v̂ ∗ v and ∆(x) := 1(v · f (x)v̂ · fˆ(x) ≤ 0) −
1(v · f (x)v · fˆ(x) ≤ 0). Assume v̂ 6= v (if v̂ = v then
k
P
the lemma clearly holds). Let a(f˜) :=
1(f˜i = 1) and
i=1

let l :=

[P r(f˜) − P r(d ∗ f˜)]

f˜∈F̃

f ∈{−1,1}k

k−1
|θ(wj ,ŵj )|
√2
( 1 )k−1
π
2(k−1) 2

max|θ(wi ,ŵi )|

[P r(f˜) − P r(d ∗ f˜)]E[∆|f˜]

f˜∈F̃

2

≥

. The

P r(v · f v̂ · fˆ ≤ 0) − P r(v · f v · fˆ ≤ 0)
= E[1(v · f v̂ · fˆ ≤ 0)] − E[1(v · f v · fˆ ≤ 0)]

= P r(v−j · f−j = 0)
P r(vj fj v−j · fˆ−j + fj fˆj < 0|v−j · f−j = 0)



term b counts coordinates where vi fˆi = sign(v · f ), while
j counts those where vi fi = sign(v · f ) and fi = fˆi .

P r(v · f v · fˆ ≤ 0)

≥ P r(v−j · f−j = 0)P r(v · f v · fˆ < 0|v−j · f−j = 0)

k−a
b−j

min i. Let F̃ := {f˜ ∈ {−1, 1}k : a(f˜) >

i:di =−1

a(d ∗ f˜) ∨ (a(f˜) = a(d ∗ f˜) ∧ f˜l = 1)}.

The fifth equality holds by expanding ∆, linearity of expectation, and a similar argument to the previous equality
to show P r(v · f v̂ · fˆ ≤ 0|f˜) = P r(v · f v · fˆ ≤ 0|d ∗ f˜).
The sixth equality holds by the rotation invariance of P and
the fact that k is odd.
For the final inequality, the right hand term is non-negative
since a(f˜) ≥ a(d ∗ f˜) and Φ is non-increasing. The left
hand term is also non-negative due to the rotation invariance assumption and the fact that ∀i, wi · ŵi ≥ 0.

Acknowledgements
Daniel McNamara was a visitor at Carnegie Mellon University during the period of this research, supported by a
Fulbright Postgraduate Scholarship.
This work was supported in part by NSF grants CCF1422910, CCF-1535967, IIS-1618714, and a Microsoft Research Faculty Fellowship.
We thank the anonymous reviewers for their useful comments.

Risk Bounds for Transferring Representations With and Without Fine-Tuning

References
Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano. Convex multi-task feature learning. Machine
Learning, 73(3):243–272, 2008.

Hoffman, Judy, Guadarrama, Sergio, Tzeng, Eric S, Hu,
Ronghang, Donahue, Jeff, Girshick, Ross, Darrell,
Trevor, and Saenko, Kate. LSDA: Large scale detection
through adaptation. In Advances in Neural Information
Processing Systems, pp. 3536–3544, 2014.

Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Tailoring continuous word representations for dependency
parsing. In Association for Computational Linguistics,
pp. 809–815, 2014.

Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan,
Michael I. Learning transferable features with deep
adaptation networks. In International Conference on
Machine Learning, pp. 97–105, 2015.

Baxter, Jonathan. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12(3):149–198,
2000.

Mansour, Yishay, Mohri, Mehryar, and Rostamizadeh, Afshin. Domain adaptation: Learning bounds and algorithms. In Conference on Learning Theory, 2009.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza,
Alex, Pereira, Fernando, and Vaughan, Jennifer Wortman. A theory of learning from different domains. Machine Learning, 79(1):151–175, 2010.

Maurer, Andreas, Pontil, Massimiliano, and RomeraParedes, Bernardino. The benefit of multitask representation learning. Journal of Machine Learning Research,
17(81):1–32, 2016.

Donahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman,
Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor. DeCAF: a deep convolutional activation feature for generic
visual recognition. In International Conference on Machine Learning, pp. 647–655, 2014.

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S., and Dean, Jeff. Distributed representations of
words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pp.
3111–3119, 2013.

Evgeniou, Theodoros and Pontil, Massimiliano. Regularized multitask learning. In International Conference on
Knowledge Discovery and Data Mining, pp. 109–117,
2004.
Galanti, Tomer, Wolf, Lior, and Hazan, Tamir. A theoretical framework for deep transfer learning. Information
and Inference, 2016.
Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain, Pascal, Larochelle, Hugo, Laviolette, François,
Marchand, Mario, and Lempitsky, Victor. Domainadversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016.
Germain, Pascal, Habrard, Amaury, Laviolette, François,
and Morvant, Emilie. A PAC-Bayesian approach for domain adaptation with specialization to linear classifiers.
In International Conference on Machine Learning, pp.
738–746, 2013.
Ghifary, Muhammad, Kleijn, W Bastiaan, and Zhang,
Mengjie. Domain adaptive neural networks for object
recognition. In Pacific Rim International Conference on
Artificial Intelligence, pp. 898–904. Springer, 2014.
Girshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,
Jitendra. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference
on Computer Vision and Pattern Recognition, pp. 580–
587, 2014.

Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of Machine Learning. MIT Press,
2012.
Pentina, Anastasia and Lampert, Christoph H. A PACBayesian bound for Lifelong Learning. In International
Conference on Machine Learning, pp. 991–999, 2014.
Qu, Lizhen, Ferraro, Gabriela, Zhou, Liyuan, Hou, Weiwei, Schneider, Nathan, and Baldwin, Timothy. Big data
small data, in domain out-of domain, known word unknown word: the impact of word representation on sequence labelling tasks. In Conference on Computational
Natural Language Learning, pp. 89–93, 2015.
Raina, Rajat, Ng, Andrew Y., and Koller, Daphne. Constructing informative priors using transfer learning. In
International Conference on Machine Learning, pp.
713–720, 2006.
Shalev-Shwartz, Shai and Ben-David, Shai. Understanding
Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.
Socher, Richard, Ganjoo, Milind, Manning, Christopher D,
and Ng, Andrew. Zero-shot learning through crossmodal transfer. In Advances in Neural Information Processing Systems, pp. 935–943, 2013.
Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson,
Hod. How transferable are features in deep neural networks? In Advances in Neural Information Processing
Systems, pp. 3320–3328, 2014.

