The Price of Differential Privacy for Online Learning
Naman Agarwal 1 Karan Singh 1

Abstract
We design differentially private algorithms for
the problem of online linear optimization in the
fullpinformation and bandit settings with optimal
Õ( T )1 regret bounds. In the full-information
setting, our results demonstrate that "-differential
privacy may be ensured for free
p – in particular,
the regret bounds scale as O( T ) + Õ 1" . For
bandit linear optimization, and as a special case,
for non-stochastic multi-armed bandits,⇣the prop ⌘
posed algorithm achieves a regret of Õ 1" T ,
while the
known best regret bound
⇣ previously
⌘
1 23
was Õ " T .

1. Introduction
In the paradigm of online learning, a learning algorithm
makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past
queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees.
Consequently, online learning algorithms are well suited
to dynamic and adversarial environments, where real-time
learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in
such settings is differential privacy (Dwork et al., 2006)
which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as
opposed to when it is absent in a dataset.
In this paper, we design differentially private algorithms for
online linear optimization with near-optimal regret, both in
*

Equal contribution 1 Computer Science, Princeton University, Princeton, NJ, USA. Correspondence to:
Naman Agarwal <namana@cs.princeton.edu>, Karan Singh
<karans@cs.princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
Here the Õ(·) notation hides polylog(T ) factors.

the full information and partial information (bandit) settings. This result improves the known best regret bounds
for a number of important online learning problems – including prediction from expert advice and non-stochastic
multi-armed bandits.
1.1. Full-Information Setting: Privacy for Free
For the full-information setting where the algorithm gets
to see the complete loss vector every round, we design
"-differentially
algorithms with regret bounds that
⇣p private
⌘
scale as O
T + Õ 1" (Theorem 3.1), partially resolving an open question
⇣ p ⌘ to improve the previously best known
bound of O 1" T posed in (Smith & Thakurta, 2013).
A decomposition of the bound on the regret bound of this
p1 , the regret incurred by
form implies that when "
T
the differentially private algorithm matches the optimal regret in the non-private setting, i.e. differential privacy is
free. Moreover even when "  p1T , our results guarantee
a sub-constant regret per round in contrast to the vacuous
constant regret per round guaranteed by existing results.
Concretely, consider the case of online linear optimization
over the cube, with unit l1 -norm-bounded loss vectors. In
this setting, (Smith
& Thakurta, 2013) achieves a regret
p
N
bound of O( 1" N T ), which is meaningful only if T
"2 .
p
Our theorems imply a regret bound of Õ( N T + N" ). This
is an improvement on the previous bound regardless of the
value of ". Furthermore, when T is between N" and "N2 , the
previous bounds are vacuous whereas our results are still
meaningful. Note that the above arguments show an improvement over existing results even for moderate value of
". Indeed, when " is very small, the magnitude of improvements are more pronounced.
Beyond the separation between T and ", the key point of
our analysis is that we obtain bounds for general regularization based algorithms which adapt to the geometry of
the underlying problem optimally, unlike the previous algorithms (Smith & Thakurta, 2013) which utilizes euclidean
regularization. This allows our results
p to get rid of a polynomial dependence on N (in the T term) in some cases.
Online linear optimization over the sphere and prediction
with expert advice are notable examples.

The Price of Differential Privacy for Online Learning

We summarize our results in Table 1.1.
1.2. Bandits: Reduction to the Non-private Setting
In the partial-information (bandit) setting, the online learning algorithm only gets to observe the loss of the prediction it prescribed. We outline a reduction technique that
translates a non-private bandit algorithm to a differentially
p
private bandit algorithm, while retaining the Õ( T ) dependency of the regret bound on the number of rounds of
play (Theorem 4.5). This allows us to derive the first "differentially private
p algorithm for bandit linear optimization achieving Õ( T ) regret, using the algorithm for the
non-private setting from (Abernethy et al., 2012). This answers
p a question from (Smith & Thakurta, 2013) asking if
Õ( T ) regret is attainable for differentially private linear
bandits .
An important case of the general bandit linear optimization
framework is the non-stochastic multi-armed bandits problem(Bubeck et al., 2012b), with applications for website
optimization, personalized medicine, advertisement placement and recommendation systems. Here, we propose
an "-differentially
private algorithm which enjoys a rep
gret of Õ( 1" N T log N ) (Theorem 4.1), improving on the
2
previously best attainable regret of Õ( 1" N T 3 )(Smith &
Thakurta, 2013).
We summarize our results in Table 1.2.
1.3. Related Work
The problem of differentially private online learning was
first considered in (Dwork et al., 2010), albeit guaranteeing privacy in a weaker setting – ensuring the privacy of
the individual entries of the loss vectors. (Dwork et al.,
2010) also introduced the tree-based aggregation scheme
for releasing the cumulative sums of vectors in a differentially private manner, while ensuring that the total amount
of noise added for each cumulative sum is only polylogarithmically dependent on the number of vectors. The
stronger notion of privacy protecting entire loss vectors was
first studied in (Jain et al., 2012), where gradient-based algorithms were proposed that achieve
pri⇣ p (", )-differntial
⌘
vacy and regret bounds of Õ 1" T log 1 . (Smith &
Thakurta, 2013) proposed a modification of Follow-theApproximate-Leader template to achieve Õ 1" log2.5 T
regret for strongly
loss functions, implying a regret
⇣ p convex
⌘
1
bound of Õ " T for general convex functions. In addition, they also demonstrated that under bandit feedback,
⇣ 2 ⌘it
is possible to obtain regret bounds that scale as Õ 1" T 3 .
(Dwork et al., 2014a; Jain & Thakurta, 2014) proved that
in the special case of prediction with expert
p advice setting,
it is possible to achieve a regret of O 1" T log N . While

most algorithms for differentially private online learning
are based on the regularization template, (Dwork et al.,
2014b) used a perturbation-based algorithm to guarantee
(", )-differential privacy for the problem of online PCA.
(Tossou & Dimitrakakis, 2016) showed that it is possible
to design "-differentially private algorithms for the stochastic multi-armed bandit problem with a separation of ", T
for the regret bound. Recently, an independent work due
to (Tossou & Dimitrakakis, 2017), which we were made
aware
⇣ pof ⌘after the first manuscript, also demonstrated a
Õ 1" T regret bound in the non-stochastic multi-armed
bandits setting. We match their results (Theorem 4.1), as
well as provide a generalization to arbitrary convex sets
(Theorem 4.5).
1.4. Overview of Our Techniques
Full Information Setting:
We consider the two
well known paradigms for online learning, Folllow-theRegularized-Leader (FTRL) and Folllow-the-PerturbedLeader (FTPL). In both cases, we ensure differential privacy by restricting the mode of access to the inputs (the
loss vectors). In particular, the algorithm can only retrieve
estimates of the loss vectors released by a tree based aggregation protocol (Algorithm 2) which is a slight modification of the protocol used in (Jain et al., 2012; Smith &
Thakurta, 2013). We outline a tighter analysis of the regret
minimization framework by crucially observing that in case
of linear losses, the expected regret of an algorithm that
injects identically (though not necessarily independently)
distributed noise per step is the same as one that injects a
single copy of the noise at the very start of the algorithm.
The regret analysis of Follow-the-Leader based algorithm
involves two components, a bias term due to the regularization and a stability term which bounds the change in the
output of the algorithm per step. In the analysis due to
(Smith & Thakurta, 2013), the stability term is affected by
the variance of the noise as it changes from step to step.
However in our analysis, since we treat the noise to have
been sampled just once, the stability analysis does not factor in the variance and the magnitude of the noise essentially appears as an additive term in the bias.
Bandit Feedback: In the bandit feedback setting, we show
a general reduction that takes a non-private algorithm and
outputs a private algorithm (Algorithm 4). Our key observation here (presented as Lemma 4.3) is that on linear
functions, in expectation the regret of an algorithm on a
noisy sequence of loss vectors is the same as its regret on
the original loss sequence as long as noise is zero mean.
We now bound the regret on the noisy sequence by conditioning out the case when the noise can be large and using exploration techniques from (Bubeck et al., 2012a) and
(Abernethy et al., 2008).

The Price of Differential Privacy for Online Learning
F UNCTION C LASS (N D IMENSIONS )

P REDICTION WITH E XPERT A DVICE

Õ

⇣p

T log N
"

O NLINE LINEAR OPTIMIZATION OVER
THE S PHERE

Õ

O NLINE LINEAR OPTIMIZATION OVER
THE C UBE

Õ

O NLINE L INEAR O PTIMIZATION

⇣p
⇣p

Õ

⌘

NT
"

⌘

O

⇣p

N log N log2 T
"

T log N +

O

⌘

NT
"

B EST

O UR R EGRET B OUND

P REVIOUS
B EST
K NOWN R EGRET

O

⇣p ⌘
T
"

⇣p

⇣p
O

T+

N log2 T
"

NT +

⇣p

T+

⌘

N log2 T
"
log2 T
"

⌘

⌘

N ON -

PRIVATE
R EGRET

p
O( T log N )
p
O( T )

⌘

p
O( N T )
p
O( T )

Table 1. Summary of our results in the full-information setting. In the last row we suppress the constants depending upon X , Y.
F UNCTION C LASS (N D IMENSIONS )

BANDIT L INEAR O PTIMIZATION
N ON - STOCHASTIC
BANDITS

M ULT- ARMED

P REVIOUS
B EST
K NOWN R EGRET
Õ

Õ

✓

✓

2

T3
"

O UR R EGRET B OUND

◆

2

NT 3
"

Õ

◆

Õ

⇣p ⌘

⇣p

N ON -

PRIVATE
R EGRET

p
O( T )

T
"

T N log N
"

B EST

⌘

p
O( N T )

Table 2. Summary of our results in the bandit setting. In the first row we suppress the specific constants depending upon X , Y.

2. Model and Preliminaries

framework.

This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.
Full-Information Setting: Online linear optimization
(Hazan et al., 2016; Shalev-Shwartz, 2011) involves repeated decision making over T rounds of play. At the beginning of every round (say round t), the algorithm chooses
a point in xt 2 X , where X ✓ RN is a (compact) convex
set. Subsequently, it observes the loss lt 2 Y ✓ RN and
suffers a loss of hlt , xt i. The success of such an algorithm,
across T rounds of play, is measured though regret, which
is defined as
X
T
T
X
Regret = E
hlt , xt i min
hlt , xi
t=1

x2K

t=1

where the expectation is over the randomness of the algorithm. In particular, achieving a sub-linear regret (o(T ))
corresponds to doing almost as good (averaging across T
rounds) as the fixed decision with the least loss in hindsight. In the non-private setting, apnumber of algorithms
have been devised to achieve O( T ) regret, with additional dependencies on other parameters dependent on the
properties of the specific decision set X and loss set Y.
(See (Hazan et al., 2016) for a survey of results.)
Following are three important instantiations of the above

• Prediction with Expert Advice: Here the underlying
decision P
set is the simplex X = N = {x 2 Rn :
n
xi
0, i=1 xi = 1} and the loss vectors are constrained to the unit cube Y = {lt 2 RN : klt k1  1}.
• OLO over the Sphere: Here the underlying decision is
the euclidean ball X = {x 2 Rn : kxk2  1} and the
loss vectors are constrained to the unit euclidean ball
Y = {lt 2 RN : klt k2  1}.
• OLO over the Cube: The decision is the unit cube
X = {x 2 Rn : kxk1  1}, while the loss vectors
are constrained to the set Y = {lt 2 RN : klt k1  1}.
Partial-Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets
to observe the value hlt , xt i, in contrast to the complete
loss vector lt 2 RN as in the full information scenario.
Therefore, the only feedback the algorithm receives is the
value of the loss it incurs for the decision it takes. This
makes designing algorithms for this feedback model challenging. Nevertheless for the general problem of bandit
linear optimization, (Abernethy et al., 2008) introduced a
computationally efficient algorithm that achieves
p an optimal dependence of the incurred regret of O( T ) on the
number of rounds of play. The non-stochastic multi-armed

The Price of Differential Privacy for Online Learning

bandit (Auer et al., 2002) problem is the bandit version of
the prediction with expert advice framework.
Differential Privacy: Differential Privacy (Dwork et al.,
2006) is a rigorous framework for establishing guarantees
on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under
composition and robustness to linkage acts (Dwork et al.,
2014a).
Definition 2.1 ((", )-Differential Privacy). A randomized
online learning algorithm A on the action set X and the
loss set Y is (", )-differentially private if for any two sequence of loss vectors L = (l1 , . . . lT ) ✓ Y T and L0 =
(l10 , . . . lT0 ) ✓ Y T differing in at most one vector – that is to
say 9t0 2 [T ], 8t 2 [T ] {t0 }, lt = lt0 – for all S ✓ X T ,
it holds that
P(A(L) 2 S)  e" P(A(L0 ) 2 S) +
Remark 2.2. The above definition of Differential Privacy
is specific to the online learning scenario in the sense that
it assumes the change of a complete loss vector. This has
been the standard notion considered earlier in (Jain et al.,
2012; Smith & Thakurta, 2013). Note that the definition
entails that the entire sequence of predictions produced by
the algorithm is differentially private.
Notation: We define kYkp = max{klt kp : lt 2
Y}, kX kp = max{kxkp : x 2 X }, and M =
maxl2Y,x2X |hl, xi|, where k · kp is the lp norm. By
Holder’s inequality, it is easy to see that M  kYkp kX kq
for all p, q
1 with p1 + 1q = 1. We define the distriN
bution Lap ( ) to be the distribution over RN such that
each coordinate is drawn independently from the Laplace
distribution with parameter .

3. Full-Information Setting: Privacy for Free
In this section, we describe an algorithmic template (Algorithm 1) for differentially private online linear optimization, based on Follow-the-Regularized-Leader scheme.
Subsequently, we outline the noise injection scheme (Algorithm 2), based on the Tree-based Aggregation Protocol
(Dwork et al., 2010), used as a subroutine by Algorithm 1
to ensure input differential privacy. The following is our
main theorem in this setting.
Theorem 3.1. Algorithm 1 when run with D = LapN ( )
where
= kYk1"log T , regularization R(x), decision set
X and loss vectors l1 , . . . lt , the regret of Algorithm 1 is
bounded by
v
u
T
u
X
Regret  tDR
max(klt k⇤r2 R(x) )2 + DLap
t=1

x2X

where
DLap = E

Z⇠D 0



max hZ, xi
x2X

DR = max R(x)

min hZ, xi

x2X

min R(x)

x2X

x2X

and D0 is the distribution induced by the sum of dlog T e
independent samples from D, k · k⇤r2 R(x) represents the
dual of the norm with respect to the hessian of R. Moreover,
the algorithm is "-differentially private, i.e. the sequence
of predictions produced (xt : t 2 [T ]) is "-differentially
private.
Algorithm 1 FTRL Template for OLO
input Noise distribution D, Regularization R(x)
1: Initialize an empty binaryP
tree B to compute different
tially private estimates of s=1 ls .
dlog T e
2: Sample n10 , . . . n0
independently from D.
Pdlog T e i
3: L̃0
n0 .
i=1
4: for t = 1 to T do
⇣
⌘
5:
Choose xt = argminx2X ⌘hx, L̃t 1 i + R(x) .
6:
Observe lt 2 Y, and suffer a loss of hlt , xt i.
7:
(L̃t , B)
TreeBasedAgg(lt , B, t, D, T ).
8: end for

The above theorem leads to following corollary where we
show the bounds obtained in specific instantiations of online linear optimization.
Corollary 3.2. Substituting the choices of , R(x) listed
below, we specify the regret bounds in each case.
1. Prediction with Expert
Choosing
PN Advice:
N log T
and
R(x)
=
x
log(x
),
i
i=1 i
"
Regret  O

p

N log2 T log N
T log N +
"

2. OLO over the Sphere Choosing
R(x) = kxk22
p

Regret  O
3. OLO over the Cube With
Regret  O

p

=

p

N log2 T
T+
"
=

log T
"

=
!

N log T
"

and

!

and R(x) = kxk22

N log2 T
NT +
"

!

The Price of Differential Privacy for Online Learning

Algorithm 2 TreeBasedAgg(lt , B, t, D, T )
input Loss vector lt , Binary tree B, Round t, Noise distribution D, Time horizon T
1: (L̃0 t , B)
PrivateSum(lt , B, t, D, T ) – Algorithm 5 ((Jain et al., 2012)) with the noise added at
each node – be it internal or leaf – sampled independently from the distribution D.
2: st
the binary representation of t as a string.
3: Find the minimum set
PSt of already populated nodes in
B that can compute s=1 ls .
4: Define Q = |S|  dlog T e. Define rt = dlog T e Q.
5: Sample n1t , . P
. . nrt t independently from D.
rt
0
6: L̃t
L̃ t + i=1
nit .
output (L̃t , B).
3.1. Proof of Theorem 3.1
We first prove the privacy guarantee, and then prove the
claimed bound on the regret. For the analysis, we define
the random variable Zt to be the net amount of noise injected by the TreeBasedAggregation (Algorithm 2) on the
true partial sums. Formally, Zt is the difference between
cumulative sum of loss vectors and its differentially private
estimate used as input to the arg-min oracle.
Zt = L̃t

t
X

li

i=1

Further, let D0 be the distribution induced by summing of
dlog T e independent samples from D.

Privacy : To make formal claims about the quality of
privacy, we ensure input differential privacy for the algorithm – that is, we ensure that the
Ptentire sequence of partial sums of the loss vectors ( s=1 ls : t 2 [T ]) is "differentially private. Since the outputs of Algorithm 1 are
strictly determined by the prefix sum estimates produced
by TreeBasedAgg, by the post-processing theorem, this
certifies that the entire sequence of choices made by the
algorithm (across all T rounds of play) (xt : t 2 [T ]) is "differentially private. We modify the standard Tree-based
Aggregation protocol to make sure that the noise on each
output (partial sum) is distributed identically (though not
necessarily independently) across time. While this modification is not essential for ensuring privacy, it simplifies the
regret analysis.
Lemma 3.3 (Privacy Guarantees with Laplacian Noise).
kYk1 log T
Choose any
. When Algorithm 2 A(D, T )
"
is run with D = LapN ( ), the following claims hold true:
• Privacy: The sequence (L̃t : t 2 [T ]) is "differentially private.
Pdlog T e
• Distribution: 8t 2 [T ], Zt ⇠
ni , where
i=1

each ni is independently sampled from LapN ( ).
Proof. By Theorem 9 ((Jain et al., 2012)), we have that the
sequence (L̃0 t : t 2 [T ]) is "-differentially private. Now the
sequence (L̃t : t 2 [T ]) is "-differentially private because
differential privacy is immune to post-processing(Dwork
et al., 2014a).
Note that the PrivateSum algorithm addsPexactly |S| int
dependent draws from the distribution D to s=1 ls , where
S is the minimum set of already populated nodes in the tree
that can compute the required prefix sum. Due to Line 6 in
Algorithm 2, it is made certain that every prefix sum released is a sum of the true prefix sum and dlog T e independent draws from D.
Regret Analysis: In this section, we show that for linear loss functions any instantiation of the Follow-theRegularized-Leader algorithm can be made differentially
private with an additive loss in regret.
Theorem 3.4. For any noise distribution D, regularization
R(x), decision set X and loss vectors l1 , . . . lt , the regret
of Algorithm 1 is bounded by
v
u
T
u
X
Regret  tDR
max(klt k⇤r2 R(x) )2 + DD0
t=1

x2X

where DD0 = EZ⇠D0 [maxx2X hZ, xi minx2X hZ, xi],
DR = maxx2X R(x) minx2X R(x), and k · k⇤r2 R(x)
represents the dual of the norm with respect to the hessian
of R.

Proof. To analyze the regret suffered by Algorithm 1, we
consider an alternative algorithm that performs a one-shot
noise injection – this alternate algorithm may not be differentially private. The observation here is that the alternate algorithm and Algorithm 1 suffer the same loss in expectation and therefore the same expected regret which we
bound in the analysis below.
Consider the following alternate algorithm which instead
of sampling noise Zt at each step instead samples noise at
the beginning of the algorithm and plays with respect to
that. Formally consider the sequence of iterates x̂t defined
as follows. Let Z ⇠ D.
X
x̂1 , x1 , x̂t , argminx2X ⌘hx, Z +
li i + R(x)
i

We have that

EZ1 ...ZT ⇠D

"

T
X
t=1

#

hlt , xt i = EZ⇠D

"

T
X
t=1

hlt , x̂t i

#

(1)

To see the above equation note that EZt ⇠D [hlt , x̂t i] =
EZ⇠D [hlt , xt i] since x, x̂t have the same distribution.

The Price of Differential Privacy for Online Learning

Therefore it is sufficient to bound the regret of the sequence
x̂1 . . . x̂t . The key idea now is to notice that the addition
of one shot noise does not affect the stability term of the
FTRL analysis and therefore the effect of the noise need
not be paid at every time step. Our proof will follow the
standard template of using the FTL-BTL (Kalai & Vempala, 2005) lemma and then bounding the stability term in
the standard way. Formally define the augmented series of
loss functions by defining
l0 (x) =

1
R(x) + hZ, xi
⌘

(2)

Algorithm 3 FTPL Template for OLO – A(D, T ) on the
action set X , the loss set Y.
1: Initialize an empty binaryP
tree B to compute different
tially private estimates of s=1 ls .
dlog
T
e
2: Sample n10 , . . . n0
independently from D.
Pdlog T e i
3: L̃0
n
.
0
i=1
4: for t = 1 to T do
5:
Choose xt = argminx2X hx, L̃t 1 i.
6:
Observe the loss vector lt 2 Y, and suffer hlt , xt i.
7:
(L̃t , B)
TreeBasedAgg(lt , B, t, D, T ).
8: end for

(3)

Theorem 3.5 (FTPL: Online Linear Optimization). Let
kX k2 = supx2X kxk2 and kYk2 = suplt 2Y klt k2 . Choosq
p
ing = max{kYk2 pN Tlog T , "N log T log log T } and

where Z ⇠ D is a sample. Now invoking the Follow the
Leader, Be the Leader Lemma (Lemma 5.3, (Hazan et al.,
2016)) we get that for any fixed u 2 X
T
X
t=0

lt (u)

T
X

lt (x̂t+1 )

t=0

Therefore we can conclude that
T
X

[lt (x̂t )

lt (u)]

[lt (x̂t )

lt (x̂t+1 )] + l0 (u)

[lt (x̂t )

1
lt (x̂t+1 )] + DR + DZ
⌘

(Abernethy et al., 2014), for the full-information setting,
wepestablish that the regret guarantees obtained scale as
O( T )+Õ( 1" log 1 ). While Theorem 3.5 is valid for all inp
stances of online linear optimization and achieves O( T )
regret, it yields sub-optimal dependence on the dimension
of the problem. The advantage of FTPL-based approaches
over FTRL is that FTPL performs linear optimization over
the decision set every round, which is possibly computationally less expensive than solving a convex program every round, as FTRL requires.

t=1




T
X

l0 (x̂1 )

t=1

T
X
t=1

where DZ , maxx2X (hZ, xi) minx2X (hZ, xi) Therefore we now need to bound the stability term lt (x̂t )
lt (x̂t+1 ). Now, the regret bound follows from the standard
analysis for the stability term in the FTRL scheme (see for
instance (Hazan et al., 2016)). Notice that the bound only
depends
P on the change in the cumulative loss per step i.e.
⌘ ( t lt + Z), for which the change is the loss vector ⌘lt+1
across time steps. Therefore we get that
lt (x̂t )

lt (x̂t+1 )  max k⌘lt k2⌘r
x2X

2 R(x)

(4)

Combining Equations (1), (3), (4) we get the regret bound
in Theorem 3.4.

3.2. Regret Bounds for FTPL
In this section, we outline algorithms based on the Followthe-Perturbed-Leader template(Kalai & Vempala, 2005).
FTPL-based algorithms ensure low-regret by perturbing the
cumulative sum of loss vectors with noise from a suitably chosen distribution. We show that the noise added in
the process of FTPL is sufficient to ensure differential privacy. More concretely, using the regret guarantees due to

D = N (0,
1
4

2

IN ), we have that RegretA(D,T ) (T ) is

O N kX k2 kYk2

p

N kX k2
log T
T+
log1.5 T log
"

!

Moreover the algorithm is "-differentially private.
The proof of the theorem is deferred to the appendix.

4. Differentially Private Multi-Armed Bandits
In this section, we state our main results regarding bandit
linear optimization, the algorithms that achieve it and prove
the associated regret bounds. The following is our main
theorem concerning non-stochastic multi-armed bandits.
Theorem 4.1 (Differentially Private Multi-Armed Bandits). Fix loss vectors (l1 . . . lT ) such that klt k1  1.
When Algorithm 4 is run with parameters D = LapN ( )
where
= 1" and algorithm A = Algorithm 5 with
q
log N
the following parameters: ⌘ =
2N T (1+2 2 log N T ) ,
p
= ⌘N 1 + 2 2 log N T and the exploration distribution µ(i) = N1 . The regret of the Algorithm 4 is
✓p
◆
N T log T log N
O
"
Moreover, Algorithm 4 is "-differentially private

The Price of Differential Privacy for Online Learning

Bandit Feedback: Reduction to the Non-private Setting
We begin by describing an algorithmic reduction that takes
as input a non-private bandit algorithm and translates it into
an "-differentially private bandit algorithm. The reduction
works in a straight-forward manner by adding the requisite
magnitude of Laplace noise to ensure differential privacy.
For the rest of this section, for ease of exposition we will
assume that both T and N are sufficiently large.
Algorithm 4 A0 (A, D) – Reduction to the Non-private Setting for Bandit Feedback
Input: Online Algorithm A, Noise Distribution D.
1: for t = 0 to T do
2:
Receive x˜t 2 X from A and output x̃t .
3:
Receive a loss value hlt , x˜t i from the adversary.
4:
Sample Zt ⇠ D.
5:
Forward hlt x˜t i + hZt , x˜t i as input to A.
6: end for
Algorithm 5 EXP2 with exploration µ
Input: learning rate ⌘; mixing coefficient ; distribution µ
1
1: q1 = N
. . . N1 2 RN .
2: for t = 1,2 . . . T do
3:
Let pt = (1
)qt + µ and play it ⇠ pt
4:
Estimate loss vector lt by l˜t = Pt+ eit eTit lt , with
⇥
⇤
Pt = Ei⇠pt ei eTi
5:
Update the exponential weights,

6: end for

qt+1 (i) = P

e
i0

⌘hei ,l̃t i

e

qt (i)

⌘hei0 ,l̃t i q

t (i

0)

The following Lemma characterizes the conditions under
which Algorithm 4 is " differentially private
Lemma 4.2 (Privacy Guarantees). Assume that each
loss vector lt is in the set Y ✓ RN , such that
hl,x̃t i
maxt,l2Y | kx̃
|  B. For D = LapN ( ) where = B" ,
t k1
the sequence of outputs (x˜t : t 2 [T ]) produced by the
Algorithm A0 (A, D) is "-differentially private.
The following lemma charaterizes the regret of Algorithm
4. In particular we show that the regret of Algorithm 4
is, in expectation, same as that of the regret of the input
algorithm A on a perturbed version of loss vectors.

Lemma 4.3 (Noisy Online Optimization). Consider a loss
sequence (l1 . . . lT ) and a convex set X . Define a perturbed
version of the sequence as random vectors (˜lt : t 2 [T ])
as ˜lt = lt + Zt where Zt is a random vector such that
{Z1 , . . . Zt } are independent and E[Zt ] = 0 for all t 2 [T ].
Let A be a full information (or bandit) online algorithm
which outputs a sequence (x̃t 2 X : t 2 [T ]) and takes as

input ˜lt (respectively h˜lt , x̃t i) at time t. Let x⇤ 2 K be a
fixed point in the convex set. Then we have that
"
" T
##
X
E{Zt } EA
(hlt , x̃t i hlt , x⇤ i)
t=1

"

= E{Zt } EA

"

T ⇣
X
t=1

h˜lt .x̃t i

h˜lt , x i
⇤

⌘

##

We provide the proof of Lemma 4.2 and defer the proof of
Lemma 4.3 to the Appendix Section B.
Proof of Lemma 4.2. Consider a pair of sequence of loss
vectors that differ at exactly one time step – say L =
(l1 , . . . lt0 . . . , lT ) and L0 = (l1 , . . . , lt0 0 , . . . lT ). Since
the prediction of produced by the algorithm at time step
any time t can only depend on the loss vectors in the past
(l1 , . . . lt 1 ), it is clear that the distribution of the output
of the algorithm for the first t0 rounds (x̃1 , . . . x̃t0 ) is unaltered. We claim that 8I ✓ R, it holds that
P(hlt0 + Zt0 , x̃t0 i 2 I)  e" P(hlt0 0 + Zt0 , x̃t0 i 2 I)
Before we justify the claim, let us see how this implies that
desired statement. To see this, note that conditioned on the
value fed to the inner algorithm A at time t0 , the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at
other time steps (discounting t0 ) stays the same (in distribution). By the above discussion, it is sufficient to demonstrate "-differential privacy for each input fed (as feedback)
to the algorithm A.
For the sake of analysis, define ltF ict as follows. If x̃t = 0,
define ltF ict = 0 2 RN . Else, define ltF ict 2 RN to be such
that (ltF ict )i = hltx̃,xi˜t i if and only if i = argmaxi2[d] |x̃i |
and 0 otherwise, where argmax breaks ties arbitrarily. Define ˜ltF ict = ltF ict + Zt . Now note that h˜ltF ict , x̃t i =
hlt x˜t i + hZt , x˜t i.

It suffices to establish that each ˜ltF ict is "-differentially
private. To argue for this, note that Laplace mechanism
(Dwork et al., 2014a) ensures the same, since the l1 norm
of ˜ltF ict is bounded by B.
4.1. Proof of Theorem 4.1
hl,x̃t i
Privacy: Note that since maxt,l2Y | kx̃
|  kYk1  1
t k1
as x̃t 2 {ei : i 2 [N ]}. Therefore by Lemma 4.2, setting
= 1" is sufficient to ensure "-differential privacy.

Regret Analysis: For the purpose of analysis we define the
following pseudo loss vectors.
˜lt = lt + Zt

The Price of Differential Privacy for Online Learning

where by definition Zt ⇠ LapN ( ). The following follows
from Fact C.1 proved in the appendix.
P(kZt k21

10

2

log2 N T ) 

E[Regret|F̄ ]
"
"

1
T2

= E{Zt } EA

Taking a union bound, we have
P(9t

kZt k21

10

2

1
log N T ) 
T
2

(5)

To bound the norm of the loss we define the event F ,
{9t : kZt k21
10 2 log2 N T }. We have from (5) that
1
P(F )  T . We now have that
E[Regret]  E[Regret|F̄ ] + P(F )E[Regret|F ]
Since the regret is always bounded by T we get that the
second term above is at most 1. Therefore we will concern
ourselves with bounding the first term above. Note that
Zt remains independent and symmetric even when conditioned on the event F̄ . Moreover the following statements
also hold.
8t E[Zt |F̄ ] = 0
(6)
8t E[kZt k21 |F̄ ]  10

2

log2 N T

(7)

Equation (6) follows by noting that Zt remains symmetric around the origin even after conditioning. It can now
be seen that Lemma 4.3 still applies even when the noise
is sampled from LapN ( ) conditioned under the event F̄
(due to Equation 6). Therefore we have that
"
" T
# #
⌘
X⇣
⇤
E[Regret|F̄ ] = E{Z } EA
h˜lt , x̃t i h˜lt , x i
F̄
t

t=1

(8)

To bound the above quantity we make use of the following
lemma which is a specialization of Theorem 1 in (Bubeck
et al., 2012a) to the case of multi-armed bandits.
Lemma 4.4 (Regret Guarantee for Algorithm 5). If ⌘ is
such that ⌘|hei , ˜lt i|  1, we have that the regret of Algorithm 5 is bounded by
XX
log N
Regret  2 T +
+ ⌘E
pt (i)hei , ˜lt i2
⌘
t
i
Now note that due to the conditioning kZt k21
10 2 log2 N T and therefore we have that
maxt,x2

N



|hZt , xi|  4 log N T.

It can be seen that the condition ⌘|hei , ˜lt i|  1 in Theorem
4.4 is satisfied for exploration µ(i) = N1 and under the
condition F̄ as long as
⌘N (1 + 4 log N T ) 

which holds by the choice of these parameters. Finally

 E{Zt }
 E{Zt }

"
"

T ⇣
X
t=1

h˜lt , x̃t i

h˜lt , x i
⇤

⌘

#

F̄

T
X
log N
+⌘
N k˜lt k21 + 2T
⌘
t=1

#
F̄

#

T
X
log N
+ 2⌘
N (klt k21 + kZt k21 ) + 2T
⌘
t=1

F̄

log N
+ 2⌘T N (1 + 2 log2 N T ) + 2T
⌘
✓q
◆
O
T N log N (1 + 2 log2 N T )
✓p
◆
N T log T log N
O
"


4.2. Differentially Private Bandit Linear Optimization
In this section we prove a general result about bandit linear
optimization over general convex sets, the proof of which
is deferred to the appendix.
Theorem 4.5 (Bandit Linear Optimization). Let X ✓ RN
be a convex set. Fix loss vectors (l1 , . . . lT ) such that
maxt,x2X |hlt , xi|  M . We have that Algorithm 4 when
1
run with parameters D = LapN ( ) (with
= kYk
" )
and algorithm A = SCRiBLe(Abernethy
et al., 2012)
q
⌫ log T
with step parameter ⌘ =
we have
2N 2 T (M 2 + 2 N kX k22 )
the following guarantees that the regret of the algorithm is
bounded by
O

p

T log T

s

✓

N kX k22 kYk21
N 2⌫ M 2 +
"2

◆!

where ⌫ is the self-concordance parameter of the convex
body X . Moreover the algorithm is "-differentially private.

5. Conclusion
In this work, we demonstrate that ensuring differential privacy leads to only a constant additive increase in the incurred regret for online linear optimization in the full feedback setting. We also show nearly optimal bounds (in terms
of T) in the bandit feedback setting. Multiple avenues for
future research arise, including extending our bandit results to other challenging partial-information models such
as semi-bandit, combinatorial bandit and contextual bandits. Another important unresolved question is whether it
is possible to achieve an additive separation in ", T in the
adversarial bandit setting.

#

The Price of Differential Privacy for Online Learning

References
Abernethy, Jacob, Hazan, Elad, and Rakhlin, Alexander.
Competing in the dark: An efficient algorithm for bandit
linear optimization. In COLT, pp. 263–274, 2008.
Abernethy, Jacob, Lee, Chansoo, Sinha, Abhinav, and
Tewari, Ambuj. Online linear optimization via smoothing. In COLT, pp. 807–823, 2014.
Abernethy, Jacob D, Hazan, Elad, and Rakhlin, Alexander. Interior-point methods for full-information and bandit online learning. IEEE Transactions on Information
Theory, 58(7):4164–4175, 2012.
Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77,
2002.
Bubeck, Sébastien, Cesa-Bianchi, Nicolo, Kakade,
Sham M, Mannor, Shie, Srebro, Nathan, and
Williamson, Robert C. Towards minimax policies
for online linear optimization with bandit feedback. In
COLT, volume 23, 2012a.
Bubeck, Sébastien, Cesa-Bianchi, Nicolo, et al. Regret
analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends R in Machine
Learning, 5(1):1–122, 2012b.
Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography Conference,
pp. 265–284. Springer, 2006.
Dwork, Cynthia, Naor, Moni, Pitassi, Toniann, and Rothblum, Guy N. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pp. 715–724. ACM, 2010.
Dwork, Cynthia, Roth, Aaron, et al. The algorithmic
foundations of differential privacy. Foundations and
Trends R in Theoretical Computer Science, 9(3–4):211–
407, 2014a.
Dwork, Cynthia, Talwar, Kunal, Thakurta, Abhradeep, and
Zhang, Li. Analyze gauss: optimal bounds for privacypreserving principal component analysis. In Proceedings
of the 46th Annual ACM Symposium on Theory of Computing, pp. 11–20. ACM, 2014b.
Hazan, Elad et al. Introduction to online convex optimization. Foundations and Trends R in Optimization, 2(3-4):
157–325, 2016.
Jain, Prateek and Thakurta, Abhradeep G. (near) dimension
independent risk bounds for differentially private learning. In Proceedings of the 31st International Conference
on Machine Learning (ICML-14), pp. 476–484, 2014.

Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep.
Differentially private online learning. In COLT, volume 23, pp. 24–1, 2012.
Kalai, Adam and Vempala, Santosh. Efficient algorithms
for online decision problems. Journal of Computer and
System Sciences, 71(3):291–307, 2005.
Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2011.
Smith, Adam and Thakurta, Abhradeep Guha. (nearly)
optimal algorithms for private online learning in fullinformation and bandit settings. In Advances in Neural
Information Processing Systems, pp. 2733–2741, 2013.
Tossou, Aristide and Dimitrakakis, Christos. Algorithms
for differentially private multi-armed bandits. In AAAI
2016, 2016.
Tossou, Aristide C. Y. and Dimitrakakis, Christos. Achieving privacy in the adversarial multi-armed bandit. In
14th International Conference on Artificial Intelligence
(AAAI 2017), 2017.

