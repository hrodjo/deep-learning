Relative Fisher Information and Natural Gradient for Learning Large
Modular Models
Ke Sun 1 Frank Nielsen 2 3

Abstract
Fisher information and natural gradient provided
deep insights and powerful tools to artificial neural networks. However related analysis becomes
more and more difficult as the learner’s structure
turns large and complex. This paper makes a preliminary step towards a new direction. We extract
a local component from a large neural system,
and define its relative Fisher information metric
that describes accurately this small component,
and is invariant to the other parts of the system.
This concept is important because the geometry
structure is much simplified and it can be easily
applied to guide the learning of neural networks.
We provide an analysis on a list of commonly
used components, and demonstrate how to use
this concept to further improve optimization.

1. Fisher Information Metric
The Fisher Information Metric (FIM) I(Θ) = (Iij ) of a
statistical parametric model p(x | Θ) of order D is defined
by a D × D positive semidefinite
(psd)
i matrix (I(Θ)  0)
h
∂l ∂l
with coefficients Iij = Ep ∂Θi ∂Θj , where l(Θ) denotes
the log-density function log p(x | Θ). Under light regularity conditions, FIM can be rewritten equivalently as

Iij = −Ep

2

∂ l
∂Θi ∂Θj



Z
=4

∂

p

p(x | Θ) ∂
∂Θi

p

p(x | Θ)
dx.
∂Θj

As its empirical counterpart, the observed FIM (Efron &
Hinkley, 1978) with respect to (wrt) a sample set Xn =
{xk }nk=1 is Î(Θ | Xn ) = −∇2 l(Θ | Xn ), which is often evaluated at the maximum likelihood estimate Θ =
Θ̂(Xn ). By the law of large numbers, Î(Θ) converges to
the (expected) FIM I(Θ) as n → ∞.
1

King Abdullah University of Science and Technology (KAUST), Saudi Arabia 2 École Polytechnique, France
3
Sony Computer Science Laboratories Inc., Japan.
Correspondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen
<Frank.Nielsen@acm.org>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The FIM is not invariant and depends on the parameterization. We can optionally write I(Θ) as IΘ (Θ) to emphasize the coordinate system. By definition, IΘ (Θ) =
∂Λi
is the JacoJ | IΛ (Λ)J where J = (Jij ), Jij = ∂Θ
j
bian matrix. For example, the FIM of regular natural exponential families (NEFs) l(Θ) = Θ| t(x) − F (Θ) (loglinear models with sufficient statistics t(x)) is I(Θ) =
∇2 F (Θ)  0, the Hessian of the log-normalizer function
F (Θ). Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the lognormalizer function may not be available in closed-form
nor computationally tractable (Montanari, 2015).
The FIM is an important concept for statistical machine
learning. It gives a Riemannian metric (Hotelling, 1929;
Rao, 1945) of the learning parameter space which is
unique (Čencov, 1982; Dowty, 2017). Hence any learning is in a space that is intrinsically curved based on the
FIM, regardless of the choice of the coordinate system. It
also gives a bound (Fréchet, 1943; Cramér, 1946; Nielsen,
2013) of learning efficiency saying that the variance of any
unbiased learning of Θ is at least I −1 (Θ)/n, where n is
the i.i.d. sample size. The FIM is applied to neural network
optimization (Amari, 1997), metric learning (Lebanon,
2005), reinforcement learning (Thomas, 2014) and manifold learning (Sun & Marchand-Maillet, 2014).
However computing the FIM is expensive. Besides the fact
that learning machines have often singularities (Watanabe,
2009) (|I(Θ)| = 0, not full rank) characterized by plateaux
in gradient learning, computing/estimating the FIM of a
large neuron system (e.g. one with millions of parameters,
Szegedy, Christian et al. 2015) is very challenging due to
the finiteness of data, and the huge number D(D+1)
of ma2
trix coefficients to evaluate. Furthermore, gradient descent
techniques require inverting this large matrix and tuning the
learning rate.
To tackle this problem, past works mainly focus on how
to approximate the FIM with a block diagonal form (Kurita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu &
Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal
form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016).
This global approach faces increasing approximation error
and increasing computational cost as the system scales up

Relative Fisher Information and Natural Gradient

and as complex and dynamic structures (Looks et al., 2017)
emerge.
This work aims at a different local approach. The idea is
to accurately describe the information geometry (IG) in a
subsystem of the large learning system, which is invariant
to the scaling up and structural change of the global system,
so that the local machinery, including optimization, can be
discussed regardless of the other parts.
For this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined. Unlike the traditional
geometric view of a high-dimensional parameter manifold,
RFIMs defines multiple projected low-dimensional geometries of subsystems. This geometry is correlated to the parameters beyond the subsystem and is therefore considered
dynamic. It can be used to characterize the efficiency of
a local learning process. Taking this stance has potential
in deep learning because a deep neural network can be decomposed into many local components such as neurons or
layers. The RFIM is well suited to the compositional block
structures of neural networks. The RFIM can be used for
out-of-core learning.
The paper is organized as follows. Sec. 2 reviews natural gradient within the context of Multi-Layer Perceptrons
(MLPs). Sec. 3 formally defines the RFIM, and gives a table of RFIMs of several commonly used subsystems. Sec. 4
discusses the advantages of using the RFIM as compared
to the FIM. Sec. 5 gives an algorithmic framework and
proof-of-concept experiments on neural network optimization. Sec. 6 presents related works on parameter diagonalization. Sec. 7 concludes this work and further hints at
perspectives.

2. Natural Gradient: Review and Insights
θ1

θL

Consider a MLP x −→ h1 · · · hL−1 −−→ y, whose statistical model is the following conditional distribution
X
p(y | x, Θ) =
p(h1 | x, θ1 ) · · · p(y | hL−1 , θL ).
h1 ,··· ,hL−1

The often intractable sum over h1 , · · · , hL−1 can
be get rid off by deteriorating p(h1 | x, θ1 ), · · · ,
p(hL−1 | hL−2 , θL−1 ) to Dirac’s deltas δ, and letting
merely the last layer p(y | hL−1 , θL ) be stochastic. Other
models such as restricted Boltzmann machines (Nair &
Hinton, 2010; Montavon & Müller, 2012), deep belief networks (Hinton et al., 2006), dropout (Wager et al., 2013),
and variational autoencoders (Kingma & Welling, 2014) do
consider the hi ’s to be stochastic.
The tensor metric of the neuromanifold (Amari, 1995)
M, consisting of all MLPs with the same architecture but different parameter values, is locally defined
by the FIM. Because a MLP corresponds to a con-

ditional distribution, its FIM is a function of the input x. By taking an empirical average over the input samples {xk }nk=1 , the P
FIM of a MLP can
ex ∂lk be

n
∂lk
,
pressed as IΘ (Θ) = n1 k=1 Ep(y | xk ,Θ) ∂Θ
|
∂Θ
where lk (Θ) = log p(y | xk , Θ) denotes the conditional
log-likelihood function wrt xk .
To understand the meaning of the Riemannian metric
IΘ (Θ), it measures the intrinsic difference between two
nearby neural networks around Θ ∈ M. A learning step
can be regarded as a tiny displacement δΘ on M. According to the FIM, the infinitesimal square distance
"
2 #
n
1X
| ∂lk
Ep(y | xk , Θ)
δΘ
hδΘ, δΘiIΘ (Θ) =
n
∂Θ
k=1
(1)
measures how much δΘ (with a radius constraint) is sta∂l
, or equivalently how much δΘ affects
tistically along ∂Θ
intrinsically the conditional distribution p(y | x, Θ).
Consider
the
log-likelihood
function
Pn negative
L(Θ) = − k=1 log p(yk | xk , Θ) wrt the observed
pairs {(xk , yk )}nk=1 , we try to minimize the loss while
maintaining a small learning step size hδΘ, δΘiIΘ (Θ) on
M. At Θt ∈ M, the target is to minimize wrt δΘ the
Lagrange function
1
hδΘ, δΘiIΘ (Θt )
2γ
1
≈ L(Θt ) + δΘ| 5Θ L(Θt ) +
δΘ| IΘ (Θt )δΘ,
2γ

L(Θt + δΘ) +

where γ > 0 is a learning rate. The optimal solution of the
above quadratic optimization gives a learning step
−1
δΘt = −γIΘ
(Θt ) 5Θ L(Θt ).

˜ Θ L(Θ) = I −1 (Θ) 5Θ L(Θ)
In this update procedure, ∇
Θ
replaces the role of the usual gradient ∇Θ L(Θ) and is
called the natural gradient (Amari, 1997).
Although the FIM depends on the chosen parameterization,
the natural gradient is invariant to reparameterization. Let
Λ be another coordinate system and J be the Jacobian matrix of the mapping Θ → Λ. Then we have
−1
IΘ
(Θ) 5Θ L(Θ) = (J | IΛ (Λ)J )

−1

J | 5Λ L(Λ)

−1
= J −1 IΛ
(Λ) 5Λ L(Λ),

˜ Θ L(Θ) and ∇
˜ Λ L(Λ) are the same dynamic
showing that ∇
up to coordinate transformation. As the learning rate γ is
not infinitesimal in practice, natural gradient descent actually depends on the coordinate system (see e.g. Martens
2014). Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux
of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 Amari 1998).

Relative Fisher Information and Natural Gradient

p(y | Θ, x)

Model:

=

P

h1

P

p(h1 | θ1 , x)

h2

p(y | θ3 , h2 )

h1 + ∆h1

x + ∆x

Mθ3

x

MΘ

Manifold:

p(h2 | θ2 , h1 )

Mθ1

Mθ2

h1

h2 + ∆h2
h2

Computational graph:

x

Θ

y

x

θ1

Θ

I(Θ)

θ2

θ1

Θ

Metric:

h1

θ2

θ1 g h1 (θ1 )

h2

y

θ3

h1

θ3

θ2 g h2 (θ2 )

θ3 g y (θ3 )

h1

h2

h2

Figure 1. (left) The traditional global geometry of a MLP; (right) information geometry of subsystems. The gray and blue meshes show
that the subsystem geometry is dynamic when the reference variable makes a tiny move. The square under the (sub-)system means the
(R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing
a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.

For the sake of simplicity, we do not discuss singular FIMs
with a subset of parameters having zero metric. This set
of parameters forms an analytic variety (Watanabe, 2009),
and technically the MLP as a statistical model is said to
be non-regular (and the parameter Θ is not identifiable).
The natural gradient has been extended (Thomas, 2014)
to cope with singular FIMs having positive semi-definite
matrices by taking the Moore-Penrose pseudo-inverse (that
coincides with the inverse matrix for full rank matrices).

subsystem parameters, resembling the long-term memory
adapting slowly to the observations (e.g. neural network
weights). The response h is a random variable that reacts
to the variations of θ. Usually, h is the output of the subsystem that is connected to neighbour subsystems (e.g. hidden
layer outputs). Formally, a subsystem which factorizes the
learning machine is characterized by the conditional distribution p(h | θ, θf ), where θ can be estimated based on h
and θf . We make the following definition.

In the family of 2nd -order optimization methods, a fuzzy
line can be drawn from the natural gradient and alternative
methods such as the Hessian-free optimization (Martens,
2010). By definition, the FIM is a property of the parameter space which is independent or weakly dependent on the
input samples. For example, the FIM of a MLP is independent of {yi }. In contrast, the Hessian (or related concepts
such as the Gauss-Newton matrix, Martens 2014) is a property of the learning cost function wrt the input samples.

Definition 1 (RFIM). Given θf , the RFIM 1 of θ wrt h is

Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a gradient descent step, thus
ensuring to stay on the manifold for any chosen learning
rate. Convergence is proven for Hadamard manifolds (of
negative curvatures). However, it is not mathematically
tractable to express the exponential map of hierarchical
model manifolds like the neuromanifold.

3. RFIM: Definition and Expressions
In general, for large parametric systems, it is impossible
to diagonalize or decorrelate all the parameters, so that we
split instead all random variables into three parts θf , θ and
h. We examine their intuitive meanings before giving the
formal definition. The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis). This is in analogy to the notion of a reference frame in physics. θ is the

g h (θ | θf )
def

= Ep(h | θ, θf )




∂
∂
log p(h | θ, θf ) | log p(h | θ, θf ) ,
∂θ
∂θ

or simply g h (θ), corresponding to the estimation of θ
based on observations of h given θf .
For example, consider a MLP. If we choose θf to be the
input features x, choose h to be the final output y, and
choose θ to be all the network weights Θ, then the RFIM
becomes the FIM: I(Θ) = g y (Θ | x).
More generally, we can choose the response h to be other
than the observables to compute the Fisher information
of subsystems, especially dynamically during the learning of the global machine. To see the meaning of the
RFIM, similar to eq. (1), the hinfinitesimal square distancei
2
∂
hδθ, δθigh (θ) = Ep(h | θ, θf ) δθ | ∂θ
log p(h | θ, θf )
measures how much δθ impacts intrinsically the stochastic
mapping θ → h which features the subsystem. We have
the following proposition following definition 1.
Proposition 2 (Relative Geometry Consistency). If θ1 consists of a subset of θ2 so that θ2 = (θ1 , θ̃1 ), then ∀θ̃1 ,
Mθ1 with the metric g h (θ1 | θ̃1 ) has exactly the same Rie1
We use the same term “relative FIM” (Zegers, 2015) with a
different definition.

Relative Fisher Information and Natural Gradient

mannian metric with the sub-manifold {θ2 ∈ Mθ2 :
θ̃1 is fixed} induced by the ambient metric g h (θ2 ).
When the response h is chosen, then different splits of
(θ, θf ) are consistent with the same ambient geometry.
Figure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s
parameter sensitivity to the external environment (x and
y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h. The two-colored
meshes show that the geometry structure is dynamic and
varies with the reference variable θf .
One should not confuse the RFIM with the diagonal blocks
of the FIM (Kurita, 1994). Both their meanings and expressions are different. The RFIM is computed by integrating out the hidden response variables h. The FIM is always computed by integrating out the observables x and
y. Hence the RFIM is a more general concept and includes the FIM as a special case. This highlights a main
difference with the backpropagated metric (Ollivier, 2013),
which essentially considers parameter sensitivity wrt the final output. Despite the fact that the FIMs of small parametric structures such as single neurons was studied (Amari,
1997), we are not looking at a small single-component system but a component embedded in a large system, targeting
at improving the large system.
In the following we provide a short table of commonly used
RFIMs for future reference (the RFIMs listed are mostly
straightforward from definition 1, with detailed derivations
given in the supplementary material). This is meaningful
since the RFIM is a new concept. We also want to demonstrate these simple closed form expressions without any approximations.
3.1. RFIMs of One Neuron
We start from the RFIM of single neuron models. Consider
a stochastic neuron with input x and weights w. After a
nonlinear activation function f , the output y is randomized
surrounding the mean f (w| x̃) with a variance. Throughout this paper x̃ = (x| , 1)| denotes the augmented vector
of x (homogeneous coordinates) so that w| x̃ contains a
bias term, and a general linear transformation can be written simply as Ax̃.
Using x as the reference, the RFIM of w with respect
to y has a common form g y (w | x) = νf (w, x)x̃x̃| ,
where νf (w, x) is a positive coefficient with large values
in the linear region, or the effective learning zone of the
neuron. This agrees with early studies on single neuron
FIMs (Amari, 1997; Kurita, 1994).
If f (t)

=

tanh(t) is the hyperbolic tangent func-

tion, then νf (w, x) = sech2 (w| x̃), where sech(t) =
2
exp(t)+exp(−t) is the hyperbolic secant function. Similarly, if f (t) = sigm(t) is the sigmoid function,
then

νf (w, x) = sigm (w| x̃) 1 − sigm (w| x̃) .
If f is defined by Parametric Rectified Linear Unit
(PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) as a special case,
so that f (t) = t (t ≥ 0), f (t) = ιt (t < 0), 0 ≤ ι < 1, then
under certain approximations (see supplementary material)
2


1−ι |
w x̃
,
νf (w, x) = ι + (1 − ι)sigm
ω
where ω > 0 is a hyper-parameter (e.g. ω = 1).
For the exponential linear unit (ELU) (Clevert et al., 2015),
f (t) = t (t ≥ 0), f (t) = α (exp(t) − 1) (t < 0), where
α > 0 is a hyper-parameter. We get

1
if w| x̃ ≥ 0
νf (w, x) =
2
|
α exp (2w x̃) if w| x̃ < 0.
3.2. RFIM of One Layer
Let D denote the dimensionality of the corresponding variable. A linear layer with input x, connection weights
W = w1 , · · · , wDy , and stochastic output y can be
represented by y ∼ G(W | x̃, σ 2 I), where I is the identity matrix, and σ is the scale of the observation noise,
and G(µ, Σ) is a multivariate Gaussian distribution with
mean µ and covariance matrix Σ. We vectorize W by
stacking its columns {wi }. Then g y (W | x) is a tensor
of size (Dx + 1)Dy × (Dx + 1)Dy , given by g y (W | x) =
diag [x̃x̃| , · · · , x̃x̃| ], where diag(·) means the (block)
diagonal matrix constructed by the given matrix entries.
A nonlinear layer increments a linear layer by adding an
element-wise activation function applied on W | x̃, and
then randomized wrt the choice of the neuron. By definition 1, its RFIM is given by
g y (W | x)
= diag [ νf (w1 , x)x̃x̃| , · · · , νf (wm , x)x̃x̃| ] ,

(2)

where νf (wi , x) is given in Subsec. 3.1.
A softmax layer, which often appears as the last layer of
a MLP, is given by y ∈ {1, . . . , m}, where p(y) = ηy =
exp(wy x̃)
Pm
. Its RFIM is a dense matrix given by
i=1 exp(wi x̃)


(η1 − η12 )x̃x̃| · · ·
−η1 ηm x̃x̃|
 −η2 η1 x̃x̃|
···
−η2 ηm x̃x̃| 


g y (W ) = 
.
..
..
..


.
.
.
−ηm η1 x̃x̃|

···

2
(ηm − ηm
)x̃x̃|

Notice that its i’th diagonal block (ηi − ηi2 )x̃x̃| resembles
the RFIM of a single sigm neuron.

Relative Fisher Information and Natural Gradient

3.3. RFIM of Two Layers
By eq. (2), the one-layer RFIM is a product metric (Jost,
2011) and does not consider the inter-neuron correlations,
which must be obtained by looking at a larger subsystem. Consider a two-layer model with stochastic output
y around the mean vector f (C | h̃), where h = f (W | x̃).
For simplicity, we ignore inter-layer correlations between
the first layer and the second layer and focus on the interneuron correlations within the first layer. To do this, both x
and C are considered as references to compute the RFIM
of W . By definition 1, g y (W | x, C) = [Gij ]Dh ×Dh and
each block has the form
Gij =

Dy
X

cil cjl νf (cl , h)νf (wi , x)νf (wj , x)x̃x̃| .

l=1

Now that we have the one-layer and two-layer RFIMs,
we can either split a given feed-forward neural network
into one-layer subsystems or into two-layer subsystems.
A trade-off is that using a larger subsystem entails greater
analytical and computational difficulty, although it could
more accurately model the global system dynamics. In the
extreme case, the FIM is obtained if the whole system is
considered as one single subsystem.

4. RFIM: Key Advantages
This section discusses the theoretical advantages of the
RFIM over the FIM. Consider wlog a MLP with Bernoulli
outputs y ∈ {0, 1}m , whose mean µ is a deterministic
function depending on the input x and the network parameters Θ. By Sec. 2, the FIM of the MLP can be computed
as (see supplementary for proof)
n

I(Θ) =

m

1
∂µj (xi ) ∂µj (xi )
1 XX
.
n i=1 j=1 µj (xi )(1 − µj (xi )) ∂Θ
∂Θ|

(3)
Therefore rank(I(Θ)) ≤ nm. The rank of a diagonal
block of I(Θ) corresponding to one layer is even smaller.
In a deep neural network (e.g. Szegedy, Christian et al.
2015), if the sample size n < dim(Θ)/m, then I(Θ)
is doomed to be singular. All methods trying to approximate the FIM suffer from this problem and therefore rely
on proper regularizations. If the network is decomposed
into layers, the RFIM of each subsystem (layer) is given
by eq. (2). Each sample can contribute maximally 1 to
the rank of the neuron-RFIM and can contribute maximally Dy to the rank of the layer-RFIM. It only requires
maxi {dim(wi )} (the maximum layer width) observations
to have a full rank RFIM, where wi is the weight vector
of the i’th neuron. The RFIM is expected to have a much
higher rank than the FIM. Higher rank means less singularity and more information is captured. Models that can

be distinguished by the RFIM may be identical in the sense
of the FIM. Essentially, the RFIM integrates the internal
randomness (Bengio, 2013) of the neural system by considering the output of each layer as a random variable. In
theory, the FIM should also consider stochastic neurons.
However it requires marginalizing the joint distribution of
h1 , h2 , · · · , y. This makes the already infeasible computation even more challenging.
The RFIM is not an approximation of the FIM but is an accurate metric, defining the geometry of θ wrt to its direct
response h in the system, or adjacent nodes in a graphical
model. By the example in fig. 1, g y (θL ) of the last layer is
exactly the corresponding block in I(Θ): they both characterize how θL affects the mapping hL−1 → y. They
start to diverge from the second to last layer. To compute
the geometry of θL−1 , the RFIM looks at how θL−1 affects the local mapping hL−2 → hL−1 , which can be measured reliably regardless of the rest of the system (think
of a “debugging” process to separate and measure a single
component). In contrast, the FIM examines how θL−1 affects the non-local mapping hL−2 → y. This is a difficult
task because it must consider the correlation between different layers. As an approximation, the block diagonalized
version of the FIM ignores such correlations and therefore
faces the loss of accuracy.
The RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems
are balanced during learning. Consider a set of interconnected subsystems with internal parameters {θl } and the
corresponding response variables {hl }. The RFIM g hl (θl )
measures how much the likelihood surface of hl is curved
wrt a small learning step δθl . By constraining the squared
Riemannian distance δθl| g hl (θl )δθl having similar scales,
different subsystems will present similar variations during
learning. Within one subsystem, the learning along sensitive parameter directions is penalized. Among different
subsystems, the learning of sensitive subsystems is penalized. Globally, the inter-subsystem stochastic connections
have similar variance, maintaining a stable reference system and achieving efficient learning. This is similar to the
idea of batch normalization (BN) (Ioffe & Szegedy, 2015)
but has a deeper theoretical foundation.
Formally, we have the following theorem.
Theorem 3. Consider a learning system represented by a
joint distribution p(x, h) of x (observables) and h (hidden variableswhich connect subsystems).
 The joint FIM
log p(x,h | Θ) log p(x,h | Θ)
has a block diagJ (Θ) = Ep
∂Θ
∂Θ|
onal form. Each block is Ep (g h (θ)), where θ is the parameters within a subsystem and h is its response variables to
neighour subsystems.
The global correspondence of the local RFIM is the joint

Relative Fisher Information and Natural Gradient

FIM.PBy theorem 3, the square distance dΘ| J (Θ)dΘ =
Ep ( l dθl| g hl (θl )dθl ) measures the system variance, including both the observables x and the hidden variables
h. An intrinsic trade-off between the RFIM and the FIM
is learning system stability versus efficiency. Normalizing the FIM is more efficient because it helps to achieve
Fisher efficiency (Amari, 1998). Normalizing the RFIM is
more stable since the hidden variations are bounded, which
only guarantees subsystem Fisher efficiency characterized
by the Cramér-Rao lower bound of local parameters.

5. Relative Natural Gradient Descent
The traditional non-parametric way of applying natural
gradient requires re-calculating the FIM and solving a large
linear system in each learning step. Besides the huge computational cost, it has a large approximation error. For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations. That is, the FIM of a
mini-batch is likely to be singular or poorly conditioned.
A recent series of efforts (Montavon & Müller, 2012; Raiko
et al., 2012; Desjardins et al., 2015) are gearing towards
a parametric approach to applying natural gradient, which
memorizes and learns a geometry. For example, natural
neural networks (Desjardins et al., 2015) augment each
layer with a redundant linear layer, and let these linear layers parametrize the geometry of the neural manifold.
By dividing the learning system into subsystems, the RFIM
potentially gives a systematical implementation of parametric natural gradient descent. The memory complexity
of storing theP
Riemannian metric has been reduced from
O(D2 ) to O( i Di2 ), where Di = dim(wi ) is the size
of the i’th neuron. Consider there are M neurons in total,
then the memory cost is reduced by a factor of M . The
computational complexity has beenPreduced from O(D% )
(% ≈ 2.373, Williams 2012) to O( i Di% ). Optimization
based on RFIM is called Relative Natural Gradient Descent
(RNGD).
The good performance of batch normalization (Ioffe &
Szegedy, 2015) provides an empirical support for the
RFIM. Basically, BN uses an inter-sample normalization
layer to transform the layer input x to z with zero mean and
unit variance and thus reduces “internal covariate shift”. In
a typical case, above this normalization layer is a linear
layer given by y = W | z̃. If each dimension of z is normalized, then the diagonal blocks of the linear layer RFIM
g y (W ) = diag[z̃ z̃ | , · · · , z̃ z̃ | ] become a covariance matrix with identity diagonal entries (after taking an empirical
average). This gives the coordinate system W a well conditioned RFIM for efficient learning.

5.1. RNGD with a relu MLP
This subsection builds a proof-of-concept experiment on
MLP optimization. We partition the MLP into layers (one
layer consists of a linear layer plus an element-wise nonlinear activation function) as the subsystems. By eq. (2),
the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 =
x) and weights {wl1 , · · · , wlml } is
h
i
diag νf (wl1 , hl−1 )h̃l−1 h̃|l−1 , · · · , νf (wlml , hl−l )h̃l−1 h̃|l−1 .

The subsystem stability during one learning
geometrically by
PL step
Pml δw can be measured
|
2
ν
(w
,
h
)(δw
h̃
)
.
Using this
li
l−1
l=1
i=1 f
li l−1
term as the geometric cost (the Lagrange term) in the trust
region approach in Sec. 2, we get the following RNGD
method. In a stochastic gradient descent scenario, each
neuron i in layer l is updated by
new
old
wli
← wli
− G−1
li

∂E
,
∂wli

where E is the cost function and Gli is a learned metric.
The consideration is that a mini-batch of samples do not
contain enough information to compute the RFIM, which
should be averaged over all training samples. Therefore,
for the i’th neuron in layer l, Gli is initialized to identity,
and is updated based on
|
Gnew
← (1 − λ)Gold
li
li + λνf (wli , hl−1 )h̃l−1 h̃l−1 + I,

where  > 0 is a hyper-parameter to avoid singularity
caused by small sample size, and the average is taken over
all samples in a mini-batch, and λ is a learning rate. In
theory, λ should be gradually reduced to zero to guarantee
the convergence of this geometry learning. To avoid solving a linear system in each iteration, every T iterations we
recompute and store G−1
li based on the most updated Gli .
In the next T iterations, this G−1
li will be used as an approximation of the inverse RFIM. For the input layer which
scales with the number of input features, and the final softmax layer, we apply instead the RFIM of the corresponding
linear layer to improve the computational efficiency.
We compare different optimizers on classifying MNIST
digits. The network has shape 784-80-80-80-10, with relu
activation units, a final soft-max layer, and uses the persample average cross-entropy with L2 -regularization as the
learning cost function. We experiment on two different architectures: one is a plain MLP (PLAIN); the other has a
batch normalization layer after each hidden layer (BNA),
where a rescaling parameter is applied to ensure enough
flexibility of the parametric structure (Ioffe & Szegedy,
2015). For simplicity, the architecture, mini-batch size
(50), and L2 regularization strength (10−3 ) are fixed to be
the same for all compared methods. The observations are
consistent when these configurations vary.

Relative Fisher Information and Natural Gradient
0.40

formative samples for each output neuron are centered and
decorrelated.

0.976
0.35

error

0.25
0.20

0.972
0.970

accuracy

0.974
PLAIN+SGD (train)
PLAIN+SGD (valid)
PLAIN+ADAM (train)
PLAIN+ADAM (valid)
PLAIN+RNGD (train)
PLAIN+RNGD (valid)

0.30

0.968

0.15

0.966

0.10
0

20

40
60
#epochs

80

100

In the above experiment, RNGD’s computational time per
each epoch is roughly 4 ∼ 10 times more than SGD and
ADAM on a modern graphic card. Therefore in terms of
wall clock time RNGD does not show advantages. This
can be improved by more efficient implementations with
low rank approximation techniques and early stopping. Our
RNGD prototype hints at a promising direction to develop
scalable 2nd -order deep learning optimizers based on the
RFIM.

(a) A plain MLP with L2 regularization
0.5

0.978

6. Related Works on FIM Diagonalization

0.977
0.976
0.975

0.3

BNA+SGD (train) 0.974
BNA+SGD (valid)
BNA+ADAM (train) 0.973
BNA+ADAM (valid)
0.972
BNA+RNGD (train)
BNA+RNGD (valid) 0.971

0.2

0.1

0

20

40
60
#epochs

80

accuracy

error

0.4

0.970
100

(b) A MLP with batch normalization and L2
regularization
Figure 2. Learning curves of different optimizers on a MLP with
two different architectures (with and without BN). The best learning rate for each method is selected based on the validation accuracy. Using this learning rate, the learning curves wrt 40 different
random initializations are shown. The mean validation curve is
shown for a clear visualization.

Figure 2 shows the learning curves of different methods.
SGD is stochastic gradient descent. ADAM is the Adam
optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999
and  = 10−8 . Our RNGD is implemented by modifying
TensorFlow’s (Abadi, Martı́n et al., 2015) SGD optimizer.
We set empirically T = 100, λ = 0.005 and ω = 1.
RNGD presents a sharper learning curve and better generalization, especially when it is combined with BN. In this
case, the final tranining error of RNGD is slightly larger
than ADAM because by validation it favors a larger learning rate, which is applied on the neural network weights
(based on RNGD) and BN parameters (based on SGD). For
the ReLU activation, νf (wi , x) is approximately binary,
emphasizing such informative samples with wi| x̃ > 0,
which are the ones contributing to the learning of wi with
non-zero gradient values. Each output neuron has a different subset of informative samples. RNGD normalizes
x differently wrt different output neurons, so that the in-

One may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert. This fundamental problem of parameter
orthogonalization was first investigated by Jeffreys (1998)
for decorrelating the parameters of interest from the nuisance parameters. Fisher diagonalization yields parameter
orthogonalization (Cox & Reid, 1987), and is proved useful when estimating Θ̂ using a maximum likelihood estimator (MLE) that is asymptotically normally distributed,
Θ̂n ∼ G(Θ, I −1 (Θ)/n), and efficient since the variance
of the estimator matches the Cramér-Rao lower bound. Using the chain rule, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying
X  ∂ 2 l  ∂Θi ∂Θj
= 0, ∀k 6= l.
E
∂Θi ∂Θj ∂Ωk ∂Ωl
i,j

(nonThus in general, we end up with D
= D(D−1)
2
2
linear) partial differential equations to satisfy (Huzurbazar,
1950).
Therefore, in general there is no solution when

D
>
D,
that is when D > 3. When D = 2, the sin2
gle differential equation is usually solvable and tractable,
and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the
location-scale families { σ1 p0 ( x−µ
σ )} that include the Gaussian family and the Cauchy family. Sometimes, the structure of the differential equation system yields a solution:
For example, Jeffreys (1998) reported a parameter orthogonalization for Pearson’s distributions of type I which is of
order D = 4. Cox and Reid (1987) further investigated this
topic with application to conditional inference, and provide
examples (including the Weibull distribution).
From the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ).
When the FIM may be degenerate, this yields a pseudoRiemannian manifold (Thomas, 2014). In differential
geometry, orthogonalization amounts to transforming the
square length infinitesimal element gij dΘi Θj of a Riemannian geometry into an orthogonal system ω with match-

Relative Fisher Information and Natural Gradient

ing square length infinitesimal element Ωii dΩ2i . However,
such a global orthogonal metric does not exist (Huzurbazar,
1950) when D > 3 for an arbitrary metric tensor, although
interesting Riemannian parameterization structures may be
derived in Riemannian 4D geometry (Grant & Vickers,
2009).
For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016)
(Θ1:k , Hk+1:D ), where H = Ep [t(x)] = ∇F (Θ) is the
moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e
denotes the subvector (vb , ..., ve )| of v. The geometry of
NEFs is a dually flat structure (Amari, 2016) induced by the
convex mgf, the potential function. It defines a dual affine
∂
∂
and ej = ∂ j = ∂Θ
coordinate systems ei = ∂i = ∂H
j
i
i
i
i
that are orthogonal: he , ej i = δj , where δj = 1 iff i = j
and δji = 0 otherwise. Hence the FIM has two diagonal
blocks. Those dual affine coordinate systems are defined
up to an affine invertible transformation: Θ̃ = AΘ + b,
H̃ = A−1 H + c. In particular, for any order-2 NEF
(D = 2), we can always obtain two mixed parameterizations (Θ1 , H2 ) or (H1 , Θ2 ).
The RFIM contributes another line of thought in parameter
diagonalization. We investigate the Fisher information of
hidden variables, or internal interfaces in the learning machine. This is novel since the majority of previous works
concentrate on the FIM of the observables, or the external
interface of the machine. From a causality perspective, we
factor out the main cause (parameters within the subsystem) of the response variable with a direct action-reaction
relationship, and regard the remaining parameters as a reference that can be easily estimated by the empirical distribution. This simplification may lead to broader applications of Fisher information in machine learning.
The particular case of a mixed coordinate system (that is
not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations. Our splits in RFIMs consider general
non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that
are the leaves of the foliation (see section 3.7 of Amari &
Nagaoka 2000).

7. Conclusion and Discussions
We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric.
The key advantage of this approach is that the local learning
dynamics can be analyzed in an accurate way without approximation. We present a core list of such local structures
in neural networks, and give their corresponding RFIMs.
This list of recipes can be used to provide guiding principles to design new optimizers for deep learning.

Our work applies to mirror descent as well since natural
gradient is related to mirror descent (Raskutti & Mukherjee, 2015) as follows: In mirror descent to minimize a cost
function E(Θ), given a strictly convex distance function
D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as:


1
Θt+1 = arg min Θ> ∇E(Θt ) + D(Θ, Θt ) .
Θ
γ
When D(Θ, Θ0 ) is chosen as a Bregman divergence
BF (Θ, Θ0 ) = F (Θ) − F (Θ0 ) − (Θ − Θ0 )> ∇F (Θ0 ) wrt
to a convex function F , it has been proved that the mirror
descent on the Θ-parameterization is equivalent (Raskutti
& Mukherjee, 2015) to the natural gradient optimization
on the induced Riemannian manifold with metric tensor
(∇2 F (Θ)) parameterized by the dual coordinate system
H = ∇F (Θ).
In general, to perform a Riemannian gradient descent for
minimizing a real-valued function f (Θ) on the manifold,
one needs to choose a proper metric tensor given in matrix form G(Θ). Thomas (2014) constructed a toy example showing that the natural gradient may diverge while
the ordinary gradient (for G = I) converges. Recently,
Thomas et al. (2016) proposed a new kind of descent
method based on what they called the Energetic Natural
Gradient that generalizes the natural gradient. The energy distance DE (p(Θ1 ), p(Θ2 ))2 = E[2dp(Θ1 ) (X, Y ) −
dp(Θ1 ) (X, X 0 ) − dp(Θ1 ) (Y, Y 0 )] where X, X 0 ∼ p(Θ1 )
and Y, Y 0 ∼ p(Θ2 ), where dp(Θ1 ) (·, ·) is a distance metric over the support. Using a Taylor’s expansion on their
energy distance, they get the Energy Information Matrix
(in a way similar to recovering the FIM from a Taylor’s
expansion of any f -divergence like the Kullback-Leibler
divergence). Their idea is to incorporate prior knowledge
on the structure of the support (observation space) to define
energy distance. Twisting the geometry of the support (say,
Wasserstein’s optimal transport) with the geometry of the
parametric distributions (Fisher-Rao geodesic distances) is
indeed important (Chizat et al., 2015). In information geometry, invariance on the support is provided by a Markov
morphism that is a probabilistic mapping of the support to
itself (Čencov, 1982). There is no neighbourhood structure
on the support in IG. Markov morphism includes deterministic transformation of a random variable by a statistic. It is
well-known that IT (Θ)  IX (Θ) with equality iff. T =
T (X) is a sufficient statistic of X. Thus to get the same invariance for the energy distance (Thomas et al., 2016), one
shall further require dp(Θ) (T (X), T (Y )) = dp(Θ) (X, Y ).
We believe that RFIMs will provide a sound methodology
to build further efficient systems for deep learning. The
full source codes to reproduce the experimental results are
available at https://www.lix.polytechnique.
fr/˜nielsen/RFIM.

Relative Fisher Information and Natural Gradient

Acknowledgements
The authors would like to thank the anonymous reviewers
and Yann Ollivier for the helpful comments. This work was
mainly conducted when the first author was a postdoctoral
researcher at École Polytechnique.

References
Abadi, Martı́n et al. TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. Software
available from tensorflow.org.
Amari, Shun’ichi. Information geometry of the EM and em
algorithms for neural networks. Neural Networks, 8(9):
1379–1408, 1995.
Amari, Shun’ichi. Neural learning in structured parameter
spaces – natural Riemannian gradient. In NIPS 9, pp.
127–133. MIT Press, 1997.
Amari, Shun’ichi. Natural gradient works efficiently in
learning. Neural Comput., 10(2):251–276, 1998.

Cox, D. R. and Reid, N. Parameter orthogonality and approximate conditional inference. Journal of the Royal
Statistical Society. Series B (Methodological), 49(1):1–
39, 1987.
Cramér, Harald. Mathematical Methods of Statistics, volume 9 of Princeton Mathematical Series. Princeton University Press, 1946.
Desjardins, Guillaume, Simonyan, Karen, Pascanu, Razvan, and Kavukcuoglu, Koray. Natural neural networks.
In NIPS 28, pp. 2071–2079. Curran Associates, Inc.,
2015.
Dowty, James G. Chentsov’s theorem for exponential families. arXiv preprints, 2017. 1701.08895 [math.ST].
Efron, Bradley and Hinkley, David V. Assessing the accuracy of the maximum likelihood estimator: Observed
versus expected Fisher information. Biometrika, 65(3):
457–487, 1978.

Amari, Shun’ichi. Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences.
Springer Japan, 2016.

Fréchet, Maurice. Sur l’extension de certaines evaluations statistiques au cas de petits echantillons. Revue de
l’Institut International de Statistique / Review of the International Statistical Institute, 11(3/4):182–205, 1943.

Amari, Shun’ichi and Nagaoka, Hiroshi. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. AMS and OUP, 2000. (Published
in Japanese in 1993).

Grant, James DE and Vickers, JA. Block diagonalization of
four-dimensional metrics. Classical and Quantum Gravity, 26(23):235014, 2009.

Bengio, Yoshua. Estimating or propagating gradients
through stochastic neurons. CoRR, abs/1305.2982, 2013.
Bonnabel, Silvère. Stochastic gradient descent on Riemannian manifolds. IEEE Trans. Automat. Contr., 58(9):
2217–2229, 2013.
Čencov, Nikolaı̌ Nikolaevich. Statistical decision rules and
optimal inference, volume 53 of Translations of Mathematical Monographs. American Mathematical Society,
1982. Translation from the Russian (published in 1972)
edited by Lev J. Leifman.
Chizat, Lenaic, Schmitzer, Bernhard, Peyré, Gabriel, and
Vialard, François-Xavier. An Interpolating Distance between Optimal Transport and Fisher-Rao. arXiv e-prints,
2015. 1506.06430 [math.AP].
Clevert, Djork-Arné, Unterthiner, Thomas, and Hochreiter,
Sepp. Fast and accurate deep network learning by exponential linear units (ELUs). CoRR, abs/1511.07289,
2015.
Cobb, Loren, Koppstein, Peter, and Chen, Neng Hsin. Estimation and moment recursion relations for multimodal
distributions of the exponential family. JASA, 78(381):
124–130, 1983.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectifiers: Surpassing humanlevel performance on ImageNet classification. In ICCV,
2015.
Hinton, Geoffrey E., Osindero, Simon, and Teh, YeeWhye. A fast learning algorithm for deep belief nets.
Neural Comput., 18(7):1527–1554, 2006.
Hotelling, Harold. Spaces of statistical parameters. American Mathematical Society Meeting, 1929. (unpublished.
Presented orally by O. Ore during the meeting).
Huzurbazar, Vasant Shankar. Probability distributions and
orthogonal parameters. Mathematical Proceedings of
the Cambridge Philosophical Society, 46(02):281–284,
1950.
Ioffe, Sergey and Szegedy, Christian. Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. In ICML; JMLR: W&CP 37, pp. 448–
456, 2015.
Jeffreys, Harold. Theory of Probability. Oxford Classic
Texts in the Physical Sciences. OUP, 3rd edition, 1998.
First published in 1939.

Relative Fisher Information and Natural Gradient

Jost, Jürgen. Riemannian Geometry and Geometric Analysis. Springer, 6th edition, 2011.
Kingma, Diederik P. and Ba, Jimmy. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.
Kingma, Diederik P and Welling, Max. Auto-encoding
variational Bayes. In ICLR, 2014. arXiv:1312.6114
[stat.ML].
Kurita, Takio. Iterative weighted least squares algorithms
for neural networks classifiers. New Generation Computing, 12(4):375–394, 1994.
Le Roux, Nicolas, Manzagol, Pierre-Antoine, and Bengio, Yoshua. Topmoumoute online natural gradient algorithm. In NIPS 20, pp. 849–856. Curran Associates,
Inc., 2008.
Lebanon, Guy. Riemannian geometry and statistical machine learning. PhD thesis, CMU, 2005.
Looks, Moshe, Herreshoff, Marcello, Hutchins, DeLesley,
and Norvig, Peter. Deep learning with dynamic computation graphs. In ICLR, 2017. arXiv:1702.02181
[cs.NE].
Marceau-Caron, Gaétan and Ollivier, Yann. Practical Riemannian neural networks. CoRR, abs/1602.08007, 2016.
Martens, James. Deep learning via Hessian-free optimization. In ICML, pp. 735–742, 2010.
Martens, James. New perspectives on the natural gradient
method. CoRR, abs/1412.1193, 2014.
Martens, James and Grosse, Roger. Optimizing neural networks with Kronecker-factored approximate curvature.
In ICML; JMLR: W&CP 37, pp. 2408–2417, 2015.
Montanari, Andrea. Computational implications of reducing data to sufficient statistics. Electron. J. Statist., 9(2):
2370–2390, 2015.
Montavon, Grégoire and Müller, Klaus-Robert. Deep
Boltzmann machines and the centering trick. In Neural
Networks: Tricks of the Trade, pp. 621–637. Springer
Berlin Heidelberg, 2nd edition, 2012.
Nair, Vinod and Hinton, Geoffrey E. Rectified linear units
improve restricted Boltzmann machines. In ICML, pp.
807–814, 2010.
Nielsen, Frank. Cramér-Rao lower bound and information
geometry. CoRR, abs/1301.3578, 2013.
Ollivier, Yann. Riemannian metrics for neural networks.
CoRR, abs/1303.0818, 2013.

Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks.
In ICLR, 2014.
arXiv:1301.3584 [cs.LG].
Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep
learning made easier by linear transformations in perceptrons. In AISTATS; JMLR W&CP 22, pp. 924–932,
2012.
Rao, Calyampudi Radhakrishna. Information and accuracy attainable in the estimation of statistical parameters.
Bull. Cal. Math. Soc., 37(3):81–91, 1945.
Raskutti, Garvesh and Mukherjee, Sayan. The information
geometry of mirror descent. In Geometric Science of
Information (GSI), volume 9389 of LNCS, pp. 359–368.
Springer, 2015.
Sun, Ke and Marchand-Maillet, Stéphane. An information geometry of statistical manifold learning. In ICML;
JMLR W&CP 32(2), pp. 1–9, 2014.
Szegedy, Christian et al. Going deeper with convolutions.
In CVPR, 2015.
Thomas, Philip. GeNGA: A generalization of natural gradient ascent with positive and negative convergence results. In ICML; JMLR W&CP 32, pp. 1575–1583, 2014.
Thomas, Philip, da Silva, B. C., Dann, C., and Brunskill,
E. Energetic natural gradient descent. In ICML, 2016.
Wager, Stefan, Wang, Sida, and Liang, Percy S. Dropout
training as adaptive regularization. In NIPS 26, pp. 351–
359. Curran Associates, Inc., 2013.
Watanabe, Sumio. Algebraic Geometry and Statistical
Learning Theory, volume 25 of Cambridge Monographs
on Applied and Computational Mathematics. CUP,
2009.
Williams, Virginia Vassilevska. Multiplying matrices faster
than Coppersmith-Winograd. In Annual ACM Symposium on Theory of Computing, STOC’12, pp. 887–898,
2012.
Zegers, Pablo. Fisher information properties. Entropy, 17:
4918–4939, 2015.

