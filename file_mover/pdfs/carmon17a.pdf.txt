“Convex Until Proven Guilty”: Dimension-Free Acceleration
of Gradient Descent on Non-Convex Functions
Yair Carmon John C. Duchi Oliver Hinder Aaron Sidford 1

Abstract
We develop and analyze a variant of Nesterov’s
accelerated gradient descent (AGD) for minimization of smooth non-convex functions. We
prove that one of two cases occurs: either
our AGD variant converges quickly, as if the
function was convex, or we produce a certificate that the function is “guilty” of being
non-convex. This non-convexity certificate allows us to exploit negative curvature and obtain deterministic, dimension-free acceleration
of convergence for non-convex functions. For
a function f with Lipschitz continuous gradient and Hessian, we compute a point x with
krf (x)k  ✏ in O(✏ 7/4 log(1/✏)) gradient and
function evaluations. Assuming additionally that
the third derivative is Lipschitz, we require only
O(✏ 5/3 log(1/✏)) evaluations.

1. Introduction
Nesterov’s seminal 1983 accelerated gradient method has
inspired substantial development of first-order methods
for large-scale convex optimization. In recent years, machine learning and statistics have seen a shift toward large
scale non-convex problems, including methods for matrix
completion (Koren et al., 2009), phase retrieval (Candès
et al., 2015; Wang et al., 2016), dictionary learning (Mairal
et al., 2008), and neural network training (LeCun et al.,
2015). In practice, techniques from accelerated gradient
methods—namely, momentum—can have substantial benefits for stochastic gradient methods, for example, in training neural networks (Rumelhart et al., 1986; Kingma and
Ba, 2015). Yet little of the rich theory of acceleration for
convex optimization is known to transfer into non-convex
optimization.
1

Stanford University, Stanford, California, USA. Correspondence to: Yair Carmon <yairc@stanford.edu>, Oliver Hinder
<ohinder@stanford.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Optimization becomes more difficult without convexity, as
gradients no longer provide global information about the
function. Even determining if a stationary point is a local minimum is (generally) NP-hard (Murty and Kabadi,
1987; Nesterov, 2000). It is, however, possible to leverage non-convexity to improve objectives in smooth optimization: moving in directions of negative curvature can
guarantee function value reduction. We explore the interplay between negative curvature, smoothness, and acceleration techniques, showing how an understanding of the
three simultaneously yields a method that provably accelerates convergence of gradient descent for a broad class of
non-convex functions.
1.1. Problem setting
We consider the unconstrained minimization problem
minimize f (x),
x

(1)

where f : Rd ! R is smooth but potentially non-convex.
We assume throughout the paper that f is bounded from
below, two-times differentiable, and has Lipschitz continuous gradient and Hessian. In Section 4 we strengthen our
results under the additional assumption that f has Lipschitz continuous third derivatives. Following the standard
first-order oracle model (Nemirovski and Yudin, 1983), we
consider optimization methods that access only values and
gradients of f (and not higher order derivatives), and we
measure their complexity by the total number of gradient
and function evaluations.
Approximating the global minimum of f to ✏-accuracy is
generally intractable, requiring time exponential in d log 1✏
(Nemirovski and Yudin, 1983, §1.6). Instead, we seek a
point x that is ✏-approximately stationary, that is,
krf (x)k  ✏.

(2)

Finding stationary points is a canonical problem in nonlinear optimization (Nocedal and Wright, 2006), and while
saddle points and local maxima are stationary, excepting
pathological cases, descent methods that converge to a stationary point converge to a local minimum (Lee et al.,
2016; Nemirovski, 1999, §3.2.2).

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

If we assume f is convex, gradient descent satisfies the
bound (2) after O(✏ 1 ) gradient evaluations, and AGD improves this rate to O(✏ 1/2 log 1✏ ) (Nesterov, 2012). Without convexity, gradient descent is significantly worse, having worst-case complexity ⇥(✏ 2 ) (Cartis et al., 2010).
More sophisticated gradient-based methods, including nonlinear conjugate gradient (Hager and Zhang, 2006) and LBFGS (Liu and Nocedal, 1989) provide excellent practical
performance, but their global convergence guarantees are
no better than O(✏ 2 ). Our work (Carmon et al., 2016) and,
independently, Agarwal et al. (2016), break this O(✏ 2 )
barrier, obtaining the rate O(✏ 7/4 log d✏ ). Before we discuss this line of work in Section 1.3, we overview our contributions.
1.2. Our contributions
“Convex until proven guilty” Underpinning our results
is the observation that when we run Nesterov’s accelerated
gradient descent (AGD) on any smooth function f , one of
two outcomes must follow:
(a) AGD behaves as though f was -strongly convex, satisfying inequality (2) in O( 1/2 log 1✏ ) iterations.
(b) There exist points u, v in the AGD trajectory that prove
f is “guilty” of not being -strongly convex,
f (u) < f (v) + rf (v)T (u

v) +

2

ku

2

vk . (3)

The intuition behind these observations is that if inequality (3) never holds during the iterations of AGD, then f
“looks” strongly convex, and the convergence (a) follows.
In Section 2 we make this observation precise, presenting an algorithm to monitor AGD and quickly find the
witness pair u, v satisfying (3) whenever AGD progresses
more slowly than it does on strongly convex functions.
We believe there is potential to apply this strategy beyond AGD, extending additional convex gradient methods
to non-convex settings.
An accelerated non-convex gradient method In Section 3 we propose a method that iteratively applies our
monitored AGD algorithm to f augmented by a proximal
regularizer. We show that both outcomes (a) and (b) above
imply progress minimizing f , where in case (b) we make
explicit use of the negative curvature that AGD exposes.
These progress guarantees translate to an overall first-order
oracle complexity of O(✏ 7/4 log 1✏ ), a strict improvement
over the O(✏ 2 ) rate of gradient descent. In Section 5
we report preliminary experimental results, showing a basic implementation of our method outperforms gradient descent but not nonlinear conjugate gradient.

Improved guarantees with third-order smoothness As
we show in Section 4, assuming Lipschitz continuous third
derivatives instead of Lipschitz continuous Hessian allows us to increase the step size we take when exploiting negative curvature, making more function progress.
Consequently, the complexity of our method improves to
O(✏ 5/3 log 1✏ ). While the analysis of the third-order setting is more complex, the method remains essentially unchanged. In particular, we still use only first-order information, never computing higher-order derivatives.
1.3. Related work
Nesterov and Polyak (2006) show that cubic regularization of Newton’s method finds a point that satisfies the stationarity condition (2) in O(✏ 3/2 ) evaluations of the Hessian. Given sufficiently accurate arithmetic operations, a
Lipschitz continuous Hessian is approximable to arbitrary
precision using finite gradient differences, and obtaining
a full Hessian requires O(d) gradient evaluations. A direct implementation of the Nesterov-Polyak method with
a first-order oracle therefore has gradient evaluation complexity O(✏ 3/2 d), improving on gradient descent only if
d ⌧ ✏ 1/2 , which may fail in high-dimensions.

In two recent papers, we (Carmon et al., 2016) and (independently) Agarwal et al. obtain better rates for first-order
methods. Agarwal et al. (2016) propose a careful implementation of the Nesterov-Polyak method, using accelerated methods for fast approximate matrix inversion. In our
earlier work, we employ a combination of (regularized) accelerated gradient descent and the Lanczos method. Both
find a point that satisfies the bound (2) with probability at
least 1
using O ✏ 7/4 log d✏ gradient and Hessianvector product evaluations.
The primary conceptual difference between our approach
and those of Carmon et al. and Agarwal et al. is that
we perform no eigenvector search: we automatically find
directions of negative curvature whenever AGD proves f
“guilty” of non-convexity. Qualitatively, this shows that
explicit second orders information is unnecessary to improve upon gradient descent for stationary point computation. Quantitatively, this leads to the following improvements:
(i) Our result is dimension-free and deterministic, with
complexity independent of the ratio d/ , compared to
the log d dependence of previous works. This is significant, as log d may be comparable to ✏ 1/4 / log 1✏ .
(ii) Our method uses only gradient evaluations, and does
not require Hessian-vector products. In practice,
Hessian-vector products may be difficult to implement and more expensive to compute than gradients.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

(iii) Under third-order smoothness assumptions we improve our method to achieve O(✏ 5/3 log 1✏ ) rate. It is
unclear how to extend previous approaches to obtain
similar guarantees.
In distinction from the methods of Carmon et al. (2016) and
Agarwal et al. (2016), our method provides no guarantees
on positive definiteness of r2 f (x); if initialized at a saddle
point it will terminate immediately. However, as we further
explain in Section D, we may combine our method with a
fast eigenvector search to recover thep
approximate positive
definiteness guarantee r2 f (x) ⌫
✏I, even improving
it to r2 f (x) ⌫ ✏2/3 I using third-order smoothness, but
at the cost of reintroducing randomization, Hessian-vector
products and a log d complexity term.

Here we introduce notation and briefly overview definitions
and results we use throughout. We index sequences by subscripts, and use xji as shorthand for xi , xi+1 , ..., xj . We use
x, y, v, u, w, p, c, q and z to denote points in Rd . Additionally, ⌘ denotes step sizes, ✏, " denote desired accuracy, ✓
denotes a scalar and k·k denotes the Euclidean norm on
Rd . We denote the nth derivative of a function h : R ! R
by h(n) . We let log+ (t) = max{0, log t}.
A function f : Rd ! R has Ln -Lipschitz nth derivative if
it is n times differentiable and for every x0 and unit vector
, the one-dimensional function h(✓) = f (x0 +✓ ) satisfies
h(n) (✓2 )  Ln |✓1

n
X
1 (i)
h (✓0 )(✓
i!
i=0

✓0 ) i 

Ln
|✓
(n + 1)!

 1

9:

if krf (yt )k  " then return (xt0 , y0t , NULL)

1: function C ERTIFY- PROGRESS(f , y0 , yt , L, , )
2:
if f (yt ) > f (y0 ) then
3:
return y0
. non-convex behavior
4:
6:
7:
8:

Set zt
yt L1 rf (yt )
2
Set (zt )
f (y0 ) f (zt ) + 2 kzt y0 k
p
if krf (yt )k2 > 2L (zt )e t/  then
return zt
. AGD has stalled
else return NULL

1: function F IND - WITNESS - PAIR(f , xt0 , y0t , wt , )
2:
for j = 0, 1, . . . , t 1 do
3:
for u = yj , wt do
4:
if eq. (8) holds with v = xj then
5:
return (u, xj )
6:

(by Corollary 1 this line is never reached)

2.1. AGD as a convexity monitor

✓2 |.

We refer to this property as nth-order smoothness, or simply smoothness for n = 1, where it coincides with the Lipschitz continuity of rf . Throughout the paper, we make extensive use of the well-known consequence of Taylor’s theorem, that the Lipschitz constant of the nth-order derivative
controls the error in the nth order Taylor series expansion
of h, i.e. for ✓, ✓0 2 R we have
h(✓)

p

p
1: Set 
L/ , !
and x0
y0
+1
2: for t = 1, 2, . . . do
3:
yt
xt 1 L1 rf (xt 1 )
4:
xt
yt + ! (yt yt 1 )
5:
wt
C ERTIFY- PROGRESS(f, y0 , yt , L, , )
6:
if wt 6= NULL then
. convexity violation
7:
(u, v)
F IND - WITNESS - PAIR(f, xt0 , y0t , wt , )
8:
return (xt0 , y0t , u, v)

5:

1.4. Preliminaries and notation

h(n) (✓1 )

Algorithm 1 AGD- UNTIL - GUILTY(f, y0 , ", L, )

✓0 |n+1 .

(4)
A function f is -strongly convex if f (u)
f (v) +
2
rf (v)T (u v) + 2 ku vk for all v, u 2 Rd .

2. Algorithm components
We begin our development by presenting the two building
blocks of our result: a monitored variation of AGD (Section 2.1) and a negative curvature descent step (Section 2.2)
that we use when the monitored version of AGD certifies
non-convexity. In Section 3, we combine these components
to obtain an accelerated method for non-convex functions.

The main component in our approach is Alg. 1, AGDUNTIL - GUILTY . We take as input an L-smooth function
f , conjectured to be -strongly convex, and optimize it
with Nesterov’s accelerated gradient descent method for
strongly convex functions (lines 3 and 4). At every iteration, the method invokes C ERTIFY- PROGRESS to test
whether the optimization is progressing as it should for
strongly convex functions, and in particular that the gradient norm is decreasing exponentially quickly (line 6). If the
test fails, F IND - WITNESS - PAIR produces points u, v proving that f violates -strong convexity. Otherwise, we proceed until we find a point y such that krf (y)k  ".
The efficacy of our method is based on the following guarantee on the performance of AGD.
Proposition 1. Let f be L-smooth, and let y0t and xt0 be the
sequence of iterates generated by AGD- UNTIL - GUILTY(f ,
y0 , L, ", ) for some " > 0 and 0 <  L. Fix w 2 Rd .
If for s = 0, 1, . . . , t 1 we have

f (u)

f (xs ) + rf (xs )T (u

xs ) +

2

ku

xs k2 (5)

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

for both u = w and u = ys , then
✓
◆t
1
f (yt ) f (w)  1 p
(w),

where  =

L

and (w) = f (y0 )

f (w) +

2

(6)
2

y0 k .

kw

Proposition 1 is essentially a restatement of established results (Nesterov, 2004; Bubeck, 2014), where we take care
to phrase the requirements on f in terms of local inequalities, rather than a global strong convexity assumption. For
completeness, we provide a proof of Proposition 1 in Section A.1 in the supplementary material.
With Proposition 1 in hand, we summarize the guarantees
of Alg. 1 as follows.
Corollary 1. Let f : Rd ! R be L-smooth, let y0 2 Rd ,
" > 0 and 0 <  L. Let (xt0 , y0t , u, v) = AGD- UNTIL GUILTY (f , y0 , ", L, ). Then the number of iterations t
satisfies
( r
✓
◆)
L
2L (zt 1 )
t  1 + max 0,
log
, (7)
"2

some 0  s < t, implying F IND - WITNESS - PAIR will return for some j  s.
1
Similarly, if wt = zt = yt
L rf (yt ) then by line 6 of
C ERTIFY- PROGRESS we must have
✓
◆t
p
1
1
2
krf (yt )k > (zt )e t/ 
1 p
(zt ).
2L


Since f is L-smooth we have the standard progress guarantee (c.f. Nesterov (2004) §1.2.3) f (yt )
f (zt )
2
1
2L krf (yt )k , again contradicting inequality (6).

To see that the bound (9) holds, note that f (ys )  f (y0 ) for
s = 0, . . . , t 1 since condition 2 of C ERTIFY- PROGRESS
did not hold. If u = yj for some 0  j < t then
f (u)  f (y0 ) holds trivially. Alternatively, if ut =
wt = zt then condition 2 did not hold at time t as well,
so we have f (yt )  f (y0 ) and also f (u) = f (zt ) 
2
1
f (yt ) 2L
krf (yt )k as noted above; therefore f (zt ) 
f (y0 ).
Before continuing, we make two remarks about implementation of Alg. 1.

2

where (z) = f (y0 ) f (z)+ 2 kz y0 k is as in line 5 of
C ERTIFY- PROGRESS. If u, v 6= NULL (non-convexity was
detected), then
f (u) < f (v) + rf (v)T (u

v) +

2

ku

vk2

(8)

where v = xj for some 0  j < t and u = yj or u = wt
(defined on line 5 of AGD- UNTIL - GUILTY). Moreover,
max{f (y1 ), . . . , f (yt

1 ), f (u)}

(9)

 f (y0 ).

Proof. The bound (7) is clear for t = 1. For t > 1, the
algorithm has not terminated at iteration t 1, and so we
know that neither the condition in line 9 of AGD- UNTIL GUILTY nor the condition in line 6 of C ERTIFY- PROGRESS
held at iteration t 1. Thus
"2 < krf (yt

1 )k

2

 2L (zt

1 )e

p
(t 1)/ 

(1) As stated, the algorithm requires evaluation of two
function gradients per iteration (at xt and yt ). Corollary 1 holds essentially unchanged if we execute line 9
of AGD- UNTIL - GUILTY and lines 4-6 of C ERTIFYPROGRESS only once every ⌧ iterations, where ⌧ is
some fixed number (say 10). This reduces the number
of gradient evaluations to 1 + ⌧1 per iteration.
(2) Direct implementation would require O(d · t) memory to store the sequences y0t , xt0 and rf (xt0 ) for later
use by F IND - WITNESS - PAIR. Alternatively, F IND WITNESS - PAIR can regenerate these sequences from
their recursive definition while iterating over j, reducing the memory requirement to O(d) and increasing
the number of gradient and function evaluations by at
most a factor of 2.

,

which gives the bound (7) when rearranged.
Now we consider the returned vectors xt0 , y0t , u, and v from
AGD- UNTIL - GUILTY. Note that u, v 6= NULL only if
wt 6= NULL. Suppose that wt = y0 , then by line 2 of
C ERTIFY- PROGRESS we have,
✓
◆t
1
f (yt ) f (wt ) > 0 = 1 p
(wt ),

since (wt ) = (y0 ) = 0. Since this contradicts the
progress bound (6), we obtain the certificate (8) by the contrapositive of Proposition 1: condition (5) must not hold for

In addition, while our emphasis is on applying AGDUNTIL - GUILTY to non-convex problems, the algorithm
has implications for convex optimization. For example,
we rarely know the strong convexity parameter
of a
given function f ; to remedy this, O’Donoghue and Candès
(2015) propose adaptive restart schemes. Instead, one may
repeatedly apply AGD- UNTIL - GUILTY and use the witnesses to update .
2.2. Using negative curvature
The second component of our approach is exploitation of
negative curvature to decrease function values; in Section 3

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Algorithm 2 E XPLOIT-NC- PAIR(f , u, v, ⌘)
1:
(u v)/ku vk
2: u+
u+⌘
3: u
u ⌘
4: return arg minz2{u ,u+ } f (z)
we use AGD- UNTIL - GUILTY to generate u, v such that
f (u) < f (v) + rf (v)T (u

v)

↵
ku
2

2

vk ,

(10)

a nontrivial violation of convexity (where ↵ > 0 is a parameter we control using a proximal term). By taking an
appropriately sized step from u in the direction ±(u v),
Alg. 2 can substantially lower the function value near u
whenever the convexity violation (10) holds. The following basic lemma shows this essential progress guarantee.
Lemma 1. Let f : Rd ! R have L2 -Lipschitz Hessian.
↵
Let ↵ > 0 and let u and v satisfy (10). If ku vk  2L
,
2
↵
then for every ⌘  L2 , E XPLOIT-NC- PAIR(f, u, v, ⌘) finds
a point z such that
f (z)  f (u)

↵⌘ 2
.
12

(11)

We give the proof of Lemma 1 in Section A.2, and we outline it here. The proof is split into two parts, both using
the Lipschitz continuity of r2 f . In the first part, we show
using (10) that f has negative curvature of at least ↵/2 in
the direction of at the point u. In the second part, we consider the Taylor series expansion of f . The first order term
predicts, due to its anti-symmetry, that either a step size of
⌘ or ⌘ in the direction reduces the objective. Adding
our knowledge of the negative curvature from the first part
yields the required progress.

3. Accelerating non-convex optimization
We now combine the accelerated convergence guarantee
of Corollary 1 and the non-convex progress guarantee of
Lemma 1 to form G UARDED - NON - CONVEX -AGD. The
idea for the algorithm is as follows. Consider iterate
k 1, denoted pk 1 . We create a proximal function fˆ by
2
adding the proximal term ↵ kx pk 1 k to f . Applying
AGD- UNTIL - GUILTY to fˆ yields the sequences x0 , . . . , xt ,
y0 , . . . , yt and possibly a non-convexity witnessing pair
u, v (line 3). If u, v are not available, we set pk = yt and
continue to the next iteration. Otherwise, by Corollary 1,
u and v certify that fˆ is not ↵ strongly convex, and therefore that f has negative curvature. E XPLOIT-NC- PAIR then
leverages this negative curvature, obtaining a point b(2) .
The next iterate pk is the best out of y0 , . . . , yt , u and b(2)
in terms of function value.

Algorithm 3
G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵, ⌘)
1: for k = 1, 2, . . . do
2
2:
Set fˆ(x) := f (x) + ↵ kx pk 1 k
t
t
3:
(x0 , y0 , u, v)
✏
AGD- UNTIL - GUILTY(fˆ, pk 1 , 10
, L1 + 2↵, ↵)
4:
if u, v = NULL then
5:
pk
yt
. fˆ effectively str. convex
6:
else
. non-convexity proof available
7:
b(1)
F IND - BEST- ITERATE(f, y0t , u, v)
(2)
8:
b
E XPLOIT-NC- PAIR(f, u, v, ⌘)
9:
pk
arg minz2{b(1) ,b(2) } f (z)
10:
11:

if krf (pk )k  ✏ then
return pk

1: function F IND - BEST- ITERATE(f , y0t , u, v)
2:
return arg minz2{u,y0 ,...,yt } f (z)

The following central lemma provides a progress guarantee
for each of the iterations of Alg. 3.
Lemma 2. Let f : Rd ! R be L1 -smooth and have L2 Lipschitz continuous Hessian, let ✏, ↵ > 0 and p0 2 Rd .
Let p1 , . . . , pK be the iterates G UARDED - NON - CONVEX AGD(f , p0 , L1 , ✏, ↵, L↵2 ) generates. Then for each k 2
{1, . . . , K 1},
⇢ 2
✏
↵3
f (pk )  f (pk 1 ) min
,
.
(12)
5↵ 64L22
We defer a detailed proof of Lemma 2 to Section B.1, and
instead sketch the main arguments. Fix an iteration k of
G UARDED - NON - CONVEX -AGD that is not the final one
(i.e. k < K). Then, if fˆ was effectively strongly convex we
must have krfˆ(yt )k  ✏/10 and standard proximal point
arguments show that we reduce the objective by ✏2 /(5↵).
Otherwise, a witness pair u, v is available for which (10)
holds by Corollary 1, and f (u)  fˆ(u)  fˆ(y0 ) = f (pk ).
To apply Lemma 1 it remains to show that ku vk 
2
↵/(2L2 ). We note that, since f (yi ) + ↵ kyi y0 k =
fˆ(yi )  f (y0 ) for every i < t, if any iterate yi is far from
y0 , f (yi ) must be substantially lower than f (y0 ), and therefore b(1) makes good progress. Formalizing the converse of
this claim gives Lemma 3, which we prove Section B.2.
Lemma 3. Let f be L1 -smooth, and ⌧
0. At any iteration of G UARDED - NON - CONVEX -AGD, if u, v 6= NULL
and the best iterate b(1) satisfies f (b(1) )
f (y0 ) ↵⌧ 2
then for 1  i < t,
kyi

y0 k  ⌧, and kxi

Consequently, ku

y0 k  3⌧.

vk  4⌧ .

Lemma 3 explains the role of b(1) produced by F IND - BESTITERATE: it is an “insurance policy” against ku
vk being

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

too large. To complete the proof of Lemma 2 we take ⌧ =
↵
8L2 , so that either
f (b(1) )  f (y0 )
or we have ku
therefore f (b
⌘ = ↵/L2 ).

(2)

↵⌧ 2 = f (y0 )
↵
2L2
3
↵
12L22

vk  4⌧ =

)  f (y0 )

↵3
,
64L22

by Lemma 3, and
by Lemma 1 (with

Lemma 2 shows we can accelerate gradient descent in
a non-convex setting. Indeed, ignoring
all problemp
dependent constants, setting ↵ = ✏ in the bound (12)
shows that we make ⌦(✏3/2 ) progress at every iteration
of G UARDED - NON - CONVEX -AGD, and consequently the
number of iterations is bounded by O(✏ 3/2 ). Arguing that
calls to AGD- UNTIL - GUILTY each require O(✏ 1/4 log 1✏ )
gradient computations yields the following complexity
guarantee, which we prove in Section B.3.
Theorem 1. Let f : Rd ! R be L1 -smooth and have L2 Lipschitz continuous Hessian. Let p0 2 Rd , f = f (p0 )
2/3 1/3
inf z2Rd f (z) and 0 < ✏  min{ f L2 , L21 /(64L2 )}.
Set
p
↵ = 2 L2 ✏
(13)
then G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵, L↵2 )
finds a point pK such that krf (pK )k  ✏ with at most
20 ·

1/2 1/4
f L1 L2
✏7/4

500L1
log
✏2

f

(14)

gradient evaluations.
The conditions on ✏ simply guarantee that the clean
bound (14) is non-trivial, as gradient descent yields better
convergence guarantees for larger values of ✏.
While we state Theorem 1 in terms of gradient evaluation
count, a similar bound holds for function evaluations as
well. Indeed, inspection of our method reveals that each iteration of Alg. 3 evaluates the function and not the gradient
at at most the three points u, u+ and u ; both complexity
measures are therefore of the same order.

4. Incorporating third-order smoothness
In this section, we show that when third-order derivatives
are Lipschitz continuous, we can improve the convergence
rate of Alg. 3 by modifying two of its subroutines. In Section 4.1 we introduce a modified version of E XPLOIT-NCPAIR that can decrease function values further using thirdorder smoothness. In Section 4.2 we change F IND - BESTITERATE to provide a guarantee that f (v) is never too large.
We combine these two results in Section 4.3 and present
our improved complexity bounds.

Algorithm 4 E XPLOIT-NC- PAIR 3 (f , u, v, ⌘)
1:
(u
p v)/ ku vk
2: ⌘ 0
⌘(⌘ + ku vk) ku vk
3: u+
u + ⌘0
4: v
v ⌘
5: return arg minz2{v ,u+ } f (z)
4.1. Making better use of negative curvature
Our first observation is that third-order smoothness allows
us to take larger steps and make greater progress when exploiting negative curvature, as the next lemma formalizes.
Lemma 4. Let f : Rd ! R have L3 -Lipschitz third-order
derivatives, u 2 Rd , and 2 Rd be a unit vector.
If
p
T 2
r f (u) = ↵2 < 0 then, for every 0  ⌘  3↵/L3 ,
min{f (u

⌘ ), f (u + ⌘ )}  f (u)

↵⌘ 2
.
8

(15)

Proof. For ✓ 2 R, define h(✓) = f (u+✓ ). By assumption
h000 is L3 -Lipschitz continuous, and therefore
h(✓)  h(0) + h0 (0)✓ + h00 (0)

✓2
✓3
✓4
+ h000 (0) + L3 .
2
6
24

Set A⌘ = h0 (0)⌘ + h000 (0)⌘ 3 /6 and set ⌘¯ = sign(A⌘ )⌘.
As h0 (0)¯
⌘ + h000 (0)¯
⌘ 3 /6 = |A⌘ |  0, we have
h(¯
⌘ )  h(0) + h00 (0)

⌘2
⌘4
+ L3
 f (u)
2
24

↵⌘ 2
,
8

the last inequality using h(0) = f (u), h00 (0) =
3↵
⌘2  L
. That f (u + ⌘¯ ) = h(¯
⌘ ) gives the result.
3

↵
2

and

Comparing Lemma 4 to the second part of the proof of
Lemma 1, we see that second-order smoothness with optimal ⌘ guarantees ↵3 /(12L22 ) function decrease, while
third-order smoothness guarantees a 3↵2 /(8L3 ) decrease.
Recalling Theorem 1, where ↵ scales as a power of ✏, this
is evidently a significant improvement. Additionally, this
benefit is essentially free: there is no increase in computational cost and no access to higher order derivatives. Examining the proof, we see that the result is rooted in the
anti-symmetry of the odd-order terms in the Taylor expansion. This rules out extending this idea to higher orders of
smoothness, as they contain symmetric fourth order terms.
Extending this insight to the setting of Lemma 1 is complicated by the fact that, at relevant scales of ku vk, it is
no longer possible to guarantee that there is negative curvature at either u or v. Nevertheless, we are able to show
that a small modification of E XPLOIT-NC- PAIR achieves
the required progress.
Lemma 5. Let f : Rd ! R have L3 -Lipschitz third-order
derivatives. Let ↵ > 0 and let u and v satisfy (10) and let

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Algorithm 5 F IND - BEST- ITERATE 3 (f , y0t , u, v)
1: Let 0  j < t be such that v = xj
2: cj
(yj + yj 1 )/2 if j > 0 else y0
3: qj
2yi + 3yj 1 if j > 0 else y0
4: return arg minz2{y0 ,...,yt ,cj ,qj ,u} f (z)
p
⌘  2↵/L3 . Then for every ku vk  ⌘/2, E XPLOITNC- PAIR 3 (f, u, v, ⌘) finds a point z such that
n
↵ 2o
↵ 2
f (z)  max f (v)
⌘ , f (u)
⌘ .
(16)
4
12
We prove Lemma 5 in Section C.1; it is essentially a more
technical version of the proof of Lemma 4, where we address the asymmetry of condition (10) by taking steps of
different sizes from u and v.
4.2. Bounding the function values of the iterates using
cubic interpolation
An important difference between Lemmas 1 and 5 is that
the former guarantees lower objective value than f (u),
while the latter only improves max{f (v), f (u)}. We invoke these lemmas for v = xj for some xj produced
by AGD- UNTIL - GUILTY, but Corollary 1 only bounds the
function value at yj and w; f (xj ) might be much larger
than f (y0 ), rendering the progress guaranteed by Lemma 5
useless. Fortunately, we are able show that whenever this
happens, there must be a point on the line that connects
xj , yj and yj 1 for which the function value is much lower
than f (y0 ). We take advantage of this fact in Alg. 5,
where we modify F IND - BEST- ITERATE to consider additional points, so that whenever the iterate it finds is not
much better than y0 , then f (xj ) is guaranteed to be close
to f (y0 ). We formalize this claim in the following lemma,
which we prove in Section C.2.
Lemma 6. Let f be L1 -smooth and have L3 -Lipschitz
conp
tinuous third-order derivatives, and let ⌧  ↵/(16L3 )
with ⌧, ↵, L1 , L3 > 0.
Consider G UARDED - NON CONVEX -AGD with F IND - BEST- ITERATE replaced by
F IND - BEST- ITERATE 3 . At any iteration, if u, v 6= NULL
and the best iterate b(1) satisfies f (b(1) )
f (y0 ) ↵⌧ 2
then,
f (v)  f (y0 ) + 14↵⌧ 2 .
We now explain the idea behind the proof of Lemma 6. Let
0  j < t be such that v = xj (such j always exists by
Corollary 1). If j = 0 then xj = y0 and the result is trivial,
so we assume j
1. Let fr : R ! R be the restriction
of f to the line containing yj 1 and yj (and also qj , cj and
xj ). Suppose now that fr is a cubic polynomial. Then, it
is completely determined by its values at any 4 points, and
f (xj ) = C1 f (qj ) + C2 f (yj 1 ) C3 f (cj ) + C4 f (yj )

for Cj
0 independent of f . By substituting the bounds
f (yj 1 ) _ f (yj )  f (y0 ) and f (qj ) ^ f (cj ) f (b(1) )
f (y0 ) ↵⌧ 2 , we obtain an upper bound on f (xj ) when fr
is cubic. To generalize this upper bound to fr with Lipschitz third-order derivative, we can simply add to it the approximation error of an appropriate third-order Taylor series expansion, which is bounded by a term proportional to
L3 ⌧ 4  ↵⌧ 2 /16.
4.3. An improved rate of convergence
With our algorithmic and analytic upgrades established,
we are ready to state the enhanced performance guarantees for G UARDED - NON - CONVEX -AGD, where from here
on we assume that E XPLOIT-NC- PAIR 3 and F IND - BESTITERATE 3 subsume E XPLOIT-NC- PAIR and F IND - BESTITERATE, respectively.
Lemma 7. Let f : Rd ! R be L1 -smooth and have L3 Lipschitz continuous third-order derivatives, let ✏, ↵ > 0
and p0 2 Rd . If pK
0 is the sequence of iterates produced
q by
G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵,
then for every 1  k < K,
⇢ 2
✏
↵2
f (pk )  f (pk 1 ) min
,
.
5↵ 32L3

2↵
L3 ),

(17)

The proof of Lemma 7 is essentially identical to the proof
of Lemma 2, where
pwe replace Lemma 1 with Lemmas 5
and 6 and set ⌧ = ↵/(32L3 ). For completeness, we give
a full proof in Section C.3. The gradient evaluation complexity guarantee for third-order smoothness then follows
precisely as in our proof of Theorem 1; see Sec. C.4 for a
proof of the following
Theorem 2. Let f : Rd ! R be L1 -smooth and have
L3 -Lipschitz continuous third-order derivatives. Let p0 2
Rd , f = f (p0 )
inf z2Rd f (z) and 0 < ✏2/3 
1/2 1/6
1/3
min{ f L3 , L1 /(8L3 )}. If we set
1/3

↵ = 2L3 ✏2/3 ,

(18)
q
2↵
G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵,
L3 )

finds a point pK such that krf (pK )k  ✏ and requires
at most
✓
◆
1/2 1/6
500L1 f
f L1 L3
20 ·
log
(19)
✏2
✏5/3
gradient evaluations.
We remark that Lemma 2 and Theorem 1 remain valid after
the modifications described in this section. Thus, Alg. 3
transitions between smoothness regimes by simply varying
the scaling of ↵ and ⌘ with ✏.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions
0

10

Gradient descent
Algorithm 3 w/o Exploit-NC-pair
Restarted AGD
Algorithm 3
Nonlinear conjugate gradient (w/ line search)

0.9

1

0.8

0.9

0.7

0.8

10-1
10-2

1
0.8

10

-3

10

-4

0.6

0.7

0.5

0.6

0.6

0.4
0.5

0.4

0.3

0.2

10

-5

10

-6

0.4
0.2
0.3

0
2
10

10

3

10

4

1

10

10

2

10

3

10

4

0.1
1
10

10

2

3

10

10

4

Figure 1. Performance on a non-convex regression problem. Left: cumulative distribution of number of steps required to achieve gradient norm < 10 4 . Center: gradient norm trace for a representative instance. Right: function value trace for the same instance. For Alg. 3, the dots correspond
to negative curvature detection and the diamonds correspond to negative curvature exploitation
(i.e. when f (b(2) ) < f (b(1) )). For RAGD, the squares indicate restarts due to non-monotonicity.

5. Preliminary experiments
The primary purpose of this paper is to demonstrate the
feasibility of acceleration for non-convex problems using
only first-order information. Given the long history of development of careful schemes for non-linear optimization,
it is unrealistic to expect a simple implementation of the
momentum-based Algorithm 3 to outperform state-of-theart methods such as non-linear conjugate gradients and LBFGS. It is important, however, to understand the degree of
non-convexity in problems we encounter in practice, and to
investigate the efficacy of the negative curvature detectionand-exploitation scheme we propose.
Toward this end, we present two experiments: (1) fitting a
non-linear regression model and (2) training a small neural
network. In these experiments we compare a basic implementation of Alg. 3 with a number baseline optimization
methods: gradient descent (GD), non-linear conjugate gradients (NCG) (Hager and Zhang, 2006), Accelerated Gradient Descent (AGD) with adaptive restart (O’Donoghue
and Candès, 2015) (RAGD), and a crippled version of
Alg. 3 without negative curvature exploitation (C-Alg. 3).
We compare the algorithms on the number of gradient
steps, but note that the number of oracle queries per step
varies between methods. We provide implementation details in Section E.1.
For our first experiment, we study robust linear regression
with the smooth
loss (Beaton and Tukey, 1974),
Pm biweight
1
T
f (x) = m
(a
x
bi ) where (✓) := ✓2 /(1 + ✓2 ).
i
i=1
For 1,000 independent experiments, we randomly generate
problem data to create a highly non-convex problem (see
Section E.2). In Figure 1 we plot aggregate convergence
time statistics, as well as gradient norm and function value
trajectories for a single representative problem instance.
The figure shows that gradient descent and C-Alg. 3 (which
does not exploit curvature) converge more slowly than the

10

1

10

2

10

3

Figure 2. Performance on neural
network training.

other methods. When C-Alg. 3 stalls it is detecting negative
curvature, which implies the stalling occurs around saddle
points. When negative curvature exploitation is enabled,
Alg. 3 is faster than RAGD, but slower than NCG. In this
highly non-convex problem, different methods often converge to local minima with (sometimes significantly) different function values. However, each method found the
“best” local minimum in a similar fraction of the generated
instances, so there does not appear to be a significant difference in the ability of the methods to find “good” local
minima in this problem ensemble.
For the second experiment we fit a neural network model1
comprising three fully-connected hidden layers containing
20, 10 and 5 units, respectively, on the MNIST handwritten
digits dataset (LeCun et al., 1998) (see Section E.3). Figure 2 shows a substantial performance gap between gradient descent and the other methods, including Alg. 3. However, this is not due to negative curvature exploitation; in
fact, Alg. 3 never detects negative curvature in this problem, implying AGD never stalls. Moreover, RAGD never
restarts. This suggests that the loss function f is “effectively convex” in large portions of the training trajectory,
consistent with the empirical observations of Goodfellow
et al. (2015); a phenomenon that may merit further investigation.
We conclude that our approach can augment AGD in the
presence of negative curvature, but that more work is necessary to make it competitive with established methods such
as non-linear conjugate gradients. For example, adaptive
schemes for setting ↵, ⌘ and L1 must be developed. However, the success of our method may depend on whether
AGD stalls at all in real applications of non-convex optimization.
1

Our approach in its current form is inapplicable to training
neural networks of modern scale, as it requires computation of
exact gradients.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Acknowledgment
OH was supported by the PACCAR INC fellowship. YC
and JCD were partially supported by the SAIL-Toyota Center for AI Research and NSF-CAREER award 1553086.
YC was partially supported by the Stanford Graduate Fellowship and the Numerical Technologies Fellowship.

References

Y. Koren, R. Bell, and C. Volinsky. Matrix factorization
techniques for recommender systems. Computer, 42(8),
2009.
Y. LeCun, C. Cortes, and C. J. Burges.
database of handwritten digits, 1998.

The MNIST

Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015.

N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and
T. Ma. Finding approximate local minima for nonconvex optimization in linear time. arXiv preprint
arXiv:1611.01146, 2016.

J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht.
Gradient descent only converges to minimizers. In 29th
Annual Conference on Learning Theory (COLT), pages
1246—1257, 2016.

A. E. Beaton and J. W. Tukey. The fitting of power series,
meaning polynomials, illustrated on band-spectroscopic
data. Technometrics, 16(2):147–185, 1974.

D. C. Liu and J. Nocedal. On the limited memory BFGS
method for large scale optimization. Mathematical Programming, 45(1):503–528, 1989.

A. Beck and M. Teboulle. Gradient-based algorithms with
applications to signal recovery. Convex optimization
in signal processing and communications, pages 42–88,
2009.

J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Supervised dictionary learning. In Advances in Neural
Information Processing Systems 21, 2008.

S. Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980, 2014.
E. J. Candès, X. Li, and M. Soltanolkotabi. Phase retrieval
via Wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.
Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for non-convex optimization. arXiv
preprint arXiv:1611.00756, 2016.
C. Cartis, N. I. Gould, and P. L. Toint. On the complexity of steepest descent, Newton’s and regularized Newton’s methods for nonconvex unconstrained optimization
problems. SIAM journal on optimization, 20(6):2833–
2852, 2010.
X. Glorot and Y. Bengio. Understanding the difficulty of
training deep feedforward neural networks. In Aistats,
volume 9, pages 249–256, 2010.
I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively
characterizing neural network optimization problems. In
International Conference on Learning Representations,
2015.
W. W. Hager and H. Zhang. A survey of nonlinear conjugate gradient methods. Pacific Journal of Optimization,
2(1):35–58, 2006.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.

K. Murty and S. Kabadi. Some NP-complete problems
in quadratic and nonlinear programming. Mathematical
Programming, 39:117–129, 1987.
A. Nemirovski.
Optimization II: Standard numerical methods for nonlinear continuous optimization.
Technion – Israel Institute of Technology,
1999. URL http://www2.isye.gatech.edu/
˜nemirovs/Lect_OptII.pdf.
A. Nemirovski and D. Yudin. Problem Complexity and
Method Efficiency in Optimization. Wiley, 1983.
Y. Nesterov. A method of solving a convex programming
problem with convergence rate O(1/k 2 ). Soviet Mathematics Doklady, 27(2):372–376, 1983.
Y. Nesterov. Squared functional systems and optimization problems. In High Performance Optimization,
volume 33 of Applied Optimization, pages 405–440.
Springer, 2000.
Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.
Y. Nesterov. How to make the gradients small. Optima 88,
2012.
Y. Nesterov and B. T. Polyak. Cubic regularization of Newton method and its global performance. Mathematical
Programming, 108(1):177–205, 2006.
J. Nocedal and S. J. Wright.
Springer, 2006.

Numerical Optimization.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

B. O’Donoghue and E. Candès. Adaptive restart for accelerated gradient schemes. Foundations of Computational
Mathematics, 15(3):715–732, 2015.
E. Polak and G. Ribière. Note sur la convergence de directions conjugées. Rev. Fr. Inform. Rech. Oper. v16, pages
35–43, 1969.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In
D. E. Rumelhart and J. L. McClelland, editors, Parallel
Distributed Processing – Explorations in the Microstructure of Cognition, chapter 8, pages 318–362. MIT Press,
1986.
G. Wang, G. B. Giannakis, and Y. C. Eldar. Solving systems of random quadratic equations via truncated amplitude flow. arXiv:1605.08285 [stat.ML], 2016.

