Doubly Accelerated Methods for Faster CCA
and Generalized Eigendecomposition

Zeyuan Allen-Zhu * 1 Yuanzhi Li * 2

1

Abstract

tradition of (Wang et al., 2016; Garber & Hazan, 2015), we

We study k-GenEV, the problem of finding the
top k generalized eigenvectors, and k-CCA, the
problem of finding the top k vectors in canonicalcorrelation analysis. We propose algorithms
LazyEV and LazyCCA to solve the two problems
with running times linearly dependent on the input size and on k. Furthermore, our algorithms
are doubly-accelerated: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both kGenEV or k-CCA. We also provide the first gapfree results,√which provide running times that depend on 1/ ε rather than the eigengap.

assume without loss of generality that λi ∈ [−1, 1].

Introduction

The Generalized Eigenvector (GenEV) problem and the
Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics. Algorithms solving these
problems are often used to extract features to compare
large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al.,
2009), classification (Karampatziakis & Mineiro, 2014),
word embeddings (Dhillon et al., 2011), and many others.
GenEV. Given two symmetric matrices A, B ∈ Rd×d
where B is positive definite. The GenEV problem is to find
generalized eigenvectors v1 , . . . , vd where each vi satisfies
 >


v Bv = 1
vi ∈ arg max v > Av  s.t.
v > Bvj = 0 ∀j ∈ [i − 1]
d
v∈R
def

The values λi = vi> Avi are known as the generalized
eigenvalues, and it satisfies |λ1 | ≥ · · · |λd |. Following the
*

Equal contribution .
Future version of this paper
shall be found at http://arxiv.org/abs/1607.06017.
1
Microsoft Research 2 Princeton University. Correspondence
to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li
<yuanzhil@cs.princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

CCA. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = n1 X > X, Sxy = n1 X > Y , Syy = n1 Y > Y ,
the CCA problem is to find canonical-correlation vectors
{(φi , ψi )}ri=1 where r = min{dx , dy } and each pair
 >
	
(φi , ψi ) ∈ arg max
φ Sxy ψ
φ∈Rdx ,ψ∈Rdy


such that

φ> Sxx φ = 1 ∧ φ> Sxx φj = 0 ∀j ∈ [i − 1]
ψ > Syy ψ = 1 ∧ ψ > Syy ψj = 0 ∀j ∈ [i − 1]
def

The values σi = φ>
i Sxy ψi ≥ 0 are known as the
canonical-correlation coefficients, and
1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisfied.
It is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx , Syy } ∈
def
>
Rd×d and A = [[0, Sxy ]; [Sxy
, 0]] ∈ Rd×d for d = dx +dy ;
see Lemma 2.3. (This reduction does not always hold if
the generalized eigenvectors are computed only approximately.)
Despite the fundamental importance and the frequent necessity in applications, there are few results on obtaining
provably efficient algorithms for GenEV and CCA until
very recently. In the breakthrough result of Ma, Lu and
Foster (Ma et al., 2015), they proposed to study algorithms
to find top k generalized eigenvectors (k-GenEV) or top k
canonical-correlation vectors (k-CCA). They designed an
alternating minimization algorithm whose running time is
only linear in the input matrix sparsity and nearly-linear in
k. Such algorithms are very appealing because in real-life
applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in
the directions where the datasets do not correlate. Unfortunately, the method of Ma, Lu and Foster has a running time
that linearly scales with κ and 1/gap, where
• κ ≥ 1 is the condition number of matrix B in GenEV,
or of matrices X > X, Y > Y in CCA; and
k+1
in GenEV, or
• gap ∈ [0, 1) is the eigengap λk −λ
λk
σk −σk+1
in
CCA.
σk
These parameters are usually not constants and scale with

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

the problem size.
Challenge 1: Acceleration
For many easier scientific computing problems, we are able
to design algorithms that have accelerated dependencies on
κ and 1/gap. As two concrete examples, k-PCA can be
√
solved with a running time linearly in 1/ gap as opposed
to 1/gap (Golub & Van Loan, 2012); computing √
B −1 w
for a vector w can be solved in time linearly in κ as
opposed to κ, where κ is the condition number of matrix
B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).
Therefore, can we obtain doubly-accelerated methods for
k-GenEV and k-CCA,√meaning that the running times lin√
early scale with both κ and 1/ gap? Before this paper,
for the general case k > 1, the method of Ge et al. (Ge
et al., 2016) made acceleration possible for parameter κ,
but not for parameter 1/gap (see Table 1).
Challenge 2: Gap-Freeness
Since gap can be even zero in the extreme case, can we
design algorithms that do not scale with 1/gap? Recall
that this is possible for the easier task of k-PCA. The block
Krylov
Musco, 2015) runs in time linear
√ method (Musco &√
in 1/ ε as opposed to 1/ gap, where ε is the approximation ratio. There is no gap-free result previously known for
k-GenEV or k-CCA even for k = 1.
Challenge 3: Stochasticity
For matrix-related problems, one can usually obtain
stochastic running times which requires some notations to
describe.
Consider a simple task of computing B −1 w for some vector √
w, where accelerated methods solve it in time linearly
in κ for κ being the condition number of B. If B =
1
>
n X X is given in the form of a covariance matrix where
X ∈ Rn×d , then (accelerated) stochastic
methods compute
p
√
B −1 w in a time linearly in (1 + κ0 /n) instead of κ,
2


maxi∈[n] {kXi k }
where κ0 =
∈ κ, nκ and Xi is the i-th
λmin (B)
p
√
row of X. (See Lemma 2.6.) Since 1 + κ0 /n ≤ O( κ),
stochastic methods are no slower than non-stochastic ones.
So, can we obtain a similar but doubly-accelerated
stochastic method for k-CCA?1 Note that, if the doublyaccelerated requirement is dropped, this task is easier and
indeed possible, see Ge et al. (Ge et al., 2016). However,
since their stochastic method is not doubly-accelerated, in
certain parameter regimes, it runs even slower than nonstochastic ones (even for k = 1, see Table 2).
Remark. In general, if designed properly, for worst case
running time:
• Accelerated results are usually better because they are
1

Note that a similar problem can be also asked for k-GenEV
when A and B are both given in their covariance matrix forms.
We refrain from doing it in this paper for notational simplicity.

no slower than non-accelerated ones in the worst-case.
• Gap-free results are better because they imply gapdependent ones.2
• Stochastic results are usually better because they are no
slower than non-stochastic ones in the worst-case.
1.1

Our Main Results

We provide algorithms LazyEV and LazyCCA that are
doubly-accelerated, gap-free, and stochastic.3
For the general k-GenEV problem, our LazyEV can be implemented to run in time4
√

κ knnz(A) + k 2 d 
e knnz(B)
or
+
O
√
√
gap
gap
√

κ knnz(A) + k 2 d 
e knnz(B)
√
√
O
+
ε
ε
in the gap-dependent and gap-free cases respectively.
Since
√
√
κ
and
gap
our running
time
only
linearly
depends
on
√
(resp. ε), our algorithm LazyEV is doubly-accelerated.
For the general k-CCA problem, our LazyCCA can be implemented to run in time
 knnz(X, Y ) · 1 + pκ0 /n + k 2 d 
e
O
or
√
gap
 knnz(X, Y ) · 1 + pκ0 /n + k 2 d 
e
√
O
ε
in the gap-dependent and gap-free cases respectively.
Here, nnz(X, Y ) = nnz(X) + nnz(Y ) and κ0 =
2 maxi {kXi k2 ,kYi k2 }
λmin (diag{Sxx ,Syy }) where Xi or Yi is the i-th row vector
of X or Y . Therefore, our algorithm LazyCCA is doublyaccelerated and stochastic.
We fully compare our results with prior work in Table 2
(for k = 1) and Table 1 (for k ≥ 1), and summarize our
main contributions:
• For k > 1, we outperform all relevant prior works (see
Table 1). Moreover, no known method was doublyaccelerated even in the non-stochastic setting.
• For k ≥ 1, we obtain the first gap-free running time.
• Even for k = 1, we outperform most of the state-ofthe-arts (see Table 2).
Note that for CCA with k > 1, previous result CCALin
only outputs the subspace spanned by the top k correlation
vectors but does not identify which vector gives the highest
correlation and so on. Our LazyCCA provides per-vector
2
If a method depends on 1/ε then one can choose ε = gap
and this translates to a gap-dependent running time.
3
Recalling Footnote 1, for notational simplicity, we only state
our k-GenEV result in non-stochastic running time.
4
e notation to hide polyThroughout the paper, we use the O
logarithmic factors with respect to κ, 1/gap, 1/ε, d, n. We use
nnz(M ) to denote the time needed to multiply M to a vector.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition
Problem

Paper

Running time

LazyEV Theorem 4.3

e
O
e
O

LazyEV Theorem 4.4

e
O

GenELin (Ge et al., 2016)
k-GenEV
Problem

Paper

√
knnz(B) κB
gap
√
knnz(B) κB
√
gap
√
knnz(B) κB
√
ε

(× for outperformed)
+
+
+

knnz(A)+k2 d 
gap
knnz(A)+k2 d 
√
gap
knnz(A)+k2 d 
√
ε

Running time

AppGrad (Ma et al., 2015)
CCALin (Ge et al., 2016)
CCALin (Ge et al., 2016)
LazyCCA (arXiv version)
k-CCA
LazyCCA (arXiv version)

e
O
e
O
e
O
e
O
e
O

√

knnz(X,Y )· 1+

(local conv.)

no

no

yes

yes

yes

gap-free?

stochastic?

×

no

no

×

no

no

×

no

yes

no

yes

yes

yes

no

doubly

yes

doubly



κ0 /n +k2 d 

gap

√



knnz(X,Y )· 1+ κ0 /n +k2 d 
√
gap


√

knnz(X,Y )· 1+
√
ε

κ0 /n

e knnz(X, Y ) · 1 +
O

LazyCCA (arXiv version)

e knnz(X, Y ) · 1 +
O

2

+k d 
κ0



√
gap·σk ·(nnz(X,Y )/kd)1/4
√

κ0
√
ε·σk ·(nnz(X,Y )/kd)1/4

Table 1: Performance comparison on k-GenEV and k-CCA.
λ −λ
(B)
In GenEV, gap = k λkk+1 ∈ [0, 1] and κB = λλmax
> 1.
min (B)
∈ [0, 1], κ =

no

(× for outperformed)

knnz(X,Y )·κ+k2 d 
gap
√
knnz(X,Y )· κ+k2 d 
gap

LazyCCA (arXiv version)

σk −σk+1
σk

negative EV?

×

√

In CCA, gap =

gap-free?

λmax (diag{Sxx ,Syy })
λmin (diag{Sxx ,Syy })

> 1, κ0 =

2 maxi {kXi k2 ,kYi k2 }
λmin (diag{Sxx ,Syy })

∈ [κ, 2nκ], and σk ∈ [0, 1].

Remark 1. Stochastic methods depend on a modified condition number κ0 . The reason κ0 ∈ [κ, 2nκ] is in Fact 2.5.
p
Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + κ0 /n ≤ O(κ).
Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.

guarantees on all the top k correlation vectors.
1.2

Our Side Results on Doubly-Stochastic Methods

Recall that when considering acceleration, there are two
parameters κ and 1/gap. One can also design stochastic methods with respect to both parameters κ and 1/gap,
meaning that
√ 0 c
κ /n
with a running time proportional to 1 + √gap
√
√
1+ κ0 /n
κ
instead of √gap (stochastic) or √gap
(non-stochastic).
The constant c is usually 1/2. We call such methods
doubly-stochastic.
Unfortunately, doubly-stochastic methods are usually
slower than stochastic ones. Take 1-CCA as an example.
The best stochastic running time (obtained
exclusively by
√
1+ κ0 /n 
e
. In contrast,
us) for 1-CCA is nnz(X, Y ) · O √
gap

if one uses a doubly-stochastic method —either (Wang
et al., 2016) or our LazyCCA—
the running time becomes
√
κ0 /n1/4 
e
nnz(X, Y ) · O 1 + √gap·σ1 . Therefore, for 1-CCA,
doubly-stochastic methods are faster than stochastic ones
only when

κ0
σ1

≤ o(n1/2 ) .

The above condition is usually not satisfied. For instance,
• κ0 is usually around n for most interesting data-sets, cf.

the experiments of (Shalev-Shwartz & Zhang, 2014);
• κ0 is between n1/2 and 100n in all the CCA experiments of (Wang et al., 2016); and
• by Fact 2.5 it satisfies κ0 ≥ d so κ0 cannot be smaller
than o(n1/2 ) unless d  n1/2 .5 Even worse, parameter
σ1 ∈ [0, 1] is usually much smaller than 1. Note that σ1
is scaling invariant: even if one scales X and Y up by
the same factor, σ1 remains unchanged.
Nevertheless, to compare our LazyCCA with all relevant
prior works, we obtain doubly-stochastic running times for
k-CCA as well. Our running time matches that of (Wang
et al., 2016) when k = 1, and no doubly-stochastic running
time for k > 1 was known before our work.
1.3

Other Related Works

For the easier task of PCA and SVD, the first gap-free
result was obtained by Musco and Musco (Musco &
Musco, 2015), the first stochastic result was obtained by
Shamir (Shamir, 2015), and the first accelerated stochastic result was obtained by Garber et al. (Garber & Hazan,
2015; Garber et al., 2016). The shift-and-invert preconditioning technique of Garber et al. is also used in this paper.
For another related problem PCR (principle compo5
Note that item (3) κ0 ≥ d may not hold in the more general
setting of CCA, see Remark A.1.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition
Problem

1-GenEV

Paper
GenELin (Ge et al., 2016)

e
O

LazyEV Theorem 4.3

e
O

LazyEV Theorem 4.4

e
O

Problem

1-CCA

Running time

Paper

√
nnz(B) κB
gap
√
nnz(B) κB
√
gap
√
nnz(B) κB
√
ε

(× for outperformed)

+ nnz(A)
×
gap
nnz(A) 
+ √gap

√
+ nnz(A)
ε

gap-free?

negative EV?

no

no

no

yes

yes

yes

(× for outperformed)

×

gap-free?

stochastic?

no

no

×

no

no

×

no

no

×

no

no

×

no

yes

×

no

yes

no

yes

yes

yes

no

doubly

no

doubly

yes

doubly

Running time

AppGrad (Ma et al., 2015)

e
nnz(X, Y ) · O

CCALin (Ge et al., 2016)

e
nnz(X, Y ) · O

ALS (Wang et al., 2016)

e
nnz(X, Y ) · O

SI (Wang et al., 2016)

e
nnz(X, Y ) · O

CCALin (Ge et al., 2016)

e
nnz(X, Y ) · O

ALS (Wang et al., 2016)

e
nnz(X, Y ) · O

LazyCCA (arXiv version)

e
nnz(X, Y ) · O

LazyCCA (arXiv version)

κ
gap
√ 
κ
gap
√ 
κ
gap2
√

κ
√
gap·σ1

√

κ0 /n 
gap
√
1+ κ0 /n 
gap2
√
1+ κ0 /n 
√
gap
1+

√

e
nnz(X, Y ) · O


e 1
nnz(X, Y ) · O

SI (Wang et al., 2016)

κ0 /n 
√
ε

√
0 /n1/4
+ √κgap·σ
1

1+

(see Remark 3)

LazyCCA (arXiv version)

e 1+
nnz(X, Y ) · O

LazyCCA (arXiv version)

e 1+
nnz(X, Y ) · O

√
κ0 /n1/4 
√
gap·σ1
√
κ0 /n1/4 
√
ε·σ1

Table 2: Performance comparison on 1-GenEV and 1-CCA.
(B)
2
In GenEV, gap = λ1λ−λ
∈ [0, 1] and κB = λλmax
> 1.
1
min (B)
In CCA, gap =

σ1 −σ2
σ1

∈ [0, 1], κ =

λmax (diag{Sxx ,Syy })
λmin (diag{Sxx ,Syy })

> 1, κ0 =

2 maxi {kXi k2 ,kYi k2 }
λmin (diag{Sxx ,Syy })

∈ [κ, 2nκ], and σ1 ∈ [0, 1].

Remark 1. Stochastic methods depend on modified condition number κ0 ; the reason κ0 ∈ [κ, 2nκ] is in Def. 2.4.
p
Remark 2. All non-stochastic CCA methods in this table have been outperformed because 1 + κ0 /n ≤ O(κ).
Remark 3. Doubly-stochastic methods are not necessarily interesting. We discuss them in Section 1.2.
Remark 4. Some CCA methods have a running time dependency on σ1 ∈ [0, 1], and this is intrinsic and cannot be removed.
In particular, if we scale the data matrix X and Y , the value σ1 stays the same.
Remark 5. The only (non-doubly-stochastic) doubly-accelerated
method
p before our work is SI (Wang et al., 2016) (for 1-CCA
p
only). Our LazyEV is faster than theirs by a factor Ω( nκ/κ0 × 1/σ1 ). Here, nκ/κ0 ≥ 1/2 and 1/σ1 ≥ 1 are two
scaling-invariant quantities usually much greater than 1.

nent regression), we recently obtained an accelerated
method (Allen-Zhu & Li, 2017) as opposed the previously
non-accelerated one (Frostig et al., 2016); however, the acceleration techniques there are not relevant to this paper.
For GenEV and CCA, many scalable algorithms have been
designed recently (Ma et al., 2015; Wang & Livescu, 2015;
Michaeli et al., 2015; Witten et al., 2009; Lu & Foster,
2014). However, as summarized by the authors of CCALin,
these cited methods are more or less heuristics and do not
have provable guarantees. Furthermore, for k > 1, the
AppGrad method (Ma et al., 2015) only provides local convergence guarantees and thus requires a warm-start whose

computational complexity is not discussed in their paper.
Finally, our algorithms on GenEV and CCA are based on
finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop
the algorithm whenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases
(such as GenELin, CCALin, AppGrad) find all k vectors at
once, therefore requiring k to be known beforehand. As a
separate note, these known approaches do not need the user
to know the desired accuracy a priori but our LazyEV and
LazyCCA algorithms do.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

2

Preliminaries

We denote by kxk or kxk2 the Euclidean norm of vector
x. We denote by kAk2 , kAkF , and kAkSq respectively the
spectral, Frobenius, and Schatten q-norm of matrix A (for
q ≥ 1). We write A  B if A, B are symmetric and A − B
is positive semi-definite (PSD), and write A  B if A, B
are symmetric but A − B is positive definite (PD). We denote by λmax (M ) and λmin (M ) the largest and smallest
eigenvalue of a symmetric matrix M , and by κM the condition number λmax (M )/λmin (M ) of a PSD matrix M .
Throughout this paper, we use nnz(M ) to denote the time
to multiply matrix M to any arbitrary vector. For two matrices X, Y , we denote by nnz(X, Y ) = nnz(X)+nnz(Y ),
and by Xi or Yi the i-th row vector of X or Y . We
also use poly(x1 , x2 , . . . , xt ) to represent a quantity that
is asymptotically at most polynomial in terms of variables x1 , . . . , xt . Given a column orthonormal matrix
U ∈ Rn×k , we denote by U ⊥ ∈ Rn×(n−k) the column
orthonormal matrix consisting of an arbitrary basis in the
space orthogonal to the span of U ’s columns.
Given a PSD matrix B and a vector v, v > Bv is the B-seminorm of v. Two vectors v, w are B-orthogonal if v > Bw =
0. We denote by B −1 the Moore-Penrose pseudoinverse
of B if B is not invertible, and by B 1/2 the matrix square
root of B (satisfying B 1/2  0). All occurrences of B −1 ,
B 1/2 and B −1/2 are for analysis purpose only. Our final
algorithms only require multiplications of B to vectors.
Definition 2.1 (GenEV). Given symmetric matrices
A, B ∈ Rd×d where B is positive definite. The generalized eigenvectors of A with respect to B are v1 , . . . , vd ,
where each vi is
(
)
 > 
v > Bv = 1


vi ∈ arg max v Av s.t. >
v Bvj = 0 ∀j ∈ [i − 1]
v∈Rd
The generalized eigenvalues λ1 , . . . , λd satisfy λi =
vi> Avi which can be negative.
Following (Wang et al., 2016; Garber & Hazan, 2015), we
assume without loss of generality that λi ∈ [−1, 1].
Definition 2.2 (CCA). Given X ∈ Rn×dx , Y ∈ Rn×dy ,
letting Sxx = n1 X > X, Sxy = n1 X > Y , Syy = n1 Y > Y ,
the canonical-correlation vectors are {(φi , ψi )}ri=1
where r = min{dx , dy } and for all i ∈ [r]:
(
(φi , ψi ) ∈

arg max

φ> Sxy ψ

such that

φ∈Rdx ,ψ∈Rdy

)
n φ> S φ = 1 ∧ φ> S φ = 0 ∀j ∈ [i − 1] o
xx
xx j
ψ > Syy ψ = 1 ∧ ψ > Syy ψj = 0 ∀j ∈ [i − 1]
The corresponding canonical-correlation coefficients
σ1 , . . . , σr satisfy σi = φ>
i Sxy ψi ∈ [0, 1].

We emphasize that σi always lies in [0, 1] and is scalinginvariant. When dealing with a CCA problem, we also denote by d = dx + dy .
Lemma 2.3 (CCA to GenEV). Given a CCA problem with
matrices X ∈ Rn×dx , Y ∈ Rn×dy , let the canonicalr
correlation vectors and coefficients be {(φ
 i , ψi , σi)}i=1
0

Sxy

where r = min{dx , dy }. Define A = S > 0
and
xy


Sxx 0
B =
0 Syy . Then, the GenEV problem of A with re-

r
spect to B has 2r eigenvalues
i=1 and
n{±σi }
ocorrespondn
φi
−φi
. The reing generalized eigenvectors
ψi ,
ψi
i=1
maining dx + dy − 2r eigenvalues are zeros.

Definition 2.4. In CCA, let A and B be as defined in
Lemma 2.3. We define condition numbers
def

κ = κB =

λmax (B)
λmin (B)

def

and κ0 =

2 maxi {kXi k2 ,kYi k2 }
λmin (B)

.

Fact 2.5. κ0 ∈ [κ, 2nκ] and κ0 ≥ d. (See full version.)
Lemma 2.6. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy ,
let A and B be as defined in Lemma 2.3. For every w ∈
Rd , the Katyusha method (Allen-Zhu, 2017) finds a vector
w0 ∈ Rd satisfying kw0 − B −1 Awk ≤ ε in time

p

κkwk2 
O nnz(X, Y ) · 1 + κ0 /n · log
.
ε

3

Leading Eigenvector via Two-Sided
Shift-and-Invert

We introduce AppxPCA± , the multiplicative approximation
algorithm for computing the two-sided leading eigenvector
of a symmetric matrix. AppxPCA± uses the shift-and-invert
framework (Garber & Hazan, 2015; Garber et al., 2016),
and shall become our building block for the LazyEV and
LazyCCA algorithms in the subsequent sections.
Our pseudo-code Algorithm 1 is a modification of Algorithm 5 in (Garber & Hazan, 2015), and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle A. The main differences between AppxPCA±
and (Garber & Hazan, 2015) are two-fold.
First, given a symmetric matrix M , AppxPCA± simultaneously considers an upper-bounding shift together with a
lower-bounding shift, and try to perform power methods
with respect to (λI − M )−1 and (λI + M )−1 . This allows us to determine approximately how close λ is to the
largest and the smallest eigenvalues of M , and decrease λ
accordingly. In the end, AppxPCA± outputs an approximate
eigenvector of M that corresponds to a negative eigenvalue
if needed. Second, we provide a multiplicative-error guarantee rather than additive as appeared in (Garber & Hazan,
2015). Without such guarantee, our final running time will
1
1 6
depend on gap·λmax
(M ) rather than gap .
6

This is why the SI method of (Wang et al., 2016) also uses

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm 1 AppxPCA± (A, M, δ× , ε, p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d , a symmetric matrix satisfying −I  M  I; δ× ∈
(0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the confidence parameter.
1: w
b0 ← RanInit(d);
s ← 0; λ(0)
+ δ×
 wb0 is a random unit vector, see Def. 3.2
 ← 136dθ
;
288dθ
,
m
←
log
;

θ
is the parameter of RanInit, see Def. 3.2
2: m1 ← 4 log
2
p2
p2 ε
δ× m1
δ × m 2
1
ε
3: εe1 ← 64m1 48
and εe2 ← 8m2 48
4: repeat
 m1 = T PM (8, 1/32, p) and m2 = T PM (2, ε/4, p), see Lemma B.1
5:
s ← s + 1;
6:
for t = 1 to m1 do


7:
Apply A to find w
bt satisfying w
bt − (λ(s−1) I − M )−1 w
bt−1  ≤ εe1 ;
8:
9:
10:
11:

(s−1)
wa ← w
bm1 /kw
bm1 k;
I − M )−m1 w
b0 then normalized

  wa is roughly (λ
(s−1)
−1
I − M ) wa  ≤ εe1 ;
Apply A to find va satisfying va − (λ
for t = 1 to m1 do


Apply A to find w
bt satisfying w
bt − (λ(s−1) I + M )−1 w
bt−1  ≤ εe1 ;

12:
13:

(s−1)
wb ← w
bm1 /kw
bm1 k;
I + M )−m1 w
b0 then normalized

  wb is roughly (λ
(s−1)
−1


Apply A to find vb satisfying vb − (λ
I + M ) wb ≤ εe1 ;

14:

∆(s) ←

1
2

1
> v ,w > v }−e
max{wa
ε1
a
b b
δ× λ(s)
12

·

and λ(s) ← λ(s−1) −

∆(s)
2 ;

15: until ∆(s) ≤
16: f ← s;
17: if the last wa> va ≥ wb> vb then
18:
for t = 1 to m2 do


19:
Apply A to find w
bt satisfying w
bt − (λ(f ) I − M )−1 w
bt−1  ≤ εe2 ;
def

20:
return (+, w) where w = w
bm2 /kw
bm2 k.
21: else
22:
for t = 1 to m2 do


bt − (λ(f ) I + M )−1 w
bt−1  ≤ εe2 ;
23:
Apply A to find w
bt satisfying w
def

24:
return (−, w) where w = w
bm2 /kw
bm2 k.
25: end if

We prove in full version the following theorem:
Theorem 3.1 (AppxPCA± , informal). Let M ∈ Rd×d be a
symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · · ≥ λd ≥
−1 and eigenvectors u1 , . . . , ud . Let λ∗ = max{λ1 , −λd }.
With probability at least 1 − p, AppxPCA± produces a pair
(sgn, w) satisfying
• if sgn = +, then w is an approx. positive eigenvector:

X
δ×  ∗ ^
w> M w ≥ 1−
λ
(w> ui )2 ≤ ε
2
i∈[d]
λi ≤(1−δ× /2)λ∗

•

λmax (λ(s) I−M ) λmax (λ(s) I+M )
,
λmin (λ(s) I−M ) λmin (λ(s) I+M )

•

1
1
,
λmin (λ(s) I−M ) λmin (λ(s) I+M )

∈ [1, δ96
] and
×
≤

48
δ× λ∗ .

We remark here that, unlike the original shift-and-invert
method which chooses a random (Gaussian) unit vector in
Line 1 of AppxPCA± , we have allowed this initial vector to
be generated from an arbitrary θ-conditioned random vector generator (for later use), defined as follows:
Definition 3.2.
An algorithm RanInit(d) is a θconditioned random vector generator if w = RanInit(d)
is a d-dimensional unit vector and, for every p ∈ (0, 1),
every unit vector u ∈ Rd , with probability at least 1 − p, it
2
satisfies (u> w)2 ≤ p9dθ .

• if sgn = −, then w is an approx. negative eigenvector:
 δ  ^
X
×
w> M w ≤ − 1−
λ∗
(w> ui )2 ≤ ε
2
This modification is needed in order to obtain our efficient
i∈[d]
λi ≥−(1−δ× /2)λ∗
implementations of GenEV and CCA. One can construct a
θ-conditioned random vector generator as follows:
e
The number of oracle calls to A is O(log(1/δ× )), and each
Proposition 3.3. Given a PSD matrix B ∈ Rd×d , if we set
1/2
def
time we call A it satisfies
RanInit(d) = B v where v is a random Gaussian
(v > Bv)0.5

shift-and-invert but depends on

1
gap·σ1

in Table 2.

vector, then RanInit(d) is a θ-conditioned random vector

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

generator for θ = κB .

4

LazyEV: Generalized Eigendecomposition

In this section, we construct an algorithm LazyEV that,
given symmetric matrix M ∈ Rd×d , computes approximately the k leading eigenvectors of M that have the largest
absolute eigenvalues. Then, for the original k-GenEV
problem, we set M = B −1/2 AB −1/2 and run LazyEV.
This is our plan to find the top k leading generalized eigenvectors of A with respect to B.
Our algorithm LazyEV is formally stated in Algorithm 2.
The algorithm applies k times AppxPCA± , each time computing an approximate leading eigenvector of M with a
multiplicative error δ× /2, and projects the matrix M into
the orthogonal space with respect to the obtained leading
eigenvector. We state our main approximation theorem below.
Theorem 4.1 (informal). Let M ∈ Rd×d be a symmetric
matrix with eigenvalues λ1 , . . . , λd ∈ [−1, 1] and corresponding eigenvectors u1 , . . . , ud , and |λ1 | ≥ · · · ≥ |λd |.
If εpca is sufficiently small,7 LazyEV outputs a (column) orthonormal matrix Vk = (v1 , . . . , vk ) ∈ Rd×k which, with
probability at least 1 − p, satisfies:
(a) kVk> U k2 ≤ ε where U = (uj , . . . , ud ) and j is the
smallest index satisfying |λj | ≤ (1 − δ× )λk .
1
|λi |.
(b) For every i ∈ [k], (1−δ× )|λi | ≤ |vi> M vi | ≤ 1−δ
×
Above, property (a) ensures the k columns of Vk have negligible correlation with the eigenvectors of M whose absolute eigenvalues are ≤ (1 − δ× )λk ; property (b) ensures
the Rayleigh quotients vi> M vi are all correct up to a 1±δ×
error. We in fact have shown two more useful properties in
the full version that may be of independent interest.
The next theorem states that, if M = B −1/2 AB −1/2 , our
LazyEV can be implemented without the necessity to compute B 1/2 or B −1/2 .
Theorem 4.2 (running time). Let A, B ∈ Rd×d be two
symmetric matrices satisfying B  0 and −B  A  B.
Suppose M = B −1/2 AB −1/2 and RanInit(d) is defined
in Proposition 3.3 with respect to B. Then, the computation of V ← B −1/2 LazyEV(A, M, k, δ× , εpca , p) can be
implemented to run in time


2
e knnz(B)+k
√ d+kΥ where Υ is the time to multiply
• O
δ×

B −1 A to a vector, or
 √

2
d
e k κB nnz(B)+knnz(A)+k
√
• O
if we use Conjugate graδ×

dient to multiply B −1 A to a vector.

1|
Meaning εpca ≤ O poly(ε, δ× , |λ|λ
, 1 ) . The complete
k+1 | d
specifications of εpca is included in the full version. Since our final
running time only depends on log(1/εpca ), we have not attempted
to improve the constants in this polynomial dependency.
7

Choosing parameter δ× as either gap or ε, our two main
theorems above immediately imply the following results
for the k-GenEV problem: (proved in full version)
Theorem 4.3 (gap-dependent GenEV, informal). Let
A, B ∈ Rd×d be two symmetric matrices satisfying B 
0 and −B  A  B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are
{(λi , ui )}di=1 , and it satisfies 1 ≥ |λ1 | ≥ · · · ≥ |λd |.
Then, LazyEV outputs V k ∈ Rd×k satisfying
>

V k BV k = I
in time

>

and kV k BW k2 ≤ ε
 k √κ nnz(B) + knnz(A) + k 2 d 
B
e
O
√
gap

Here, W = (uk+1 , . . . , ud ) and gap =

|λk |−|λk+1 |
.
|λk |

Theorem 4.4 (gap-free GenEV, informal). In the same
setting as Theorem 4.3, our LazyEV outputs V k =
>
(v 1 , . . . , v k ) ∈ Rd×k satisfying V k BV k = I and
 h

|λs | i

∀s ∈ [k] : v >
s Av s ∈ (1 − ε)|λs |,
1−ε
 k √κ nnz(B) + knnz(A) + k 2 d 
B
e
√
.
in time O
ε

5

Ideas Behind Theorems 4.1 and 4.2

In Section 5.1 we discuss how to ensure accuracy: that is,
why does LazyEV guarantee to approximately find the top
eigenvectors of M . In the full version of this paper, we also
discuss how to implement LazyEV without compute B 1/2
explicitly, thus proving Theorem 4.2.
5.1

Ideas Behind Theorem 4.1

Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative
methods to find the top k eigenvectors of a PSD matrix M (Allen-Zhu & Li, 2016). That method is called
LazySVD and we summarize it as follows.
At a high level, LazySVD finds the top k eigenvectors oneby-one and approximately. Starting with M0 = M , in the
s-th iteration where s ∈ [k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it
vs . Then, LazySVD projects Ms ← (I − vs vs> )Ms−1 (I −
vs vs> ) and proceeds to the next iteration.
While the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas. Most
notably, if vs is an approximate leading eigenvector of
Ms−1 , then one needs to prove that the small eigenvectors
of Ms−1 somehow still “embed” into that of Ms after projection. This is achieved by a gap-free variant of the Wedin
theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview
section of (Allen-Zhu & Li, 2016).

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Algorithm 2 LazyEV(A, M, k, δ× , εpca , p)
Input: A, an approximate matrix inversion method; M ∈ Rd×d , a matrix satisfying −I  M  I; k ∈ [d], the desired
rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a confidence parameter.
1: M0 ← M ; V0 = [];
2: for s = 1 to k do
3:
(∼, vs0 ) ← AppxPCA± (A, M
 vs0 is an approximate two-sided leading eigenvector of Ms−1
s−1 , δ× /2, εpca , p/k);

⊥
>
0
>
0
 project vs0 to Vs−1
4:
vs ← (I − Vs−1 Vs−1 )vs /(I − Vs−1 Vs−1 )vs ;
5:
Vs ← [Vs−1 , vs ];
6:
Ms ← (I − vs vs> )Ms−1 (I − vs vs> )
 we also have Ms = (I − Vs Vs> )M (I − Vs Vs> )
7: end for
8: return Vk .

In this paper, to relax the assumption that M is PSD, and to
find leading eigenvectors whose absolute eigenvalues are
large, we have to make several non-trivial changes. On
the algorithm side, LazyEV uses our two-sided shift-andinvert method in Section 3 to find the leading eigenvector
of Ms−1 with largest absolute eigenvalue. On the analysis
side, we have to make sure all lemmas properly deal with
negative eigenvalues. For instance:
• If we perform a projection M 0 ← (I − vv > )M (I −
vv > ) where v correlates by at most ε with all eigenvectors of M whose absolute eigenvalues are smaller than a
threshold µ, then, after the projection, we need to prove
that these eigenvectors can be approximately “embedded” into the eigenspace spanned by all eigenvectors of
M 0 whose absolute eigenvalues are smaller than µ + τ .
The approximation of this embedding should depend on
ε, µ and τ .
The full proof of Theorem 4.1 is in the arXiv version. It relies on a few matrix algebraic lemmas (including the aforementioned “embedding lemma”).

6

Conclusion

In this paper we propose new iterative methods to solve
the generalized eigenvector and the canonical correlation
analysis problems. Our methods find the most significant k
eigenvectors or correlation vectors, and have running times
that linearly scales with k.
Most importantly, our methods are doubly-accelerated: the
running times have square-root dependencies both with respect to the condition number of the matrix (i.e., κ) and
with respect to the eigengap (i.e., gap). They are the first
doubly-accelerated iterative methods at least for k > 1.
They can also be made gap-free, and are the first gap-free
iterative methods even for 1-GenEV or 1-CCA.
Although this is a theory paper, we believe that if implemented carefully, our methods can outperform not only
previous iterative methods (such as GenELin, AppGrad,
CCALin), but also the commercial mathematics libraries
for sparse matrices of dimension more than 10, 000. We

leave it a future work for such careful comparisons.

References
Allen-Zhu, Zeyuan. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.
Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016.
Allen-Zhu, Zeyuan and Li, Yuanzhi. Faster Principal Component Regression and Stable Matrix Chebyshev Approximation. In Proceedings of the 34th International
Conference on Machine Learning, ICML ’17, 2017.
Allen-Zhu, Zeyuan and Orecchia, Lorenzo. Linear Coupling: An Ultimate Unification of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS ’17, 2017.
Allen-Zhu, Zeyuan and Yuan, Yang. Improved SVRG
for Non-Strongly-Convex or Sum-of-Non-Convex Objectives. In ICML, 2016.
Allen-Zhu, Zeyuan, Richtárik, Peter, Qu, Zheng, and Yuan,
Yang. Even faster accelerated coordinate descent using
non-uniform sampling. In ICML, 2016.
Arora, Sanjeev, Rao, Satish, and Vazirani, Umesh V. Expander flows, geometric embeddings and graph partitioning. Journal of the ACM, 56(2), 2009.
Aujol, J-F and Dossal, Ch. Stability of over-relaxations
for the forward-backward algorithm, application to fista.
SIAM Journal on Optimization, 25(4):2408–2433, 2015.
Axelsson, Owe. A survey of preconditioned iterative methods for linear systems of algebraic equations. BIT Numerical Mathematics, 25(1):165–187, 1985.
Chaudhuri, Kamalika, Kakade, Sham M, Livescu, Karen,
and Sridharan, Karthik. Multi-view clustering via canonical correlation analysis. In ICML, pp. 129–136, 2009.

Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition

Dhillon, Paramveer, Foster, Dean P, and Ungar, Lyle H.
Multi-view learning of word embeddings via cca. In
NIPS, pp. 199–207, 2011.
Frostig, Roy, Musco, Cameron, Musco, Christopher, and
Sidford, Aaron. Principal Component Projection Without Principal Component Analysis. In ICML, 2016.
Garber, Dan and Hazan, Elad. Fast and simple PCA via
convex optimization. ArXiv e-prints, September 2015.
Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Robust shift-and-invert preconditioning: Faster
and more sample efficient algorithms for eigenvector
computation. In ICML, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized
Loss Minimization. In Proceedings of the 31st International Conference on Machine Learning, ICML 2014,
pp. 64–72, 2014.
Shamir, Ohad. A Stochastic PCA and SVD Algorithm with
an Exponential Convergence Rate. In ICML, pp. 144—153, 2015.
Shewchuk, Jonathan Richard. An introduction to the conjugate gradient method without the agonizing pain, 1994.
Wang, Weiran and Livescu, Karen. Large-scale approximate kernel canonical correlation analysis. arXiv
preprint, abs/1511.04773, 2015.

Ge, Rong, Jin, Chi, Kakade, Sham M., Netrapalli, Praneeth, and Sidford, Aaron. Efficient Algorithms for
Large-scale Generalized Eigenvector Computation and
Canonical Correlation Analysis. In ICML, 2016.

Wang, Weiran, Wang, Jialei, Garber, Dan, and Srebro,
Nathan. Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis. In NIPS,
2016.

Golub, Gene H. and Van Loan, Charles F. Matrix Computations. The JHU Press, 4th edition, 2012. ISBN
1421407949.

Witten, Daniela M, Tibshirani, Robert, and Hastie, Trevor.
A penalized matrix decomposition, with applications to
sparse principal components and canonical correlation
analysis. Biostatistics, pp. kxp008, 2009.

Kakade, Sham M and Foster, Dean P. Multi-view regression via canonical correlation analysis. In Learning theory, pp. 82–96. Springer, 2007.
Karampatziakis, Nikos and Mineiro, Paul. Discriminative
features via generalized eigenvectors. In ICML, pp. 494–
502, 2014.
Lu, Yichao and Foster, Dean P. Large scale canonical correlation analysis with iterative least squares. In NIPS,
pp. 91–99, 2014.
Ma, Zhuang, Lu, Yichao, and Foster, Dean. Finding linear
structure in large datasets with scalable canonical correlation analysis. In ICML, pp. 169–178, 2015.
Michaeli, Tomer, Wang, Weiran, and Livescu, Karen.
Nonparametric canonical correlation analysis. arXiv
preprint, abs/1511.04839, 2015.
Musco, Cameron and Musco, Christopher. Randomized
block krylov methods for stronger and faster approximate singular value decomposition. In NIPS, pp. 1396–
1404, 2015.
Nesterov, Yurii. A method of solving a convex programming problem with convergence rate O(1/k 2 ). In Doklady AN SSSR (translated as Soviet Mathematics Doklady), volume 269, pp. 543–547, 1983.
Shalev-Shwartz, Shai. SDCA without Duality, Regularization, and Individual Convexity. In ICML, 2016.

