Sketched Ridge Regression: Optimization Perspective,
Statistical Perspective, and Model Averaging

Shusen Wang 1 Alex Gittens 2 Michael W. Mahoney 1

Abstract
We address the statistical and optimization impacts of using classical sketch versus Hessian
sketch to solve approximately the Matrix Ridge
Regression (MRR) problem. Prior research has
considered the effects of classical sketch on least
squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a
similar effect upon the optimization properties
of MRR as it does on those of LSR‚Äînamely,
it recovers nearly optimal solutions. In contrast,
Hessian sketch does not have this guarantee; instead, the approximation error is governed by a
subtle interplay between the ‚Äúmass‚Äù in the responses and the optimal objective value. For
both types of approximations, the regularization
in the sketched MRR problem gives it significantly different statistical properties from the
sketched LSR problem. In particular, there is
a bias-variance trade-off in sketched MRR that
is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the
variance is significantly increased when classical
sketches are used, while the bias is significantly
increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that
are higher by an order-of-magnitude than those
of the optimal MRR solutions. We establish theoretically and empirically that model averaging
greatly decreases this gap. Thus, in the distributed setting, sketching combined with model
averaging is a powerful technique that quickly
obtains near-optimal solutions to the MRR prob1
International Computer Science Institute and Department of
Statistics, University of California at Berkeley, USA 2 Department
of Computer Science, Rensselaer Polytechnic Institute, USA.
Correspondence to: Shusen Wang <shusen@berkeley.edu>,
Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

lem while greatly mitigating the statistical risks
incurred by sketching.

1. Introduction
Regression is one of the most fundamental problems in machine learning. The simplest and most thoroughly studied
regression model is least squares regression (LSR). Given
features X = [xT1 ; . . . , xTn ] ‚àà Rn√ód and responses y =
[y1 , . . . , yn ]T ‚àà Rn , the LSR problem minw kXw ‚àí yk22
can be solved in O(nd2 ) time using the QR decomposition
or in O(ndt) time using accelerated gradient descent algorithms. Here, t is the number of iterations, which depends
on the initialization, the condition number of X, and the
stopping criterion.
This paper considers the n  d problem, where there
is much redundancy in X. Matrix sketching, as used
within Randomized Linear Algebra (RLA) (Mahoney,
2011; Woodruff, 2014), works by reducing the size of X
without losing too much information; this operation can
be modeled as taking actual rows or linear combinations
of the rows of X with a sketching matrix S to form the
sketch ST X. Here S ‚àà Rn√ós satisfies d < s  n so that
ST X generically has the same rank but much fewer rows
as X. Sketching has been used to speed up LSR (Drineas
et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng &
Mahoney, 2013; Nelson & NguyeÃÇn, 2013) by solving the
sketched LSR problem minw kST Xw ‚àí ST yk22 instead
of the original LSR problem. Solving sketched LSR costs
either O(sd2 + Ts ) time using the QR decomposition or
O(sdt + Ts ) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the
time cost of sketching. For example, Ts = O(nd log s)
when S is the subsampled randomized Hadamard transform (Drineas et al., 2011), and Ts = O(nd) when S is
a CountSketch matrix (Clarkson & Woodruff, 2013).
There has been much work in RLA on analyzing the
quality of sketched LSR with different sketching methods
and different objectives; see the reviews (Mahoney, 2011;
1
The condition number of XT SST X is very close to that of
XT X, and thus the number of iterations t is almost unchanged.

Sketched Ridge Regression
Table 1. The time cost of the solutions to MRR. Here Ts (X) and
Ts (Y) denote the time cost of forming the sketches ST X ‚àà
Rs√ód and ST Y ‚àà Rs√óm .
Optimal
Classical
Hessian

Definition
(2)
(3)
(4)

Time
O(nd2 + nmd)
O(sd2 + smd) + Ts (X) + Ts (Y)
O(sd2 + nmd) + Ts (X)

Wc
W

Woodruff, 2014) and the references therein.
The concept of sketched LSR originated in the theoretical computer science literature, e.g., (Drineas et al., 2006;
2011), where the behavior of sketched LSR was studied
from an optimization perspective. Let w? be the optimal
LSR solution and wÃÉ be the solution to sketched LSR. This
line of work established that if s = O(d/ + poly(d)),
2
then the objective function value kXwÃÉ ‚àí y2 is at most 
2
times worse than kXw? ‚àí y . These works also bounded
2

kwÃÉ ‚àí w? k22 in terms of the difference in the objective function values and the condition number of XT X.
A more recent line of work has studied sketched LSR from
a statistical perspective: (Ma et al., 2015; Raskutti & Mahoney, 2016; Pilanci & Wainwright, 2015; Wang et al.,
2016b) considered statistical properties of sketched LSR
like the bias and variance. In particular, Pilanci & Wainwright (2015) showed that sketched LSR has much higher
variance than the optimal solution.
Both of these perspectives are important and of practical
interest. The optimization perspective is relevant when the
data can be taken as deterministic values. The statistical
perspective is relevant in machine learning and statistics
applications where the data are random, and the regression
coefficients are therefore themselves random variables.
In practice, regularized regression, e.g., ridge regression
and LASSO, exhibit more attractive bias-variance tradeoffs and generalization errors than vanilla LSR. Furthermore, the matrix generalization of LSR, where multiple
responses are to be predicted, is often more useful than
LSR. However, the properties of sketched regularized matrix regression are largely unknown. Hence, the question:
How, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize
to sketched regularized regression? We answer this question for sketched matrix ridge regression (MRR).
Recall that X is n √ó d. Let Y ‚àà Rn√óm denote the matrix
of corresponding responses. We study the MRR problem
min
W

n
f (W) ,



1
XW
n

o
2
‚àí YF + Œ≥kWk2F ,

(1)

which has optimal solution
W

?

=

T

‚Ä†

T

(X X + nŒ≥Id ) X Y.

LSR is a special case of MRR, with m = 1 and Œ≥ = 0. The
optimal solution W? can be obtained in O(nd2 + nmd)
time using a QR decomposition of X. Sketching can be
applied to MRR in two ways:

(2)

Here, (¬∑)‚Ä† denotes the Moore-Penrose inversion operation.

h

=
=

(XT SST X + nŒ≥Id )‚Ä† (XT SST Y),
T

T

‚Ä†

T

(X SS X + nŒ≥Id ) X Y.

(3)
(4)

Following the convention of Pilanci & Wainwright (2015);
Wang et al. (2016a), we call Wc classical sketch and Wh
Hessian sketch, which approximate the optimal solution
W? . Table 1 lists the time costs of the three solutions to
MRR.
1.1. Main Results and Contributions
We first study classical and Hessian sketches from the optimization perspective. Theorems 1 and 2 show that
‚Ä¢ Classical sketch achieves relative error in the objective
value. With sketch size s = OÃÉ(d/), the objective
satisfies f (Wc ) ‚â§ (1 + )f (W? ).
‚Ä¢ Hessian sketch does not achieve relative error in the
objective value. In particular, if n1 kYk2F is much
larger than f (W? ), then f (Wh ) can be far larger than
f (W? ).
‚Ä¢ For both classical and Hessian sketch, the relative
quality of approximation improves as the regularization parameter Œ≥ increases.
We then study classical and Hessian sketches from the statistical perspective, by modeling Y = XW0 + Œû as the
sum of a true linear model and random noise, decomposing the risk R(W) = EkXW ‚àí XW0 k2F into bias and
variance terms, and bounding these terms. We draw the
following conclusions (see Theorems 4, 5, 6):
‚Ä¢ The bias of the classical sketch can be nearly as
small as
 that of the optimal solution. The variance
is Œò ns times that of the optimal solution; this bound
is optimal. Therefore over-regularization, i.e., large
Œ≥, should be used to supress the variance. (As Œ≥
increases, the bias increases, and the variance decreases.)
‚Ä¢ Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution. However, Hessian sketch has high bias, especially when nŒ≥ is small compared to kXk22 . This indicates that over-regularization is necessary for Hessian
sketch to have low bias.
Our empirical evaluations bear out these theoretical results.
In particular, in Section 4, we show in Figure 2 that even
when the regularization parameter Œ≥ is fine-tuned, the risks
of classical sketch and Hessian sketch are worse than that

Sketched Ridge Regression

of the optimal solution by an order of magnitude. This is
an empirical demonstration of the fact that the near-optimal
properties of sketching from the optimization perspective
are much less relevant in a statistical setting than its suboptimal statistical properties.
We propose to use model averaging, which averages the
solutions of g sketched MRR problems, to attain lower optimization and statistical errors. Without ambiguity, we denote classical and Hessian sketches with model averaging
by Wc and Wh , respectively. Theorems 7, 8, 10, 11 give
the following results:
‚Ä¢ Classical Sketch. Assume the sketch size s = OÃÉ( d )
and  ‚â§ g1 ; then the bound on f (Wc ) ‚àí f (W? ) is
proportional to g . Assume that s = OÃÉ( d2 ) and 2 ‚â§
1
g ; the bias does not increase; the variance bound is
proportional to g1 .
‚Ä¢ Hessian Sketch. Assume that s = OÃÉ( d ) and  ‚â§ g12 ;
then the bound on f (Wh ) ‚àí f (W? ) is proportional
to g2 . Assume that s = OÃÉ( d2 ); the variance does
not increase; if, additionally,  ‚â§ g1 and nŒ≥ is much
smaller than the squared spectral norm of X, then the
bias bound is proportional to g .
Note that classical sketch with uniform sampling and
model averaging is very well known as bagging (Breiman,
1996) (or pasting (Breiman, 1999) or bootstrap aggregating). Different from bagging, our model averaging approach is not limited to uniform sampling.
Classical sketch with model averaging has three immediate
applications. In the single-machine setting,
‚Ä¢ Classical sketch with model averaging offers a way to
improve the statistical performance in the presence
‚àö of
heavy noise. Assume the sketch size is s = OÃÉ( nd).
As g grows larger than ns , the variance of the averaged
solution can be even lower than the optimal solution.
See Remark 1 for further discussion. This observation
is in accordance with the observation that bagging reduces variance.
In the distributed setting, the feature-response pairs
(x1 , y1 ), ¬∑ ¬∑ ¬∑ , (xn , yn ) ‚àà Rd √ó Rm are divided among g
machines. Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement.
In this setting, the model averaging procedure will communicate the g local models only once to return the final
estimate; this process has very low communication complexity and latency, and it suggests two further applications
of classical sketch with model averaging:

‚Ä¢ Model Averaging for Machine Learning. If a lowprecision solution is acceptable, the averaged solution
can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication. If ng is big enough compared to d and the row
coherence of X is small, then ‚Äúone-shot‚Äù model averaging has bias and variance comparable to the optimal
solution.
‚Ä¢ Model Averaging for Optimization.
If a highprecision solution to MRR is required, then an iterative numerical optimization algorithm must be used.
The cost of such numerical optimization algorithms
heavily depends on the quality of the initialization.2
A good initialization saves lots of iterations. The averaged model is provably close to the optimal solution,
so model averaging provides a high-quality initialization for more expensive algorithms.
1.2. Prior Work
The body of work on sketched LSR mentioned earlier
(Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013;
Meng & Mahoney, 2013; Nelson & NguyeÃÇn, 2013) shares
many similarities with our results. However, the theories of
sketched LSR developed from the optimization perspective
do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR
is unbiased while MRR has a nontrivial bias and therefore
has a bias-variance tradeoff that must be considered.
Lu et al. (2013) has considered a different application of
sketching to ridge regression: they assume d  n, reduce
the number of features in X using sketching, and conduct
statistical analysis. Our setting differs in that we consider
n  d, reduce the number of samples by sketching, and
allow for multiple responses.
The model averaging analyzed in this paper is similar in
spirit to the AVG M algorithm of (Zhang et al., 2013). When
classical sketch is used with uniform row sampling without
replacement, our model averaging procedure is a special
case of AVG M. However, our results do not follow from
those of (Zhang et al., 2013): first, we make no assumption
on the data, whereas they assumed x1 , ¬∑ ¬∑ ¬∑ , xn are i.i.d.
from an unknown distribution; second, our results apply
to many other sketching ensembles than uniform sampling
without replacement; and third, we provide both optimization and statistical perspectives, whereas they provide only
a statistical perspective. Our results clearly indicate that the
2

For example, the conjugate gradient method satisfies

kW(t) ‚àíW? k2
F
kW(0) ‚àíW? k2
F

‚â§ Œ∏1t ; the stochastic block coordinate descent (Tu
(t)

?

(W )‚àíf (W )
et al., 2016) satisfies Ef
‚â§ Œ∏2t . Here W(t) is the
f (W(0) )‚àíf (W? )
output of the t-th iteration; Œ∏1 , Œ∏2 ‚àà (0, 1) depend on the condition number of XT X + nŒ≥Id and some other factors.

Sketched Ridge Regression

performance critically depends on the row coherence of X;
this dependence is not captured in (Zhang et al., 2013). For
similar reasons, our work is different from the divide-andconquer kernel ridge regression algorithm of (Zhang et al.,
2015).
Iterative Hessian sketch has been studied by Pilanci &
Wainwright (2015); Wang et al. (2016a). By way of comparison, all the algorithms in this paper are ‚Äúone-shot‚Äù
rather than iterative. Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016; Thanei
et al., 2017). Avron et al. (2016) studied classical sketch
from the optimization perspective, and Thanei et al. (2017)
studied LSR with model averaging.
1.3. Paper Organization
Section 2 defines our notation and introduces the sketching
schemes we consider. Section 3 presents our theoretical
results. Section 4 conducts experiments to verify our theories and demonstrates the usefulness of model averaging.
Proofs of our claims and more empirical evaluations can be
found in the technical report version (Wang et al., 2017).

2. Preliminaries
Throughout, we take In to be the n √ó n identity matrix and
0 to be a vector or matrix of all zeroes of the appropriate
size. Given a matrix A = [aij ], the i-th row is denoted
by ai: , and a:j denotes the j-th column. The Frobenius
and spectral norms of A are written as, respectively, kAkF
and kAk2 . The set {1, 2, ¬∑ ¬∑ ¬∑ , n} is written [n]. Let O, ‚Ñ¶,
and Œò be the standard asymptotic notation. Let OÃÉ conceal
logarithm factors.
Throughout, we fix X ‚àà Rn√ód as our matrix of features.
We set œÅ = rank(X) and write the SVD of X as X =
UŒ£VT , where U, Œ£, V are respectively n √ó œÅ, œÅ √ó œÅ,
and d √ó œÅ matrices. We let œÉ1 ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉœÅ > 0 be the
singular values of X. The Moore-Penrose inverse of X
is defined by X‚Ä† = VŒ£‚àí1 UT . The row leverage scores
of X are li = ku:i k22 for i ‚àà [n]. The row coherence of
X is ¬µ(X) = nœÅ maxi ku:i k22 . Throughout, we let ¬µ be
shorthand for ¬µ(X).
Matrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear
regression. We denote the process of sketching a matrix
X ‚àà Rn√ód by X0 = ST X. Here, S ‚àà Rn√ós is called a
sketching matrix and X0 ‚àà Rs√ód is called a sketch of X.
In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled from N (0, 1/s)), the sketching
matrix S is not formed explicitly. Matrix sketching can be
accomplished by random sampling or random projection.
Random sampling corresponds to sampling rows of X

i.i.d. with replacement according to given row sampling
probabilities p1 , ¬∑ ¬∑ ¬∑ , pm ‚àà (0, 1). The corresponding (random) sketching matrix S ‚àà Rn√ós has exactly one non-zero
entry per column, whose position indicates the index of the
selected row; in practice, this S is not explicitly formed.
Uniform sampling fixes p1 = ¬∑ ¬∑ ¬∑ = pn = n1 . Leverage
score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X. In
practice shrinked leverage score sampling can be a better
choice than leverage score sampling (Ma et al., 2015). The
sampling probabilities of shrinked leverage
score sampling

are defined by pi = 21 Pnli lj + n1 .3
j=1

Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson & Lindenstrauss, 1984). Let G ‚àà Rm√ós be a standard Gaussian matrix, i.e., each entry is sampled independently from
N (0, 1). The matrix S = ‚àö1s G is a Gaussian projection
matrix. It takes O(nds) time to apply S ‚àà Rn√ós to any
n √ó d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.
Subsampled randomized Hadamard transform (SRHT)
(Drineas et al., 2011; Lu et al., 2013; Tropp, 2011) is
a more efficient alternative to Gaussian projection. Let
Hn ‚àà Rn√ón be the Walsh-Hadamard matrix with +1 and
‚àí1 entries, D ‚àà Rn√ón be a diagonal matrix with diagonal
entries sampled uniformly from {+1, ‚àí1}, and P ‚àà Rn√ós
be the uniform row sampling matrix defined above. The
matrix S = ‚àö1n DHn P ‚àà Rn√ós is an SRHT matrix, and
can be applied to any n √ó d matrix in O(nd log s) time.
In practice, the subsampled randomized Fourier transform
(SRFT) (Woolfe et al., 2008) is often used in lieu of the
SRHT, because the SRFT exists for all values of n, whereas
Hn exists only for some values of n. Their performance
and theoretical analyses are very similar.
CountSketch can be applied to any X ‚àà Rn√ód in O(nd)
time (Charikar et al., 2004; Clarkson & Woodruff, 2013;
Meng & Mahoney, 2013; Nelson & NguyeÃÇn, 2013; Pham
& Pagh, 2013; Weinberger et al., 2009). Though more efficient to apply, CountSketch requires a bigger sketch size
than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees. The readers
can refer to (Woodruff, 2014) for a detailed description of
CountSketch.

3. Main Results
Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives. Sec3

In fact, pi can be any convex combination of

Pnli

j=1 lj

and

1
n

(Ma et al., 2015). We use the weight 21 for simplicity; our conclusions extend in a straightforward manner to other weightings.

Sketched Ridge Regression

tions 3.3 and 3.4 capture the impacts of model averaging
on, respectively, the optimization and statistical properties
of sketched MRR.
We described six sketching methods in Section 2. For simplicity, in this section, we refer to leverage score sampling,
shrinked leverage score sampling, Gaussian projection, and
SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch. The notation defined in Table 2 are used throughout.
Table 2. The commonly used notation.
Notation
X ‚àà Rn√ód
Y ‚àà Rn√óm
UŒ£VT
¬µ
Œ≥
Œ≤
S ‚àà Rn√ós

Definition
each row is a data sample (feature vector)
each row contains the corresponding responses
the SVD of X
the row coherence of X
the regularization parameter
Œ≤=

kXk2
2
kXk2
2 +nŒ≥

‚â§1

holds with probability at least 0.9.
These two results imply that f (Wc ) and f (Wh ) can be
close to f (W? ). When this is the case, curvature of the
objective function ensures that the sketched solutions Wc
and Wh are close to the optimal solution W? . Lemma 3
studies the Mahalanobis distance kM(W ‚àí W? )k2F . Here
M is any non-singular matrix; in particular, it can be the
identity matrix or (XT X)1/2 .
Lemma 3. Let f be the objective function of MRR defined
in (1), W ‚àà Rd√óm be arbitrary, and W? be the optimal
solution defined in (2). For any non-singular matrix M, the
Mahalanobis distance satisfies

f (W) ‚àí f (W? )
1
M(W ‚àí W? )2 ‚â§

.
2
F
T
n
œÉmin (X SST X + nŒ≥Id )1/2 M‚àí1

By choosing M = (XT X)1/2 , we can bound n1 kXW ‚àí
XW? k2F in terms of the difference in the objective values:


1
XW
n

a sketching matrix

3.1. Sketched MRR: Optimization Perspective
Theorem 1 shows that f (Wc ), the objective value of classical sketch, is very close to the optimal objective value
f (W? ). The approximation quality improves as Œ≥ increases.
Theorem 1 (Classical Sketch).
For the four sketching

methods with s = OÃÉ Œ≤d
 , uniform sampling with s =

2
d
O ¬µ Œ≤d log
, and CountSketch with s = O Œ≤d , the in
equality
f (Wc ) ‚àí f (W? ) ‚â§

2
‚àí XW? F



Œ≤ f (W) ‚àí f (W? ) .

‚â§

With Lemma 3, we can directly apply Theorems 1 or 2 to
bound n1 kXWc ‚àí XW? k2F or n1 kXWh ‚àí XW? k2F .
3.2. Sketched MRR: Statistical Perspective
We consider the following fixed design model. Let X ‚àà
Rn√ód be the observed feature matrix, W0 ‚àà Rd√óm be the
true and unknown model, Œû ‚àà Rn√óm contain unknown
random noise, and
Y = XW0 + Œû

(5)

be the observed responses. We make the following standard
weak assumptions on the noise:

 f (W? )

E[Œû] = 0

and

E[ŒûŒûT ] = Œæ 2 In .

We observe X and Y and seek to estimate W0 .

holds with probability at least 0.9.

We can evaluate the quality of the estimate by the risk:
The corresponding guarantee for the performance of Hessian sketch is given in Theorem 2. It is weaker than the
guarantee for classical sketch, especially when n1 kYk2F is
far larger than f (W? ). If Y is nearly noiseless‚ÄîY is wellexplained by a linear combination of the columns of X‚Äî
and Œ≥ is small, then f (W? ) is close to zero, and consequently f (W? ) can be far smaller than n1 kYk2F . Therefore, in this case which is ideal for MRR, f (Wh ) is not
close to f (W? ) and our theory suggests Hessian sketch
does not perform as well as classical sketch. This is verified by our experiments, which show that unless Œ≥ is big or
a large portion of Y is outside the column space of X, the
(Wh )
ratio ff(W
? ) can be large.
Theorem 2 (Hessian Sketch). For the four sketching meth2 
ods with s = OÃÉ Œ≤ d , uniform sampling with s =

2
2 2
O ¬µŒ≤ dlog d , and CountSketch with s = O( Œ≤ d ), the inequality
f (Wh ) ‚àí f (W? )

‚â§





kYk2
F
n


‚àí f (W? ) .

R(W) =



1 
E XW
n

2
‚àí XW0 F ,

(6)

where the expectation is taken w.r.t. the noise Œû. We study
the risk functions R(W? ), R(Wc ), and R(Wh ) in the following.
Theorem 4 (Bias-Variance Decomposition). We consider
the data model described in this subsection. Let W be W? ,
Wc , or Wh , as defined in (2), (3), (4), respectively; then
the risk function can be decomposed as
R(W)

=

bias2 (W) + var(W).

Recall the SVD of X: X = UŒ£VT . The bias and variance
terms can be written as

‚àö 


= Œ≥ n(Œ£2 + nŒ≥IœÅ )‚àí1 Œ£VT W0  ,
F
2
2


Œæ
‚àí1


‚àí2
var W? =
 IœÅ + nŒ≥Œ£
 ,
n 
F


‚Ä†
‚àö


T
T
c
bias W = Œ≥ n Œ£U SS UŒ£ + nŒ≥IœÅ Œ£VT W0  ,
bias W?



F

var W

c


2
Œæ2  T T

‚àí2 ‚Ä† T
=
U SST  ,
 U SS U + nŒ≥Œ£
n
F

Sketched Ridge Regression


‚àö 

bias Wh = Œ≥ n Œ£‚àí2 +

UT SST U‚àíIœÅ
nŒ≥





¬∑ UT SST U + nŒ≥Œ£
Œ£VT W0  ,
F

2
Œæ2  T T
‚àí2 ‚Ä† 
h
var W =
 U SS U + nŒ≥Œ£
 .
n
F
‚àí2 ‚Ä†

Theorem 5 provides upper and lower bounds on the bias
and variance of the classical sketch. In particular, we see
that that bias(Wc ) is within a factor of (1¬±) of bias(W? ).
However, var(Wc ) is Œò( ns ) times worse than var(W? ).
Theorem 5 (Classical Sketch). For Gaussian projection
and SRHT sketching with s = OÃÉ( d2 ), uniform sampling
d
d2
with s = O(¬µ d log
2 ), or CountSketch with s = O( 2 ), the
inequalities
bias(Wc )
‚â§ 1 + ,
bias(W? )
var(Wc )
n
n
‚â§ (1 + )
(1 ‚àí ) ‚â§
s
var(W? )
s
1‚àí ‚â§

hold with probability at least 0.9.
d
For shrinked leverage score sampling with s = O( d log
2 ),
these inequalities, except for the lower bound on the variance,4 hold with probability at least 0.9.

Theorem 6 establishes similar upper and lower bounds on
the bias and variance of Hessian sketch. The situation is the
reverse of that with classical sketch: the variance of Wh is
close to that of W? if s is large enough, but as the regularization parameter Œ≥ goes to zero, bias(Wh ) becomes much
larger than bias(W? ).
Theorem 6 (Hessian Sketch). For the four sketching
methods with s = OÃÉ( d2 ), uniform sampling with s =
d
d2
O(¬µ d log
2 ), and CountSketch with s = O( 2 ), the inequalities

kXk22 
bias(Wh )
‚â§ (1 + ) 1 +
,
?
bias(W )
nŒ≥
1‚àí ‚â§

var(Wh )
‚â§ 1+
var(W? )

hold with probability at least 0.9. Further assume that
œÉœÅ2 ‚â• nŒ≥
 . Then

bias(Wh )
1  œÉœÅ2
‚â•
‚àí1
?
bias(W )
1 +  nŒ≥

holds with probability at least 0.9.
The lower bound on the bias shows that Hessian sketch
can suffer from a much higher bias than the optimal solution. The gap between bias(Wh ) and bias(W? ) can be
For shrinked leverage score sampling, kSk22 does not enjoy
nontrivial lower bound. This is why we do not have a lower bound
on the variance.
4

lessened by increasing the regularization parameter Œ≥, but
such over-regularization increases the baseline bias(W? )
itself. It is also worth mentioning that unlike bias(W? )
and bias(Wc ), bias(Wh ) is not monotonically increasing
with Œ≥, as is empirically verified in Figure 2.
In sum, our theories show that classical and Hessian
sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of
regularization. In practice, the regularization parameter Œ≥
should be tuned to optimize the prediction accuracy. Our
experiments in Figure 2 show that even with fine-tuned Œ≥,
the risks of classical and Hessian sketches can be higher
than the risk of the optimal solution by an order of magnitude. Formally speaking, minŒ≥ R(Wc )  minŒ≥ R(W? )
and minŒ≥ R(Wh )  minŒ≥ R(W? ) hold in practice.
Our empirical study in Figure 2 suggests classical and Hessian sketches both require over-regularization, i.e., setting
Œ≥ larger than what is best for the optimal solution W? . Formally speaking, argminŒ≥ R(Wc ) > argminŒ≥ R(W? ) and
argminŒ≥ R(Wh ) > argminŒ≥ R(W? ). Although this is the
case for both types of sketches, the underlying explanations
are different. Classical sketch has a high variance, so a
large Œ≥ is required to supress the variance (its variance is
non-increasing with Œ≥). Hessian sketch has very high bias
when Œ≥ is small, so a reasonably large Œ≥ is necessary to
lower its bias.
3.3. Model Averaging: Optimization Perspective
We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions. The model
averaging procedure is straightforward: one independently
draws g sketching matrices S1 , ¬∑ ¬∑ ¬∑ , Sg ‚àà Rn√ós , uses these
to form g sketched MRR solutions, denoted by {Wic }gi=1
or {Wih }gi=1 , and averages
Pg these solutions to obtain
Pg the final estimate Wc = g1 i=1 Wic or Wh = g1 i=1 Wih .
Practical applications of model averaging are enumerated
in Section 1.1.
Theorems 7 and 8 present guarantees on the optimization
accuracy of using model averaging to combine the classical
or Hessian sketch solutions. We can contrast these with the
guarantees provided for sketched MRR in Theorems 1 and
2. For classical sketch with model averaging, we see that
when  ‚â§ g1 , the bound on f (Wh )‚àíf (W? ) is proportional
to /g. From Lemma 3 we can see that the distances from
Wc to W? also decreases accordingly.
Theorem 7 (Classical Sketch with
 Model Averaging). For
the four methods, let s = OÃÉ Œ≤d
 ; for uniform sampling, let

¬µŒ≤d log d
s=O
. Then the inequality

f (Wc ) ‚àí f (W? )

‚â§




g


+ Œ≤ 2 2 f (W? )

Sketched Ridge Regression

10-4
10

-5

10-6

10-10

10-8

Œ≥

10-6

10-4

10

10-10

10-8

Œ≥

10-6

10-4

10-2

0.01

0.005
10-12

-2

Leverage Sampling
10-10

10-8

Œ≥

10-6

10-4

10-2

Shrinkage Lev. Sampling
Gaussian Projection

0.05

SRFT
Count Sketch

-3

Objective Function

Objective Function

0.02

Uniform Sampling

10-12

10-2

10-2

Wh

-3

10-4

10-7 -12
10

10

10

Œæ = 10‚àí1
0.05

Objective Function

Wc

10

-2

Objective Function

10

-3

Objective Function

10

Œæ = 10‚àí2

Objective Function

Œæ = 10‚àí3
-2

10-4

10-3

10-5
10-6

10
10-7 -12
10

10-10

10-8

Œ≥

10-6

10-4

10-2

Optimal Solution
0.02

0.01
0.01
-5
0.003
0.001
-6
0.0003
0.0001
3e-05
3e-06
3e-07
10
10 -7
-13
10
10-14

-4

10

-12

10

-10

10

-8

Œ≥

10

-6

10

-4

10

-2

0.005
10-12

10-10

10-8

Œ≥

10-6

10-4

10-2

Figure 1. Empirical study of classical sketch and Hessian sketch from optimization perspective. The x-axis is the regularization parameter Œ≥ (log-scale); the y-axis is the objective function values (log-scale).

holds with probability at least 0.8.
For Hessian sketch with model averaging, if Œ≤2 ‚â§ g12 , then
the bound on f (Wh ) ‚àí f (W? ) is proportional to g2 .
Theorem 8 (Hessian Sketch with Model Averaging). For
2 
the four methods let s = OÃÉ Œ≤ d , and for uniform sam
2
pling let s = O ¬µŒ≤ dlog d , then the inequality
f (Wh ) ‚àí f (W? )

‚â§


g2

+

2
Œ≤2



kYk2
F
n


‚àí f (W? ) .

holds with probability at least 0.8.
3.4. Model Averaging: Statistical Perspective
Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketch solutions.
Our first result conducts a bias-variance decomposition for
the averaged solution of sketched MRR.
Theorem 9 (Bias-Variance Decomposition). We consider
the fixed design model (5). The risk function defined in (6)
can be decomposed as

g
T
T

‚àö 
 1 X ‚àí2 U Si Si U ‚àí IœÅ 
Œ£ +
bias Wh = Œ≥ n
g i=1
nŒ≥


T
T
‚àí2 ‚Ä†
¬∑ U Si Si U + nŒ≥Œ£
Œ£VT W0  ,
F

var W

h

 g
‚Ä† 
Œæ2  1 X
2
=
UT Si STi U + nŒ≥Œ£‚àí2  .

n g i=1
F

Theorems 10 and 11 provide upper bounds on the bias
and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch. We can contrast
them with Theorems 5 and 6 to see the statistical benefits
of model averaging.
Theorem 10 (Classical Sketch with Model Averaging).
For shrinked leverage score sampling, Gaussian projection, SRHT with s = OÃÉ d2 , or uniform sampling with

d
s = O ¬µd log
, the inequalities
2
bias(Wc )
‚â§ 1 + ,
bias(W? )
q
2
var(Wc )
n  1+/g
‚â§
+

g
var(W? )
s

hold with probability at least 0.8.
Remark 1. From this result, we see that if  ‚â§

‚àö1 ,
g

then

the variance is proportional to g1 . If g and s are at least
R(W)

=

bias2 (W) + var(W).

The bias and variance terms are
g


‚Ä†
‚àö 

1 X
bias Wc = Œ≥ n
Œ£UT Si STi UŒ£ + nŒ≥IœÅ Œ£VT W0  ,
g i=1
F

var W

c

 g
2
‚Ä†
Œæ2  1 X

=
UT Si STi U + nŒ≥Œ£‚àí2 UT Si STi  ,

n g i=1
F

g=O

n
s

and

s = OÃÉ

‚àö


nd ,

then the risk R(Wc ) is close to R(W? ). If g and s are
larger, then the variance var(Wc ) can even be even lower
than var(W? ).
Theorem 11 shows that model averaging decreases the bias
of Hessian sketch without increasing the variance. For Hessian sketch without model averaging, recall that bias(Wh )

Sketched Ridge Regression

Bias2

10-2

10-3

10

10

-4

10-4

10-4

10-5

10-5

10-5

-6

10

-6

10

-7

10

-8

-7

10

-7

10

-8

10

-9

10-9

10

-10

10-10

10

-10

10

-11

10-11

10

-11

10

-12

10

-10

10

-8

Œ≥

10

-6

10

-4

10

10-12 -12
10

-2

10
10

10

-10

10

-8

Œ≥

10

-6

10

-4

10

-2

-9

10

10

-2

10-3

10

-3

10-4

10-4

10

-4

10-5

10-5

10-5

10

10-7

10-7
10

-6

10-8

-8

-9

10-9

10

10-10

10-10

-11

-11

10

10-12 -12
10

10
10-10

10-8

Œ≥

10-6

10-4

10-2

10-12 -12
10

10

-6

10

-7

10

-8

10

-9

Risk

Bias2

10

Variance

10-2

-3

-6

the min risk of the
e opt
o
optimal solution

Uniform Sampling

10-8

10-2
10

the min risk of the classical
assic sketch

-3

10-6

Risk

Bias2

Variance

-3

10-12 -12
10

Wh

-2

10

10

Wc

Risk = Bias2 + Var

Var
10

10-2

10

-10

10

-8

Œ≥

10

-6

10

-4

10

-2

10

-10

10

-11

10

-12

-12

10

10

-10

10

-8

Leverage Sampling

optimal Œ≥ for the
classical
sical
a sketch

optimal Œ≥ for the
optimal solution
tion

Œ≥

10

-6

10

-4

10

Shrinkage Lev. Sampling
Gaussian Projection

-2

SRFT
Count Sketch

the min risk of the Hessian
esssia sketch

Optimal Solution

the min risk of the
e op
o
optimal solution
optimal Œ≥ for the
ssian sketch
Hessian

optimal Œ≥ for the
tion
optimal solution
-12

10

-10

10

-8
8

Œ≥

10

-6

10

-4

10

0.01
-5
0.003
0.001
0.0003
0.0001
-7
3e-05
3e-06
3e-07
10 -6
-14
-13
10
10
-2

Figure 2. Empirical study of classical sketch and Hessian sketch from statistical perspective. The x-axis is the regularization parameter
Œ≥ (log-scale); the y-axes are respectively bias2 , variance, and risk (log-scale). We annotate the minimum risks and optimal Œ≥ in the plots.

is larger than bias(W? ) by a factor of O(kXk22 /(nŒ≥)).
Theorem 11 shows that model averaging reduces this ratio by a factor of g when  ‚â§ g1 .

hold with probability at least 0.8.

We conducted experiments on synthetic data to verify Theorems 5 and 6 and to show the effects of classical and Hessian sketching on the bias and variance. We set the noise
intensity to be Œæ = 0.1. In Figure 2, we plot the analytical
expressions for the squared bias, variance, and risk stated in
Theorem 4 against the regularization parameter Œ≥. The results of this experiment match our theory: classical sketch
magnified the variance, and Hessian sketch increased the
bias. Even if Œ≥ is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal
solution.5 Our experiments also indicate that classical and
Hessian sketches require setting Œ≥ larger than the best regularization parameter for the optimal solution W? .

4. Sketched Ridge Regression Experiments

5. Conclusions

Following (Ma et al., 2015; Yang et al., 2016), we
constructed X ‚àà Rn√ód to have condition number
Œ∫(XT X) = 1012 and high row coherence, fixed w0 =
[10.2d ; 0.110.6d ; 10.2d ], and set y = Xw0 + Œµ ‚àà Rn , where
the entries of Œµ ‚àà Rn were i.i.d. sampled from N (0, Œæ 2 ).
The details of this data model are given in the technical report version (Wang et al., 2017). Let S ‚àà Rn√ós be any of
the six sketching methods considered in this paper. We fix
n = 105 , d = 500, and s = 5, 000. Because the analytical expressions involve the random sketching matrix S, we
randomly generate S, repeat this procedure 10 times, and
report the averaged results.

We studied sketched matrix ridge regression (MRR) from
optimization and statistical perspectives. Using classical
sketch, by taking a large enough sketch, one can obtain
an -accurate approximate solution. Counterintuitively and
in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both
classical and Hessian sketches can have statistical risks that
are worse than the risk of the optimal solution by an order
of magnitude. We proposed the use of model averaging
to attain better optimization and statistical properties. We
have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning.

Theorem 11 (Hessian Sketch with Model Averaging). For
the four methods with s = OÃÉ d2 , or uniform sampling

d
, the inequalities
with s = O ¬µd log
2
2
bias(Wh )

2  kXk2
‚â§
1
+

+
+

,
bias(W? )
g
nŒ≥

var(Wh )
‚â§ 1+
var(W? )

In Figure 1, we plot the objective function value f (w) =
1
2
2
n kXw ‚àí yk2 + Œ≥kwk2 against Œ≥, under different settings
of noise intensity Œæ. The results verify our theory: classical
sketch wc is always close to optimal; Hessian sketch wh
is much worse than the optimal when Œ≥ is small and y is
mostly in the column space of X.

5

In the experiment yielding Figure 2, Hessian sketch had
lower risk than classical sketch. This is not generally true: if we
used a smaller Œæ, so that the variance is dominated by bias, then
classical sketch results in lower risks than Hessian sketch.

Sketched Ridge Regression

Acknowledgements
We thank the anonymous reviewers for their helpful suggestions. We thank the Army Research Office and the Defense Advanced Research Projects Agency for partial support of this work.

References
Avron, Haim, Clarkson, Kenneth L., and Woodruff, David P.
Sharper bounds for regression and low-rank approximation
with regularization. arXiv preprint arXiv:1611.03225, 2016.

Pham, Ninh and Pagh, Rasmus. Fast and scalable polynomial kernels via explicit feature maps. In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD),
2013.
Pilanci, Mert and Wainwright, Martin J. Iterative Hessian sketch:
Fast and accurate solution approximation for constrained leastsquares. Journal of Machine Learning Research, pp. 1‚Äì33,
2015.
Raskutti, Garvesh and Mahoney, Michael W. A statistical perspective on randomized sketching for ordinary least-squares.
Journal of Machine Learning Research, 17(214):1‚Äì31, 2016.

Breiman, Leo. Bagging predictors. Machine Learning, 24(2):
123‚Äì140, 1996.

Thanei, Gian-Andrea, Heinze, Christina, and Meinshausen, Nicolai. Random projections for large-scale regression. arXiv
preprint arXiv:1701.05325, 2017.

Breiman, Leo. Pasting small votes for classification in large
databases and on-line. Machine Learning, 36(1-2):85‚Äì103,
1999.

Tropp, Joel A. Improved analysis of the subsampled randomized
Hadamard transform. Advances in Adaptive Data Analysis, 3
(01n02):115‚Äì126, 2011.

Charikar, Moses, Chen, Kevin, and Farach-Colton, Martin. Finding frequent items in data streams. Theoretical Computer Science, 312(1):3‚Äì15, 2004.

Tu, Stephen, Roelofs, Rebecca, Venkataraman, Shivaram, and
Recht, Benjamin. Large scale kernel learning using block coordinate descent. arXiv preprint arXiv:1602.05310, 2016.

Clarkson, Kenneth L. and Woodruff, David P. Low rank approximation and regression in input sparsity time. In Annual ACM
Symposium on theory of computing (STOC), 2013.
Drineas, Petros, Mahoney, Michael W., and Muthukrishnan, S.
Sampling algorithms for `2 regression and applications. In Annual ACM-SIAM Symposium on Discrete Algorithm (SODA),
2006.
Drineas, Petros, Mahoney, Michael W., Muthukrishnan, S., and
SarloÃÅs, TamaÃÅs. Faster least squares approximation. Numerische
Mathematik, 117(2):219‚Äì249, 2011.
Drineas, Petros, Magdon-Ismail, Malik, Mahoney, Michael W.,
and Woodruff, David P. Fast approximation of matrix coherence and statistical leverage. Journal of Machine Learning Research, 13:3441‚Äì3472, 2012.
Johnson, William B. and Lindenstrauss, Joram. Extensions of
Lipschitz mappings into a Hilbert space. Contemporary mathematics, 26(189-206), 1984.
Lu, Yichao, Dhillon, Paramveer, Foster, Dean P, and Ungar,
Lyle. Faster ridge regression via the subsampled randomized
Hadamard transform. In Advances in Neural Information Processing Systems (NIPS), 2013.
Ma, Ping, Mahoney, Michael W, and Yu, Bin. A statistical perspective on algorithmic leveraging. Journal of Machine Learning Research, 16(1):861‚Äì911, 2015.

Wang, Jialei, Lee, Jason D, Mahdavi, Mehrdad, Kolar, Mladen,
and Srebro, Nathan. Sketching meets random projection in
the dual: A provable recovery algorithm for big and highdimensional data. arXiv preprint arXiv:1610.03045, 2016a.
Wang, Shusen, Gittens, Alex, and Mahoney, Michael W. Sketched
ridge regression: Optimization perspective, statistical perspective, and model averaging. arXiv preprint arXiv:1702.04837,
2017.
Wang, Yining, Yu, Adams Wei, and Singh, Aarti. Computationally feasible near-optimal subset selection for linear
regression under measurement constraints. arXiv preprint
arXiv:1601.02068, 2016b.
Weinberger, Kilian, Dasgupta, Anirban, Langford, John, Smola,
Alex, and Attenberg, Josh. Feature hashing for large scale multitask learning. In International Conference on Machine Learning (ICML), 2009.
Woodruff, David P. Sketching as a tool for numerical linear algeR in Theoretical Computer Scibra. Foundations and Trends
ence, 10(1‚Äì2):1‚Äì157, 2014.
Woolfe, Franco, Liberty, Edo, Rokhlin, Vladimir, and Tygert,
Mark. A fast randomized algorithm for the approximation of
matrices. Applied and Computational Harmonic Analysis, 25
(3):335‚Äì366, 2008.

Mahoney, Michael W. Randomized algorithms for matrices and
data. Foundations and Trends in Machine Learning, 3(2):123‚Äì
224, 2011.

Yang, Jiyan, Meng, Xiangrui, and Mahoney, Michael W. Implementing randomized matrix algorithms in parallel and distributed environments. Proceedings of the IEEE, 104(1):58‚Äì
92, 2016.

Meng, Xiangrui and Mahoney, Michael W. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Annual ACM Symposium on Theory
of Computing (STOC), 2013.

Zhang, Yuchen, Duchi, John C., and Wainwright, Martin J.
Communication-efficient algorithms for statistical optimization. Journal of Machine Learning Research, 14:3321‚Äì3363,
2013.

Nelson, John and NguyeÃÇn, Huy L. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In
IEEE Annual Symposium on Foundations of Computer Science
(FOCS), 2013.

Zhang, Yuchen, Duchi, John, and Wainwright, Martin. Divide
and conquer kernel ridge regression: a distributed algorithm
with minimax optimal rates. Journal of Machine Learning Research, 16:3299‚Äì3340, 2015.

