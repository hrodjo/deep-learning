Adaptive Feature Selection: Computationally Efficient Online Sparse Linear
Regression under RIP

Satyen Kale 1 Zohar Karnin 2 Tengyuan Liang 3 Dávid Pál 4

Abstract
Online sparse linear regression is an online problem where an algorithm repeatedly chooses a
subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued
prediction, receives the true label, and incurs the
squared loss. The goal is to design an online
learning algorithm with sublinear regret to the
best sparse linear predictor in hindsight. Without any assumptions, this problem is known to
be computationally intractable. In this paper, we
make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of
the problem. In the first variant, the true label is
generated according to a sparse linear model with
additive Gaussian noise. In the second, the true
label is chosen adversarially.

1. Introduction
In modern real-world sequential prediction problems, samples are typically high dimensional, and construction of
the features may itself be a computationally intensive task.
Therefore in sequential prediction, due to the computation
and resource constraints, it is preferable to design algorithms that compute only a limited number of features for
each new data example. One example of this situation,
from (Cesa-Bianchi et al., 2011), is medical diagnosis of a
disease, in which each feature is the result of a medical test
on the patient. Since it is undesirable to subject a patient to
1
2
Google Research, New York.
Amazon, New York.
University of Chicago, Booth School of Business, Chicago.
4
Yahoo Research, New York.
Work done while the authors were at Yahoo Research, New York.
Correspondence to: Satyen Kale <satyenkale@google.com>, Zohar Karnin <zkarnin@gmail.com>,
Tengyuan Liang
<Tengyuan.Liang@chicagobooth.edu>,
Dávid
Pál
<dpal@yahoo-inc.com>.
3

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

a battery of medical tests, we would like to adaptively design diagnostic procedures that rely on only a few, highly
informative tests.
Online sparse linear regression (OSLR) is a sequential prediction problem in which an algorithm is allowed to see
only a small subset of coordinates of each feature vector.
The problem is parameterized by 3 positive integers: d,
the dimension of the feature vectors, k, the sparsity of the
linear regressors we compare the algorithm’s performance
to, and k0 , a budget on the number of features that can be
queried in each round by the algorithm. Generally we have
k  d and k0 ≥ k but not significantly larger (our algorithms need1 k0 = Õ(k)).
In the OSLR problem, the algorithm makes predictions
over a sequence of T rounds. In each round t, nature
chooses a feature vector xt ∈ Rd , the algorithm chooses
a subset of {1, 2, . . . , d} of size at most k 0 and observes
the corresponding coordinates of the feature vector. It then
makes a prediction ybt ∈ R based on the observed features,
observes the true label yt , and suffers loss (yt − ybt )2 . The
goal of the learner is to make the cumulative loss comparable to that of the best k-sparse linear predictor w in hindsight. The performance of the online learner is measured
by the regret, which is defined as the difference between
the two losses:
RegretT =

T
X
t=1

2

(yt − ybt ) −

min

w: kwk0 ≤k

T
X

2

(yt − hxt , wi) .

t=1

The goal is to construct algorithms that enjoy regret that is
sub-linear in T , the total number of rounds. A sub-linear
regret implies that in the asymptotic sense, the average perround loss of the algorithm approaches the average perround loss of the best k-sparse linear predictor.
Sparse regression is in general a computationally hard
problem. In particular, given k, x1 , x2 , . . . , xT and
y1 , y2 , . . . , yT as inputs, the offline P
problem of finding a
T
k-sparse w that minimizes the error t=1 (yt − hxt , wi)2
does not admit a polynomial time algorithm under standard
complexity assumptions (Foster et al., 2015). This hard1
In this paper, we use the Õ(·) notation to suppress factors that
are polylogarithmic in the natural parameters of the problem.

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

ness persists even under the assumption that there exists a
k-sparse w∗ such that yt = hxt , w∗ i for all t. Furthermore,
the computational hardness is present even when the solue
tion is required to be only O(k)-sparse
solution and has to
minimize the error only approximately; see (Foster et al.,
2015) for details. The hardness result was extended to online sparse regression by (Foster et al., 2016). They showed
that for all δ > 0 there exists no polynomial-time algorithm
with regret O(T 1−δ ) unless N P ⊆ BP P .
Foster et al. (2016) posed the open question of what additional assumptions can be made on the data to make the
problem tractable. In this paper, we answer this open question by providing efficient algorithms with sublinear regret
under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) (Candes & Tao,
2005). It has been shown that if RIP holds and there exists
a sparse linear predictor w∗ such that yt = hxt , w∗ i + ηt
where ηt is independent noise, the offline sparse linear
regression problem admits computationally efficient algorithms, e.g., (Candes & Tao, 2007). RIP and related Restricted Eigenvalue Condition (Bickel et al., 2009) have
been widely used as a standard assumption for theoretical
analysis in the compressive sensing and sparse regression
literature, in the offline case. In the online setting, it is
natural to ask whether sparse regression avoids the computational difficulty under an appropriate form of the RIP
condition. In this paper, we answer this question in a positive way, both in the realizable setting and in the agnostic
setting. As a by-product, we resolve the adaptive feature
selection problem as the efficient algorithms we propose in
this paper adaptively choose a different “sparse” subset of
features to query at each round. This is closely related to
attribute-efficient learning (see discussion in Section 1.2)
and online model selection.
1.1. Summary of Results
We design polynomial-time algorithms for online sparse
linear regression for two models for the sequence
(x1 , y1 ), (x2 , y2 ), . . . , (xT , yT ). The first model is called
the realizable and the second is called agnostic. In both
models, we assume that, after proper normalization, for all
large enough t, the matrix Xt formed from the first t feature
vectors x1 , x2 , . . . , xt satisfies the restricted isometry property. The two models differ in the assumptions on yt . The
realizable model assumes that yt = hxt , w∗ i + ηt where
w∗ is k-sparse and ηt is an independent noise. In the agnostic model, yt can be arbitrary, and therefore, the regret
bounds we obtain are worse than in the realizable setting.
The models and corresponding algorithms are presented in
Sections 2 and 3 respectively. Interestingly enough, the algorithms and their corresponding analyses are completely
different in the realizable and agnostic case.

Our algorithms allow for somewhat more flexibility than
the problem definition: they are designed to work with a
budget k0 on the number of features that can be queried that
may be larger than the sparsity parameter k of the comparator. The regret bounds we derive improve with increasing
values of k0 . In the case when k0 ≈ k, the dependence on
d in the regret bounds is polynomial, as can be expected
in limited feedback settings (this is analogous to polynomial dependence on d in bandit settings). In the extreme
case when k0 = d, i.e. we have access to all the features,
the dependence on the dimension d in the regret bounds we
prove is only logarithmic. The interpretation is that if we
have full access to the features, but the goal is to compete
with just k sparse linear regressors, then the number of data
points that need to be seen to achieve good predictive accuracy has only logarithmic dependence on d. This is analogous to the (offline) compressed sensing setting where the
sample complexity bounds, under RIP, only depend logarithmically on d.
A major building block in the solution for the realizable setting (Section 2) consists of identifying the best k-sparse linear predictor for the past data at any round in the prediction
problem. This is done by solving a sparse regression problem on the observed data. The solution of this problem cannot be obtained by a simple application of say, the Dantzig
selector (Candes & Tao, 2007) since we do not observe the
data matrix X, but rather a subsample of its entries. Our
algorithm is a variant of the Dantzig selector that incorporates random sampling into the optimization, and computes
a near-optimal solution by solving a linear program. The
e
resulting algorithm has a regret bound of O(log
T ). This
bound has optimal dependence on T , since even in the full
information setting where all features are observed there is
a lower bound of Ω(log T ) (Hazan & Kale, 2014).
The algorithm for the agnostic setting relies on the theory
of submodular optimization. The analysis in (Boutsidis
et al., 2015) shows that the RIP assumption implies that
the set function defined as the minimum loss achievable
by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that
is still strong enough to show performance bounds for the
standard greedy feature selection algorithm for solving the
sparse regression problem. We then employ a technique
developed by Streeter & Golovin (2008) to construct an
online learning algorithm that mimics the greedy feature
selection algorithm. The resulting algorithm has a regret
e 2/3 ). It is unclear if this bound has the opbound of O(T
timal √
dependence on T : it is easy to prove a lower bound
of Ω( T ) on the regret using standard arguments for the
multiarmed bandit problem.

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

1.2. Related work
A related setting is attribute-efficient learning (CesaBianchi et al., 2011; Hazan & Koren, 2012; Kukliansky &
Shamir, 2015). This is a batch learning problem in which
the examples are generated i.i.d., and the goal is to simply
output a linear regressor using only a limited number of
features per example with bounded excess risk compared
to the optimal linear regressor, when given full access to
the features at test time. Since the goal is not prediction
but simply computing the optimal linear regressor, efficient
algorithms exist and have been developed by the aforementioned papers.
Without any assumptions, only inefficient algorithms for
the online sparse linear regression problem are known Zolghadr et al. (2013); Foster et al. (2016). Kale (2014) posed
the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret
bound. This question was answered in the negative by Foster et al. (2016), who showed that efficiency can only be
obtained under additional assumptions on the data. This
paper shows that the RIP assumption yields tractability in
the online setting just as it does in the batch setting.
In the realizable setting, the linear program at the heart of
the algorithm is motivated from Dantzig selection (Candes
& Tao, 2007) and error-in-variable regression (Rosenbaum
& Tsybakov, 2010; Belloni et al., 2016). The problem of
finding the best sparse linear predictor when only a sample
of the entries in the data matrix is available is also discussed
by Belloni et al. (2016) (see also the references therein). In
fact, these papers solve a more general problem where we
observe a matrix Z rather than X that is an unbiased estimator of X. While we can use their results in a black-box
manner, they are tailored for the setting where the variance
of each Zij is constant and it is difficult to obtain the exact
dependence on this variance in their bounds. In our setting,
this variance can be linear in the dimension of the feature
vectors, and hence we wish to control the dependence on
the variance in the bounds. Thus, we use an algorithm that
is similar to the one in (Belloni et al., 2016), and provide
an analysis for it (in the supplementary material). As an
added bonus, our algorithm results in solving a linear program rather than a conic or general convex program, hence
admits a solution that is more computationally efficient.
In the agnostic setting, the computationally efficient algorithm we propose is motivated from (online) supermodular optimization (Natarajan, 1995; Boutsidis et al., 2015;
Streeter & Golovin, 2008). The algorithm is computationally efficient and enjoys sublinear regret under an RIP-like
condition, as we will show in Section 3. This result can
be contrasted with the known computationally prohibitive
algorithms for online sparse linear regression (Zolghadr
et al., 2013; Foster et al., 2016), and the hardness result

without RIP (Foster et al., 2015; 2016).
1.3. Notation and Preliminaries
For d ∈ N, we denote by [d] the set {1, 2, . . . , d}. For a
vector in x ∈ Rd , denote by x(i) its i-th coordinate. For
a subset S ⊆ [d], we use the notation RS to indicate the
vector space spanned by the coordinate axes indexed by S
(i.e. the set of all vectors w supported on the set S). For a
vector x ∈ Rd , denote by x(S) ∈ Rd the projection of x
on RS . That is, the coordinates of x(S) are
(
x(i) if i ∈ S,
x(S)(i) =
for i = 1, 2, . . . , d.
0
if i 6∈ S,
P
Let hu, vi = i u(i) · v(i) be the inner product of vectors
u and v.
d
For p ∈ [0, ∞], the `p -norm of a vector
P x ∈ R is denoted
by kxkp . For p ∈ (0, ∞), kxkp = ( i |xi |p )1/p , kxk∞ =
maxi |xi |, and kxk0 is the number of non-zero coordinates
of x.

The following definition will play a key role:
Definition 1 (Restricted Isometry Property (Candes & Tao,
2007)). Let  ∈ (0, 1) and k ≥ 0. We say that a matrix
X ∈ Rn×d satisfies restricted isometry property (RIP) with
parameters (, k) if for any w ∈ Rd with kwk0 ≤ k we
have
1
(1 − ) kwk2 ≤ √ kXwk2 ≤ (1 + ) kwk2 .
n
One can show that RIP holds with overwhelming probability if n = Ω(−2 k log(ed/k)) and each row of the matrix
is sampled independently from an isotropic sub-Gaussian
distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the “small ball” analysis introduced in Mendelson (2014), since we only require one-sided lower isometry
property.
1.4. Proper Online Sparse Linear Regression
We introduce a variant of online sparse regression (OSLR),
which we call proper online sparse linear regression
(POSLR). The adjective “proper” is to indicate that the algorithm is required to output a weight vector in each round
and its prediction is computed by taking an inner product
with the feature vector.
We assume that there is an underlying sequence
(x1 , y1 ), (x2 , y2 ), . . . , (xT , yT ) of labeled examples in
Rd × R. In each round t = 1, 2, . . . , T , the algorithm behaves according to the following protocol:
1. Choose a vector wt ∈ Rd such that kwt k0 ≤ k.

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

2. Choose St ⊆ [d] of size at most k0 .

2.1. Algorithm

3. Observe xt (St ) and yt , and incur loss (yt −hxt , wt i)2 .

bt of the
The algorithm maintains an unbiased estimate X
bt are vectors x
matrix Xt . The rows of X
bT1 , x
bT2 , . . . , x
bTt
which are unbiased estimates of xT1 , xT2 , . . . , xTt . To construct the estimates, in each round t, the set St ⊆ [d] is
chosen uniformly at random from the collection of all subsets of [d] of size k0 . The estimate is

Essentially, the algorithm makes the prediction ybt :=
hxt , wt i in round t. The regret after T rounds of an algorithm with respect to w ∈ Rd is
RegretT (w) =

T
X

2

(yt − hxt , wt i) −

t=1

T
X

d
· xt (St ).
k0

(2)

t=1

The regret after T rounds of an algorithm with respect to
the best k-sparse linear regressor is defined as
RegretT =

x
bt =

2

(yt − hxt , wi) .

max

w: kwk0 ≤k

RegretT (w) .

Note that any algorithm for POSLR gives rise to an algorithm for OSLR. Namely, if an algorithm for POSLR
chooses wt and St , the corresponding algorithm for OSLR
queries the coordinates St ∪ {i : wt (i) 6= 0}. The algorithm for OSLR queries at most k0 + k coordinates and has
the same regret as the algorithm for POSLR.
Additionally, POSLR allows parameters settings which do
not have corresponding counterparts in OSLR. Namely, we
can consider the sparse “full information” setting where
k0 = d and k  d.
We denote by Xt the t × d matrix of first t unlabeled samples i.e. rows of Xt are xT1 , xT2 , . . . , xTt . Similarly, we
denote by Yt ∈ Rt the vector of first t labels y1 , y2 , . . . , yt .
We use the shorthand notation X, Y for XT and YT respectively.
In order to get computationally efficient algorithms, we assume that that for all t ≥ t0 , the matrix Xt satisfies the
restricted isometry condition. The parameter t0 and RIP
parameters k,  will be specified later.

2. Realizable Model
In this section we design an algorithm for POSLR for the
realizable model. In this setting we assume that there is a
vector w∗ ∈ Rd such that kw∗ k0 ≤ k and the sequence
of labels y1 , y2 , . . . , yT is generated according to the linear
model
yt = hxt , w∗ i + ηt ,
(1)

To compute the predictions of the algorithm, we consider
the linear program



1 T 

bt w + 1 D
bt Yt − X
b t w
minimize kwk1 s.t. 
X
t

t
∞
s


d
d log(td/δ)
≤C
σ+
.
tk0
k0
(3)
Here, C > 0 is a universal constant, and δ ∈ (0, 1) is the
b t , defined in equation (5), is
allowed failure probability. D
bT X
bt ).
a diagonal matrix that offsets the bias on the diag(X
t
The linear program (3) is called the Dantzig selector. We
denote its optimal solution by w
bt+1 . (We define w
b1 = 0.)
Based on w
bt , we construct w
et ∈ Rd . Let |w
bt (i1 )| ≥
|w
bt (i2 )| ≥ · · · ≥ |w
bt (id )| be the coordinates sorted according to the their absolute value, breaking ties according
to their index. Let Set = {i1 , i2 , . . . , ik } be the top k coordinates. We define w
et as
w
et = w
bt (Set ).

(4)

The actual prediction wt is either zero if t ≤ t0 or w
es for
some s ≤ t and it gets updated whenever t is a power of 2.
The algorithm queries at most k + k0 features each round,
and the linear program can be solved in polynomial time
using simplex method or interior point method. The algorithm solves the linear program only dlog2 T e times by
using the same vector in the rounds 2s , . . . , 2s+1 − 1. This
lazy update improves both the computational aspects of the
algorithm and the regret bound.
2.2. Main Result
The main result in this section provides a logarithmic regret
bound under the following assumptions 2

where η1 , η2 , . . . , ηT are independent random variables
from N (0, σ 2 ). We assume that the standard deviation σ,
or an upper bound of it, is given to the algorithm as input.
We assume that kw∗ k1 ≤ 1 and kxt k∞ ≤ 1 for all t.

• The feature vectors have the property that for any
t ≥ t0 , the matrix Xt satisfies the RIP condition with
( 51 , 3k), with t0 = O(k log(d) log(T )).

For convenience, we use η to denote the vector
(η1 , η2 , . . . , ηT ) of noise variables.

2
A more precise statement with the exact dependence on the
problem parameters can be found in the supplementary material.

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Algorithm 1 Dantzig Selector for POSLR
Require: T , σ, t0 , k, k0
1: for t = 1, 2, . . . , T do
2:
if t ≤ t0 then
3:
Predict wt = 0
4:
else if t is a power of 2 then
5:
Let w
bt be the solution of linear program (3)
6:
Compute w
et according to (4)
7:
Predict wt = w
et
8:
else
9:
Predict wt = wt−1
10:
end if
11:
Let St ⊆ [d] be a random subset of size k0
12:
Observe xt (St ) and yt
13:
Construct estimate x
bt according to (2)
bt−1 to form X
bt ∈ Rt×d
14:
Append x
bTt to X
15: end for

• The underlying POSLR online prediction problem has
a sparsity budget of k and observation budget k0 .
• The model is realizable as defined in equation (1) with
i.i.d unbiased Gaussian noise with standard deviation
σ = O(1).
Theorem 2. For any δ > 0, with probability at least 1 − δ,
Algorithm 1 satisfies

RegretT = O k 2 log(d/δ)(d/k0 )3 log(T ) .
The theorem asserts that an O(log T ) regret bound is efficiently achievable in the realizable setting. Furthermore
when k0 = Ω(d) the regret scales as log(d) meaning that
we do not necessarily require T ≥ d to obtain a meaningful
result. We note that the complete expression for arbitrary
t0 , σ is given in (13) in the supplementary material.
The algorithm can be easily understood via the error-invariable equation
yt = hxt , w∗ i + ηt ,
x
bt = xt + ξt .
with E[ξt ] = E[b
xt − xt ] = 0, where the expectation is
taken over random sampling introduced by the algorithm
when performing feature exploration. The learner observes
yt as well as the “noisy” feature vector x
bt , and aims to
recover w∗ .
As mentioned above, we (implicitly) need an unbiased esbtT X
bt it is easy to verify
timator of XtT Xt . By taking X
that the off-diagonal entries are indeed unbiased however
this is not the case for the diagonal. To this end we define
Dt ∈ Rd×d as the diagonal matrix compensating for the

btT X
bt
sampling bias on the diagonal elements of X



d
Dt =
− 1 · diag XtT Xt
k0
and the estimated bias from the observed data is




k0
btT X
bt .
b
· diag X
Dt = 1 −
d

(5)

Therefore, program (1) can be viewed as Dantzig selector
with plug-in unbiased estimates for XtT Yt and XtT Xt using
limited observed features.
2.3. Sketch of Proof
The main building block in proving Theorem 2 is stated in
Lemma 3. It proves that the sequence of solutions w
bt converges to the optimal response w∗ based on which the signal yt is created. More accurately, ignoring
√all second order
terms, it shows that kw
bt − w∗ k1 ≤ O(1/ t). In Lemma 4
we show that the same applies for the sparse approximation wt of w
bt . Now, since kxt k∞ ≤ 1 we get that the
difference between our response hxt , wt i and
√ the (almost)
optimal response hxt , w∗ i is bounded by 1/ t. Given this,
a careful calculation of the difference of losses leads to a
regret bound w.r.t. w∗ . Specifically, an elementary analysis
of the loss expression leads to the equality
RegretT (w∗ ) =

T
X

2ηt hxt , w∗ − wt i + (hxt , w∗ − wt i)

2

t=1

A bound on both summands can√clearly be expressed in
terms of | hxt , w∗ − wt i | = O(1/ t). The right summand
requires a martingale concentration bound and the left is
trivial. For both we obtain a bound of O(log(T )).
We are now left with two technicalities. The first is that
w∗ is not necessarily the empirically optimal response. To
this end we provide, in Lemma 16 in the supplementary
material, a constant (independent of T ) bound on the regret
of w∗ compared to the empirical optimum. The second
technicality is the fact that we do not solve for w
bt in every
round, but in exponential gaps. This translates to an added
factor of 2 to the bound kwt − w∗ k1 that affects only the
constants in the O(·) terms.
Lemma 3 (Estimation Rates). Assume that the matrix
Xt ∈ Rt×d satisfies the RIP condition with (, 3k) for some
 < 1/5. Let w
bn+1 ∈ Rd be the optimal solution of program (3). With probability at least 1 − δ,
s


d k log(d/δ)
d
∗
·
σ+
,
kw
bt+1 − w k2 ≤ C ·
k0
t
k0
s


d k 2 log(d/δ)
d
∗
kw
bt+1 − w k1 ≤ C ·
σ+
.
k0
t
k0

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Here C > 0 is some universal constant and σ is the standard deviation of the noise.
Note the w
bt may not be sparse; it can have many non-zero
coordinates that are small in absolute value. However, we
take the top k coordinates of w
bt in absolute value. Thanks
√
to the Lemma 4 below, we lose only a constant factor 3.
Lemma 4. Let w
b ∈ Rd be an arbitrary vector and let
∗
d
w ∈ R be a k-sparse vector. Let Se ⊆ [d] be the top
k coordinates of w
b in absolute value. Then,


√

 e
b − w ∗ k2 .
b S) − w∗  ≤ 3 kw
w(
2

3. Agnostic Setting
In this section we focus on the agnostic setting, where
we don’t impose any distributional assumption on the sequence. In this setting, there is no “true” sparse model, but
the learner — with limited access to features — is competing with the best k-sparse model defined using full inforT
mation {(xt , yt )}t=1 .
As before, we do assume that xt and yt are bounded. Without loss of generality, kxt k∞ ≤ 1, and |yt | ≤ 1 for all
t. Once again, without any regularity condition on the design matrix, Foster et al. (2016) have shown that achieving
a sub-linear regret O(T 1−δ ) is in general computationally
hard, for any constant δ > 0 unless NP ⊆ BPP.
We give an efficient algorithm that achieves sub-linear regret under the assumption that the design matrix of any
(sufficiently long) block of consecutive data points has
bounded restricted condition number, which we define below:
Definition 5 (Restricted Condition Number). Let k ∈ N be
a sparsity parameter. The restricted condition number for
sparsity k of a matrix X ∈ Rn×d is defined as
kXvk
.
v,w: kvk=kwk=1, kXwk
sup

kvk0 ,kwk0 ≤k

It is easy to see that if a matrix X satisfies RIP with parameters (, k), then its restricted condition number for sparsity
1+
k is at most 1−
. Thus, having bounded restricted condition
number is a weaker requirement than RIP.
We now define the Block Bounded Restricted Condition
Number Property (BBRCNP):
Definition 6 (Block Bounded Restricted Condition Number Property). Let κ > 0 and k ∈ N. A sequence of feature
vectors x1 , x2 , . . . , xT satisfies BBRCNP with parameters
(κ, K) if there is a constant t0 such that for any sequence
of consecutive time steps T with |T | ≥ t0 , the restricted
condition number for sparsity k of X, the design matrix of
the feature vectors xt for t ∈ T , is at most κ.

Note that in the random design setting where xt , for t ∈
[T ], are isotropic sub-Gaussian vectors, t0 = O(log T +
k log d) suffices to satisfy BBRCNP with high probability,
where the O(·) notation hides a constant depending on κ.
We assume in this section that the sequence of feature vectors satisfies BBRCNP with parameters (κ, K) for some
K = O(k log(T )) to be defined in the course of the analysis.
3.1. Algorithm
The algorithm in the agnostic setting is of distinct nature
from that in the stochastic setting. Our algorithm is motivated from literature on maximization of sub-modular
set function (Natarajan, 1995; Streeter & Golovin, 2008;
Boutsidis et al., 2015). Though the problem being NPhard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically,
(Streeter & Golovin, 2008) considered online optimization
of super/sub-modular set functions using expert algorithm
as sub-routine. (Natarajan, 1995; Boutsidis et al., 2015)
cast the sparse linear regression as maximization of weakly
supermodular function. We will introduce an algorithm that
blends various ideas from referred literature, to attack the
online sparse regression with limited features.
First, let’s introduce the notion of a weakly supermodular
function.
Definition 7. For parameters k ∈ N and α ≥ 1, a set
function g : [d] → R is (k, α)-weakly supermodular if for
any two sets S ⊆ T ⊆ [d] with |T | ≤ k, the following two
inequalities hold:
1. (monotonicity) g(T ) ≤ g(S), and
2. (approximately decreasing marginal gain)
X
g(S) − g(T ) ≤ α
[g(S) − g(S ∪ {i})].
i∈T \S

The definition is slightly stronger than that in (Boutsidis
et al., 2015). We will show that sparse linear regression can
be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.
Now we outline the algorithm (see Algorithm 2). We divide
the rounds 1, 2, . . . , T into mini-batches of size B each (so
there are T /B such batches). The b-th batch thus consists
of the examples (xt , yt ) for t ∈ Tb := {(b − 1)B + 1, (b −
1)B + 1, . . . , bB}. Within the b-th batch, our algorithm
queries the same subset of features of size at most k0 .
The algorithm consists of few key steps. First, one can
show that under BBRCNP, as long as B is large enough,

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

the loss within batch b defines a weakly supermodular set
function
X
1
gt (S) =
inf
(yt − hxt , wi)2 .
B w∈RS
t∈Tb

Therefore, we can formulate the original online sparse regression problem into online weakly supermodular minimization problem. For the latter problem, we develop
an online greedy algorithm along the lines of (Streeter &
Golovin, 2008). We employ k1 = O∗ (k) budgeted experts
algorithms (Amin et al., 2015), denoted BEXP, with budget parameter3 kk01 . The precise characteristics of BEXP
are given in Theorem 8 (adapted from Theorem 2 in (Amin
et al., 2015)).
Theorem 8. For the problem of prediction from expert advice, let there be d experts, and let k ∈ [d] be a budget
parameter. In each prediction round t, the BEXP algorithm
chooses an expert jt and a set of experts Ut containing jt
of size at most k, obtains as feedback the losses of all the
experts in Ut , suffers the lossq
of expert jt , and guarantees
an expected regret bound of 2
rounds.

d log(d)
T
k

over T prediction

At the beginning of each mini-batch b, the BEXP algorithms are run. Each BEXP algorithm outputs a set of
coordinates of size kk10 as well as a special coordinate in
that set. The union of all of these sets is then used as the
set of features to query throughout the subsequent minibatch. Within the mini-batch, the algorithm runs the standard Vovk-Azoury-Warmuth algorithm for linear prediction with square loss restricted to set of special coordinates
output by all the BEXP algorithms.
At the end of the mini-batch, every BEXP algorithm is provided carefully constructed losses for each coordinate that
was output as feedback. These losses ensure that the set of
special coordinates chosen by the BEXP algorithms mimic
the greedy algorithm for weakly supermodular minimization.
3.2. Main Result
In this section, we will show that Algorithm 2 achieves sublinear regret under BBRCNP.
Theorem 9. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (κ, k1 + k) for k1 =
1 2
3 κ k log(T ), and assume that T is large enough so that
T 1/3
t0 ≤ ( κk20dk
) . Then if Algorithm 2 is run with paramek0 T 1/3
ters B = ( κ2 dk )
and k1 as specified above, its expected
8
4
regret is at most Õ(( κ kdk
)1/3 T 2/3 ).
0
Proof. The proof relies on a number of lemmas whose
3

We assume, for convenience, that k0 is divisible by k1 .

Algorithm 2 Online Greedy Algorithm for POSLR
Require: Mini-batch size B, sparsity parameters k0 and
k1
(i)
1: Set up k1 budgeted prediction algorithms BEXP for
i ∈ [k1 ], each using the coordinates in [d] as “experts”
with a per-round budget of kk10 .
2: for b = 1, 2, . . . , T /B do
(i)
3:
For each i ∈ [k1 ], obtain a coordinate jb and subset
(i)
(i)
of coordinates Ub from BEXP(i) such that jb ∈
(i)
Ub .
(0)
(i)
4:
Define Vb = ∅ and for each i ∈ [k1 ] define Vb =
(i0 )

5:
6:
7:

{jb | i0 ≤ i}.
Set up the Vovk-Azoury-Warmuth (VAW) algorithm
(k )
for predicting using the features in Vb 1 .
for t ∈ Tb do S
(i)
Set St = i∈[k1 ] Ub , obtain xt (St ), and pass
(k )

8:
9:
10:
11:

xt (Vb 1 ) to VAW.
Set wt to be the weight vector output by VAW.
Obtain the true label yt and pass it to VAW.
end for
Define the function
X
1
inf
(yt − hxt , wi)2 .
(6)
gb (S) =
B w∈RS
t∈Tb

(i)

(i−1)

For each j ∈ Ub , compute gb (Vb
∪ {j}) and
(i)
pass it BEXP as the loss for expert j.
13: end for
12:

proofs can be found in the supplementary material. We begin with the connection between sparse linear regression,
weakly supermodular function and RIP, formally stated in
Lemma 10. This lemma is a direct consequence of Lemma
5 in (Boutsidis et al., 2015).
Lemma 10. Consider a sequence of examples (xt , yt ) ∈
Rd × R for t = 1, 2, . . . , B, and let X be the design matrix
for the sequence. Consider the set function associated with
least squares optimization:
g(S) = inf

w∈RS

B
1 X
(yt − hxt , wi)2 .
B t=1

Suppose the restricted condition number of X for sparsity
k is bounded by κ. Then g(S) is (k, κ2 )-weakly supermodular.
Even though minimization of weakly supermodular functions is NP-hard, the greedy algorithm provides a good approximation, as shown in the next lemma.
Lemma 11. Consider a (k, α)-weakly supermodular set
function g(·). Let j ∗ := arg minj g({j}). Then, for any

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

4. Conclusions and Future Work

subset V of size at most k, we have


1
g({j ∗ }) − g(V ) ≤ 1 − α|V
| [g(∅) − g(V )].
The BEXP algorithms essentially implement the greedy algorithm in an online fashion. Using the properties of the
BEXP algorithm, we have the following regret guarantee:
Lemma 12. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (, k1 + k). Then for any set
V of coordinates of size at most k, we have


T /B
X
(k )
E
gb (Vb 1 ) − gb (V )

In this paper, we gave computationally efficient algorithms
for the online sparse linear regression problem under the
assumption that the design matrices of the feature vectors
satisfy RIP-type properties. Since the problem is hard without any assumptions, our work is the first one to show that
assumptions that are similar to the ones used to sparse recovery in the batch setting yield tractability in the online
setting as well.

Several open questions remain in this line of work and will
be the basis for future work. Is it possible to improve the
regret bound in the agnostic setting? Can we give matching lower bounds on the regret in various settings? Is it
b=1
possible to relax the RIP assumption on the design maT /B 
q
k1
trices and still have efficient algorithms? Some obvious
X
[gb (∅) − gb (V )] + 2κ2 k dk1 klog(d)T
1 − κ21|V |
.
≤
weakenings of the RIP assumption we have made don’t
0B
b=1
yield tractability. For example, simply assuming that the
final matrix XT satisfies RIP rather than every intermediFinally, within every mini-batch, the VAW algorithm guarate matrix Xt for large enough t is not sufficient; a simantees the following regret bound, an immediate conseple tweak to the lower bound construction of Foster et al.
quence of Theorem 11.8 in Cesa-Bianchi & Lugosi (2006):
(2016) shows this. This tweak consists of simply padding
the construction with enough dummy examples which are
Lemma 13. Within every batch b, the VAW algorithm genwell-conditioned enough to overcome the ill-conditioning
erates weight vectors wt for t ∈ Tb such that
of the original construction so that RIP is satisfied by XT .
X
(k1 )
We note however that in the realizable setting, our analy2
(yt − hxt , wt i) − Bgb (Vb ) ≤ O(k1 log(B)).
sis can be easily adapted to work under weaker conditions
t∈Tb
such as irrepresentability (Zhao & Yu, 2006; Javanmard &
Montanari, 2013).
We can now prove Theorem 9. Combining the bounds of
lemma 12 and 13, we conclude that for any subset of coorReferences
dinates V of size at most k, we have
" T
#
Amin, Kareem, Kale, Satyen, Tesauro, Gerald, and Turaga,
X
2
Deepak S. Budgeted prediction with expert advice. In
E
(yt − hxt , wt i)
(7)
AAAI, pp. 2490–2496, 2015.
t=1
T /B

≤

X

Bgb (V ) + B(1 −

k1
1
κ2 |V | ) [gb (∅)

− gb (V )] (8)

b=1



q
T
dk1 log(d)BT
2
+O κ k
+ k1 log(B) .
k0
B

(9)

Finally, note that
T /B

X
b=1

Bgb (V ) ≤ inf

T
X

w∈RV

(yt − hxt , wi)2 ,

Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexandre B. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, pp. 1705–1732, 2009.

t=1

Boutsidis, Christos, Liberty, Edo, and Sviridenko, Maxim.
Greedy minimization of weakly supermodular set functions. arXiv preprint arXiv:1502.06528, 2015.

and
T /B

X

Belloni, Alexandre, Rosenbaum, Mathieu, and Tsybakov,
Alexandre B. Linear and conic programming estimators
in high dimensional errors-in-variables models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 2016. ISSN 1467-9868.

B(1 −

k1
1
κ2 |V | ) [gb (∅)

− gb (V )] ≤ T · exp(− κk21k ),

b=1

because gb (∅) ≤ 1. Using these bounds in (9), and plugging in the specified values of B and k1 , we get the stated
regret bound.

Candes, Emmanuel and Tao, Terence. The Dantzig selector: statistical estimation when p is much larger than n.
The Annals of Statistics, pp. 2313–2351, 2007.
Candes, Emmanuel J and Tao, Terence. Decoding by linear

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

programming. IEEE transactions on information theory,
51(12):4203–4215, 2005.
Cesa-Bianchi, Nicolò and Lugosi, Gábor. Prediction,
learning, and games. Cambridge University Press, 2006.
Cesa-Bianchi, Nicolò, Shalev-Shwartz, Shai, and Shamir,
Ohad. Efficient learning with partially observed attributes. Journal of Machine Learning Research, 12
(Oct):2857–2878, 2011.
Foster, Dean, Karloff, Howard, and Thaler, Justin. Variable
selection is hard. In COLT, pp. 696–709, 2015.
Foster, Dean, Kale, Satyen, and Karloff, Howard. Online
sparse linear regression. In COLT, 2016.
Hazan, Elad and Kale, Satyen.
Beyond the regret
minimization barrier: optimal algorithms for stochastic strongly-convex optimization. Journal of Machine
Learning Research, 15(1):2489–2512, 2014.
Hazan, Elad and Koren, Tomer. Linear regression with limited observation. In ICML, 2012.
Javanmard, Adel and Montanari, Andrea. Model selection
for high-dimensional regression under the generalized
irrepresentability condition. In NIPS, pp. 3012–3020,
2013.
Kale, Satyen. Open problem: Efficient online sparse regression. In COLT, pp. 1299–1301, 2014.
Kukliansky, Doron and Shamir, Ohad. Attribute efficient
linear regression with distribution-dependent sampling.
In ICML, pp. 153–161, 2015.
Mendelson, Shahar. Learning without concentration. In
COLT, pp. 25–39, 2014.
Natarajan, Balas Kausik. Sparse approximate solutions to
linear systems. SIAM journal on computing, 24(2):227–
234, 1995.
Rosenbaum, Mathieu and Tsybakov, Alexandre B. Sparse
recovery under matrix uncertainty. The Annals of Statistics, 38(5):2620–2651, 2010.
Streeter, Matthew J. and Golovin, Daniel. An online algorithm for maximizing submodular functions. In NIPS,
pp. 1577–1584, 2008.
Zhao, Peng and Yu, Bin. On model selection consistency
of lasso. Journal of Machine learning research, 7(Nov):
2541–2563, 2006.
Zolghadr, Navid, Bartók, Gábor, Greiner, Russell, György,
András, and Szepesvári, Csaba. Online learning with
costly features and labels. In NIPS, pp. 1241–1249,
2013.

