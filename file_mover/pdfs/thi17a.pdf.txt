Stochastic DCA for the Large-sum of Non-convex Functions Problem and its
Application to Group Variable Selection in Classification

Hoai An Le Thi 1 Hoai Minh Le 1 Duy Nhat Phan 1 Bach Tran 1

Abstract

Nowadays, the growth of technologies leads to exponential augmentation of large-scale data where the number of
both variables and samples are huge. Thus, optimization
methods for solving the problem (1) are faced with a great
challenge that is the number of samples n can be extremely
large. Among existing methods for this problem, stochastic
programming has been proved to be suitable thanks to its
ability to exploit the advantage of the sum structure of the
problem. In (Schmidt et al., 2015), the authors considered
a special case of the large-sum problem (1) where fi are
convex and smooth functions and p corresponds to the `2
regularization. Stochastic Average Gradient was developed
to solve the resulting problem. Reddi et al. (Reddi et al.,
2016) developed Proximal Stochastic Gradient method for
the case where fi are smooth (can be non-convex) and p
is convex, non-smooth function. Motivated by its success,
we will study stochastic programming for solving (1) in order to deal with data having an extremely large number of
samples.

In this paper, we present a stochastic version
of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems
whose objective function is a large sum of nonconvex functions and a regularization term. We
consider the `2,0 regularization to deal with the
group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points
onto balls that is explicitly computed. As an application, we applied our algorithm for the group
variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the
efficiency of our algorithm and its superiority
over well-known methods, with respect to classification accuracy, sparsity of solution as well
as running time.

1. Introduction
We consider the following optimization problem
)
(
n
1X
fi (x) + Œªp(x) ,
min f (x) =
n i=1

(1)

whose objective function f is a large sum of non-convex
functions fi (x) and a regularization term p(x), where fi (x)
corresponds to a criteria to optimize and Œª ‚â• 0 is a tradeoff parameter between the two terms. This model covers
a very vast class of problems arising from several fields
such as machine learning, signal processing, etc. For instance, least-squares regression, logistic regression problem, etc can be expressed in the form of (1).
1

Laboratory of Theoretical and Applied Computer Science,
University of Lorraine, France. Correspondence to: Hoai Minh
Le <minh.le@univ-lorraine.fr>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

On the other hand, in real-world applications such as image processing, microarray analysis, etc. datasets contain
a very large number of variables. In such of cases, we are
often to face with the problem of redundant and irrelevant
variables. Redundant variables contain information already
presented by other variables while irrelevant variables do
not contain useful information. Variables selection methods that consist of selecting important variables for a considered task, are a popular and efficient way to deal with
redundant and irrelevant variables. In this direction, a natural idea is to formulate the variables selection problem as
a minimization of the `0 -norm (or k.k0 ). The sparse optimization has been extensively studied on both theoretical
and practical aspects. The readers can refer to Le Thi et al.
(Le Thi et al., 2015) for an extensive overview of existing
approaches for the minimization of `0 -norm.
Nevertheless, when the data possesses certain group structures, we are naturally interested in selecting important
groups of variables rather than individual ones. For
instance, in multi-factor analysis of variance, a factor
with several levels may be expressed through a group of
dummy variables. In genomic data analysis, the correlations between genes sharing the biological pathway can

Stochastic DCA for the Large-sum of Non-convex Functions Problem

be high. Hence these genes should be considered as a
group. Recently, the mixed-norm regularization has been
developed for the group variable selection. It consists
in using the `2,0 regularization term. Assume that x =
(x1 , ..., xm ) ‚àà Rm is partitioned into J non-overlapping
groups x(1)
, ..., x(J) , then the `2,0 -norm of
	 x is defined by
kxk2,0 = | j ‚àà {1, ..., J} : kx(j) k2 6= 0 |. Clearly, `2,0 norm is non-convex that makes the optimization problem
involving `2,0 challenging. Several works have been developed to solve the problem of mixed-norm regularization
`2,0 . The first approach, named the group Lasso (`2,1 norm) (Yuan & Lin, 2006), is closely connected to the
Lasso (`1 -norm) - an approximation of the `0 -norm (Tibshirani, 1994). This approach was widely used for selecting groups of variables in multi-task learning (Obozinski
et al., 2006), multiclass support vector machine (Blondel
et al., 2013), principal component analysis (Khan et al.,
2015), linear discriminant analysis (Gu et al., 2011), and
compressed sensing (Sun et al., 2009), etc. The second approach consists in replacing the `2,0 -norm by a DC (Difference of Convex functions) approximation. In (Wang et al.,
2007), the authors used the smoothly clipped absolute deviation (SCAD) approximation and developed a group coordinate descent based algorithm for the sparse linear regression. Later, Huang et al. (Huang et al., 2012) used the
minimax concave penalty (MCP) for the same problem. In
(Lee et al., 2016), the authors considered both above approximations and developed DC programming and DCA
(DC algorithm) based method for the resulting problems.
Recently, Phan et al. (Phan et al., 2017) proposed DCA
based algorithms for bi-level variable selection using the
combination of the `0 -norm and `q,0 -norm.
Paper‚Äôs contribution: In this paper, we aim at developing
efficient methods to solve the problem (1) where n is extremely large and p(x) corresponds to `2,0 regularization
(in order to deal with the group variables selection). The
large-sum optimization (1) becomes
)
(
n
1X
fi (x) + Œªkxk2,0 .
(2)
min f (x) =
n i=1
We assume that fi (x) is differentiable with L-Lipschitz
gradient. This assumption is broad enough to cover several applications. Various important problems in machine
learning such as Multi-task feature selection, Sparse logistic regression, Minimizing an expected loss in stochastic
programming, etc. can be expressed in the form of (2).
As we have mentioned above, the `2,0 -norm can be approximated by a convex (e.g. `2,1 -norm) or non-convex
function. Using a non-convex approximation will lead to a
‚Äùharder‚Äù optimization problem but it has been proved that
non-convex approximations perform better than convex approximations in terms of sparsity (Le Thi et al., 2015). The
resulting problem is then reformulated as a DC program

and DCA based algorithm will be developed to solve it.
We exploit the special structure of the problem to propose
an efficient DC decomposition for which the corresponding
DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. On
the other hand, in order to deal with data having a large
number of samples, we present stochastic version DCA.
The convergence properties of the proposed algorithm is
rigorously studied to show that the convergence is guaranteed with probability one.
As an application of our algorithm, we consider the group
variables selection in multiclass logistic regression. We
perform an empirical comparison of stochastic DCA with
DCA and standard methods on very large synthetic and
real-world datasets, and show that the stochastic DCA is
efficient in group variable selection ability and classification accuracy as well as running time.
The remainder of the paper is organized as follows. Solution method based on Stochastic DCA for solving (2) is
developed in Section 2. In Section 3, we apply the proposed
algorithm to the group variables selection in multiclass logistic regression. Finally Section 4 concludes the paper.

2. Solution method via stochastic DCA
2.1. Outline of DC programming and DCA
DC programming and DCA constitute the backbone of
smooth/non-smooth non-convex programming and global
optimization (Pham Dinh & Le Thi, 1997; 1998; Le Thi &
Pham Dinh, 2005). They address the problem of minimizing a DC function on the whole space Rn or on a closed
convex set ‚Ñ¶ ‚äÇ Rn . Generally speaking, a standard DC
program takes the form:
Œ± = inf{F (x) := G(x) ‚àí H(x) | x ‚àà Rn }

(Pdc ),

where G, H are lower semi-continuous proper convex
functions on Rn . Such a function F is called a DC function, and G‚àíH is a DC decomposition of F while G and H
are the DC components of F . A DC program with convex
constraint x ‚àà ‚Ñ¶ can be equivalently expressed as (Pdc ) by
adding the indicator function œá‚Ñ¶ (œá‚Ñ¶ (x) = 0 if x ‚àà ‚Ñ¶ and
+‚àû otherwise) to the first DC component G.
The modulus of strong convexity of Œ∏ on ‚Ñ¶, denoted by
¬µ(Œ∏, ‚Ñ¶) or ¬µ(Œ∏) if ‚Ñ¶ = Rn , is given by
¬µ(Œ∏, ‚Ñ¶) = sup{¬µ ‚â• 0 : Œ∏ ‚àí (¬µ/2)k.k2 is convex on ‚Ñ¶}
One says that Œ∏ is strongly convex on ‚Ñ¶ if ¬µ(Œ∏, ‚Ñ¶) > 0.
For a convex function Œ∏, the subdifferential of Œ∏ at x0 ‚àà
domŒ∏ := {x ‚àà Rn : Œ∏(x0 ) < +‚àû}, denoted by ‚àÇŒ∏(x0 ), is

Stochastic DCA for the Large-sum of Non-convex Functions Problem

defined by
n

‚àÇŒ∏(x0 ) := {y ‚àà R : Œ∏(x) ‚â• Œ∏(x0 ) + hx ‚àí x0 , yi,
‚àÄx ‚àà Rn }.
The subdifferential ‚àÇŒ∏(x0 ) generalizes the derivative in the
sense that Œ∏ is differentiable at x0 if and only if ‚àÇŒ∏(x0 ) ‚â°
{‚àáx Œ∏(x0 )}.
A point x‚àó is called a critical point of G ‚àí H, or a
generalized Karush-Kuhn-Tucker point (KKT) of (Pdc )) if
‚àÇH(x‚àó ) ‚à© ‚àÇG(x‚àó ) 6= ‚àÖ.
The main idea of DCA is simple: each iteration l of DCA
approximates the concave part ‚àíH by its affine majorization (that corresponds to taking v l ‚àà ‚àÇH(xl )) and then
computes xl+1 by solving the resulting convex problem.

Each function fi (x) can be rewritten as
hœÅ
i
œÅ
fi (x) = kxk2 ‚àí kxk2 ‚àí fi (x) .
2
2
Since
fi (x) is differentiable with L-Lipschitz gradient,
œÅ
2
kxk
‚àí fi (x) is strongly convex with œÅ > L. Hence,
2
fi (x) is a DC function. Consequently, f (x) is a DC function with the following DC decomposition
f (x) = g(x) ‚àí h(x),

(4)

where g(x) and h(x) are convex functions defined by
g(x) = œÅ2 kxk2 + ŒªgÃÉ(x),
n
P
h(x) = n1
hi (x); hi (x) = œÅ2 kxk2 ‚àí fi (x) + ŒªhÃÉ(x).
i=1

minn {G(x) ‚àí hv l , xi}.

x‚ààR

The sequence {xl } generated by DCA enjoys the following
properties (Pham Dinh & Le Thi, 1997; 1998; Le Thi &
Pham Dinh, 2005):

DCA for solving (3) amounts to computing two sequences
{xl } and {v l } such that v l ‚àà ‚àÇh(xl ) and xl+1 is an optimal
solution of the following convex problem

	
min g(x) ‚àí hv l , xi .
(5)

(i) The sequence {F (xl )} is decreasing;
(ii) If F (xl+1 ) = F (xl ), then xl is a critical point of (Pdc )
and DCA terminates at l-th iteration.
(iii) If ¬µ(G) + ¬µ(H) > 0 then the series {kxk+1 ‚àí xk k2
converges.
(iv) If the optimal value Œ± of (Pdc ) is finite and the infinite
sequence {xl } is bounded then every limit point of the
sequence {xl } is a critical point of G ‚àí H.
2.2. Stochastic DCA for solving the problem (2)
In this section, we introduce a stochastic version of DCA
for solving (2) that exploits the structure of objective function f . We consider a family of DC approximations pÃÉ(x)
of `2,0 -norm, defined by
pÃÉ(x) =

J
X

Œ∑(kx(j) k2 ),

j=1

where Œ∑ is a non-convex penalty function which includes
SCAD, MCP, Capped-`1 , exponential function, `p+ with
0 < p < 1, `p‚àí with p < 0 (see (Le Thi et al., 2015) for
more details). pÃÉ(x) can be expressed as pÃÉ(x) = gÃÉ(x) ‚àí
hÃÉ(x), where
gÃÉ(x) = Œ±

J
X
j=1

kx(j) k2 and hÃÉ(x) = Œ±

J
X

kx(j) k2 ‚àí pÃÉ(x).

The computation of subgradients of h requires the one of
all components hi . This can be expensive when n is very
large. Hence we propose a stochastic version of DCA in
which we only compute the subgradients of a small subset
of components hi . Precisely, at each iteration l, we compute vil ‚àà ‚àÇhi (xl ) for i ‚àà sl and keep vil = vil‚àí1 for i 6‚àà sl ,
where sl ‚äÇ {1, ..., n} is a randomly chosen set of index.
The computation of vil ‚àà ‚àÇhi (xl ) can be given as vil =
œÅxl ‚àí ‚àáfi (xl ) + y l , where y l ‚àà Œª‚àÇ hÃÉ(xl ) for all i ‚àà sl . The
convex problem (5) take the form
Ô£º
Ô£±
n
J
Ô£Ω
Ô£≤ X
X
1
œÅ
vil , xi . (6)
min ŒªŒ±
kx(j) k2 + kxk2 ‚àí h
Ô£æ
Ô£≥
2
n i=1
j=1
We observe that the objective of (6) is separable in groups
of x, then the solution to this problem can be computed by
solving J independent sub-problems of the same form:
n
o
œÅ
l
min ŒªŒ±kx(j) k2 + kx(j) k2 ‚àí hv(j)
, x(j) i , (7)
2
P
n
1
l
l
where v(j)
= œÅn
i=1 (vi )(j) for j = 1, ..., J. The solution of (7) can be explicitly computed by


l
xl+1
(j) = kv(j) k2 ‚àí ŒªŒ±/œÅ

l
v(j)
+

l k
kv(j)
2

,

(8)

j=1

Hence, the approximate problem of (2) can be written as
)
(
n
i
1 Xh
fi (x) + ŒªgÃÉ(x) ‚àí ŒªhÃÉ(x) . (3)
min f (x) =
x‚ààRm
n i=1

Thus, the stochastic DCA (SDCA) for solving the problem
(3) is described in Algorithm 1.
Now we will prove that the convergence properties of
SDCA are guaranteed with probability one.

Stochastic DCA for the Large-sum of Non-convex Functions Problem

Algorithm 1 SDCA for solving the problem (3)
Initialization: Choose x0 ‚àà Rm , œÅ > L and s0 =
{1, ..., n}, l ‚Üê 0.
Repeat
1. Compute vil ‚àà ‚àÇhi (xl ) for i ‚àà sl and keep vil =
l‚àí1
vi for i 6‚àà sl .
2. Compute xl+1 by using (8).
3. Set l ‚Üê l+1 and randomly choose a small subset
sl ‚äÇ {1, ..., n}.
Until Stopping criterion.

Theorem 1. If Œ±‚àó = inf f (x) > ‚àí‚àû and |sl | = b for all
l ‚â• 1, then SDCA generates the sequence {xl } such that

b) Since yil‚àí1 ‚àà Œª‚àÇ hÃÉ(xl‚àí1
i ), we have
l‚àí1 l‚àí1
ŒªhÃÉ(x) ‚â• ŒªhÃÉ(xl‚àí1
i.
i ) + hx ‚àí xi , yi

(11)

Since fi (x) is a differentiable function with L-Lipschitz
gradient, we have
L
l‚àí1
l‚àí1
l‚àí1 2
fi (x) ‚â§ fi (xl‚àí1
i )+hx‚àíxi , ‚àáfi (xi )i+ kx‚àíxi k .
2
Thus, we get that
fi (x) + ŒªpÃÉ(x) ‚â§ Til‚àí1 (x) +

L‚àíœÅ
2
kx ‚àí xl‚àí1
i k . (12)
2

From (9) and (12), we have
œÅ‚àíL X l
2
T l (xl+1 ) ‚â§ T l‚àí1 (xl ) ‚àí
kx ‚àí xl‚àí1
i k . (13)
2n i‚ààs
l

a) {f (xl )} is the almost sure convergent sequence.
P‚àû
l
l‚àí1 2
b)
k is almost surely finite and
l=1 kx ‚àí x
l
liml‚Üí‚àû kx ‚àí xl‚àí1 k = 0 almost surely.
c) Every limit point of {xl } is a critical point of f with
probability one.
Proof. a) Let xli = xl and yil = y l for i ‚àà sl , xli = xl‚àí1
i
and yil = y l‚àí1 for i 6‚àà sl . We denote Til the function given
by
œÅ
Til (x) = ŒªgÃÉ(x) + kx ‚àí xli k2 ‚àí hx ‚àí xli , yil ‚àí ‚àáfi (xli i
2
l
+ fi (xi ) ‚àí hÃÉ(xli ),
Pn
and T l (x) = n1 i=1 Til (x). From the step 2 in Algorithm
1, it follows that xl+1 = arg min T l (x). Hence, we have
T l (xl+1 ) ‚â§ T l (xl ) =T l‚àí1 (xl ) +

1X
[fi (xl )
n i‚ààs
l

l

+ ŒªpÃÉ(x ) ‚àí

(9)

Til‚àí1 (xl )].

Let Fl denote the œÉ-algebra generated by the entire history of SDCA up to the iteration l, i.e., F0 = œÉ(x0 ) and
Fl = œÉ(x0 , ..., xl , s0 , ..., sl‚àí1 ) for all l ‚â• 1. By taking
the expectation of the inequality (9) conditioned on Fl , we
have



b  l‚àí1 l
E T l (xl+1 )|Fl ‚â§ T l‚àí1 (xl ) ‚àí
T (x ) ‚àí f (xl ) .
n
By the supermartingale convergence theorem, we can conclude that the sequence {T l‚àí1 (xl ) ‚àí Œ±‚àó } converges almost
surely. Moreover,
‚àû
X
 l‚àí1 l

T (x ) ‚àí f (xl ) < ‚àû,

(10)

l=1
l

almost surely and hence {f (x )} converges almost surely.

By taking the expectation of the inequality (13) conditioned
on Fl , we have
n
 l l+1

b(œÅ ‚àí L) X l l‚àí1 2
l‚àí1 l
kx ‚àíxi k .
E T (x )|Fl ‚â§ T (x )‚àí
2n2 i=1
By the supermartingale convergence theorem, we conclude
that
‚àû X
n
X
2
kxl ‚àí xl‚àí1
(14)
i k < ‚àû,
l=1 i=1

is almost surely satisfied. In particular, we have
‚àû
X

kxl ‚àí xl‚àí1 k2 < ‚àû,

(15)

l=1

almost surely and hence liml‚Üí‚àû kxl ‚àí xl‚àí1 k = 0 almost
surely.
c) Assume that there exists a sub-sequence {xlk } of {xl }
such that xlk ‚Üí x‚àó almost surely. From (14) and (15),
we have kxlk +1 ‚àí xlik k ‚Üí 0 almost surely. Without
loss of generality, we can suppose that the sub-sequences
yilk ‚Üí y ‚àó almost surely. We note that yilk ‚àà Œª‚àÇ hÃÉ(xlik ) and
by the closed property of the subdifferential mapping ‚àÇ hÃÉ,
we have y ‚àó ‚àà Œª‚àÇ hÃÉ(x‚àó ) with probability one. It follows
from xlk +1 ‚àà arg min T lk (x) that T lk (xlk +1 ) ‚â§ T lk (x).
Taking k ‚Üí ‚àû, we get that
n

1X
œÅ
‚àáfi (x‚àó )i,
ŒªgÃÉ(x‚àó ) ‚â§ ŒªgÃÉ(x)+ kx‚àíx‚àó k2 ‚àíhx‚àíx‚àó , y ‚àó ‚àí
2
n i=1
is almost surely satisfied for all x ‚àà Rm . Thus, we have
n

y‚àó ‚àí

1X
‚àáfi (x‚àó ) ‚àà ‚àÇŒªgÃÉ(x‚àó ),
n i=1

with probability one. Therefore,
"
#
n
1X
‚àó
‚àó
‚àó
y ‚àà ‚àá
fi (x ) + ‚àÇŒªgÃÉ(x ) ‚à© ‚àÇŒªhÃÉ(x‚àó ),
n i=1

(16)

(17)

with probability one. This implies that x‚àó is a critical point
of f with probability one and the proof is complete.

Stochastic DCA for the Large-sum of Non-convex Functions Problem

3. Application to Group Variables Selection in
Multiclass Logistic Regression
Logistic regression, introduced by D. Cox in 1958 (Cox,
1958), is a popular method in supervised learning. Logistic
regression has been successfully applied in various real-life
problems such as cancer detection (Kim et al., 2008), medical (Bagley et al., 2001; Subasi & ErcÃßelebi, 2005), social
science (King & Zeng, 2001), etc. Especially, logistic regression combined with feature selection has been proved
to be suitable for high dimensional problems, for instance,
document classification (Genkin et al., 2007) and microarray classification (Liao & Chin, 2007; Kim et al., 2008).
We describe the multiclass logistic regression problem as
follows. Let W be a d √ó Q matrix, where d and Q are the
number of features and number of classes, respectively. We
denote the i-th column of W by W:,i and b = (b1 , ..., bQ ) ‚àà
RQ . In the multiclass logistic classification problem, a new
instance x‚àó is classified to class y ‚àó by using the rule y ‚àó =
arg maxk p(Y = k|X = x‚àó ), where p(Y = y|X = x) is
the conditional probability defined by

based on the piecewise exponential penalty function. This
approximation function has shown its efficiency in several problems, for instance, variables selection in SVM
(Bradley & Mangasarian, 1998; Le Thi et al., 2008), semisupervised support vector machines (Le et al., 2015), sparse
multiclass support vector machines (Le Thi & Nguyen,
2017), sparse signal recovery (Le Thi et al., 2013), sparse
linear discriminant analysis (Le Thi & Phan, 2016a;b),
variables selection in SVM with uncertain data (Le Thi
et al., 2014), etc. Using the piecewise exponential penalty
function, the corresponding approximate problem of (20)
takes the form:
)
( n
1X
fi (W, b) + ŒªpÃÉ(W ) ,
(21)
min
W,b
n i=1
Pd
where pÃÉ(W ) =
j=1 Œ∑Œ± (kWj,: k2 ) with Œ∑Œ± (t) = 1 ‚àí
exp(‚àíŒ±|t|). The function pÃÉ(W ) can be expressed as a DC
function:
pÃÉ(W ) = Œ±

d
X

kWj,: k2 ‚àí hÃÉ(W ),

j=1

p(Y = y|X = x) =

T
exp(by + W:,y
x)
.
Q
P
T x)
exp(bk + W:,k

(18)

k=1

Given a training set containing n instances xi and their corresponding labels yi ‚àà {1, ..., Q}, we aim to find (W, b) for
which the total probability of the training instances xi belonging to its correct classes yi is maximized. To estimate
(W, b), we maximize the log-likelihood function defined as
1
L(W, b) := ‚àí
n

n
X

where hÃÉ(W ) =

`(xi , yi , W, b)

(19)

where `(xi , yi , W, b) = ‚àí log p(Y = yi |X = xi ). As
mentioned above, to deal with irrelevant and/or redundant
variables in high-dimensional data, we use variables selection method. Note that a variable j is to be removed if and
only if all components in the row j of W are zero. Therefore, we can consider each row of W as a group. Denote
by Wj,: the j-th row of the matrix W . The `2,0 -norm of W ,
i.e., the number of non-zero rows of W , is defined by
kW k2,0 = |{j ‚àà {1, ..., d} : kWj,: k2 6= 0}|.
Hence, the `2,0 regularized multiclass logistic regression
problem is formulated as
)
( n
1X
min
`(xi , yi , W, b) + ŒªkW k2,0 .
(20)
W,b
n i=1
Observe that the problem (20) takes the form of (2) where
the function fi (W, b) = `(xi , yi , W, b). In this application, we use a non-convex approximation of the `2,0 -norm

j=1 [‚àí1+Œ±|Wj,: k2 +exp(‚àíŒ±kWj,: k2 )].

According to the SDCA scheme in Algorithm 1, at each
iteration l, we have to compute (vil , zil ) = œÅ(W l , bl ) ‚àí
‚àáfi (W l , bl ) + (y l , 0) for i ‚àà sl , where
‚àábk fi (W l , bl )
= plk (xi ) ‚àí Œ¥kyi 
l l
‚àáW:,k fi (W , b ) = plk (xi ) ‚àí Œ¥kyi xi
with plk (xi ) =

i=1

Pd

l
exp(blk +(W:,k
)T xi )
PQ
l +(W l )T x )
b
i
h=1 h
:,h

(22)

and Œ¥kyi = 1 if k = yi

and 0 otherwise. The computation of y l is given by
Ô£±
l
Ô£≤0
if kWj,:
k2 = 0
l
l
.
yj,: = ŒªŒ±Œ∑Œ± (kWj,: k2 ) l
Ô£≥ kW l k
Wj,: otherwise

(23)

j,: 2

SDCA for solving (21) is described in Algorithm 2.
3.1. Numerical Experiment
3.1.1. DATASETS
To illustrate the performances of algorithms, we performed
numerical tests on real datasets (aloi, covertype, madelon
and sensorless) and simulated datasets (sim 1, sim 2 and
sim 3). Dataset Aloi is a library of object images 1 while
covertype, madelon, sensorless, are taken from the wellknown UCI data repository.
We used the same way as proposed in (Witten & Tibshirani, 2011) to generate simulated datasets. In sim 1, features are independent with different means in each class.
1

http://aloi.science.uva.nl/

Stochastic DCA for the Large-sum of Non-convex Functions Problem

Algorithm 2 SDCA for solving the problem (21)
Initialization: Choose W 0 ‚àà Rd√óQ , b0 ‚àà RQ , œÅ > L,
s0 = {1, ..., n}, l ‚Üê 0.
Repeat
1. Compute (vil , zil ) = œÅ(W l , bl ) ‚àí ‚àáfi (W l , bl ) +
l
(y , 0) for i ‚àà sl using (22)-(23) and keep (vil , zil ) =
(vil‚àí1 , zil‚àí1 ) for i 6‚àà sl .
2. Compute (W l+1 , bl+1 ) by
Pn
1
l
bl+1
= œÅn
i=1 zi

(24)
vl
l+1
l
Wj,:
= kvj,:
k2 ‚àí ŒªŒ±/œÅ + kvlj,:k2 ,
j:,

Pn

1
l
l
where vj,:
= œÅn
i=1 (vi )j,: for j = 1, ..., d.
3. Set l ‚Üê l+1 and randomly choose a small subset
sl ‚äÇ {1, ..., n}.
Until Stopping criterion.

In sim 2, features also have different means in each class,
however they are dependent. The dataset sim 3 has different one-dimensional means in each class with independent
features. The procedure for generating simulated datasets
is described as follows.
For sim 1: this dataset consists of four classes. The class
k is sampled from the multivariate normal distribution
N (¬µk , I), where the mean vector ¬µk ‚àà R50 is given by
¬µkj = 0.5 if 10(k ‚àí 1) + 1 ‚â§ j ‚â§ 10k and 0 otherwise.
We generate 25, 000 samples for each class.
For sim 2: we generate a dataset with three classes sampled
from the multivariate normal distributions N (¬µk , Œ£), k =
1, 2, 3, where ¬µk ‚àà R50 is defined by ¬µkj = 0.4(k ‚àí 1) if
j ‚â§ 40 and 0 otherwise. We use the block diagonal matrix
Œ£ with five blocks of dimension 10 √ó 10 whose element
0
(j, j 0 ) is 0.6|j‚àíj | . 150, 000 instances are generated.
For sim 3: we generate a dataset including four classes as
follows: xi ‚àà Ck then xij ‚àº N ((k ‚àí 1)/3, 1) if j ‚â§ 100,
k = 1, 2, 3, 4 and xij ‚àº N (0, 1) otherwise. We generate
250, 000 instances with equal probabilities for each class.
For pre-processing data, we use standardization to scale the
data.
3.1.2. C OMPARATIVE ALGORITHMS
We compare our algorithm with two algorithms: msgl and
liblinear. msgl (Vincent & Hansen, 2014) is a coordinate
gradient descent algorithm for solving the multiclass
logistic regression using
regularization term, i.e., the
 `2,1

n
P
1
convex problem min n
`(xi , yi , W, b) + ŒªkW k2,1 .
W,b

i=1

LibLinear (Fan et al., 2008) is a well-known
package for solving large-scale problems by using
the coordinate descent algorithm.
We use the `1 -

regularized logistic regression solver of LibLinear
to (
solve the binary logistic regression
problem
)
n
d
P
P
T
min
log(1 + e‚àíyi w xi ) + Œª
|wj | , and then
w

i=1

j=1

the one-vs-the-rest strategy is used for the multiclass case.
3.1.3. E XPERIMENT SETTING
The comparison of algorithms are performed in terms of
three criteria: classification accuracy on test set, sparsity of
solution and running time. Sparsity is computed as the percentage of selected features, where a feature j ‚àà {1, . . . , d}
is considered to be removed if all absolute values of components of row Wj,: are smaller than a threshold  = 10‚àí8 .
The cross-validation procedure is used for experiments. We
randomly take 80% of the whole dataset as a training set
and the rest is used as test set (20%). This process is repeated 10 times and we report the mean and standard deviation of each criterion.
We use the early-stopping condition for SDCA. This is a
well-know technique in machine learning, especially in
stochastic learning which permits to avoid the over-fitting
problem. After each epoch, we compute the accuracy based
on the validation set, then we stop SDCA if the accuracy is
not improved after npatience = 5 epochs. For comparative
algorithms, we use their default stopping parameters. We
also stop algorithms if they exceed 2 hours of running time
in the training process.
For SDCA, we 	 set the trade-off parameter Œª ‚àà

10‚àí4 , 10‚àí3 , ...1 and the parameter for controlling the
tightness of zero-norm approximation Œ± ‚àà {0.5, 1, 2, 5}.
For both LibLinear and msgl, the trade-off parameter
is chosen in interval {10‚àí3 , . . . , 104 }.
All experiments are performed on a PC Intel (R) Xeon (R)
E5-2630 v2 @2.60 GHz of 32GB RAM.
3.1.4. E XPERIMENT 1 : COMPARISON OF SDCA AND
DCA
Firstly, we will study the impact of batch size on the quality
of solution and the running time of SDCA. The batch size
refers to the size of set of index sl , i.e., the number of components hi that are used to compute the subgradients of hÃÑ
at each iteration (c.f Algorithm 1). In DCA, or full-batch
DCA, all components hi are used, i.e., sl ‚â° {1, . . . , n}.
The Table 1 reports the accuracy and the running time of
SDCA as the batch size varies on an arbitrary chosen dataset
(sensorless).
We observe that the running time is smallest (1.78s) with
batch size equals to 10% while giving the second best classification accuracy 85.23%, only 0.07% smaller than the
best one. Hence, we choose the batch size as 10% through-

Stochastic DCA for the Large-sum of Non-convex Functions Problem
Table 1. Performance of SDCA as batch size varies

Batch Size
Time (s)
Accuracy (%)

5%
10%
15%
20%
25%
30%
40%
50%
2.93¬±0.1 1.78¬±0.1 2.53¬±0.1 2.06¬±0.2 1.54¬±0.2 2.24¬±0.3 2.98¬±0.4 1.94¬±0.50
84.62¬±1.1 85.23¬±0.7 83.25¬±1.2 85.05¬±0.9 82.77¬±1.1 84.54¬±1.1 85.30¬±0.7 82.88¬±1.1

Table 2. Comparative results on both simulated and real datasets.
Bold values correspond to best results for each dataset. NA means that the algorithm fails to furnish a result. n, d and Q is the number
of instances, the number of dimensions and the number of classes respectively.

aloi
(n √ó d) = (108,000√ó128)
Q = 1,000

DCA (Full-batch)
SDCA
MSGL
LibLinear

Accuracy (%)
Mean
STD
85.05
0.44
82.98
0.47
NA
NA
81.61
0.20

covertype
(n √ó d) = (581,012√ó54)
Q=7

DCA (Full-batch)
SDCA
MSGL
LibLinear

71.30
71.58
71.22
71.54

0.09
0.13
0.02
0.19

322.98
5.86
525.49
264.88

0.27
1.14
1.10
26.83

100.00
84.72
68.52
100.00

0.00
6.48
0.00
0.00

madelon
(n √ó d) = (2,600√ó500)
Q=2

DCA (Full-batch)
SDCA
MSGL
LibLinear

61.35
62.60
60.48
61.54

0.39
1.38
2.37
2.72

5.25
0.12
23.92
0.08

0.02
0.12
0.12
0.01

0.80
0.43
0.67
0.58

0.04
0.23
0.00
0.06

sensorless
(n √ó d) = (58,509√ó48)
Q = 11

DCA (Full-batch)
SDCA
MSGL
LibLinear

90.21
85.11
85.06
75.55

0.41
0.83
0.31
0.24

34.65
11.76
199.00
216.48

1.79
4.60
41.75
72.05

38.19
40.00
50.00
100.00

1.20
4.52
0.00
0.00

sim 1
(n √ó d) = (100,000√ó50)
Q=4

DCA (Full-batch)
SDCA
MSGL
LibLinear

72.11
72.24
72.33
72.62

0.57
0.42
0.18
0.38

24.67
1.33
214.83
2038.85

6.39
0.23
25.40
5.05

84.00
80.00
82.00
80.00

2.00
0.00
0.00
0.00

sim 2
(n √ó d) = (150,000√ó50)
Q=3

DCA (Full-batch)
SDCA
MSGL
LibLinear

65.53
68.60
68.42
66.92

3.70
0.22
0.03
0.02

1.57
0.27
53.52
0.23

79.33
80.00
82.00
80.00

1.15
0.00
0.00
0.00

sim 3
(n √ó d) = (250,000√ó500)
Q=4

DCA (Full-batch)
SDCA
MSGL
LibLinear

99.87
99.93
99.93
99.03

0.02
0.01
0.01
0.00

76.04
1.74
367.29
2.04
15
151.72
22.83
1581.44
50.50

4.75
2.55
14.76
2.96

80.53
80.00
80.20
97.16

3.13
0.00
0.00
0.50

Dataset

Algorithm

out our experiments.
To illustrate the potential gain of SDCA, we compare it with
a DCA for solving the problem (21). From the Table 2, we
see that the gain of running time of SDCA ranges from 11.6
times (aloi) to 55.1 times (covertype).
Concerning the classification accuracy, SDCA and DCA are

Time (s)
Mean
STD
2414.72 77.12
208.67
50.03
NA
NA
2732.96 46.38

Sparsity (%)
Mean
STD
97.66
1.35
63.87
3.57
NA
NA
100.00
0.00

comparable. SDCA gives slightly better accuracy than DCA
on covertype, sim 1, sim 3, with a gain ranges from 0.06%
to 0.28%. The gain of SDCA is higher on 2 datasets (madelon, sim 2), 1.35% and 3.07%. DCA furnishes a better result on aloi and sensorless, especially the gain is up to 4.9%
on sensorless. The results prove that SDCA can greatly improve the running time of DCA while archiving a similar

Stochastic DCA for the Large-sum of Non-convex Functions Problem

accuracy.
3.1.5. E XPERIMENT 2 : SIMULATED DATASET
For synthetic datasets (sim 1, sim 2 and sim 3), we know
in advance the informative features that were used to generate the datasets. Hence, the purpose of this experiment
is to study the ability of algorithms to select these informative features in order to furnish a good classification accuracy. The comparison is performed with 3 algorithms,
SDCA, msgl and LibLinear. We report the results in
Table 2, and observe that.
For sim 1 dataset, LibLinear gives a slightly better classification accuracy (72.62%) comparing to SDCA (72.24%)
and msgl (72.33%). However, SDCA is by far the fastest
algorithm. SDCA is 1532 (resp. 136) times faster than
LibLinear (resp. msgl). Furthermore, SDCA and
LibLinear successfully suppress the 20% uninformative
features, which it also matches with our procedure of generating this synthetic dataset. msgl fails on this purpose
by selecting 82% of features.
For sim 2 dataset, SDCA is the best algorithm on both criteria: classification accuracy and running time. Similarly to
sim 1 dataset, only SDCA and LibLinear can correctly
select the informative features (80%).
For sim 3 dataset, SDCA exceeds LibLinear and
GLASSO on all three comparison criteria: classification accuracy, sparsity and speed. LibLinear almost selects all
the features (97.16% selected) but gives 0.89% accuracy
lower than SDCA (99.93%), and it is also 2 times slower
than SDCA. Among the three algorithm, only SDCA successfully selects the informative features.
To summarize, for all three synthetic datasets, SDCA
successfully selects the exact informative features.
LibLinear selects the exact features on 2 out of 3
datasets while GLASSO fails on all three datasets.
3.1.6. E XPERIMENT 3 : REAL - WORLD DATASETS
In this experiment, we perform the comparative study
between SDCA, msgl and LibLinear on real-world
datasets . We observe from Table 2 that.
For aloi dataset, SDCA only selects 63.87% of features
for a classification accuracy of 82.98% while LibLinear
has a worse accuracy with 100% of features used. Moreover, SDCA is the 12.6 times faster than LibLinear while
msgl fails to furnish a result after 2 hours of running time.
For covertype dataset, SDCA furnishes better classification
accuracy than LibLinear and msgl. Moreover, SDCA is
by far faster than the two others. SDCA is SDCA is 45 times
faster than LibLinear and 89 times faster than msgl.
Concerning the sparsity of solution, msgl is the best while

LibLinear fails to suppress features.
The dataset madelon is known to be non-linear. Hence, all
three algorithms furnish quite low classification accuracy
(62.38% for SDCA, 61.54% for LibLinear and 60.48%
for msgl). As for the sparsity, SDCA suppresses more features than LibLinear and msgl.
For sensorless dataset, SDCA is better than both
LibLinear and msgl on all three aspects: classification accuracy, sparsity and running time. In terms of classification accuracy, the gain of SDCA versus msgl (resp.
LibLinear) is 3.89% (resp. 2.26%). Regarding the running time, SDCA is 113 times faster than msgl and 137
times faster than LibLinear. As for the sparsity, SDCA
selects 10% less features than msgl while LibLinear
fails to suppress features.
Overall, SDCA gives the best among the three in term of
classification accuracy on all 4 datasets. As for running
time, SDCA is by far the fastest algorithm. Concerning the
sparsity of solution, SDCA suppresses more features than
the two others on 3 out of 4 datasets.

4. Conclusions
We have rigorously studied the large-sum optimization
problem involving `2,0 regularization. The `2,0 -norm is approximated by a DC function, namely the piecewise exponential function. The resulting problem is then reformulated as a DC program and we developed stochastic DCA
to solve it. Exploiting the fact that each component fi (x)
is differentiable with L-Lipschitz gradient, we propose, a
stochastic version of DCA that is very inexpensive. At
each iteration, the algorithm only requires the computing
the subgradients of a small subset of functions and the projection of points onto balls that is explicitly computed. We
have also proved that the convergence is guaranteed with
probability one. As an application, we applied our algorithm to the group variables selection in multiclass logistic
regression problem. Numerical experiments were carefully
conducted on both synthetic and real-world datasets. The
numerical results show that SDCA greatly improves the
running time of DCA while giving similar accuracy. Moreover, our algorithm SDCA outperforms standard algorithms
(Liblinear and msgl) on all 3 criteria: classification
accuracy, sparsity of solution and running time. Especially,
the gain in running time is huge. SDCA is up to 210 times
faster than msgl and 1537 times faster than LibLinear.
We are convinced that stochastic DCA is a promising approach for handling very large-scale datasets in machine
learning.

Stochastic DCA for the Large-sum of Non-convex Functions Problem

References
Bagley, S. C., White, H., and Golomb, B. A. Logistic regression in the medical literature: Standards for use and
reporting, with particular attention to one medical domain. Journal of Clinical Epidemiology, 54(10):979‚Äì
985, 2001.
Blondel, Mathieu, Seki, Kazuhiro, and Uehara, Kuniaki. Block coordinate descent algorithms for large-scale
sparse multiclass classification. Machine Learning, 93
(1):31‚Äì52, 2013.
Bradley, Paul S. and Mangasarian, O. L. Feature Selection via Concave Minimization and Support Vector Machines. In Proceedings of the Fifteenth International
Conference on Machine Learning, ICML ‚Äô98, pp. 82‚Äì
90, San Francisco, CA, USA, 1998. Morgan Kaufmann
Publishers Inc.
Cox, David. The regression analysis of binary sequences
(with discussion). J Roy Stat Soc B, 20:215‚Äì242, 1958.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and
Lin, C.-J. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871‚Äì
1874, 2008. URL https://cran.r-project.
org/web/packages/LiblineaR/index.html.
Genkin, Alexander, Lewis, David D., and Madigan, David.
Large-scale Bayesian logistic regression for text categorization. Technometrics, 49(3):291‚Äì304, 2007.
Gu, Quanquan, Li, Zhenhuif, and Han, Jiawei. Linear discriminant dimensionality reduction. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 549‚Äì564. Springer, 2011.
Huang, Jian, Wei, Fengrong, and Ma, Shuangge. Semiparametric Regression Pursuit. Statistica Sinica, 22(4):
1403‚Äì1426, 2012.
Khan, Z., Shafait, F., and Mian, A. Joint Group Sparse PCA
for Compressed Hyperspectral Imaging. IEEE Transactions on Image Processing, 24(12):4934‚Äì4942, 2015.
Kim, Jinseog, Kim, Yuwon, and Kim, Yongdai. A
Gradient-Based Optimization Algorithm for LASSO.
Journal of Computational and Graphical Statistics, 17
(4):994‚Äì1009, 2008.
King, Gary and Zeng, Langche. Logistic Regression in
Rare Events Data. Political Analysis, 9:137‚Äì163, 2001.
Le, H. M., Le Thi, H. A., and Nguyen, M. C. Sparse semisupervised support vector machines by DC programming and DCA. Neurocomputing, 153:62‚Äì76, 2015.

Le Thi, H. A. and Nguyen, M. C. DCA based algorithms
for feature selection in multi-class support vector machine. Annals of Operations Research, 249(1):273‚Äì300,
2017.
Le Thi, H. A., Nguyen, T. B. T, and Le, H. M. Sparse Signal Recovery by Difference of Convex Functions Algorithms. In Intelligent Information and Database Systems,
pp. 387‚Äì397. Springer, Berlin, Heidelberg, 2013.
Le Thi, H. A., Vo, X. T., and Pham Dinh, T. Feature selection for linear SVMs under uncertain data: Robust
optimization based on difference of convex functions algorithms. Neural Networks, 59:36‚Äì50, 2014.
Le Thi, H. A., Pham Dinh, T., Le, H. M., and Vo, X. T.
DC approximation approaches for sparse optimization.
European Journal of Operational Research, 244(1):26‚Äì
46, 2015.
Le Thi, Hoai An and Pham Dinh, Tao. The DC (Difference
of Convex Functions) Programming and DCA Revisited
with DC Models of Real World Nonconvex Optimization
Problems. Annals of Operations Research, 133(1-4):23‚Äì
46, 2005.
Le Thi, Hoai An and Phan, Duy Nhat. DC Programming
and DCA for Sparse Optimal Scoring Problem. Neurocomput., 186(C):170‚Äì181, 2016a.
Le Thi, Hoai An and Phan, Duy Nhat. DC programming
and DCA for sparse Fisher linear discriminant analysis.
Neural Computing and Applications, pp. 1‚Äì14, 2016b.
Le Thi, Hoai An, Le, Hoai Minh, Nguyen, Van Vinh, and
Pham, Dinh Tao. A DC programming approach for feature selection in support vector machines learning. Advances in Data Analysis and Classification, 2(3):259‚Äì
278, 2008.
Lee, Sangin, Oh, Miae, and Kim, Yongdai. Sparse optimization for nonconvex group penalized estimation.
Journal of Statistical Computation and Simulation, 86
(3):597‚Äì610, 2016.
Liao, J. G. and Chin, Khew-Voon. Logistic regression for
disease classification using microarray data: Model selection in a large p and small n case. Bioinformatics, 23
(15):1945‚Äì1951, 2007.
Obozinski, Guillaume, Taskar, Ben, and Jordan, Michael.
Multi-task feature selection. Statistics Department, UC
Berkeley, Tech. Rep, 2, 2006.
Pham Dinh, Tao and Le Thi, Hoai An. Convex analysis
approach to dc programming: Theory, algorithms and
applications. Acta Mathematica Vietnamica, 22(1):289‚Äì
355, 1997.

Stochastic DCA for the Large-sum of Non-convex Functions Problem

Pham Dinh, Tao and Le Thi, Hoai An. A D. C. Optimization Algorithm for Solving the Trust-Region Subproblem. SIAM Journal of Optimization, 8(2):476‚Äì505,
1998.
Phan, Duy Nhat, Le Thi, Hoai An, and Pham Dinh, Tao.
Efficient bi-level variable selection and application to estimation of multiple covariance matrices. In Advances in
Knowledge Discovery and Data Mining: 21st PacificAsia Conference, PAKDD 2017, Proceedings, Part I,
volume 10234, pp. 304‚Äì316. Springer International Publishing, 2017.
Reddi, Sashank J., Sra, Suvrit, Poczos, Barnabas, and
Smola, Alexander J. Proximal stochastic methods for
Nonsmooth Nonconvex Finite-Sum Optimization. In
Advances in Neural Information Processing Systems, pp.
1145‚Äì1153, 2016.
Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
Mathematical Programming, 162(1):83‚Äì112, 2015.
Subasi, Abdulhamit and ErcÃßelebi, Ergun. Classification of
EEG signals using neural network and logistic regression. Computer Methods and Programs in Biomedicine,
78(2):87‚Äì99, 2005.
Sun, Liang, Liu, Jun, Chen, Jianhui, and Ye, Jieping. Efficient Recovery of Jointly Sparse Vectors. In Bengio, Y.,
Schuurmans, D., Lafferty, J. D., Williams, C. K. I., and
Culotta, A. (eds.), Advances in Neural Information Processing Systems 22, pp. 1812‚Äì1820. Curran Associates,
Inc., 2009.
Tibshirani, Robert. Regression Shrinkage and Selection
Via the Lasso. Journal of the Royal Statistical Society,
Series B, 58:267‚Äì288, 1994.
Vincent, Martin and Hansen, Niels Richard. Sparse group
lasso and high dimensional multinomial classification.
Comput. Stat. Data Anal., 71:771‚Äì786, 2014.
Wang, Lifeng, Chen, Guang, and Li, Hongzhe. Group
SCAD regression analysis for microarray time course
gene expression data. Bioinformatics, 23(12):1486‚Äì
1494, 2007.
Witten, Daniela M. and Tibshirani, Robert. Penalized
classification using Fisher‚Äôs linear discriminant. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73(5):753‚Äì772, 2011.
Yuan, Ming and Lin, Yi. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society, Series B, 68:49‚Äì67, 2006.

