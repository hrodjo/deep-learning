An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum 1 ZoltÃ¡n SzabÃ³ 2 Arthur Gretton 1

Abstract
A new computationally efficient dependence measure, and an adaptive statistical test of independence,
are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at
a finite set of locations (features). These features are
chosen so as to maximize a lower bound on the test
power, resulting in a test that is data-efficient, and
that runs in linear time (with respect to the sample
size n). The optimized features can be interpreted
as evidence to reject the null hypothesis, indicating
regions in the joint domain where the joint distribution and the product of the marginals differ most.
Consistency of the independence test is established,
for an appropriate choice of features. In real-world
benchmarks, independence tests using the optimized
features perform comparably to the state-of-the-art
quadratic-time HSIC test, and outperform competing
O(n) and O(n log n) tests.

1. Introduction
We consider the design of adaptive, nonparametric statistical
tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals Px Py
with the null hypothesis that H0 : X and Y are independent. While classical tests of dependence, such as Pearsonâ€™s
correlation and Kendallâ€™s Ï„ , are able to detect monotonic
relations between univariate variables, more modern tests
can address complex interactions, for instance changes in
variance of X with the value of Y . Key to many recent
tests is to examine covariance or correlation between data
features. These interactions become significantly harder to
detect, and the features are more difficult to design, when
the data reside in high dimensions.
ZoltÃ¡n SzabÃ³â€™s ORCID ID: 0000-0001-6183-7603. Arthur Grettonâ€™s ORCID ID: 0000-0003-3169-7624. 1 Gatsby Unit, University College London, UK. 2 CMAP, Ã‰cole Polytechnique, France.
Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

A basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables (Gretton et al.,
2005; 2008). Each random variable X and Y is mapped
to a respective reproducing kernel Hilbert space Hk and
Hl . For sufficiently rich mappings, the covariance operator
norm is zero if and only if the variables are independent. A
second basic nonlinear dependence measure is the smoothed
difference between the characteristic function of the joint
distribution, and that of the product of marginals. When
a particular smoothing function is used, the statistic corresponds to the covariance between distances of X and Y variable pairs (Feuerverger, 1993; SzÃ©kely et al., 2007; SzÃ©kely
& Rizzo, 2009), yielding a simple test statistic based on
pairwise distances. It has been shown by Sejdinovic et al.
(2013) that the distance covariance (and its generalization
to semi-metrics) is an instance of HSIC for an appropriate
choice of kernels. A disadvantage of these feature covariance statistics, however, is that they require quadratic time
to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo &
SzÃ©kely (2016) achieve an O(n log n) cost). Moreover, the
feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of
an expensive eigenvalue problem (e.g. Zhang et al., 2011) is
required for consistent estimation of the quantiles. Several
approaches were proposed by Zhang et al. (2017) to obtain
faster tests along the lines of HSIC. These include computing HSIC on finite-dimensional feature mappings chosen as
random Fourier features (RFFs) (Rahimi & Recht, 2008),
a block-averaged statistic, and a NystrÃ¶m approximation
to the statistic. Key to each of these approaches is a more
efficient computation of the statistic and its threshold under
the null distribution: for RFFs, the null distribution is a
finite weighted sum of Ï‡2 variables; for the block-averaged
statistic, the null distribution is asymptotically normal; for
NystrÃ¶m, either a permutation approach is employed, or
the spectrum of the NystrÃ¶m approximation to the kernel
matrix is used in approximating the null distribution. Each
of these methods costs significantly less than the O(n2 ) cost
of the full HSIC (the cost is linear in n, but also depends
quadratically on the number of features retained). A potential disadvantage of the NystrÃ¶m and Fourier approaches is
that the features are not optimized to maximize test power,

An Adaptive Test of Independence with Analytic Kernel Embeddings

but are chosen randomly. The block statistic performs worse
than both, due to the large variance of the statistic under the
null (which can be mitigated by observing more data).

In these experiments, we outperform competing linear and
O(n log n) time tests.

In addition to feature covariances, correlation measures have
also been developed in infinite dimensional feature spaces:
in particular, Bach & Jordan (2002); Fukumizu et al. (2008)
proposed statistics on the correlation operator in a reproducing kernel Hilbert space. While convergence has been
established for certain of these statistics, their computational cost is high at O(n3 ), and test thresholds have relied
on permutation. A number of much faster approaches to
testing based on feature correlations have been proposed,
however. For instance, Dauxois & Nkiet (1998) compute
statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order
B-splines. The cost of this approach is O(n). This idea
was extended by Lopez-Paz et al. (2013), who computed
the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they
performed a copula transform on the inputs, with a total
cost of O(n log n). Finally, space partitioning approaches
have also been proposed, based on statistics such as the
KL divergence, however these apply only to univariate variables (Heller et al., 2016), or to multivariate variables of
low dimension (Gretton & GyÃ¶rfi, 2010) (that said, these
tests have other advantages of theoretical interest, notably
distribution-independent test thresholds).

2. Independence Criteria and Statistical Tests

The approach we take is most closely related to HSIC on a
finite set of features. Our simplest test statistic, the Finite
Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each
of X and Y . A normalized version of the statistic (NFSIC)
yields a distribution-independent asymptotic test threshold.
We show that our test is consistent, despite a finite number
of analytic features being used, via a generalization of arguments in Chwialkowski et al. (2015). As in recent work
on two-sample testing by Jitkrittum et al. (2016), our test
is adaptive in the sense that we choose our features on a
held-out validation set to optimize a lower bound on the
test power. The design of features for independence testing
turns out to be quite different to the case of two-sample
testing, however: the task is to find correlated feature pairs
on the respective marginal domains, rather than attempting
to find a single, high-dimensional feature representation on
the tensor product of the marginals, as we would need to
do if we were comparing distributions Pxy and Qxy . While
the use of coupled feature pairs on the marginals entails a
smaller feature space dimension, it introduces significant
complications in the proof of the lower bound, compared
with the two-sample case. We demonstrate the performance
of our tests on several challenging artificial and real-world
datasets, including detection of dependence between music
and its year of appearance, and between videos and captions.

We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle
that dependence can be measured in terms of the covariance between data features. Next, we propose a normalized
version of this statistic (NFSIC), with a simpler asymptotic
distribution when Pxy = Px Py . We show how to select
features for the latter statistic to maximize a lower bound
on the power of its corresponding statistical test.
2.1. The Finite Set Independence Criterion
We begin by recalling the Hilbert-Schmidt Independence
Criterion (HSIC) as proposed in Gretton et al. (2005), since
our unnormalized statistic is built along similar lines. Consider two random variables X âˆˆ X âŠ† Rdx and Y âˆˆ Y âŠ†
Rdy . Denote by Pxy the joint distribution between X and Y ;
Px and Py are the marginal distributions of X and Y . Let âŠ—
denote the tensor product, such that (a âŠ— b) c = a hb, ci. Assume that k : X Ã— X â†’ R and l : Y Ã— Y â†’ R are positive
definite kernels associated with reproducing kernel Hilbert
spaces (RKHS) Hk and Hl , respectively. Let k Â· kHS be the
norm on the space of Hl â†’ Hk Hilbert-Schmidt operators.
Then, HSIC between X and Y is defined as

2
HSIC(X, Y ) = Âµxy âˆ’ Âµx âŠ— Âµy 
HS

=E

(x,y),(x0 ,y0 )

0

0

[k(x, x )l(y, y )]

+ Ex Ex0 [k(x, x0 )]Ey Ey0 [l(y, y0 )]
âˆ’ 2E(x,y) [Ex0 [k(x, x0 )]Ey0 [l(y, y0 )]] ,

(1)

where Ex := Exâˆ¼Px , Ey := Eyâˆ¼Py , Exy := E(x,y)âˆ¼Pxy ,
and x0 is an independent copy of x. The mean embedding of
Pxy belongs to the space
R of Hilbert-Schmidt operators from
Hl to Hk , Âµxy := X Ã—Y k(x, Â·) âŠ— l(y, Â·) dPxy (x, y) âˆˆ
are Âµx :=
HS(H
l , Hk ), and the marginal mean embeddings
R
R
k(x,
Â·)
dP
(x)
âˆˆ
H
and
Âµ
:=
l(y,
Â·)
dP
x
k
y
y (y) âˆˆ
X
Y
Hl (Smola et al., 2007). Gretton et al. (2005, Theorem 4)
show that if the kernels k and l are universal (Steinwart
& Christmann, 2008) on compact domains X and Y, then
HSIC(X, Y ) = 0 if and only if X and Y are independent.
Given a joint sample Zn = {(xi , yi )}ni=1 âˆ¼ Pxy , an empirical estimator of HSIC can be computed in O(n2 ) time
by replacing the population expectations in (1) with their
corresponding empirical expectations based on Zn .
We now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC). Let
X âŠ† Rdx and Y âŠ† Rdy be open sets.
Let
Âµx Âµy (x, y) := Âµx (x)Âµy (y) The idea is to see Âµxy (v, w) =
Exy [k(x, v)l(y, w)], Âµx (v) = Ex [k(x, v)] and Âµy (w) =
Ey [l(y, w)] as smooth functions, and consider a new dis-

An Adaptive Test of Independence with Analytic Kernel Embeddings

tance between Âµxy and Âµx Âµy instead of a Hilbert-Schmidt
distance as in HSIC (Gretton et al., 2005). The new measure is given by the average of squared differences between Âµxy and Âµx Âµy , evaluated at J random test locations
VJ := {(vi , wi )}Ji=1 âŠ‚ X Ã— Y.
J
1X
2
FSIC (X, Y ) :=
[Âµxy (vi , wi ) âˆ’ Âµx (vi )Âµy (wi )]
J i=1
2

=

J
1X 2
1
u (vi , wi ) = kuk22 ,
J i=1
J

where
u(v, w) := Âµxy (v, w) âˆ’ Âµx (v)Âµy (w)

= Exy [k(x, v)l(y, w)] âˆ’ Ex [k(x, v)]Ey [l(y, w)], (2)

= covxy [k(x, v), l(y, w)],

u := (u(v1 , w1 ), . . . , u(vJ , wJ ))> , and {(vi , wi )}Ji=1
are realizations from an absolutely continuous distribution
(wrt the Lebesgue measure).
Our first result in Proposition 2 states that FSIC(X, Y )
almost surely defines a dependence measure for the random
variables X and Y , provided that the product kernel on
the joint space X Ã— Y is characteristic and analytic (see
Definition 1).
Definition 1 (Analytic kernels (Chwialkowski et al., 2015)).
Let X be an open set in Rd . A positive definite kernel
k : X Ã— X â†’ R is said to be analytic on its domain X Ã— X
if for all v âˆˆ X , f (x) := k(x, v) is an analytic function on
X.

Assumption A. The kernels k : X Ã— X â†’ R and
l : Y Ã— Y â†’ R are bounded by Bk and Bl respectively
[supx,x0 âˆˆX k(x, x0 ) â‰¤ Bk , supy,y0 âˆˆY l(y, y0 ) â‰¤ Bl ] , and
the product kernel g((x, y), (x0 , y0 )) := k(x, x0 )l(y, y0 ) is
characteristic (Sriperumbudur et al., 2010, Definition 6),
and analytic (Definition 1) on (X Ã— Y) Ã— (X Ã— Y).
Proposition 2 (FSIC is a dependence measure). Assume
that assumption A holds, and that the test locations
VJ = {(vi , wi )}Ji=1 are drawn from an absolutely continuous distribution Î·. Then, Î·-almost surely, it holds that
FSIC(X, Y ) = âˆš1J kuk2 = 0 if and only if X and Y are
independent.

Proof. Since g is characteristic, the mean embedding map
Î g : P 7â†’ E(x,y)âˆ¼P [g((x, y), Â·)] is injective (Sriperumbudur et al., 2010, Section 3), where P is a probability
distribution on X Ã— Y. Since g is analytic, by Lemma 10
(Appendix), Âµxy and Âµx Âµy are analytic functions. Thus,
Lemma 11 (Appendix, setting Î› = Î g ) guarantees that
FSIC(X, Y ) = 0 â‡â‡’ Pxy = Px Py â‡â‡’ X and Y are
independent almost surely.

FSIC uses Âµxy as a proxy for Pxy , and Âµx Âµy as a proxy for
Px Py . Proposition 2 states that, to detect the dependence
between X and Y , it is sufficient to evaluate the difference
of the population joint embedding Âµxy and the embedding
of the product of the marginal distributions Âµx Âµy at a finite number of locations (defined by VJ ). The intuitive explanation of this property is as follows. If Pxy = Px Py ,
then u(v, w) = 0 everywhere, and FSIC(X, Y ) = 0
for any VJ . If Pxy 6= Px Py , then u will not be a zero
function, since the mean embedding map is injective (requires the product kernel to be characteristic). Using the
same argument as in Chwialkowski et al. (2015), since k
and l are analytic, u is also analytic, and the set of roots
Ru := {(v, w) | u(v, w) = 0} has Lebesgue measure zero.
Thus, it is sufficient to draw (v, w) from an absolutely continuous distribution to have (v, w) âˆˆ
/ Ru Î·-almost surely,
and hence FSIC(X, Y ) > 0. We note that a characteristic
kernel which is not analytic may produce u such that Ru has
a positive Lebesgue measure. In this case, there is a positive
probability that (v, w) âˆˆ Ru , resulting in a potential failure
to detect the dependence.
The next proposition shows that Gaussian kernels k and l
yield a product kernel which is characteristic and analytic; in
other words, this is an example when Assumption A holds.
Proposition 3 (A product of Gaussian kernels
is characteristic and analytic).
Let k(x, x0 )
=

exp âˆ’(x âˆ’ x0 )> A(x âˆ’ x0 )
and
l(y, y0 )
=
be
Gaussian
kerexp âˆ’(y âˆ’ y0 )> B(y âˆ’ y0 )
nels on Rdx Ã— Rdx and Rdy Ã— Rdy respectively,
for positive definite matrices A and B.
Then,
g((x, y), (x0 , y0 )) = k(x, x0 )l(y, y0 ) is characteristic and analytic on (Rdx Ã— Rdy ) Ã— (Rdx Ã— Rdy ).
Proof (sketch). The main idea is to use the fact that a Gaussian kernel is analytic, and a product of Gaussian kernels is
a Gaussian kernel on the pair of variables. See the full proof
in Appendix D.
Plug-in Estimator Assume that we observe a
i.i.d.
joint sample Zn := {(xi , yi )}ni=1 âˆ¼ Pxy . Unbiased estimators of Âµxy
w) and Âµx Âµy (v, w)
P(v,
n
1
are ÂµÌ‚xy (v, w)
:=
k(xi , v)l(yi , w) and
nP i=1P
n
1
Âµ[
:= n(nâˆ’1) i=1 j6=i k(xi , v)l(yj , w),
x Âµy (v, w)
respectively. A straightforward empirical estimator of
FSIC2 is then given by
J
X
\2 (Zn ) = 1
FSIC
uÌ‚(vi , wi )2 ,
J i=1

uÌ‚(v, w) := ÂµÌ‚xy (v, w) âˆ’ Âµ[
(3)
x Âµy (v, w)
X
2
=
h(v,w) ((xi , yi ), (xj , yj )), (4)
n(n âˆ’ 1) i<j

where h(v,w) ((x, y), (x0 , y0 ))
k(x0 , v))(l(y, w) âˆ’ l(y0 , w)).

1
:=
2 (k(x, v) âˆ’
For conciseness, we

An Adaptive Test of Independence with Analytic Kernel Embeddings

define uÌ‚ := (uÌ‚1 , . . . , uÌ‚J )> âˆˆ RJ where uÌ‚i := uÌ‚(vi , wi )
\2 (Zn ) = 1 uÌ‚> uÌ‚.
so that FSIC
J
\2 can be efficiently computed in O((dx +dy )Jn) time
FSIC
which is linear in n [see (3) which does not have nested
double sums], assuming that the runtime complexity of
evaluating k(x, v) is O(dx ) and that of l(y, w) is O(dy ).
Since FSIC satisfies FSIC(X, Y ) = 0 â‡â‡’ X âŠ¥ Y , in
principle its empirical estimator can be used as a test statistic
for an independence test proposing a null hypothesis H0 :
â€œX and Y are independentâ€ against an alternative H1 : â€œX
and Y are dependent.â€ The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however, and depends on the unknown Pxy .
This prompts us to consider a normalized version of FSIC
whose asymptotic null distribution takes a more convenient
form. We first derive the asymptotic distribution of uÌ‚ in
Proposition 4, which we use to derive the normalized test
statistic in Theorem 5. As a shorthand, we write z := (x, y),
t := (v, w), covz is covariance,Vz stands for variance.
Proposition 4 (Asymptotic distribution of uÌ‚). Define
u := (u(t1 ), . . . , u(tJ ))> , kÌƒ(x, v) := k(x, v) âˆ’
Ex0 k(x0 , v), and Ëœl(y, w) := l(y, w) âˆ’ Ey0 l(y0 , w).
Let Î£ = [Î£ij ] âˆˆ RJÃ—J be the positive semi-definite
matrix with entries Î£ij = covz (uÌ‚(ti ), uÌ‚(tj )) =
Exy [kÌƒ(x, vi )Ëœl(y, wi )kÌƒ(x, vj )Ëœl(y, wj )]âˆ’u(ti )u(tj ). Then,
under both H0 and H1 , for any fixed test locations
{t1 , . . . , tJ } for which Î£ is full rank, andâˆš 0 <
Vz [htj (z)] < âˆž for j = 1, . . . , J, it holds that n(uÌ‚ âˆ’
d

u) â†’ N (0, Î£).

Proof. For a fixed {t1 , . . . , tJ }, uÌ‚ is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht .
Thus, by Lehmann (1999, Theorem 6.1.6) and Kowalski & Tu (2008, Section 5.1, Theorem 1), it follows diâˆš
d
rectly that n(uÌ‚ âˆ’ u) â†’ N (0, Î£) where we note that
Exy [kÌƒ(x, v)Ëœl(y, w)] = u(v, w).
Recall from Proposition 2 that u = 0 holds almost surely under H0 . The asymptotic normality described in Proposition
\2 = n uÌ‚> uÌ‚ converges in distribution
4 implies that nFSIC
J
to a sum of J dependent weighted Ï‡2 random variables.
The dependence comes from the fact that the coordinates
uÌ‚1 . . . , uÌ‚J of uÌ‚ all depend on the sample Zn . This null distribution is not analytically tractable, and requires a large
number of simulations to compute the rejection threshold
TÎ± for a given significance value Î±.

chi-squared distribution with J degrees of freedom. We
\ 2 is
then show that the independence test defined by NFSIC
consistent. These results are given in Theorem 5.
\ 2 is consisTheorem 5 (Independence test based on NFSIC
tent). Let Î£Ì‚ be a consistent estimate of Î£ based on the joint
sample Zn , where Î£ is defined in Proposition 4. Assume
that VJ = {(vi , wi )}Ji=1 âˆ¼ Î· where Î· is absolutely contin\ 2 statistic is
uous wrt the Lebesgue measure. The NFSIC

âˆ’1
defined as Î»Ì‚n := nuÌ‚> Î£Ì‚ + Î³n I
uÌ‚ where Î³n â‰¥ 0 is a
regularization parameter. Assume that
1. Assumption A holds.
2. Î£ is invertible Î·-almost surely.
3. limnâ†’âˆž Î³n = 0.
Then, for any k, l and VJ satisfying the assumptions,
d

1. Under H0 , Î»Ì‚n â†’ Ï‡2 (J) as n â†’ âˆž.


2. Under H1 , for any r âˆˆ R, limnâ†’âˆž P Î»Ì‚n â‰¥ r = 1
Î·-almost surely. That is, the independence test based on
\ 2 is consistent.
NFSIC
>
(Î£Ì‚ + Î³n I)âˆ’1 uÌ‚ asymptotProof (sketch) . Under H0 , nuÌ‚âˆš
2
ically follows Ï‡ (J) because nuÌ‚ is asymptotically normally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u 6= 0 under H1 ; it
follows using the convergence of uÌ‚ to u. The full proof can
be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for
any fixed threshold. Asymptotically the test threshold TÎ± is
given by the (1 âˆ’ Î±)-quantile of Ï‡2 (J) and is independent
of n. The assumption on the consistency of Î£Ì‚ is required
to obtain the asymptotic chi-squared distribution. The regularization parameter Î³n is to ensure that (Î£Ì‚ + Î³n I)âˆ’1 can
be stably computed. In practice, Î³n requires no tuning, and
can be set to be a very small constant. We emphasize that J
need not increase with n for test consistency.

2.2. Normalized FSIC and Adaptive Test

The next proposition states that the computational com\ 2 estimator is linear in both the input
plexity of the NFSIC
dimension and sample size, and that it can be expressed
in terms of the K =[K ij ] = [k(vi , xj )] âˆˆ RJÃ—n , L =
[Lij ] = [l(wi , yj )] âˆˆ RJÃ—n matrices. In contrast to typical
kernel methods, a large Gram matrix of size n Ã— n is not
\ 2.
needed to compute NFSIC

For the purpose of an independence test, we will consider
\2 , which we call NFSIC
\ 2,
a normalized variant of FSIC
whose tractable asymptotic null distribution is Ï‡2 (J), the

\ 2 ). Let
Proposition 6 (An empirical estimator of NFSIC
1n := (1, . . . , 1)> âˆˆ Rn . Denote by â—¦ the element-wise
matrix product. Then,

An Adaptive Test of Independence with Analytic Kernel Embeddings

1. uÌ‚ =

(Kâ—¦L)1n
nâˆ’1

âˆ’

(K1n )â—¦(L1n )
.
n(nâˆ’1)

2. A consistent estimator for Î£ is Î£Ì‚ =

Î“Î“>
n

where

âˆ’1
b >
Î“ := (K âˆ’ nâˆ’1 K1n 1>
L1n 1>
n ) â—¦ (L âˆ’ n
n ) âˆ’ uÌ‚ 1n ,

uÌ‚b = nâˆ’1 (K â—¦ L) 1n âˆ’ nâˆ’2 (K1n ) â—¦ (L1n ) .

Assume that the complexity of the kernel evaluation is linear in the input dimension. Then the test statistic Î»Ì‚n =

âˆ’1
nuÌ‚> Î£Ì‚ + Î³n I
uÌ‚ can be computed in O(J 3 + J 2 n +

(dx + dy )Jn) time.

Proof (sketch). Claim 1 for uÌ‚ is straightforward. The expression for Î£Ì‚ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4. The consistency of Î£Ì‚ can be obtained by noting that the finite sample
bound for P(kÎ£Ì‚ âˆ’ Î£kF > t) decreases as n increases. This
is implicitly shown in Appendix F.2.2 and its following
sections.
Although the dependency of the estimator on J is cubic, we
empirically observe that only a small value of J is required
(see Section 3). The number of test locations J relates to
the number of regions in X Ã— Y of pxy and px py that differ
(see Figure 1).
Theorem 5 asserts the consistency of the test for any test
locations VJ drawn from an absolutely continuous distribution. In practice, VJ can be further optimized to increase
the test power for a fixed sample size. Our final theoretical
\ 2 i.e.,
result gives a lower bound on the test power of NFSIC
the probability of correctly rejecting H0 . We will use this
lower bound as the objective function to determine VJ and
the kernel parameters. Let k Â· kF be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2 (X, Y ) := Î»n := nu> Î£âˆ’1 u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that

2
Î¾3 :=âˆš8c1 B 2 J, c3 := 4Bâˆš
J cÌƒ2 , Î¾4 := 28 B 4 J 2 c21 , c1 :=
2
4B J J cÌƒ, and c2 := 4B J cÌƒ. Moreover, for sufficiently
large fixed n, L(Î»n ) is increasing in Î»n .

We provide the proof in Appendix F. To put
Theorem
7 into
assume that

o K =
n
 perspective,
kxâˆ’vk2
2
2
2
| Ïƒx âˆˆ [Ïƒx,l , Ïƒx,u ]
=: Kg
(x, v) 7â†’ exp âˆ’ 2Ïƒ2
x

2
2
for some 0 < Ïƒx,l
< Ïƒx,u
< âˆž and L =
n


o
2
kyâˆ’wk
2
2
2
(y, w) 7â†’ exp âˆ’ 2Ïƒ2
| Ïƒy âˆˆ [Ïƒy,l
, Ïƒy,u
] =: Lg
y

2
2
for some 0 < Ïƒy,l
< Ïƒy,u
< âˆž are Gaussian kernel
classes. Then, in Theorem 7, B = Bk = Bl = 1,
and B âˆ— = 2. The assumption cÌƒ < âˆž is a technical condition to guarantee that the test power lower
bound is finite for all Î¸ defined
 by the feasible sets
K, L, and V. Let V,r := VJ | kvi k2 , kw	i k2 â‰¤
r and kvi âˆ’ vj k22 + kwi âˆ’ wj k22 â‰¥ , for all i 6= j . If we
set K = Kg , L = Lg , and V = V,r for some , r > 0, then
cÌƒ < âˆž as Kg , Lg , and V,r are compact. In practice, these
conditions do not necessarily create restrictions as they
almost always hold implicitly. We show in Appendix C that
the objective function used to choose VJ will discourage
any two locations to be in the same neighborhood.

Parameter Tuning Let Î¸ be the collection of all tuning
parameters of the test. If k âˆˆ Kg and l âˆˆ Lg (i.e., Gaussian kernels), then Î¸ = {Ïƒx2 , Ïƒy2 , VJ }. The test power
lower bound L(Î»n ) in Theorem 7 is a function of Î»n =
nu> Î£âˆ’1 u which is the population counterpart of the test
statistic Î»Ì‚n . As in FSIC, it can be shown that Î»n = 0 if
and only if X are Y are independent (from Proposition 2).
According to Theorem 7, for a sufficiently large n, the test
power lower bound is increasing in Î»n . One can therefore
think of Î»n (a function of Î¸) as representing how easily the
test rejects H0 given a problem Pxy . The higher the Î»n , the
greater the lower bound on the test power, and thus the more
likely it is that the test will reject H0 when it is false.

In light of this reasoning, we propose to set Î¸ by maximizing the lower bound on the test power i.e., set Î¸ to
1. There exist finite Bk and Bl such that
Î¸âˆ— = arg maxÎ¸ L(Î»n ). Assume that n is sufficiently large
supkâˆˆK supx,x0 âˆˆX |k(x, x0 )|
â‰¤
Bk
and
so that Î»n 7â†’ L(Î»n ) is an increasing function. Then,
suplâˆˆL supy,y0 âˆˆY |l(y, y0 )| â‰¤ Bl .
arg maxÎ¸ L(Î»n ) = arg maxÎ¸ Î»n . That this procedure is
also valid under H0 can be seen as follows. Under H0 ,
2. cÌƒ := supkâˆˆK suplâˆˆL supVJ âˆˆV kÎ£âˆ’1 kF < âˆž.
Î¸âˆ— = arg maxÎ¸ 0 will be arbitrary. Since Theorem 7 guarand
Then, for any k âˆˆK, l âˆˆ L,
V
âˆˆ
V,
and
Î»
â‰¥
r,
the
test
n
 J
tees that Î»Ì‚n â†’ Ï‡2 (J) as n â†’ âˆž for any Î¸, the asymptotic
power satisfies P Î»Ì‚n â‰¥ r â‰¥ L(Î»n ) where
null distribution does not change by using Î¸âˆ— . In practice,
Î»n is a population quantity which is unknown. We propose
2
âˆ’b0.5nc(Î»n âˆ’r)2 /[Î¾2 n2 ]
âˆ’Î¾1 Î³n
(Î»n âˆ’r)2 /n
dividing the sample Zn into two disjoint sets: training and
L(Î»n ) = 1 âˆ’ 62e
âˆ’ 2e
test sets. The training set is used to compute Î»Ì‚n (an estimate
2
2
2
âˆ’ 2eâˆ’[(Î»n âˆ’r)Î³n (nâˆ’1)/3âˆ’Î¾3 nâˆ’c3 Î³n n(nâˆ’1)] /[Î¾4 n (nâˆ’1)] , of Î»n ) to optimize for Î¸âˆ— , and the test set is used for the actual independence test with the optimized Î¸âˆ— . The splitting
1
âˆ—
bÂ·c is the floor function, Î¾1 := 32 c2 J 2 B âˆ— , B is a constant
is to guarantee the independence of Î¸âˆ— and the test sample
1
2
2
depending on only Bk and Bl , Î¾2 := 72c2 JB , B := Bk Bl ,
to avoid overfitting.

An Adaptive Test of Independence with Analytic Kernel Embeddings

(a) ÂµÌ‚xy (v, w)

(b) Âµ[
x Âµy (v, w)

b
(c) Î£(v,
w)

(d) Statistic Î»Ì‚n (v, w)

\ 2.
Figure 1: Illustration of NFSIC
\ 2 , we visualTo better understand the behaviour of NFSIC
ize ÂµÌ‚xy (v, w), Âµ[
Âµ
(v,
w)
and
Î£Ì‚(v,
w)
as a function of
x y
one test location (v, w) on a simple toy problem. In this
problem, Y = âˆ’X + Z where Z âˆ¼ N (0, 0.32 ) is an independent noise variable. As we consider only one location
(J = 1), Î£Ì‚(v, w) is a scalar. The statistic can be written
2
Âµx Âµy (v,w))
(ÂµÌ‚xy (v,w)âˆ’\
as Î»Ì‚n = n
. These components are
Î£Ì‚(v,w)
shown in Figure 1, where we use Gaussian kernels for both
X and Y , and the horizontal and vertical axes correspond
to v âˆˆ R and w âˆˆ R, respectively.
Intuitively, uÌ‚(v, w) = ÂµÌ‚xy (v, w) âˆ’ Âµ[
x Âµy (v, w) captures
the difference of the joint distribution and the product of
the marginals as a function of (v, w). Squaring uÌ‚(v, w)
and dividing it by the variance shown in Figure 1c gives the
statistic (also the parameter tuning objective) shown in Figure 1d. The latter figure illustrates that the parameter tuning
objective function can be non-convex: non-convexity arises
since there are multiple ways to detect the difference between the joint distribution and the product of the marginals.
In this case, the lower left and upper right regions equally
indicate the largest difference. A convex objective would
not be able to capture this phenomenon.

3. Experiments
In this section, we empirically study the performance of
the proposed method on both toy (Section 3.1) and real
problems (Section 3.2). We are interested in challenging problems requiring a large number of samples, where
a quadratic-time test might be computationally infeasible. Our goal is not to outperform a quadratic-time test
with a linear-time test uniformly over all testing problems.
We will find, however, that our test does outperform the
quadratic-time test in some cases. Code is available at
https://github.com/wittawatj/fsic-test.
We compare the proposed NFSIC with optimization (NFSIC\2
opt) to five multivariate nonparametric tests. The NFSIC
test without optimization (NFSIC-med) acts as a baseline,
allowing the effect of parameter optimization to be clearly

seen. For pedagogical reason, we consider the original HSIC
test of Gretton et al. (2005) denoted by QHSIC, which is a
quadratic-time test. NystrÃ¶m HSIC (NyHSIC) uses a NystrÃ¶m approximation to the kernel matrices of X and Y when
computing the HSIC statistic. FHSIC is another variant of
HSIC in which a random Fourier feature approximation
(Rahimi & Recht, 2008) to the kernel is used. NyHSIC and
FHSIC are studied in Zhang et al. (2017) and can be computed in O(n), with quadratic dependency on the number
of inducing points in NyHSIC, and quadratic dependency
on the number of random features in FHSIC. Finally, the
Randomized Dependence Coefficient (RDC) proposed in
Lopez-Paz et al. (2013) is also considered. The RDC can be
seen as the primal form (with random Fourier features) of
the kernel canonical correlation analysis of Bach & Jordan
(2002) on copula-transformed data. We consider RDC as a
linear-time test even though preprocessing by an empirical
copula transform costs O((dx + dy )n log n).
We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the independence
test, where the Gaussian widths Ïƒx and Ïƒy are set according to the widely used median heuristic i.e., Ïƒx =
median ({kxi âˆ’ xj k2 | 1 â‰¤ i < j â‰¤ n}), and Ïƒy is set in
the same way using {yi }ni=1 . The J locations for NFSICmed are randomly drawn from the standard multivariate
normal distribution in each trial. For a sample of size n,
NFSIC-opt uses half the sample for parameter tuning, and
the other disjoint half for the test. We permute the sample
300 times in RDC1 and HSIC to simulate from the null
distribution and compute the test threshold. The null distributions for FHSIC and NyHSIC are given by a finite sum of
weighted Ï‡2 (1) random variables given in Eq. 8 of Zhang
et al. (2017). Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1 âˆ’ Î±)-quantile of
Ï‡2 (J). To provide a fair comparison, we set J = 10, use 10
inducing points in NyHSIC, and 10 random Fourier features
in FHSIC and RDC.
Optimization of NFSIC-opt The parameters of NFSIC-opt
are Ïƒx , Ïƒy , and J locations of size (dx + dy )J. We treat all
the parameters as a long vector in R2+(dx +dy )J and use gradient ascent to optimize Î»Ì‚n/2 . We observe that initializing
VJ by randomly picking J points from the training sample
yields good performance. The regularization parameter Î³n
in NFSIC is fixed to a small value, and is not optimized. It is
worth emphasizing that the complexity of the optimization
procedure is still linear-time.2
1
We use a permutation test for RDC, following the authorsâ€™ implementation (https://github.com/lopezpaz/
randomized_dependence_coefficient, referred commit: b0ac6c0).
2
Our claim on linear runtime (with respect to n) is for the
gradient ascent procedure to find a local optimum for Î¸. We do not

An Adaptive Test of Independence with Analytic Kernel Embeddings

100

dx and dy

200

(a) SG (Î± = 0.05)

0.04
50

100 150 200
dx and dy

250

Test power

10

0

Test power

Type-I error

Time (s)

101

0.06

0.5
0.0

Test power

1.0 1.0

1.0
102

0.5 0.5

0.0
1
2
3
4
5
6 0.0
1 12
Ï‰ in 1 + sin(Ï‰x) sin(Ï‰y)

(b) SG (Î± = 0.05)

(c) Sin

23 3 4 4 5 5 6 6
dx

dx

NFSIC-opt
NFSIC-med
QHSIC
NyHSIC
FHSIC
RDC

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.
Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent
only if both the number of features increases with n. By
constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed. We refer
the reader to Appendix C for a brief investigation of the test
power vs. increasing J. The test power does not necessarily
monotonically increase with J.
3.1. Toy Problems
We consider three toy problems.
1. Same Gaussian (SG). The two variables are independently drawn from the standard multivariate normal distribution i.e., X âˆ¼ N (0, Idx ) and Y âˆ¼ N (0, Idy ) where Id
is the d Ã— d identity matrix. This problem represents a case
in which H0 holds.
2. Sinusoid (Sin). Let pxy be the probability density of Pxy .
In the Sinusoid problem, the dependency of X and Y is characterized by (X, Y ) âˆ¼ pxy (x, y) âˆ 1 + sin(Ï‰x) sin(Ï‰y),
where the domains of X , Y = (âˆ’Ï€, Ï€) and Ï‰ is the frequency of the sinusoid. As the frequency Ï‰ increases, the
drawn sample becomes more similar to a sample drawn
from Uniform((âˆ’Ï€, Ï€)2 ). That is, the higher Ï‰, the harder
to detect the dependency between X and Y . This problem
was studied in Sejdinovic et al. (2013). Plots of the density
for a few values of Ï‰ are shown in Figures 6 and 7 in the
appendix. The main characteristic of interest in this problem
is the local change in the density function.
3. Gaussian Sign (GSign). In this problem, Y =
Qdx
|Z| i=1
sgn(Xi ), where X âˆ¼ N (0, Idx ), sgn(Â·) is the
sign function, and Z âˆ¼ N (0, 1) serves as a source of noise.
The full interaction of X = (X1 , . . . , Xdx ) is what makes
the problem challenging. That is, Y is dependent on X,
yet it is independent of any proper subset of {X1 , . . . , Xd }.
Thus, simultaneous consideration of all the coordinates of
X is required to successfully detect the dependency.
We fix n = 4000 and vary the problem parameters. Each
problem is repeated for 300 trials, and the sample is redrawn
each time. The significance level Î± is set to 0.05. The reclaim a linear runtime to find a global optimum.

sults are shown in Figure 2. It can be seen that in the SG
problem (Figure 2b) where H0 holds, all the tests achieve
roughly correct type-I errors at Î± = 0.05. In particular,
we point out that NFSIC-optâ€™s rejection rate is well controlled as the sample used for testing and the sample used
for parameter tuning are independent. The rejection rate
would have been much higher had we done the optimization
and testing on the same sample (i.e., overfitting). In the
Sin problem, NFSIC-opt achieves high test power for all
considered Ï‰ = 1, . . . , 6, highlighting its strength in detecting local changes in the joint density. The performance of
NFSIC-med is significantly lower than that of NFSIC-opt.
This phenomenon clearly emphasizes the importance of the
optimization to place the locations at the relevant regions in
X Ã—Y. RDC has a remarkably high performance in both Sin
and GSign (Figure 2c, 2d) despite no parameter tuning. The
ability to simultaneously consider interacting features of
NFSIC-opt is indicated by its superior test power in GSign,
especially at the challenging settings of dx = 5, 6.
NFSIC vs. QHSIC. We observe that NFSIC-opt outperforms the quadratic-time QHSIC in these two problems.
QHSIC is defined as the RKHS norm of the witness function u (see (2)). Intuitively, one can think of the RKHS
norm as taking into account all the locations (v, w). By
contrast, the proposed NFSIC evaluates the witness function
at J locations. If the differences in pxy and px py are local
(e.g., Sin problem), or there are interacting features (e.g.,
GSign problem), then only small regions in the space of
(X, Y ) are relevant in detecting the difference of pxy and
px py . In these cases, pinpointing exact test locations by
the optimization of NFSIC performs well. On the other
hand, taking into account all possible test locations as done
implicitly in QHSIC also integrates over regions where the
difference between pxy and px py is small, resulting in a
weaker indication of dependence. Whether QHSIC is better
than NFSIC depends heavily on the problem, and there is
no one best answer. If the difference between pxy and px py
is large only in localized regions, then the proposed linear
time statistic has an advantage. If the difference is spatially
diffuse, then QHSIC has an advantage. No existing work
has proposed a procedure to optimally tune kernel parameters for QHSIC; by contrast, NFSIC has a clearly defined
objective for parameter tuning.

An Adaptive Test of Independence with Analytic Kernel Embeddings
1.0 1.0

103

104
Sample size n

105

(a) SG. dx = dy = 250.

0.04
104
Sample size n

105

0.5
0.0

103

(b) SG. dx = dy = 250.

104
Sample size n

NFSIC-opt
NFSIC-med
QHSIC
NyHSIC
FHSIC
RDC

Test power

100

0.06

Test power

102

Test power

Type-I error

Time (s)

1.0

0.5 0.5

0.0
105 0.0 1 1023

4
3 10
4 size5n
Sample
d

610

5

x

(c) Sin. Ï‰ = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

3.2. Real Problems
We now examine the performance of our proposed test on
real problems.
Million Song Data (MSD) We consider a subset of the
Million Song Data3 (Bertin-Mahieux et al., 2011), in which
each song (X) out of 515,345 is represented by 90 features,
of which 12 features are timbre average (over all segments)
of the song, and 78 features are timbre covariance. Most of
the songs are western commercial tracks from 1922 to 2011.
The goal is to detect the dependency between each song and
its year of release (Y ). We set Î± = 0.01, and repeat for
300 trials where the full sample is randomly subsampled
to n points in each trial. Other settings are the same as
in the toy problems. To make sure that the type-I error
is correct, we use the permutation approach in the NFSIC
tests to compute the threshold. Figure 4b shows the test
powers as n increases from 500 to 2000. To simulate the
case where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are shown
in Figure 5 in the appendix.
Evidently, NFSIC-opt has the highest test power among all
3
Million Song Data subset: https://archive.ics.
uci.edu/ml/datasets/YearPredictionMSD.

NFSIC-opt

NFSIC-med

QHSIC

1.0

NyHSIC

FHSIC

RDC

0.5
500

Test power

1.0
Type-I error

Test power

To investigate the sample efficiency of all the tests, we fix
dx = dy = 250 in SG, Ï‰ = 4 in Sin, dx = 4 in GSign, and
increase n. Figure 3 shows the results. The quadratic dependency on n in QHSIC makes it infeasible both in terms of
memory and runtime to consider n larger than 6000 (Figure 3a). By constrast, although not the most time-efficient,
NFSIC-opt has the highest sample-efficiency for GSign, and
for Sin in the low-sample regime, significantly outperforming QHSIC. Despite the small additional overhead from the
optimization, we are yet able to conduct an accurate test
with n = 105 , dx = dy = 250 in less than 100 seconds.
We observe in Figure 3b that the two NFSIC variants have
correct type-I errors across all sample sizes. We recall from
Theorem 5 that the NFSIC test with random test locations
will asymptotically reject H0 if it is false. A demonstration
of this property is given in Figure 3c, where the test power
of NFSIC-med eventually reaches 1 with n higher than 105 .

0.02
0.01
0.00

1000
1500
Sample size n

(a) MSD problem.

500
2000

0.5

1000 2000
1500 4000
2000 6000
Sample size n Sample size n

8000

(b) Videos & Captions problem.

Figure 4: Probability of rejecting H0 as n increases in the
two real problems. Î± = 0.01.

the linear-time tests for all the sample sizes. Its test power
is second to only QHSIC. We recall that NFSIC-opt uses
half of the sample for parameter tuning. Thus, at n = 500,
the actual sample for testing is 250, which is relatively
small. The fact that there is a vast power gain from 0.4
(NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that
the optimization procedure can perform well even at a lower
sample sizes.
Videos and Captions Our last problem is based on the
VideoStory46K4 dataset (Habibian et al., 2014). The
dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors
of Wang & Schmid (2013). Each caption is represented
as a bag of words with each feature being the frequency
of one word. After filtering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is sufficiently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample efficiency among the
linear-time tests, showing that the optimization procedure is
also practical in a high dimensional setting.
4
VideoStory46K dataset: https://ivi.fnwi.uva.nl/
isis/mediamill/datasets/videostory.php.

An Adaptive Test of Independence with Analytic Kernel Embeddings

Acknowledgement
We thank the Gatsby Charitable Foundation for the financial
support. The major part of this work was carried out while
ZoltÃ¡n SzabÃ³ was a research associate at the Gatsby Computational Neuroscience Unit, University College London.

References
Anderson, Theodore W. An Introduction to Multivariate
Statistical Analysis. Wiley, 2003.
Bach, Francis R. and Jordan, Michael I. Kernel independent component analysis. Journal of Machine Learning
Research, 3:1â€“48, 2002.
Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whitman,
Brian, and Lamere, Paul. The million song dataset. In
International Conference on Music Information Retrieval
(ISMIR), 2011.
Chwialkowski, Kacper P., Ramdas, Aaditya, Sejdinovic,
Dino, and Gretton, Arthur. Fast Two-Sample Testing
with Analytic Representations of Probability Measures.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1981â€“1989. 2015.
Dauxois, Jacques and Nkiet, Guy Martial. Nonlinear canonical analysis and independence tests. The Annals of Statistics, 26(4):1254â€“1278, 1998.
Feuerverger, Andrey. A consistent test for bivariate dependence. International Statistical Review, 61(3):419â€“433,
1993.
Fukumizu, Kenji, Gretton, Arthur, Sun, Xiaohai, and
SchÃ¶lkopf, Bernhard. Kernel measures of conditional
dependence. In Advances in Neural Information Processing Systems (NIPS), pp. 489â€“496, 2008.
Gretton, Arthur and GyÃ¶rfi, LÃ¡szlÃ³. Consistent nonparametric tests of independence. Journal of Machine Learning
Research, 11:1391â€“1423, 2010.
Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and
SchÃ¶lkopf, Bernhard. Measuring Statistical Dependence
with Hilbert-Schmidt Norms. In Algorithmic Learning
Theory (ALT), pp. 63â€“77. 2005.
Gretton, Arthur, Fukumizu, Kenji, Teo, Choon H., Song,
Le, SchÃ¶lkopf, Bernhard, and Smola, Alex J. A Kernel
Statistical Test of Independence. In Advances in Neural
Information Processing Systems (NIPS), pp. 585â€“592.
2008.
Habibian, Amirhossein, Mensink, Thomas, and Snoek,
Cees GM. Videostory: A new multimedia embedding
for few-example recognition and translation of events. In

ACM International Conference on Multimedia, pp. 17â€“26,
2014.
Heller, Ruth, Heller, Yair, Kaufman, Shachar, Brill, Barak,
and Gorfine, Malka. Consistent distribution-free ksample and independence tests for univariate random
variables. Journal of Machine Learning Research, 17
(29):1â€“54, 2016.
Huo, Xiaoming and SzÃ©kely, GÃ¡bor J. Fast computing
for distance covariance. Technometrics, 58(4):435â€“447,
2016.
Jitkrittum, Wittawat, SzabÃ³, ZoltÃ¡n, Chwialkowski, Kacper,
and Gretton, Arthur. Interpretable Distribution Features
with Maximum Testing Power. In Advances in Neural
Information Processing Systems (NIPS), pp. 181â€“189.
2016.
Kowalski, Jeanne and Tu, Xin M. Modern Applied UStatistics. John Wiley & Sons, 2008.
Lehmann, Eric L. Elements of Large-Sample Theory.
Springer Science & Business Media, 1999.
Lopez-Paz, David, Hennig, Philipp, and SchÃ¶lkopf, Bernhard. The Randomized Dependence Coefficient. In Advances in Neural Information Processing Systems (NIPS),
pp. 1â€“9. 2013.
Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), pp. 1177â€“1184.
2008.
Sejdinovic, Dino, Sriperumbudur, Bharath, Gretton, Arthur,
and Fukumizu, Kenji. Equivalence of distance-based and
RKHS-based statistics in hypothesis testing. The Annals
of Statistics, 41(5):2263â€“2291, 2013.
Serfling, Robert J. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.
Smola, Alex, Gretton, Arthur, Song, Le, and SchÃ¶lkopf,
Bernhard. A Hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory (ALT), pp. 13â€“31, 2007.
Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu,
Kenji, SchÃ¶lkopf, Bernhard, and Lanckriet, Gert R. G.
Hilbert Space Embeddings and Metrics on Probability
Measures. Journal of Machine Learning Research, 11:
1517â€“1561, 2010.
Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer Science & Business Media, 2008.

An Adaptive Test of Independence with Analytic Kernel Embeddings

SzÃ©kely, GÃ¡bor J. and Rizzo, Maria L. Brownian distance
covariance. The Annals of Applied Statistics, 3(4):1236â€“
1265, 2009.
SzÃ©kely, GÃ¡bor J., Rizzo, Maria L., and Bakirov, Nail K.
Measuring and testing dependence by correlation of distances. The Annals of Statistics, 35(6):2769â€“2794, 2007.
van der Vaart, Aad. Asymptotic Statistics. Cambridge University Press, 2000.
Wang, Heng and Schmid, Cordelia. Action recognition with
improved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pp. 3551â€“3558, 2013.
Zhang, Kun, Peters, Jonas, Janzing, Dominik, and
SchÃ¶lkopf, Bernhard. Kernel-based conditional independence test and application in causal discovery. In Conference on Uncertainty in Artificial Intelligence (UAI), pp.
804â€“813, 2011.
Zhang, Qinyi, Filippi, Sarah, Gretton, Arthur, and Sejdinovic, Dino. Large-Scale Kernel Methods for Independence Testing. Statistics and Computing, pp. 1â€“18, 2017.

