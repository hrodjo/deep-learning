An Adaptive Test of Independence with Analytic Kernel Embeddings

Wittawat Jitkrittum 1 Zoltán Szabó 2 Arthur Gretton 1

Abstract
A new computationally efficient dependence measure, and an adaptive statistical test of independence,
are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at
a finite set of locations (features). These features are
chosen so as to maximize a lower bound on the test
power, resulting in a test that is data-efficient, and
that runs in linear time (with respect to the sample
size n). The optimized features can be interpreted
as evidence to reject the null hypothesis, indicating
regions in the joint domain where the joint distribution and the product of the marginals differ most.
Consistency of the independence test is established,
for an appropriate choice of features. In real-world
benchmarks, independence tests using the optimized
features perform comparably to the state-of-the-art
quadratic-time HSIC test, and outperform competing
O(n) and O(n log n) tests.

1. Introduction
We consider the design of adaptive, nonparametric statistical
tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals Px Py
with the null hypothesis that H0 : X and Y are independent. While classical tests of dependence, such as Pearson’s
correlation and Kendall’s τ , are able to detect monotonic
relations between univariate variables, more modern tests
can address complex interactions, for instance changes in
variance of X with the value of Y . Key to many recent
tests is to examine covariance or correlation between data
features. These interactions become significantly harder to
detect, and the features are more difficult to design, when
the data reside in high dimensions.
Zoltán Szabó’s ORCID ID: 0000-0001-6183-7603. Arthur Gretton’s ORCID ID: 0000-0003-3169-7624. 1 Gatsby Unit, University College London, UK. 2 CMAP, École Polytechnique, France.
Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

A basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the
Hilbert-Schmidt norm of the covariance operator between
feature mappings of the random variables (Gretton et al.,
2005; 2008). Each random variable X and Y is mapped
to a respective reproducing kernel Hilbert space Hk and
Hl . For sufficiently rich mappings, the covariance operator
norm is zero if and only if the variables are independent. A
second basic nonlinear dependence measure is the smoothed
difference between the characteristic function of the joint
distribution, and that of the product of marginals. When
a particular smoothing function is used, the statistic corresponds to the covariance between distances of X and Y variable pairs (Feuerverger, 1993; Székely et al., 2007; Székely
& Rizzo, 2009), yielding a simple test statistic based on
pairwise distances. It has been shown by Sejdinovic et al.
(2013) that the distance covariance (and its generalization
to semi-metrics) is an instance of HSIC for an appropriate
choice of kernels. A disadvantage of these feature covariance statistics, however, is that they require quadratic time
to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo &
Székely (2016) achieve an O(n log n) cost). Moreover, the
feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of
an expensive eigenvalue problem (e.g. Zhang et al., 2011) is
required for consistent estimation of the quantiles. Several
approaches were proposed by Zhang et al. (2017) to obtain
faster tests along the lines of HSIC. These include computing HSIC on finite-dimensional feature mappings chosen as
random Fourier features (RFFs) (Rahimi & Recht, 2008),
a block-averaged statistic, and a Nyström approximation
to the statistic. Key to each of these approaches is a more
efficient computation of the statistic and its threshold under
the null distribution: for RFFs, the null distribution is a
finite weighted sum of χ2 variables; for the block-averaged
statistic, the null distribution is asymptotically normal; for
Nyström, either a permutation approach is employed, or
the spectrum of the Nyström approximation to the kernel
matrix is used in approximating the null distribution. Each
of these methods costs significantly less than the O(n2 ) cost
of the full HSIC (the cost is linear in n, but also depends
quadratically on the number of features retained). A potential disadvantage of the Nyström and Fourier approaches is
that the features are not optimized to maximize test power,

An Adaptive Test of Independence with Analytic Kernel Embeddings

but are chosen randomly. The block statistic performs worse
than both, due to the large variance of the statistic under the
null (which can be mitigated by observing more data).

In these experiments, we outperform competing linear and
O(n log n) time tests.

In addition to feature covariances, correlation measures have
also been developed in infinite dimensional feature spaces:
in particular, Bach & Jordan (2002); Fukumizu et al. (2008)
proposed statistics on the correlation operator in a reproducing kernel Hilbert space. While convergence has been
established for certain of these statistics, their computational cost is high at O(n3 ), and test thresholds have relied
on permutation. A number of much faster approaches to
testing based on feature correlations have been proposed,
however. For instance, Dauxois & Nkiet (1998) compute
statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order
B-splines. The cost of this approach is O(n). This idea
was extended by Lopez-Paz et al. (2013), who computed
the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they
performed a copula transform on the inputs, with a total
cost of O(n log n). Finally, space partitioning approaches
have also been proposed, based on statistics such as the
KL divergence, however these apply only to univariate variables (Heller et al., 2016), or to multivariate variables of
low dimension (Gretton & Györfi, 2010) (that said, these
tests have other advantages of theoretical interest, notably
distribution-independent test thresholds).

2. Independence Criteria and Statistical Tests

The approach we take is most closely related to HSIC on a
finite set of features. Our simplest test statistic, the Finite
Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each
of X and Y . A normalized version of the statistic (NFSIC)
yields a distribution-independent asymptotic test threshold.
We show that our test is consistent, despite a finite number
of analytic features being used, via a generalization of arguments in Chwialkowski et al. (2015). As in recent work
on two-sample testing by Jitkrittum et al. (2016), our test
is adaptive in the sense that we choose our features on a
held-out validation set to optimize a lower bound on the
test power. The design of features for independence testing
turns out to be quite different to the case of two-sample
testing, however: the task is to find correlated feature pairs
on the respective marginal domains, rather than attempting
to find a single, high-dimensional feature representation on
the tensor product of the marginals, as we would need to
do if we were comparing distributions Pxy and Qxy . While
the use of coupled feature pairs on the marginals entails a
smaller feature space dimension, it introduces significant
complications in the proof of the lower bound, compared
with the two-sample case. We demonstrate the performance
of our tests on several challenging artificial and real-world
datasets, including detection of dependence between music
and its year of appearance, and between videos and captions.

We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle
that dependence can be measured in terms of the covariance between data features. Next, we propose a normalized
version of this statistic (NFSIC), with a simpler asymptotic
distribution when Pxy = Px Py . We show how to select
features for the latter statistic to maximize a lower bound
on the power of its corresponding statistical test.
2.1. The Finite Set Independence Criterion
We begin by recalling the Hilbert-Schmidt Independence
Criterion (HSIC) as proposed in Gretton et al. (2005), since
our unnormalized statistic is built along similar lines. Consider two random variables X ∈ X ⊆ Rdx and Y ∈ Y ⊆
Rdy . Denote by Pxy the joint distribution between X and Y ;
Px and Py are the marginal distributions of X and Y . Let ⊗
denote the tensor product, such that (a ⊗ b) c = a hb, ci. Assume that k : X × X → R and l : Y × Y → R are positive
definite kernels associated with reproducing kernel Hilbert
spaces (RKHS) Hk and Hl , respectively. Let k · kHS be the
norm on the space of Hl → Hk Hilbert-Schmidt operators.
Then, HSIC between X and Y is defined as

2
HSIC(X, Y ) = µxy − µx ⊗ µy 
HS

=E

(x,y),(x0 ,y0 )

0

0

[k(x, x )l(y, y )]

+ Ex Ex0 [k(x, x0 )]Ey Ey0 [l(y, y0 )]
− 2E(x,y) [Ex0 [k(x, x0 )]Ey0 [l(y, y0 )]] ,

(1)

where Ex := Ex∼Px , Ey := Ey∼Py , Exy := E(x,y)∼Pxy ,
and x0 is an independent copy of x. The mean embedding of
Pxy belongs to the space
R of Hilbert-Schmidt operators from
Hl to Hk , µxy := X ×Y k(x, ·) ⊗ l(y, ·) dPxy (x, y) ∈
are µx :=
HS(H
l , Hk ), and the marginal mean embeddings
R
R
k(x,
·)
dP
(x)
∈
H
and
µ
:=
l(y,
·)
dP
x
k
y
y (y) ∈
X
Y
Hl (Smola et al., 2007). Gretton et al. (2005, Theorem 4)
show that if the kernels k and l are universal (Steinwart
& Christmann, 2008) on compact domains X and Y, then
HSIC(X, Y ) = 0 if and only if X and Y are independent.
Given a joint sample Zn = {(xi , yi )}ni=1 ∼ Pxy , an empirical estimator of HSIC can be computed in O(n2 ) time
by replacing the population expectations in (1) with their
corresponding empirical expectations based on Zn .
We now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC). Let
X ⊆ Rdx and Y ⊆ Rdy be open sets.
Let
µx µy (x, y) := µx (x)µy (y) The idea is to see µxy (v, w) =
Exy [k(x, v)l(y, w)], µx (v) = Ex [k(x, v)] and µy (w) =
Ey [l(y, w)] as smooth functions, and consider a new dis-

An Adaptive Test of Independence with Analytic Kernel Embeddings

tance between µxy and µx µy instead of a Hilbert-Schmidt
distance as in HSIC (Gretton et al., 2005). The new measure is given by the average of squared differences between µxy and µx µy , evaluated at J random test locations
VJ := {(vi , wi )}Ji=1 ⊂ X × Y.
J
1X
2
FSIC (X, Y ) :=
[µxy (vi , wi ) − µx (vi )µy (wi )]
J i=1
2

=

J
1X 2
1
u (vi , wi ) = kuk22 ,
J i=1
J

where
u(v, w) := µxy (v, w) − µx (v)µy (w)

= Exy [k(x, v)l(y, w)] − Ex [k(x, v)]Ey [l(y, w)], (2)

= covxy [k(x, v), l(y, w)],

u := (u(v1 , w1 ), . . . , u(vJ , wJ ))> , and {(vi , wi )}Ji=1
are realizations from an absolutely continuous distribution
(wrt the Lebesgue measure).
Our first result in Proposition 2 states that FSIC(X, Y )
almost surely defines a dependence measure for the random
variables X and Y , provided that the product kernel on
the joint space X × Y is characteristic and analytic (see
Definition 1).
Definition 1 (Analytic kernels (Chwialkowski et al., 2015)).
Let X be an open set in Rd . A positive definite kernel
k : X × X → R is said to be analytic on its domain X × X
if for all v ∈ X , f (x) := k(x, v) is an analytic function on
X.

Assumption A. The kernels k : X × X → R and
l : Y × Y → R are bounded by Bk and Bl respectively
[supx,x0 ∈X k(x, x0 ) ≤ Bk , supy,y0 ∈Y l(y, y0 ) ≤ Bl ] , and
the product kernel g((x, y), (x0 , y0 )) := k(x, x0 )l(y, y0 ) is
characteristic (Sriperumbudur et al., 2010, Definition 6),
and analytic (Definition 1) on (X × Y) × (X × Y).
Proposition 2 (FSIC is a dependence measure). Assume
that assumption A holds, and that the test locations
VJ = {(vi , wi )}Ji=1 are drawn from an absolutely continuous distribution η. Then, η-almost surely, it holds that
FSIC(X, Y ) = √1J kuk2 = 0 if and only if X and Y are
independent.

Proof. Since g is characteristic, the mean embedding map
Πg : P 7→ E(x,y)∼P [g((x, y), ·)] is injective (Sriperumbudur et al., 2010, Section 3), where P is a probability
distribution on X × Y. Since g is analytic, by Lemma 10
(Appendix), µxy and µx µy are analytic functions. Thus,
Lemma 11 (Appendix, setting Λ = Πg ) guarantees that
FSIC(X, Y ) = 0 ⇐⇒ Pxy = Px Py ⇐⇒ X and Y are
independent almost surely.

FSIC uses µxy as a proxy for Pxy , and µx µy as a proxy for
Px Py . Proposition 2 states that, to detect the dependence
between X and Y , it is sufficient to evaluate the difference
of the population joint embedding µxy and the embedding
of the product of the marginal distributions µx µy at a finite number of locations (defined by VJ ). The intuitive explanation of this property is as follows. If Pxy = Px Py ,
then u(v, w) = 0 everywhere, and FSIC(X, Y ) = 0
for any VJ . If Pxy 6= Px Py , then u will not be a zero
function, since the mean embedding map is injective (requires the product kernel to be characteristic). Using the
same argument as in Chwialkowski et al. (2015), since k
and l are analytic, u is also analytic, and the set of roots
Ru := {(v, w) | u(v, w) = 0} has Lebesgue measure zero.
Thus, it is sufficient to draw (v, w) from an absolutely continuous distribution to have (v, w) ∈
/ Ru η-almost surely,
and hence FSIC(X, Y ) > 0. We note that a characteristic
kernel which is not analytic may produce u such that Ru has
a positive Lebesgue measure. In this case, there is a positive
probability that (v, w) ∈ Ru , resulting in a potential failure
to detect the dependence.
The next proposition shows that Gaussian kernels k and l
yield a product kernel which is characteristic and analytic; in
other words, this is an example when Assumption A holds.
Proposition 3 (A product of Gaussian kernels
is characteristic and analytic).
Let k(x, x0 )
=

exp −(x − x0 )> A(x − x0 )
and
l(y, y0 )
=
be
Gaussian
kerexp −(y − y0 )> B(y − y0 )
nels on Rdx × Rdx and Rdy × Rdy respectively,
for positive definite matrices A and B.
Then,
g((x, y), (x0 , y0 )) = k(x, x0 )l(y, y0 ) is characteristic and analytic on (Rdx × Rdy ) × (Rdx × Rdy ).
Proof (sketch). The main idea is to use the fact that a Gaussian kernel is analytic, and a product of Gaussian kernels is
a Gaussian kernel on the pair of variables. See the full proof
in Appendix D.
Plug-in Estimator Assume that we observe a
i.i.d.
joint sample Zn := {(xi , yi )}ni=1 ∼ Pxy . Unbiased estimators of µxy
w) and µx µy (v, w)
P(v,
n
1
are µ̂xy (v, w)
:=
k(xi , v)l(yi , w) and
nP i=1P
n
1
µ[
:= n(n−1) i=1 j6=i k(xi , v)l(yj , w),
x µy (v, w)
respectively. A straightforward empirical estimator of
FSIC2 is then given by
J
X
\2 (Zn ) = 1
FSIC
û(vi , wi )2 ,
J i=1

û(v, w) := µ̂xy (v, w) − µ[
(3)
x µy (v, w)
X
2
=
h(v,w) ((xi , yi ), (xj , yj )), (4)
n(n − 1) i<j

where h(v,w) ((x, y), (x0 , y0 ))
k(x0 , v))(l(y, w) − l(y0 , w)).

1
:=
2 (k(x, v) −
For conciseness, we

An Adaptive Test of Independence with Analytic Kernel Embeddings

define û := (û1 , . . . , ûJ )> ∈ RJ where ûi := û(vi , wi )
\2 (Zn ) = 1 û> û.
so that FSIC
J
\2 can be efficiently computed in O((dx +dy )Jn) time
FSIC
which is linear in n [see (3) which does not have nested
double sums], assuming that the runtime complexity of
evaluating k(x, v) is O(dx ) and that of l(y, w) is O(dy ).
Since FSIC satisfies FSIC(X, Y ) = 0 ⇐⇒ X ⊥ Y , in
principle its empirical estimator can be used as a test statistic
for an independence test proposing a null hypothesis H0 :
“X and Y are independent” against an alternative H1 : “X
and Y are dependent.” The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however, and depends on the unknown Pxy .
This prompts us to consider a normalized version of FSIC
whose asymptotic null distribution takes a more convenient
form. We first derive the asymptotic distribution of û in
Proposition 4, which we use to derive the normalized test
statistic in Theorem 5. As a shorthand, we write z := (x, y),
t := (v, w), covz is covariance,Vz stands for variance.
Proposition 4 (Asymptotic distribution of û). Define
u := (u(t1 ), . . . , u(tJ ))> , k̃(x, v) := k(x, v) −
Ex0 k(x0 , v), and ˜l(y, w) := l(y, w) − Ey0 l(y0 , w).
Let Σ = [Σij ] ∈ RJ×J be the positive semi-definite
matrix with entries Σij = covz (û(ti ), û(tj )) =
Exy [k̃(x, vi )˜l(y, wi )k̃(x, vj )˜l(y, wj )]−u(ti )u(tj ). Then,
under both H0 and H1 , for any fixed test locations
{t1 , . . . , tJ } for which Σ is full rank, and√ 0 <
Vz [htj (z)] < ∞ for j = 1, . . . , J, it holds that n(û −
d

u) → N (0, Σ).

Proof. For a fixed {t1 , . . . , tJ }, û is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht .
Thus, by Lehmann (1999, Theorem 6.1.6) and Kowalski & Tu (2008, Section 5.1, Theorem 1), it follows di√
d
rectly that n(û − u) → N (0, Σ) where we note that
Exy [k̃(x, v)˜l(y, w)] = u(v, w).
Recall from Proposition 2 that u = 0 holds almost surely under H0 . The asymptotic normality described in Proposition
\2 = n û> û converges in distribution
4 implies that nFSIC
J
to a sum of J dependent weighted χ2 random variables.
The dependence comes from the fact that the coordinates
û1 . . . , ûJ of û all depend on the sample Zn . This null distribution is not analytically tractable, and requires a large
number of simulations to compute the rejection threshold
Tα for a given significance value α.

chi-squared distribution with J degrees of freedom. We
\ 2 is
then show that the independence test defined by NFSIC
consistent. These results are given in Theorem 5.
\ 2 is consisTheorem 5 (Independence test based on NFSIC
tent). Let Σ̂ be a consistent estimate of Σ based on the joint
sample Zn , where Σ is defined in Proposition 4. Assume
that VJ = {(vi , wi )}Ji=1 ∼ η where η is absolutely contin\ 2 statistic is
uous wrt the Lebesgue measure. The NFSIC

−1
defined as λ̂n := nû> Σ̂ + γn I
û where γn ≥ 0 is a
regularization parameter. Assume that
1. Assumption A holds.
2. Σ is invertible η-almost surely.
3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
d

1. Under H0 , λ̂n → χ2 (J) as n → ∞.


2. Under H1 , for any r ∈ R, limn→∞ P λ̂n ≥ r = 1
η-almost surely. That is, the independence test based on
\ 2 is consistent.
NFSIC
>
(Σ̂ + γn I)−1 û asymptotProof (sketch) . Under H0 , nû√
2
ically follows χ (J) because nû is asymptotically normally distributed (see Proposition 4). Claim 2 builds on
the result in Proposition 2 stating that u 6= 0 under H1 ; it
follows using the convergence of û to u. The full proof can
be found in Appendix E.

Theorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for
any fixed threshold. Asymptotically the test threshold Tα is
given by the (1 − α)-quantile of χ2 (J) and is independent
of n. The assumption on the consistency of Σ̂ is required
to obtain the asymptotic chi-squared distribution. The regularization parameter γn is to ensure that (Σ̂ + γn I)−1 can
be stably computed. In practice, γn requires no tuning, and
can be set to be a very small constant. We emphasize that J
need not increase with n for test consistency.

2.2. Normalized FSIC and Adaptive Test

The next proposition states that the computational com\ 2 estimator is linear in both the input
plexity of the NFSIC
dimension and sample size, and that it can be expressed
in terms of the K =[K ij ] = [k(vi , xj )] ∈ RJ×n , L =
[Lij ] = [l(wi , yj )] ∈ RJ×n matrices. In contrast to typical
kernel methods, a large Gram matrix of size n × n is not
\ 2.
needed to compute NFSIC

For the purpose of an independence test, we will consider
\2 , which we call NFSIC
\ 2,
a normalized variant of FSIC
whose tractable asymptotic null distribution is χ2 (J), the

\ 2 ). Let
Proposition 6 (An empirical estimator of NFSIC
1n := (1, . . . , 1)> ∈ Rn . Denote by ◦ the element-wise
matrix product. Then,

An Adaptive Test of Independence with Analytic Kernel Embeddings

1. û =

(K◦L)1n
n−1

−

(K1n )◦(L1n )
.
n(n−1)

2. A consistent estimator for Σ is Σ̂ =

ΓΓ>
n

where

−1
b >
Γ := (K − n−1 K1n 1>
L1n 1>
n ) ◦ (L − n
n ) − û 1n ,

ûb = n−1 (K ◦ L) 1n − n−2 (K1n ) ◦ (L1n ) .

Assume that the complexity of the kernel evaluation is linear in the input dimension. Then the test statistic λ̂n =

−1
nû> Σ̂ + γn I
û can be computed in O(J 3 + J 2 n +

(dx + dy )Jn) time.

Proof (sketch). Claim 1 for û is straightforward. The expression for Σ̂ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4. The consistency of Σ̂ can be obtained by noting that the finite sample
bound for P(kΣ̂ − ΣkF > t) decreases as n increases. This
is implicitly shown in Appendix F.2.2 and its following
sections.
Although the dependency of the estimator on J is cubic, we
empirically observe that only a small value of J is required
(see Section 3). The number of test locations J relates to
the number of regions in X × Y of pxy and px py that differ
(see Figure 1).
Theorem 5 asserts the consistency of the test for any test
locations VJ drawn from an absolutely continuous distribution. In practice, VJ can be further optimized to increase
the test power for a fixed sample size. Our final theoretical
\ 2 i.e.,
result gives a lower bound on the test power of NFSIC
the probability of correctly rejecting H0 . We will use this
lower bound as the objective function to determine VJ and
the kernel parameters. Let k · kF be the Frobenius norm.

Theorem 7 (A lower bound on the test power). Let
NFSIC2 (X, Y ) := λn := nu> Σ−1 u. Let K be a kernel
class for k, L be a kernel class for l, and V be a collection
with each element being a set of J locations. Assume that

2
ξ3 :=√8c1 B 2 J, c3 := 4B√
J c̃2 , ξ4 := 28 B 4 J 2 c21 , c1 :=
2
4B J J c̃, and c2 := 4B J c̃. Moreover, for sufficiently
large fixed n, L(λn ) is increasing in λn .

We provide the proof in Appendix F. To put
Theorem
7 into
assume that

o K =
n
 perspective,
kx−vk2
2
2
2
| σx ∈ [σx,l , σx,u ]
=: Kg
(x, v) 7→ exp − 2σ2
x

2
2
for some 0 < σx,l
< σx,u
< ∞ and L =
n


o
2
ky−wk
2
2
2
(y, w) 7→ exp − 2σ2
| σy ∈ [σy,l
, σy,u
] =: Lg
y

2
2
for some 0 < σy,l
< σy,u
< ∞ are Gaussian kernel
classes. Then, in Theorem 7, B = Bk = Bl = 1,
and B ∗ = 2. The assumption c̃ < ∞ is a technical condition to guarantee that the test power lower
bound is finite for all θ defined
 by the feasible sets
K, L, and V. Let V,r := VJ | kvi k2 , kw	i k2 ≤
r and kvi − vj k22 + kwi − wj k22 ≥ , for all i 6= j . If we
set K = Kg , L = Lg , and V = V,r for some , r > 0, then
c̃ < ∞ as Kg , Lg , and V,r are compact. In practice, these
conditions do not necessarily create restrictions as they
almost always hold implicitly. We show in Appendix C that
the objective function used to choose VJ will discourage
any two locations to be in the same neighborhood.

Parameter Tuning Let θ be the collection of all tuning
parameters of the test. If k ∈ Kg and l ∈ Lg (i.e., Gaussian kernels), then θ = {σx2 , σy2 , VJ }. The test power
lower bound L(λn ) in Theorem 7 is a function of λn =
nu> Σ−1 u which is the population counterpart of the test
statistic λ̂n . As in FSIC, it can be shown that λn = 0 if
and only if X are Y are independent (from Proposition 2).
According to Theorem 7, for a sufficiently large n, the test
power lower bound is increasing in λn . One can therefore
think of λn (a function of θ) as representing how easily the
test rejects H0 given a problem Pxy . The higher the λn , the
greater the lower bound on the test power, and thus the more
likely it is that the test will reject H0 when it is false.

In light of this reasoning, we propose to set θ by maximizing the lower bound on the test power i.e., set θ to
1. There exist finite Bk and Bl such that
θ∗ = arg maxθ L(λn ). Assume that n is sufficiently large
supk∈K supx,x0 ∈X |k(x, x0 )|
≤
Bk
and
so that λn 7→ L(λn ) is an increasing function. Then,
supl∈L supy,y0 ∈Y |l(y, y0 )| ≤ Bl .
arg maxθ L(λn ) = arg maxθ λn . That this procedure is
also valid under H0 can be seen as follows. Under H0 ,
2. c̃ := supk∈K supl∈L supVJ ∈V kΣ−1 kF < ∞.
θ∗ = arg maxθ 0 will be arbitrary. Since Theorem 7 guarand
Then, for any k ∈K, l ∈ L,
V
∈
V,
and
λ
≥
r,
the
test
n
 J
tees that λ̂n → χ2 (J) as n → ∞ for any θ, the asymptotic
power satisfies P λ̂n ≥ r ≥ L(λn ) where
null distribution does not change by using θ∗ . In practice,
λn is a population quantity which is unknown. We propose
2
−b0.5nc(λn −r)2 /[ξ2 n2 ]
−ξ1 γn
(λn −r)2 /n
dividing the sample Zn into two disjoint sets: training and
L(λn ) = 1 − 62e
− 2e
test sets. The training set is used to compute λ̂n (an estimate
2
2
2
− 2e−[(λn −r)γn (n−1)/3−ξ3 n−c3 γn n(n−1)] /[ξ4 n (n−1)] , of λn ) to optimize for θ∗ , and the test set is used for the actual independence test with the optimized θ∗ . The splitting
1
∗
b·c is the floor function, ξ1 := 32 c2 J 2 B ∗ , B is a constant
is to guarantee the independence of θ∗ and the test sample
1
2
2
depending on only Bk and Bl , ξ2 := 72c2 JB , B := Bk Bl ,
to avoid overfitting.

An Adaptive Test of Independence with Analytic Kernel Embeddings

(a) µ̂xy (v, w)

(b) µ[
x µy (v, w)

b
(c) Σ(v,
w)

(d) Statistic λ̂n (v, w)

\ 2.
Figure 1: Illustration of NFSIC
\ 2 , we visualTo better understand the behaviour of NFSIC
ize µ̂xy (v, w), µ[
µ
(v,
w)
and
Σ̂(v,
w)
as a function of
x y
one test location (v, w) on a simple toy problem. In this
problem, Y = −X + Z where Z ∼ N (0, 0.32 ) is an independent noise variable. As we consider only one location
(J = 1), Σ̂(v, w) is a scalar. The statistic can be written
2
µx µy (v,w))
(µ̂xy (v,w)−\
as λ̂n = n
. These components are
Σ̂(v,w)
shown in Figure 1, where we use Gaussian kernels for both
X and Y , and the horizontal and vertical axes correspond
to v ∈ R and w ∈ R, respectively.
Intuitively, û(v, w) = µ̂xy (v, w) − µ[
x µy (v, w) captures
the difference of the joint distribution and the product of
the marginals as a function of (v, w). Squaring û(v, w)
and dividing it by the variance shown in Figure 1c gives the
statistic (also the parameter tuning objective) shown in Figure 1d. The latter figure illustrates that the parameter tuning
objective function can be non-convex: non-convexity arises
since there are multiple ways to detect the difference between the joint distribution and the product of the marginals.
In this case, the lower left and upper right regions equally
indicate the largest difference. A convex objective would
not be able to capture this phenomenon.

3. Experiments
In this section, we empirically study the performance of
the proposed method on both toy (Section 3.1) and real
problems (Section 3.2). We are interested in challenging problems requiring a large number of samples, where
a quadratic-time test might be computationally infeasible. Our goal is not to outperform a quadratic-time test
with a linear-time test uniformly over all testing problems.
We will find, however, that our test does outperform the
quadratic-time test in some cases. Code is available at
https://github.com/wittawatj/fsic-test.
We compare the proposed NFSIC with optimization (NFSIC\2
opt) to five multivariate nonparametric tests. The NFSIC
test without optimization (NFSIC-med) acts as a baseline,
allowing the effect of parameter optimization to be clearly

seen. For pedagogical reason, we consider the original HSIC
test of Gretton et al. (2005) denoted by QHSIC, which is a
quadratic-time test. Nyström HSIC (NyHSIC) uses a Nyström approximation to the kernel matrices of X and Y when
computing the HSIC statistic. FHSIC is another variant of
HSIC in which a random Fourier feature approximation
(Rahimi & Recht, 2008) to the kernel is used. NyHSIC and
FHSIC are studied in Zhang et al. (2017) and can be computed in O(n), with quadratic dependency on the number
of inducing points in NyHSIC, and quadratic dependency
on the number of random features in FHSIC. Finally, the
Randomized Dependence Coefficient (RDC) proposed in
Lopez-Paz et al. (2013) is also considered. The RDC can be
seen as the primal form (with random Fourier features) of
the kernel canonical correlation analysis of Bach & Jordan
(2002) on copula-transformed data. We consider RDC as a
linear-time test even though preprocessing by an empirical
copula transform costs O((dx + dy )n log n).
We use Gaussian kernel classes Kg and Lg for both
X and Y in all the methods. Except NFSIC-opt, all
other tests use full sample to conduct the independence
test, where the Gaussian widths σx and σy are set according to the widely used median heuristic i.e., σx =
median ({kxi − xj k2 | 1 ≤ i < j ≤ n}), and σy is set in
the same way using {yi }ni=1 . The J locations for NFSICmed are randomly drawn from the standard multivariate
normal distribution in each trial. For a sample of size n,
NFSIC-opt uses half the sample for parameter tuning, and
the other disjoint half for the test. We permute the sample
300 times in RDC1 and HSIC to simulate from the null
distribution and compute the test threshold. The null distributions for FHSIC and NyHSIC are given by a finite sum of
weighted χ2 (1) random variables given in Eq. 8 of Zhang
et al. (2017). Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1 − α)-quantile of
χ2 (J). To provide a fair comparison, we set J = 10, use 10
inducing points in NyHSIC, and 10 random Fourier features
in FHSIC and RDC.
Optimization of NFSIC-opt The parameters of NFSIC-opt
are σx , σy , and J locations of size (dx + dy )J. We treat all
the parameters as a long vector in R2+(dx +dy )J and use gradient ascent to optimize λ̂n/2 . We observe that initializing
VJ by randomly picking J points from the training sample
yields good performance. The regularization parameter γn
in NFSIC is fixed to a small value, and is not optimized. It is
worth emphasizing that the complexity of the optimization
procedure is still linear-time.2
1
We use a permutation test for RDC, following the authors’ implementation (https://github.com/lopezpaz/
randomized_dependence_coefficient, referred commit: b0ac6c0).
2
Our claim on linear runtime (with respect to n) is for the
gradient ascent procedure to find a local optimum for θ. We do not

An Adaptive Test of Independence with Analytic Kernel Embeddings

100

dx and dy

200

(a) SG (α = 0.05)

0.04
50

100 150 200
dx and dy

250

Test power

10

0

Test power

Type-I error

Time (s)

101

0.06

0.5
0.0

Test power

1.0 1.0

1.0
102

0.5 0.5

0.0
1
2
3
4
5
6 0.0
1 12
ω in 1 + sin(ωx) sin(ωy)

(b) SG (α = 0.05)

(c) Sin

23 3 4 4 5 5 6 6
dx

dx

NFSIC-opt
NFSIC-med
QHSIC
NyHSIC
FHSIC
RDC

(d) GSign

Figure 2: (a): Runtime. (b): Probability of rejecting H0 as problem parameters vary. Fix n = 4000.
Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent
only if both the number of features increases with n. By
constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed. We refer
the reader to Appendix C for a brief investigation of the test
power vs. increasing J. The test power does not necessarily
monotonically increase with J.
3.1. Toy Problems
We consider three toy problems.
1. Same Gaussian (SG). The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx ) and Y ∼ N (0, Idy ) where Id
is the d × d identity matrix. This problem represents a case
in which H0 holds.
2. Sinusoid (Sin). Let pxy be the probability density of Pxy .
In the Sinusoid problem, the dependency of X and Y is characterized by (X, Y ) ∼ pxy (x, y) ∝ 1 + sin(ωx) sin(ωy),
where the domains of X , Y = (−π, π) and ω is the frequency of the sinusoid. As the frequency ω increases, the
drawn sample becomes more similar to a sample drawn
from Uniform((−π, π)2 ). That is, the higher ω, the harder
to detect the dependency between X and Y . This problem
was studied in Sejdinovic et al. (2013). Plots of the density
for a few values of ω are shown in Figures 6 and 7 in the
appendix. The main characteristic of interest in this problem
is the local change in the density function.
3. Gaussian Sign (GSign). In this problem, Y =
Qdx
|Z| i=1
sgn(Xi ), where X ∼ N (0, Idx ), sgn(·) is the
sign function, and Z ∼ N (0, 1) serves as a source of noise.
The full interaction of X = (X1 , . . . , Xdx ) is what makes
the problem challenging. That is, Y is dependent on X,
yet it is independent of any proper subset of {X1 , . . . , Xd }.
Thus, simultaneous consideration of all the coordinates of
X is required to successfully detect the dependency.
We fix n = 4000 and vary the problem parameters. Each
problem is repeated for 300 trials, and the sample is redrawn
each time. The significance level α is set to 0.05. The reclaim a linear runtime to find a global optimum.

sults are shown in Figure 2. It can be seen that in the SG
problem (Figure 2b) where H0 holds, all the tests achieve
roughly correct type-I errors at α = 0.05. In particular,
we point out that NFSIC-opt’s rejection rate is well controlled as the sample used for testing and the sample used
for parameter tuning are independent. The rejection rate
would have been much higher had we done the optimization
and testing on the same sample (i.e., overfitting). In the
Sin problem, NFSIC-opt achieves high test power for all
considered ω = 1, . . . , 6, highlighting its strength in detecting local changes in the joint density. The performance of
NFSIC-med is significantly lower than that of NFSIC-opt.
This phenomenon clearly emphasizes the importance of the
optimization to place the locations at the relevant regions in
X ×Y. RDC has a remarkably high performance in both Sin
and GSign (Figure 2c, 2d) despite no parameter tuning. The
ability to simultaneously consider interacting features of
NFSIC-opt is indicated by its superior test power in GSign,
especially at the challenging settings of dx = 5, 6.
NFSIC vs. QHSIC. We observe that NFSIC-opt outperforms the quadratic-time QHSIC in these two problems.
QHSIC is defined as the RKHS norm of the witness function u (see (2)). Intuitively, one can think of the RKHS
norm as taking into account all the locations (v, w). By
contrast, the proposed NFSIC evaluates the witness function
at J locations. If the differences in pxy and px py are local
(e.g., Sin problem), or there are interacting features (e.g.,
GSign problem), then only small regions in the space of
(X, Y ) are relevant in detecting the difference of pxy and
px py . In these cases, pinpointing exact test locations by
the optimization of NFSIC performs well. On the other
hand, taking into account all possible test locations as done
implicitly in QHSIC also integrates over regions where the
difference between pxy and px py is small, resulting in a
weaker indication of dependence. Whether QHSIC is better
than NFSIC depends heavily on the problem, and there is
no one best answer. If the difference between pxy and px py
is large only in localized regions, then the proposed linear
time statistic has an advantage. If the difference is spatially
diffuse, then QHSIC has an advantage. No existing work
has proposed a procedure to optimally tune kernel parameters for QHSIC; by contrast, NFSIC has a clearly defined
objective for parameter tuning.

An Adaptive Test of Independence with Analytic Kernel Embeddings
1.0 1.0

103

104
Sample size n

105

(a) SG. dx = dy = 250.

0.04
104
Sample size n

105

0.5
0.0

103

(b) SG. dx = dy = 250.

104
Sample size n

NFSIC-opt
NFSIC-med
QHSIC
NyHSIC
FHSIC
RDC

Test power

100

0.06

Test power

102

Test power

Type-I error

Time (s)

1.0

0.5 0.5

0.0
105 0.0 1 1023

4
3 10
4 size5n
Sample
d

610

5

x

(c) Sin. ω = 4.

(d) GSign. dx = 4.

Figure 3: (a) Runtime. (b): Probability of rejecting H0 as n increases in the toy problems.

3.2. Real Problems
We now examine the performance of our proposed test on
real problems.
Million Song Data (MSD) We consider a subset of the
Million Song Data3 (Bertin-Mahieux et al., 2011), in which
each song (X) out of 515,345 is represented by 90 features,
of which 12 features are timbre average (over all segments)
of the song, and 78 features are timbre covariance. Most of
the songs are western commercial tracks from 1922 to 2011.
The goal is to detect the dependency between each song and
its year of release (Y ). We set α = 0.01, and repeat for
300 trials where the full sample is randomly subsampled
to n points in each trial. Other settings are the same as
in the toy problems. To make sure that the type-I error
is correct, we use the permutation approach in the NFSIC
tests to compute the threshold. Figure 4b shows the test
powers as n increases from 500 to 2000. To simulate the
case where H0 holds in the problem, we permute the sample
to break the dependency of X and Y . The results are shown
in Figure 5 in the appendix.
Evidently, NFSIC-opt has the highest test power among all
3
Million Song Data subset: https://archive.ics.
uci.edu/ml/datasets/YearPredictionMSD.

NFSIC-opt

NFSIC-med

QHSIC

1.0

NyHSIC

FHSIC

RDC

0.5
500

Test power

1.0
Type-I error

Test power

To investigate the sample efficiency of all the tests, we fix
dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and
increase n. Figure 3 shows the results. The quadratic dependency on n in QHSIC makes it infeasible both in terms of
memory and runtime to consider n larger than 6000 (Figure 3a). By constrast, although not the most time-efficient,
NFSIC-opt has the highest sample-efficiency for GSign, and
for Sin in the low-sample regime, significantly outperforming QHSIC. Despite the small additional overhead from the
optimization, we are yet able to conduct an accurate test
with n = 105 , dx = dy = 250 in less than 100 seconds.
We observe in Figure 3b that the two NFSIC variants have
correct type-I errors across all sample sizes. We recall from
Theorem 5 that the NFSIC test with random test locations
will asymptotically reject H0 if it is false. A demonstration
of this property is given in Figure 3c, where the test power
of NFSIC-med eventually reaches 1 with n higher than 105 .

0.02
0.01
0.00

1000
1500
Sample size n

(a) MSD problem.

500
2000

0.5

1000 2000
1500 4000
2000 6000
Sample size n Sample size n

8000

(b) Videos & Captions problem.

Figure 4: Probability of rejecting H0 as n increases in the
two real problems. α = 0.01.

the linear-time tests for all the sample sizes. Its test power
is second to only QHSIC. We recall that NFSIC-opt uses
half of the sample for parameter tuning. Thus, at n = 500,
the actual sample for testing is 250, which is relatively
small. The fact that there is a vast power gain from 0.4
(NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that
the optimization procedure can perform well even at a lower
sample sizes.
Videos and Captions Our last problem is based on the
VideoStory46K4 dataset (Habibian et al., 2014). The
dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding
text captions (Y ) uploaded by the users. Each video is
represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors
of Wang & Schmid (2013). Each caption is represented
as a bag of words with each feature being the frequency
of one word. After filtering only words which occur in at
least six video captions, we obtain dy = 1878 words. We
examine the test powers as n increases from 2000 to 8000.
The results are given in Figure 4. The problem is sufficiently challenging that all linear-time tests achieve a low
power at n = 2000. QHSIC performs exceptionally well
on this problem, achieving a maximum power throughout.
NFSIC-opt has the highest sample efficiency among the
linear-time tests, showing that the optimization procedure is
also practical in a high dimensional setting.
4
VideoStory46K dataset: https://ivi.fnwi.uva.nl/
isis/mediamill/datasets/videostory.php.

An Adaptive Test of Independence with Analytic Kernel Embeddings

Acknowledgement
We thank the Gatsby Charitable Foundation for the financial
support. The major part of this work was carried out while
Zoltán Szabó was a research associate at the Gatsby Computational Neuroscience Unit, University College London.

References
Anderson, Theodore W. An Introduction to Multivariate
Statistical Analysis. Wiley, 2003.
Bach, Francis R. and Jordan, Michael I. Kernel independent component analysis. Journal of Machine Learning
Research, 3:1–48, 2002.
Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whitman,
Brian, and Lamere, Paul. The million song dataset. In
International Conference on Music Information Retrieval
(ISMIR), 2011.
Chwialkowski, Kacper P., Ramdas, Aaditya, Sejdinovic,
Dino, and Gretton, Arthur. Fast Two-Sample Testing
with Analytic Representations of Probability Measures.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1981–1989. 2015.
Dauxois, Jacques and Nkiet, Guy Martial. Nonlinear canonical analysis and independence tests. The Annals of Statistics, 26(4):1254–1278, 1998.
Feuerverger, Andrey. A consistent test for bivariate dependence. International Statistical Review, 61(3):419–433,
1993.
Fukumizu, Kenji, Gretton, Arthur, Sun, Xiaohai, and
Schölkopf, Bernhard. Kernel measures of conditional
dependence. In Advances in Neural Information Processing Systems (NIPS), pp. 489–496, 2008.
Gretton, Arthur and Györfi, László. Consistent nonparametric tests of independence. Journal of Machine Learning
Research, 11:1391–1423, 2010.
Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and
Schölkopf, Bernhard. Measuring Statistical Dependence
with Hilbert-Schmidt Norms. In Algorithmic Learning
Theory (ALT), pp. 63–77. 2005.
Gretton, Arthur, Fukumizu, Kenji, Teo, Choon H., Song,
Le, Schölkopf, Bernhard, and Smola, Alex J. A Kernel
Statistical Test of Independence. In Advances in Neural
Information Processing Systems (NIPS), pp. 585–592.
2008.
Habibian, Amirhossein, Mensink, Thomas, and Snoek,
Cees GM. Videostory: A new multimedia embedding
for few-example recognition and translation of events. In

ACM International Conference on Multimedia, pp. 17–26,
2014.
Heller, Ruth, Heller, Yair, Kaufman, Shachar, Brill, Barak,
and Gorfine, Malka. Consistent distribution-free ksample and independence tests for univariate random
variables. Journal of Machine Learning Research, 17
(29):1–54, 2016.
Huo, Xiaoming and Székely, Gábor J. Fast computing
for distance covariance. Technometrics, 58(4):435–447,
2016.
Jitkrittum, Wittawat, Szabó, Zoltán, Chwialkowski, Kacper,
and Gretton, Arthur. Interpretable Distribution Features
with Maximum Testing Power. In Advances in Neural
Information Processing Systems (NIPS), pp. 181–189.
2016.
Kowalski, Jeanne and Tu, Xin M. Modern Applied UStatistics. John Wiley & Sons, 2008.
Lehmann, Eric L. Elements of Large-Sample Theory.
Springer Science & Business Media, 1999.
Lopez-Paz, David, Hennig, Philipp, and Schölkopf, Bernhard. The Randomized Dependence Coefficient. In Advances in Neural Information Processing Systems (NIPS),
pp. 1–9. 2013.
Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), pp. 1177–1184.
2008.
Sejdinovic, Dino, Sriperumbudur, Bharath, Gretton, Arthur,
and Fukumizu, Kenji. Equivalence of distance-based and
RKHS-based statistics in hypothesis testing. The Annals
of Statistics, 41(5):2263–2291, 2013.
Serfling, Robert J. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.
Smola, Alex, Gretton, Arthur, Song, Le, and Schölkopf,
Bernhard. A Hilbert space embedding for distributions.
In International Conference on Algorithmic Learning
Theory (ALT), pp. 13–31, 2007.
Sriperumbudur, Bharath K., Gretton, Arthur, Fukumizu,
Kenji, Schölkopf, Bernhard, and Lanckriet, Gert R. G.
Hilbert Space Embeddings and Metrics on Probability
Measures. Journal of Machine Learning Research, 11:
1517–1561, 2010.
Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer Science & Business Media, 2008.

An Adaptive Test of Independence with Analytic Kernel Embeddings

Székely, Gábor J. and Rizzo, Maria L. Brownian distance
covariance. The Annals of Applied Statistics, 3(4):1236–
1265, 2009.
Székely, Gábor J., Rizzo, Maria L., and Bakirov, Nail K.
Measuring and testing dependence by correlation of distances. The Annals of Statistics, 35(6):2769–2794, 2007.
van der Vaart, Aad. Asymptotic Statistics. Cambridge University Press, 2000.
Wang, Heng and Schmid, Cordelia. Action recognition with
improved trajectories. In IEEE International Conference
on Computer Vision (ICCV), pp. 3551–3558, 2013.
Zhang, Kun, Peters, Jonas, Janzing, Dominik, and
Schölkopf, Bernhard. Kernel-based conditional independence test and application in causal discovery. In Conference on Uncertainty in Artificial Intelligence (UAI), pp.
804–813, 2011.
Zhang, Qinyi, Filippi, Sarah, Gretton, Arthur, and Sejdinovic, Dino. Large-Scale Kernel Methods for Independence Testing. Statistics and Computing, pp. 1–18, 2017.

