Hierarchy Through Composition with Multitask LMDPs
Andrew M. Saxe 1 Adam C. Earle 2 Benjamin Rosman 2 3

Abstract
Hierarchical architectures are critical to the
scalability of reinforcement learning methods.
Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel
alternative to these control hierarchies based on
concurrent execution of many actions in parallel.
Our scheme exploits the guaranteed concurrent
compositionality provided by the linearly solvable
Markov decision process (LMDP) framework,
which naturally enables a learning agent to draw
on several macro-actions simultaneously to solve
new tasks. We introduce the Multitask LMDP
module, which maintains a parallel distributed
representation of tasks and may be stacked to form
deep hierarchies abstracted in space and time.

1. Introduction
Real world tasks unfold at a range of spatial and temporal
scales, such that learning solely at the finest scale is likely to
be slow. Hierarchical reinforcement learning (HRL) (Barto
& Madadevan, 2003; Parr & Russell, 1998; Dietterich, 2000)
attempts to remedy this by learning a nested sequence of
ever more detailed plans. Hierarchical schemes have a number of desirable properties. Firstly, they are intuitive, as
humans seldom plan at the level of raw actions, typically preferring to reason at a higher level of abstraction (Botvinick
et al., 2009; Ribas-Fernandes et al., 2011; Solway et al.,
2014). Secondly, they constitute one approach to tackling
the curse of dimensionality (Bellman, 1957; Howard, 1960).
In real world MDPs, the number of states typically grows
dramatically in the size of a domain. A similar curse of dimensionality afflicts actions, such that a robot with multiple
joints, for instance, must operate in the product space of
1

Center for Brain Science, Harvard University 2 School of
Computer Science and Applied Mathematics, University of
the Witwatersrand 3 Council for Scientific and Industrial Research, South Africa. Correspondence to: Andrew M. Saxe
<asaxe@fas.harvard.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

actions for each joint individually (Mausam & Weld, 2008).
Finally, the ‘tasks’ performed by an agent may come in
great variety, and can also suffer from a curse of dimensionality: a robot that learns policies to navigate to each of ten
rooms would need to learn 10 choose 2 policies to navigate
to the closer of each pair of rooms. Transferring learning
across tasks is therefore vital (Taylor & Stone, 2009; Foster
& Dayan, 2002; Drummond, 1998; Fernández & Veloso,
2006; Bonarini et al., 2006; Barreto et al., 2016).
Many HRL schemes rely on a serial call/return procedure,
in which temporally extended macro-actions or ‘options’
can call other (macro-)actions. In this sense, these schemes
draw on a computer metaphor, in which a serial processor
chooses sequential primitive actions, occasionally pushing
or popping new macro-actions onto a stack. The influential
options framework (Sutton et al., 1999), MAXQ method
(Dietterich, 2000; Jonsson & Gómez, 2016) and the hierarchy of abstract machines (Parr & Russell, 1998; Burridge
et al., 1999) all share this sequential-execution structure.
Concurrent MDPs (Mausam & Weld, 2008) relax the assumption of serial execution to allow multiple actions to be
executed simultaneously at each time step, at the cost of a
combinatorial increase in the size of the action space. In
this paper, we develop a novel hierarchical scheme with a
parallel and distributed execution structure that overcomes
the combinatorial increase in action space by exploiting the
guaranteed optimal compositionality afforded by the linearly solvable Markov decision process (LMDP) framework
(Todorov, 2006; Kappen, 2005).
We present a Multitask LMDP module that executes concurrent blends of previously learned tasks. Here we use
the term ‘concurrent’ or ‘parallel’ to refer to ‘parallel distributed processing’ or neural network-like methods that
form weighted blends of many simple elements to achieve
their aim. In standard schemes, if a robot arm has acquired
policies to individually reach two points in some space, this
knowledge typically does not aid it in optimally reaching to
either point. However in our scheme, such behaviours can
be expressed as different weighted task blends, such that
an agent can simultaneously draw on several sub-policies
to achieve goals not explicitly represented by any of the
individual behaviours, including tasks the agent has never
performed before.

Hierarchy Through Composition with Multitask LMDPs

Next we show how this Multitask LMDP module can be
stacked, resulting in a hierarchy of more abstract modules
that communicate distributed representations of tasks between layers. We give a simple theoretical analysis showing
that hierarchy can yield qualitative efficiency improvements
in learning time and memory. Finally, we demonstrate the
operation of the method on a navigation domain, and show
that its multitasking ability can speed learning of new tasks
compared to a traditional options-based agent.
Our scheme builds on a variety of prior work. Like the options framework (Sutton et al., 1999), we build a hierarchy
in time. Similar to MAXQ Value Decomposition (Dietterich, 2000), we decompose a target MDP into a hierarchy
of smaller SMDPs which progressively abstract over states.
From Feudal RL, we draw the idea of a managerial hierarchy in which higher layers prescribe goals but not details for
lower layers (Dayan & Hinton, 1993). Most closely related
to our scheme, (Jonsson & Gómez, 2016) develop a MAXQ
decomposition within the LMDP formalism (see Supplementary Material for extended discussion). Our method
differs from all of the above approaches in permitting a
graded, concurrent blend of tasks at each level, and developing a uniform, stackable module capable of performing a
variety of tasks.

2. The Multitask LMDP: A compositional
action module
Our goal in this paper is to describe a flexible actionselection module which can be stacked to form a hierarchy, such that the full action at any given point in time is
composed of the concurrent composition of sub-actions
within sub-actions. By analogy to perceptual deep networks, restricted Boltzmann machines (RBMs) form a component module from which a deep belief network can be
constructed by layerwise stacking (Hinton et al., 2006; Hinton & Salakhutdinov, 2006). We seek a similar module in
the context of action or control. This section describes the
module, the Multitask LMDP (MLMDP), before turning to
how it can be stacked. Our formulation relies on the linearly
solvable Markov decision process (LMDP) framework introduced by Todorov (2006) (see also Kappen (2005)). The
LMDP differs from the standard MDP formulation in fundamental ways, and enjoys a number of special properties.
We first briefly describe the canonical MDP formulation, in
order to explain what the switch to the LMDP accomplishes
and why it is necessary.
2.1. Canonical MDPs
In its standard formulation, an MDP is a four-tuple M =
hS, A, P, Ri, where S is a set of states, A is a set of discrete
actions, P is a transition probability distribution P : S ×
A × S → [0, 1], and R is an expected instantaneous reward

function R : S × A → R. The goal is to determine an
optimal policy π : S → A specifying which action to
take in each state. This optimal policy can be computed
from the optimal value function V : S → R, defined as
the expected reward starting in a given state and acting
optimally thereafter. The value function obeys the wellknown Bellman optimality condition
(
)
X
V (s) = max R(s, a) +
P (s0 |s, a)V (s0 ) . (1)
a∈A

s0

This formalism is the basis of most practical and theoretical
studies of decision-making under uncertainty and reinforcement learning (Bellman, 1957; Howard, 1960; Sutton &
Barto, 1998). See, for instance, (Mnih et al., 2015; Lillicrap
et al., 2015; Levine et al., 2016) for recent successes in
challenging domains.
For the purposes of a compositional hierarchy of actions,
this formulation presents two key difficulties.

1. Mutually exclusive sequential actions First, the
agent’s actions are discrete and execute serially. Exactly one (macro-)action operates at any given time
point. Hence there is no way to build up an action at a
single time point out of several ‘subactions’ taken in
parallel. For example, a control signal for a robotic arm
cannot be composed of a control decision for the elbow
joint, a control decision for the shoulder joint, and a
control decision for the gripper, each taken in parallel
and combined into a complete action for a specific time
point.
2. Non-composable optimal policies The maximization
in Eqn. (1) over a discrete set of actions is nonlinear. This means that optimal solutions, in general, do
not compose in a simple way. Consider two standard
MDPs M1 = hS, A, P, R1 i and M2 = hS, A, P, R2 i
which have identical state spaces, action sets, and transition dynamics but differ in their instantaneous rewards R1 and R2 . These may be solved independently
to yield value functions V1 and V2 . But the value function of the MDP M1+2 = hS, A, P, R1 + R2 i, whose
instantaneous rewards are the sum of the first two, is
not V1+2 = V1 + V2 . In general, there is no simple
procedure for deriving V1+2 from V1 and V2 ; it must
be found by solving Eqn. (1) again.
2.2. Linearly Solvable MDPs
The LMDP (Todorov, 2009a; Dvijotham & Todorov, 2010;
Todorov, 2009b; Dvijotham & Todorov, 2010) is defined
by a three-tuple L = hS, P, Ri, where S is a set of
states, P is a passive transition probability distribution
P : S × S → [0, 1], and R is an expected instantaneous reward function R : S → R. The LMDP framework replaces

Hierarchy Through Composition with Multitask LMDPs

the traditional discrete set of actions A with a continuous
probability distribution over next states a : S × S → [0, 1].
That is, the ‘control’ or ‘action’ chosen by the agent in state
s is a transition probability distribution over next states,
a(·|s). The controlled transition distribution may be interpreted either as directly constituting the agent’s dynamics,
or as a stochastic policy over deterministic actions which
affect state transitions (Todorov, 2006; Jonsson & Gómez,
2016). Swapping a discrete action space for a continuous action space is a key change which will allow for concurrently
selected ‘subactions’ and distributed representations.
The LMDP framework additionally posits a specific form
for the cost function to be optimized. The instantaneous
reward for taking action a(·|s) in state s is
R(s, a) = R(s) − λKL (a(·|s)||P (·|s)) ,

(2)

where the KL term is the Kullback-Leibler divergence between the selected control transition probability and the
passive dynamics. This term implements a control cost, encouraging actions to conform to the natural passive dynamics of a domain. In a cart-pole balancing task, for instance,
the passive dynamics might encode the transition structure
arising from physics in the absence of control input. Any
deviation from these dynamics will require energy input.
In more abstract settings, such as navigation in a 2D grid
world, the passive dynamics might encode a random walk,
expressing the fact that actions cannot transition directly
to a far away goal but only move some limited distance
in a specific direction. Examples of standard benchmark
domains in the LMDP formalism are provided in the Supplementary Material. The parameter λ in Eqn. (2) acts to
trade-off the relative value between the reward of being in a
state and the control cost, and determines the stochasticity
of the resulting policies.
We consider first-exit problems (see Dvijotham & Todorov
(2011) for infinite horizon and other formulations), in which
the state space is divided into a set of absorbing boundary
states B ⊂ S and non-absorbing interior states I ⊂ S, with
S = B ∪ I. In this formulation, an agent acts in a variable
length episode that consists of a series of transitions through
interior states before a final transition to a boundary state
which terminates the episode. The goal is to find the policy
a∗ which maximizes the total expected reward across the
episode,
(τ −1
)
X
a∗ = argmaxa E st+1 ∼a(·|st )
R(st , a) + R(sτ ) .
τ =min{t:st ∈B}

t=1

(3)

Because of the carefully chosen structure of the reward
R(s, a) and the continuous action space, the Bellman equation simplifies greatly. In particular define the desirability
function z(s) = eV (s)/λ as the exponentiated cost-to-go

function, and define q(s) = eR(s)/λ to be the exponentiated
instantaneous rewards. Let N be the number of states, and
Ni and Nb be the number of internal and boundary states
respectively. Represent z(s) and q(s) with N -dimensional
column vectors z and q, and the transition dynamics P (s0 |s)
with the N -by-Ni matrix P , where column index corresponds to s and row index corresponds to s0 . Let zi and zb
denote the partition of z into internal and boundary states,
respectively, and similarly for qi and qb . Finally, let Pi denote the Ni -by-Ni submatrix of P containing transitions
between internal states, and Pb denote the Nb -by-Ni submatrix of P containing transitions from internal states to
boundary states.
As shown in Todorov (2009b), the Bellman equation in this
setting reduces to
(I − Mi PiT )zi = Mi PbT zb

(4)

where Mi = diag(qi ) and, because boundary states are
absorbing, zb = qb . The exponentiated Bellman equation
is hence a linear system, the key advantage of the LMDP
framework. A variety of special properties flow from the
linearity of the Bellman equation, which we exploit in the
following.
Solving for zi may be done explicitly as zi = (I −
Mi PiT )−1 Mi PbT zb or via the z-iteration method (akin to
value iteration),
zi ← Mi PiT zi + Mi PbT zb .

(5)

Finally, the optimal policy may be computed in closed form
as
P (s0 |s)z(s0 )
a∗ (s0 |s) =
,
(6)
G[z](s)

where
the normalizing constant G[z](s)
=
P
0
0
Detailed derivations of these res0 P (s |s)z(s ).
sults are given in (Todorov, 2009a;b; Dvijotham & Todorov,
2011). Intuitively, the hard maximization ofPEqn. (1)
has been replaced by a soft maximization log( exp(·)),
and the continuous action space enables closed form
computation of the optimal policy.
Compared to the standard MDP formulation, the LMDP has
1. Continuous concurrent actions Actions are expressed as transition probabilities over next states, such
that these transition probabilities can be influenced by
many subtasks operating in parallel, and in a graded
fashion.
2. Compositional optimal policies In the LMDP, linearly blending desirability functions yields the correct
composite desirability function (Todorov, 2009a;b). In
particular, consider two LMDPs L1 = hS, P, qi , qb1 i

More generally,
we suggest
that most
domains
interest
have some
notion
of ‘efficient’
actio
112
More
generally,
we suggest
thatofmost
domains
of interest
have
some notion
o
making a control
cost a reasonably
natural
and universal
phenomenon.
it is possible
113
making
a control cost
a reasonably
natural
and universalIndeed,
phenomenon.
Indeet
the standard114MDP
is overly
general,
usefuldiscarding
structure useful
in moststructure
real wo
theformulation
standard MDP
formulation
is discarding
overly general,
domains–namely,
a preference for aefficient
actions.
Standardactions.
MDP formulations
commonly
pl
115
domains–namely,
preference
for efficient
Standard MDP
formulatio
small negative
on each action
instantiate
thistoefficiency
goal,
they retain
thebut
flexibil
116 rewards
small negative
rewardstoon
each action
instantiate
thisbut
efficiency
goal,
they
to, for instance,
prefer
energetically
inefficient
trajectories
by placing
positive
rewardspositi
on ea
117
to,
for instance,
prefer
energetically
inefficient
trajectories
by placing
action. The 118
drawback
ofThe
this drawback
flexibility of
is the
of maximization
Eqn. (1), which
action.
thisunstructured
flexibility is maximization
the unstructured
ofpreve
Eqn.
compositionality.
119
compositionality.

a (s |s) =
the optimal policy.
the optimal103
policy.

The specific108form
of specific
Eqn. (2)form
can seem
to limit
theseem
applicability
of the
LMDP framework.
Yet
The
of Eqn.
(2) can
to limit the
applicability
of the LMDP
shown in a variety
of recent
work [***],
and in
the [***],
examples
most given
standard
domains
109
shown
in a variety
of recent
work
and given
in thelater,
examples
later,
most stc
be translated110to the
framework;
and there
exists aand
general
for embedding
be MDP
translated
to the MDP
framework;
thereprocedure
exists a general
proceduretraditio
for e
MDPs in the111LMDP
framework
?. framework ?.
MDPs
in the LMDP

b

⇡

Our hierarchical
scheme
is built on
two key
properties
ofkey
the properties
LMDP. of the LMDP.
105
Our
hierarchical
scheme
is built
on two
105

Desirabliity
Instantaneous	
Concurrent
and distributed
representations
of tasks
104
2.3subactions
Concurrent
subactions
and
distributed
representations
of tasks
trajectories
function	z
reward	q

2.3
104

0
P (s0 |s)Z(s
) P (s0 |s)Z(s0 )
a⇤ (s0 |s) ,=
,
G[Z](s)
G[Z](s)
P
P
0
0
where the normalizing
G[Z](s)constant
= s0 PG[Z](s)
(s0 |s)Z(s
derivations
of these
resu
100
where constant
the normalizing
= 0 ). sDetailed
). Detailed
derivati
0 P (s |s)Z(s
are given in101[***].
Intuitively,
thePhard
maximization
Eqn. (1) has of
been
replaced
as
arePgiven
in [***].
Intuitively,
the hardofmaximization
Eqn.
(1) hasbybeen
maximization
exp(·)), and
space enables
closed
computation
102 log(
maximization
log(the continuous
exp(·)), andaction
the continuous
action
spaceform
enables
closed f
the optimal 103
policy.
the optimal policy.

Finally, the optimal
policy
be computed
in closed
form asin closed form as
99
Finally,
themay
optimal
policy may
be computed

1
Solving for z97i may
be done
zi =explicitly
(I Qi P
Qi(I
Pb z b Q
ori Pvia
Solving
for explicitly
zi may beas
done
asi )zi 1=
Qiz-iteration
Pb zb or viameth
the
i ) the
(akin to value98iteration),
(akin to value iteration),
zi
Q i Pi z i + Q
zii Pb zQ
b . i Pi z i + Q i Pb z b .

where Qi =94diag(q
because
states boundary
are absorbing,
qb . The exponentiat
where
Qi =
diag(qiboundary
) and, because
stateszare
zb = q b .
i ) and,
b =absorbing,
Bellman equation
is henceequation
a linear system,
key advantage
ofkey
the advantage
LMDP framework.
A variety
95
Bellman
is hencethe
a linear
system, the
of the LMDP
fram
special properties
flow from
the linearity
of the
which we
exploitwhich
in the we
followi
96
special
properties
flow from
theBellman
linearityequation,
of the Bellman
equation,
expl

P (s
|s)Z(s
0
⇤ 0 maximization
100 Intuitively,
where
the
normalizing
constant
=)Eqn.
(s |s)Z(s
). P
Detailed
of these results
where
the normalizing
constant
=
Poptimal
(s0 |s)Z(s
).sDetailed
derivations
of these
0 P
en
in
[***].
the
hard
of
(1)
has
been
replaced
byresults
aexponentiated
soft
0 G[Z](s)
P
0 derivations
0as
sthe
99 94
Finally,
may
computed
in0 closed
aG[Z](s)
(s
|s)
=are
,are
(6)
0where
0 boundary
94
where
QP
diag(qi0 )P94
and,
because
absorbing,
zib) =
qpolicy
exponentiated
100
the
constant
G[Z](s)
Pdiag(q
(s
).
Detailed
derivations
ofexponentiated
these
Qas
diag(q
)where
and,
where
because
Q
boundary
diag(q
and,
states
94The
where
absorbing,
boundary
Qi be
=
zdiag(q
94= where
qib)are
.=
and,
The
absorbing,
Qbecause
exponentiated
zboundary
) form
and,
qb . The
states
because
are
boundary
absorbing,
states
zb =areqbabsorbing,
. The
zb =resul
qb . T
i ==
b .because
ng
constant
G[Z](s)
(s
|s)Z(s
derivations
of
these
results
i ).=Detailed
istates
i = normalizing
bstates
i s=
b |s)Z(s
i=
olicy
may
be
computed
in
closed
form
0
0
0
0
s
101
are
given
in
[***].
Intuitively,
the
hard
maximization
of
Eqn.
(1)
has
been
replaced
by
a framework.
soft
are
given
in
[***].
Intuitively,
the
hard
maximization
of
Eqn.
(1)
has
been
replaced
by
a
soft
G[Z](s)
P
(s
|s)Z(s
)
95
Bellman
equation
is
hence
a
linear
system,
the
key
advantage
of
the
LMDP
framework.
A
variety
of
P
(s
|s)Z(s
)
equation
is95⇤hence
Bellman
agiven
linear
equation
system,
isthe
hence
key
95
linear
Bellman
system,
of the
equation
the
LMDP
key
95 advantage
isframework.
Bellman
hencemaximization
a linear
of
equation
A
thevariety
system,
LMDP
is hence
offramework.
the of
key
a linear
advantage
A
system,
variety
ofthe
the
ofkey
LMDP
advantage
of the A
LMDP
variety
framew
ofso
ization
andofthe
action
space
enables
closed
form
computation
of
P
P 95 Bellman
⇤
0 aadvantage
0 been
101
are
in
[***].
Intuitively,
the
hard
Eqn.
(1)
has
been
replaced
by
a
Intuitively,log(
the hardexp(·)),
maximization
Eqn.continuous
(1)
has
replaced
by
a
soft
0
0
P
a
(s
|s)
=
,
(6)
a
(s
|s)
=
,
(6)
96
special102
properties
flow 96
from
the linearity
ofthe
the
Bellman
equation,
which
we
exploit
inenables
the following.
0 special
0 and
properties
flow
96
from
special
the
properties
linearity
of
flow
thecontinuous
from
Bellman
the
96 linearity
equation,
special
properties
ofwhich
the Bellman
96
we
flow
exploit
special
from
in
properties
the
thelinearity
which
following.
we
offrom
exploit
the Bellman
the
the
following.
of thewhich
Bellman
we exploit
equation,
in which
the following.
we explo
P
(sflow
|s)Z(s
)inlinearity
maximization
exp(·)),
andcomputation
the
action
space
enables
closed
form
computation
of computation
maximization
log(
continuous
action
space
closed
form
computation
ofequation,
P exp(·)),
(s
|s)Z(s
)log(
⇤continuous
0equation,
exp(·)),
and the⇤ continuous
action
space
enables
closed
form
of
P
0
102
maximization
log(
exp(·)),
and
the
action
space
enables
closed
form
o
G[Z](s)
imal
policy.
G[Z](s)
a (s
|s)
, Q P )method
(6
1
(s z|s)
= optimal
, as
1
1=
1
1
0(6)
97
Solvinga
for
may
be 97done
explicitly
zi ==
(I done
Qi explicitly
Pi )P
Qzi P0may
z-iteration
method
Solving
for
zi may
97
be
Solving
for
as
be
=via
done
(I the
97
explicitly
Q
Solving
Qasi P
for
zbiz=
(I
may
97via Solving
be
Q
the
done
z-iteration
explicitly
Qi P
zib zof
may
method
oras
be
via
zidone
the
= (I
z-iteration
explicitly
zi i P
=b z(Ib or Q
viai Pthe
Qi Pb zb ormethod
via the z
i the
b zbzior
103
policy.
iP
i)
bzior
iP
i ) for
bG[Z](s)
i i as Q
i ) z-iteration
thenormalizing
optimal
policy.
the
constant
G[Z](s)
(s
|s)Z(s
).
Detailed
derivations
these
results
G[Z](s)
0
103
the
optimal
policy.
98
(akin to value iteration),
(akin to value iteration),
98
(akinsP
to value iteration),
98P
towith
value
iteration),
98
(akin
value iteration),
0 Multitask
0 toLMDPs
0
0 (akinP
Hierarchy
Composition
P 98 the
100 Intuitively,
normalizing
constant
G[Z](s)
=Eqn.
(sQ
|s)Z(s
Detailed
derivations
of these
results
where
the
normalizing
constant
=Qi Pb zsb .0 Through
P
|s)Z(s
Detailed
of these
results
0
0 ziG[Z](s)
Qi Pi zmaximization
in [***].
the
hard
of
has
been
soft
zi(s
Q
Q).
.(1)
Pi zderivations
Qi).
Pb zP
(5)
Qby
(5)
(5)
i+
iP
i zi +
i Ps
b0zbz
i
i(5)
i+
b replaced
i Pi z0ia
i Pzbizb . Q
i Pi z i + Q i Pb z b .
gen
constant
G[Z](s)
=where
P
(s
|s)Z(s
).
Detailed
derivations
of
these
results
0
P
100tasks
where the normalizing constant G[Z](s) = . s0 P (szi0 |s)Z(s
).+ Q
Detailed
derivations of these resul
s representations of
ubactions
andin
distributed
Concurrent
subactions
and
distributed
representations
of
tasks
101
are
given
in
[***].
Intuitively,
the
hard
maximization
of
Eqn.
(1)
has
been
replaced
by
a
soft
are
given
[***].
Intuitively,
the
hard
maximization
of
Eqn.
(1)
has
been
replaced
by
a
soft
Intuitively,
the
hard
maximization
of
Eqn.
(1)
has
been
replaced
by
a
soft
2.3P
Concurrent
subactions
and
distributed
representations
ofberepresentations
tasks
2.3 99 Concurrent
subactions
and
distributed
representations
tasks
(c)
ization
log(104
exp(·)),
and
the
continuous
action
space
enables
closed
form
computation
of(1)tasks
P
(a)
(d)
(f)
(g)
104
2.3
Concurrent
subactions
and
distributed
of
Finally,
the optimal
policy
may
be computed
in99policy
closed
form
101
are
given
in [***].
Intuitively,
the
hard
maximization
Eqn.
beenform
replaced
by a so
99
Finally,
the
optimal
Finally,
may
the
beas
computed
optimal
policy
in
closed
may
99 form
be
Finally,
computed
asofthe
optimal
in
closed
99(e)
Finally,
policy
form
may
as
the optimal
computed
policyinof
may
closed
be computed
form
as has
in closed
as
P
exp(·)),
and the
continuous
action space
enables
closed form
computation
of enables
102
maximization
log(
exp(·)),
and
the
continuous
action
space
enables
closed
form
computation
of0 computation o
maximization
log(
exp(·)),
and
the
continuous
action
space
closed
form
computation
of
0
0
⇡
\
⇡
(7)
0
0
0
0
0
0
102 Pmaximization
log(P (s exp(·)),
action space
enables
form
2
imal policy.
(s |s)Z(s )
|s)Z(s ) and the
P (scontinuous
|s)Z(s )
P (s |s)Z(s
) closed
P (s0 |s)Z(s
)
⇤ 0
1. Continuous
concurrent
actions
106
1. Continuous
concurrent actions

2. Compositional
policiesoptimal policies
107
2. optimal
Compositional
107

⇡

⇡

optimal
optimal policy.
the optimal policy.
106 policy.
1. the
Continuous
concurrent
actions
1.the Continuous
concurrent
actions
103

103

3

a⇤,(s0 |s) =(6)

, a⇤ (s0 |s) =

106

103

\2

+w2 ⇥
(7)
(7
= wpolicy.
(7)(7)=
⇡
1⇥
103
the
optimal
G[Z](s)
G[Z](s)
G[Z](s)+ · · · + wn ⇥
G[Z](s)
G[Z](s)
eme is built on two key properties of the LMDP.
P
P
P
P
P
0
0
0
0
0 key properties
0
0
0
0
100
where
the
constant
G[Z](s)
=105
(sis
|s)Z(s
). Detailed
derivations
of these
Our
hierarchical
scheme
is
built
onsof
two
the
LMDP.
bactions
and distributed
of
tasks
100
where
the normalizing
100
constant
where
theG[Z](s)
normalizing
constant
Pthe
(s
100|s)Z(s
G[Z](s)
where
). the
Detailed
=results
normalizing
Pderivations
(s
where
|s)Z(s
constant
the
of
). normalizing
Detailed
these
G[Z](s)
results
derivations
=of
constant
of
G[Z](s)
|s)Z(s
these results
).
= Detailed
|s)Z(s0 ). Detailed
of these derivatio
results
105normalizing
Our
hierarchical
scheme
built
on =two
properties
the
LMDP.
0key
0100
Our
hierarchical
scheme
is built
on
two
key
properties
of
LMDP.
s0 P
sthe
s0 P (s
s0 P (sderivations
erarchical
scheme
isrepresentations
built
on
key
properties
of
LMDP.
concurrent
actions
Concurrent
subactions
distributed
representations
101
are given
in [***].
Intuitively,
the two
hard
maximization
of
Eqn.
(1)distributed
has
been
ainsoft
101Concurrent
areand
given
in [***].
101 Intuitively,
are
given
the
in
[***].
hard
maximization
Intuitively,
101replaced
are
the
ofof
given
Eqn.
hard
maximization
(1)
[***].
has
101 been
are
Intuitively,
given
replaced
of Eqn.
in representations
the
[***].
by
(1)
ahard
has
soft
Intuitively,
been
maximization
replaced
the of
hard
by
of tasks
Eqn.
a maximization
soft(1) has been
of replaced
Eqn. (1) by
hasabeen
soft r
104
2.3P
subactions
and
representations
of
tasks
2.3
Concurrent
subactions
and
distributed
representations
ofbytasks
tasks
P 2.3
P
P
P
104
Concurrent
subactions
and
distributed
102
maximization log( exp(·)),
action
space
enables
closed
form
computation
log(102 exp(·)),
maximization
and the
log(
continuous
exp(·)),
action
102
and
maximization
space
the continuous
enablesof
log(
102
closed
action
maximization
exp(·)),
form
space
computation
and
enables
log(
the continuous
closed
of
exp(·)),
formaction
and
computation
thespace
continuous
enables
of action
closedspace
formenables
computation
closedoffor
\1 maximization
⇡ and the continuous
nal optimal policies ⇡ 102
106
1. Continuous(7)
concurrent actions
,

0
a⇤ (s(6)
|s) =

a⇤ (s0 |s)
, (6)
=

,

(6)

the optimal policy.
103
the optimal policy.

103

3

i

\2
⇡

a⇤ (s0 |s) =

me
is built on two keyconcurrent
properties of theactions
LMDP.
. Continuous
+w2 ⇥ policies + · · · + wn ⇥
(7)
=Compositional
w1 ⇥
(7)(7)=
(7
⇡
107
2.
optimal
Eqn.104(2)2.
canCompositional
seem
the
of104
the
LMDP
framework.
Yet
as 2.3
2.3
Concurrent
subactions
and
distributed
representations
of
tasks
107 to limit
2. applicability
Compositional
optimal
policies
104
2.3
Concurrent
subactions
2.3
Concurrent
and
distributed
subactions
representations
104 and
distributed
Concurrent
of tasks
104
representations
subactions
2.3 Concurrent
and
of tasks
distributed
subactions
representations
and distributed
of tasks
representations of tasks
optimal
policies
work actions
[***],
inoptimal
the
examples
given
later,Our
most
standard
can
105
scheme
is built onoftwo
properties of the LMDP.
105
Our
hierarchical
scheme
is hierarchical
built
on domains
two
key
properties
thekey
LMDP.
Our
hierarchical
scheme
is built
on
two
key
properties
of
the
LMDP.
concurrent
(b) and
.recent
Compositional
erarchical
scheme
built
onpolicies
two
key
properties
of
the
LMDP.
⇡
\
⇡
⇡
\
⇡ (2) can⇡(7)
\1 ⇡to limit the applicability
(7) ⇡ \1 ⇡ of the
(7)
⇡ LMDP
\1 ⇡ framework. Yet
(7) a
MDP
framework;
and thereisexists
a general
procedure
for
embedding
traditional
1
1
108
The
specific
form
of
Eqn.
seem
nal
optimal
108form
The
ofproperties
Eqn.
(2)
to limit the
of the LMDP
Yet as
The
ofspecific
Eqn.
(2)
cankeyseem
to of
limit
theseem
applicability
of applicability
the LMDP framework.
Yetframework.
as
framework
?.policies
105specific
Our
hierarchical
scheme
is built
onform
two
thecan
LMDP.

hierarchical109
scheme
Our
is built
hierarchical
onContinuous
key
scheme
properties
is built
ofon
Our
the
two
hierarchical
LMDP.
key properties
scheme
Our
of
hierarchical
the
is built
LMDP.
on two
scheme
key properties
is built
onof
two
the
key
LMDP.
properties
of the LMDP.
shown
atwovariety
of
recent
work
[***],
and
in
the
examples
given
later,
most standard
domains ca
106
1.in
concurrent
actions
106
1. Our
Continuous
concurrent
actions
1. Continuous
concurrent
actions
105

105

105

105

The
120

The
120

119

118

117

116

115

114

113

112

111

110

109

108

Sample

103

102

101

100

99

98

97

96

95

94

109
shown
in
a variety
ofto
recent
work
[***],
and
in
the
examples
given
later,domains
standard
can
in aseem
variety
of of
recent
work
[***],
and
inthe
theapplicability
examples
given
later,
most
standard
can
Start
ecific
form
ofdomains
(2)
can
seem
limit
of
the 106LMDP
framework.
Yet
as domains
suggest
interest
have
some
notion
of
‘efficient’
actions,
.shown
Continuous
concurrent
actions
110the
be
translated
to the
MDP
framework;
andconcurrent
there
exists
amost
general
procedure
for embedding
tradition
Eqn.
(2)that
canmost
toEqn.
limit the
applicability
of
LMDP
Yet
as optimal
106
1. Continuous
concurrent
106
1.actions
Continuous
106 concurrent
1.2.framework.
Continuous
actions
concurrent
106
actions
1. Continuous
1. Continuous
actions
concurrent
actions
End
107
Compositional
policies
110
be
translated
to
the
MDP
framework;
and
there
exists
a
general
procedure
for
embedding
traditional
be
translated
toand
the
MDP
framework;
and
there
exists
a general
procedure
for embedding
traditional
st
aa
reasonably
natural
and
universal
phenomenon.
Indeed,
itexamples
ispolicies
possible
that
107
2.
Compositional
optimal
2. Compositional
optimal
policies
recent
work
[***],
in
the107
examples
given
later,
most
standard
domains
can
111
MDPs
the
LMDP
framework
?. 107
in
variety
recent
work
and
in
the
given
later,
most
standard
domains
can
107
2. of
Compositional
optimal2.[***],
policies
Compositional
107
optimal
2. in
Compositional
policies
optimal
107
policies
2.
Compositional
optimal
2. Compositional
policies
optimal policies
ormulation
overly
general,
discarding
usefulframework
structure
in most
real world
.DP
Compositional
optimal
policies
111
MDPs
in
LMDP
?. traditional
framework;
and
there
exists
a the
general
procedure
for embedding
MDPs
inisthe
the
LMDP
framework
?.
slated
to
MDP
framework;
and
there
exists
a
general
procedure
for
traditional
108
The
specific
form
of
Eqn.
(2)
can
seem
toembedding
limit
the
applicability
ofwith
LMDP
Yet
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
112
More
generally,
we
suggest
that
most
domains
of
interest
have
some
notion
offramework.
‘efficient’
action
ramework
?.
108
The
specific
form
of Eqn.
(2)
can
seem
to
limit
the
applicability
of
the
LMDP
framework.
Yet
as
Figure
1.
Distributed
task
representations
with
the
Multitask
LMDP.
(a)
Example
2DOF
arm
constrained
to(2)
the
plane
state
space
108
The(2)
specific
form
108
ofEqn.
Eqn.
The
(2)
specific
cancan
seem
form
toof
limit
Eqn.
the
(2)
108
applicability
can
The
seem
specific
to
of
limit
the
form
108
the
LMDP
ofThe
applicability
Eqn.
framework.
specific
(2)
can
form
of
seem
the
Yet
of
LMDP
to
as
Eqn.
limit
framework.
the
can
applicability
seem
Yet
tothe
limit
asof
theYet
applicability
LMDP
framework.
of the LMDP
Yet
as fa
108form
The
specific
form
of
(2)
seem
to
limit
the
applicability
of
the
LMDP
framework.
as
The
specific
of
Eqn.
can
seem
to
limit
the
applicability
of
the
LMDP
framework.
Yet
as
ds
on
each
action
to
instantiate
this
efficiency
goal,
but
they
retain
the
flexibility
109
shown
in
a
variety
of
recent
work
[***],
and
in
the
examples
given
later,
most
standard
domains
can
112
More
generally,
we
suggest
that
most
domains
of
interest
have
some
notion
of
‘efficient’
actions,
More
generally,
we
suggest
that
most
domains
of
interest
have
some
notion
of
‘efficient’
actions,
109
shown
in a variety
of
recent
shown
work
in
a6 [***],
variety
of
in
recent
the
examples
109
work
shown
[***],
given
inand
alater,
variety
in[***],
the
109
most
examples
ofshown
recent
standard
in
work
given
adomains
variety
[***],
later,
ofcan
most
and
recent
in
standard
the
work
examples
[***],
domains
and
given
can
inmost
later,
the
examples
most
standard
given
later,
domains
most
can
stan
109109
shown
in
a
variety
of
recent
work
and
in
the
examples
given
later,
standard
domains
ca
6 2and
in
the
LMDP
framework
?.
113
making
a
control
cost
a
reasonably
natural
and
universal
phenomenon.
Indeed,
it
is
possible
th
consisting
of
shoulder
and
elbow
joint
angles
respectively.
(b)
A
novel
task
is
specified
as
an
instantaneous
reward
,
∈
[−π,
π]
1
uggest
most
domains
of
interest
have
some
notion
of
‘efficient’
actions,
110 that
be
toshown
theof
MDP
framework;
andto
there
exists
a general
procedure
for
embedding
traditional
109
in
acontrol
variety
of
recent
work
[***],
and
examples
given
later,
most
standard
domains
can
shown
inatranslated
acontrol
variety
recent
work
[***],
and
inthe
the
examples
given
later,
most
standard
domains
can
110
be
translated
theto
110
MDP
be
framework;
translated
toand
the
there
MDP
exists
framework;
110a in
general
be the
translated
and
procedure
there
toexists
110
the
forMDP
be
aembedding
general
translated
framework;
procedure
traditional
to theand
MDP
for
there
embedding
framework;
exists
a it
general
traditional
and
there
procedure
exists afor
general
embedding
procedure
traditional
for emb
fer
energetically
inefficient
trajectories
by
placing
positive
rewards
on
each
ecific
form
of
Eqn.
(2)
can
seem
limit
applicability
of
the
LMDP
framework.
Yet
as
113
making
a
cost
a
reasonably
natural
and
universal
phenomenon.
Indeed,
is
possible
that
110
be
translated
to
the
MDP
framework;
and
there
exists
a
general
procedure
for
embedding
tradition
making
cost
a
reasonably
natural
and
universal
phenomenon.
Indeed,
it
is
possible
that
114
the
standard
MDP
formulation
is
overly
general,
discarding
useful
structure
in
most
real
wor
function
over
terminal
states.
In
this
example,
the
task
“reach
to
the
black
rectangle”
is
encoded
by
rewarding
any
terminal
state
with
the
a reasonably
and
universal
Indeed,
it is
possible
that
111
MDPs natural
in the
LMDP
framework
?.phenomenon.
111
MDPs
in
the
LMDP
111framework
MDPs
in?.
the
LMDP
framework
111 ?.
MDPs
in theaLMDP
111
MDPs
framework
in
the?.LMDP framework
?.
ktbe
of
this
flexibility
issuggest
the
unstructured
maximization
of
Eqn.
(1),
prevents
110to
be
translated
to
the
MDP
there
exists
general
procedure
for
embedding
traditional
the
MDP
framework;
and
there
exists
aand
general
procedure
for
traditional
generally,
we
that
most
domains
of
interest
have
some
of
‘efficient’
actions,
end
effector
in work
black
rectangle
(all
successful
configurations
(c-g)
Solutions
viaembedding
the
LMDP.
Top
row:
Instantaneous
rewards
in commonly plac
111
MDPs
in
the
LMDP
framework
?.notion
in
variety
ofMDP
recent
[***],
and
inframework;
the
examples
given
later,
most
standard
domains
can
domains–namely,
aofshown).
preference
for
efficient
actions.
Standard
MDP
formulations
114
the
standard
MDP
formulation
iswhich
overly
general,
discarding
useful
structure
in
most
real
world
theatranslated
standard
is 115
overly
general,
discarding
useful
structure
in
most
real
world
ormulation
is overly
general,
discarding
useful
structure
in
most
real
world
112
More
generally,
weformulation
suggest
that generally,
most
domains
of
interest
have
some
notion
of
‘efficient’
actions,
112
More
we
112suggest
More
that
generally,
most
domains
we
suggest
112
interest
that
More
most
have
generally,
domains
some
notion
112
of
we
interest
More
suggest
of
‘efficient’
generally,
have
that
some
most
actions,
we
notion
domains
suggest
of
‘efficient’
of
that
interest
most domains
actions,
have some
of notion
interestofhave
‘efficient’
some notion
actions,
of
111
MDPs
in
the
LMDP
framework
?.
MDPs
in
the
LMDP
framework
?.
the
space
of
joint
angles.
Middle
row:
Optimal
desirability
function
with
sample
trajectories
starting
at
green
circles,
finishing
at
red
reference
forthe
efficient
Standard
MDP
formulations
commonly
place
116
small
negative
rewards
on
each
action
to
instantiate
this
efficiency
goal,
but
they
retain
flexibili
113 to
making
a MDP
control
a
reasonably
natural
and
universal
phenomenon.
Indeed,
it
is
possible
that
115
domains–namely,
a
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
gslated
a control
cost
aactions.
reasonably
natural
and
universal
phenomenon.
Indeed,
it
is
possible
that
113
making
a
control
cost
113
a
making
reasonably
a
control
natural
cost
and
a
reasonably
universal
113
making
phenomenon.
natural
a
control
and
113
universal
Indeed,
cost
making
a
reasonably
it
phenomenon.
is
a
control
possible
natural
cost
that
Indeed,
a
and
reasonably
universal
it
is
possible
natural
phenomenon.
that
and
universal
Indeed,
phenomenon.
it isthe
possible
Indeed,
that
domains–namely,
acost
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
framework;
and
exists
a
general
procedure
for
embedding
traditional
112 there
More
generally,
we
suggest
that
most
domains
of
interest
have
some
notion
of
‘efficient’
action
114
the
standard
MDP
formulation
is overly
general,
discarding
useful
structure
in energetically
most
real
world
s on each
action
to instantiate
goal,
but
they
retain
thein
flexibility
circles.
Bottom
row:
Strobe
plot
of114formulation
sample
trajectories
Cartesian
Trajectories
at useful
green
circle,
end
atby
redoverly
circle.
Column
(c): in
114 this
theefficiency
standard
MDP
the
standard
is action
overly
MDP
general,
formulation
114
discarding
the
isspace.
overly
standard
useful
general,
MDP
structure
114 discarding
formulation
thestart
in
standard
most
real
isMDP
overly
world
structure
formulation
general,
in most
discarding
is
real
world
general,
useful
structure
discarding
most
usefulreal
structure
world
in
117
to,
for
instance,
prefer
inefficient
trajectories
placing
positive
rewards
on
eac
116
small
negative
rewards
on
each
to
instantiate
this
efficiency
goal,
but
they
retain
the
flexibility
small
negative
rewards
on
each
action
to
instantiate
this
efficiency
goal,
but
they
retain
the
flexibility
112formulation
More
generally,
suggest
that
most
domains
of
interest
have
some
of
‘efficient’
actions,
generally,
weaBellman
suggest
most
domains
of
interest
have
some
notion
of
actions,
ndard
MDP
isthat
overly
general,
discarding
useful
structure
in‘efficient’
most
real
world
in
the
framework
?.
113
making
arewards
control
cost
afor
reasonably
and
universal
phenomenon.
Indeed,
it commonly
is possible
115 LMDP
domains–namely,
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
115
domains–namely,
a115
preference
domains–namely,
forparticular
efficient
a preference
actions.
115
Standard
domains–namely,
efficient
MDP
actions.
formulations
115natural
a domains–namely,
Standard
preference
commonly
MDP
for
efficient
formulations
place
anotion
preference
actions.
commonly
for
Standard
efficient
place
MDP
actions.
formulations
Standard
MDP
formulations
placeth
erMore
energetically
inefficient
trajectories
by we
placing
positive
on
each
The
linear
equation
is
solved
for
this
instantaneous
boundary
reward
structure
to
obtain
the
optimal
value
function.
118116 action.
Thetrajectories
drawback
of
this
flexibility
is efficiency
the
unstructured
maximization
ofon
Eqn.
(1), which
preven
116
small
negative
rewards
onainstance,
each
action
to prefer
instantiate
this
efficiency
goal,
but
retain
theuniversal
flexibility
117is the
to,
for
energetically
inefficient
trajectories
by
placing
positive
rewards
116
small
negative
rewards
on
small
each
negative
action
to
rewards
instantiate
onthey
each
116
this
action
efficiency
small
to
negative
instantiate
goal,
rewards
116
butpositive
this
they
small
on
retain
negative
each
the
action
goal,
flexibility
to
butinstantiate
they
onon
each
retain
action
this
the
flexibility
to
goal,
but
thiseach
they
efficiency
goal,
thereal
flexibility
but
they
to,this
forflexibility
instance,
prefer
energetically
inefficient
by
placing
each
113
making
areasonably
control
cost
athe
reasonably
natural
and
phenomenon.
Indeed,
itefficiency
isinstantiate
possible
that
making
a control
cost
and
phenomenon.
Indeed,
itrewards
is
possible
that
of
unstructured
maximization
of
Eqn.
(1), universal
which
prevents
114natural
standard
MDP
formulation
is
overly
general,
discarding
useful
structure
in retain
most
worre
ns–namely,
a preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
(d-g):
Solution
via
compositional
Multitask
LMDP.
The
instantaneous
reward
structure
isrewards
expressed
astrajectories
a weighted
combination
117
to, for Columns
instance,
prefer
energetically
inefficient
trajectories
by placing
on
each
117
to, for instance,
prefer
117 compositionality.
to,
energetically
for instance,
inefficient
preferpositive
energetically
trajectories
117 rewards
to, forby
inefficient
instance,
placing
117trajectories
positive
prefer
to, for
energetically
rewards
instance,
by placing
onprefer
inefficient
each
positive
energetically
rewards inefficient
on by
each
placing
trajectories
positive by
rewards
placing
onpositive
each
3
119
118
action.
The
drawback
of
this
flexibility
is
the
unstructured
maximization
of
Eqn.
(1),
which
prevents
generally,
we
suggest
that
most
domains
of
interest
have
some
notion
of
‘efficient’
actions,
action.
The
drawback
of
this
flexibility
is
the
unstructured
maximization
of
Eqn.
(1),
which
prevents
115
domains–namely,
a
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
plac
114
the
standard
MDP
formulation
is
overly
general,
discarding
useful
structure
in
most
real
world
the
standard
MDP
formulation
is
overly
general,
discarding
useful
structure
in
most
real
world
of
previously-learned
subtasks
(d-f),
here
chosen
to
be
navigation
to
specific
points
in
state
space.
(g)
Because
of
the
linearity
of
the
118
action.
The drawback
of this
flexibility
is to
the unstructured
maximization
of Eqn.
(1),action.
which
prevents
118
action.
The drawback
118 ofaction.
this flexibility
The
drawback
is theefficiency
unstructured
this118flexibility
maximization
isThe
thedrawback
unstructured
118
of Eqn.
action.
of(1),
this
maximization
The
which
flexibility
drawback
prevents
isofthe
of
Eqn.
this
unstructured
(1),
flexibility
which maximization
prevents
is the unstructured
of Eqn.
maximization
(1), which prevents
of Eqn.
(
egative
rewards
on each
action
instantiate
this
goal,
but
they
retain
the
flexibility
119
compositionality.
119
compositionality.
119
compositionality.
119
compositionality.
119
compositionality.
119
compositionality.
compositionality.
Bellman
equation,
the
resulting
combined
value
function
is
optimal,
and
the
system
can
act
instantly
despite
no
explicit
training
on
the
116
small
negative
rewards
on
each
action
to
instantiate
this
efficiency
goal,
but
they
retain
the
flexibili
120
The
115
domains–namely,
a
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
domains–namely,
a
preference
for
efficient
actions.
Standard
MDP
formulations
commonly
place
ginstance,
a controlprefer
cost a energetically
reasonably natural
and
universal
phenomenon.
Indeed,
it
is
possible
that
inefficient
trajectories
byenergetically
placing
positive
rewards
on by
each
reach-to-rectangle
task.
same
fixed
basis
can this
be used
to120express
wide
of tasks
(reach
toflexibility
a they
cross;
reach
tothe
a circle;
etc). rewards on eac
120
The 116
117120
for set
instance,
inefficient
trajectories
placing
positive
The
The
The agoal,
120
The
small120negative
rewards
on
each
action
toprefer
instantiate
thisvariety
efficiency
goal,
but
retain
flexibility
small
negative
rewards
on
eachThe
action
toto,
instantiate
efficiency
but
they
retain
the

ndard
MDP120formulation
is overly
general,
discarding
useful structure
in
most
real
world
The
The drawback
The
of
this
flexibility
is the
unstructured
maximization
ofpositive
Eqn.
(1),
which
prevents
118
action.
Thetrajectories
drawback
of by
thisplacing
flexibility
is the
unstructured
maximization
Eqn.
(1), which preven
117
to,
for
instance,
prefer
energetically
inefficient
trajectories
by
placing
positive
rewardsofon
each
to,
for
instance,
prefer
energetically
inefficient
rewards
on
each
3
ns–namely,
a preference for efficient
actions. Standard
MDP formulations
commonly
place
3
3compositionality.
3
3
3
3
119
sitionality.
action.
ofis this
flexibility
isstate
the
unstructured
offunctions
Eqn. (1),z t ,which
prevents
action. The118
drawback
ofThe
thisdrawback
flexibility
thehave
unstructured
maximization
ofmaximization
Eqn. desirability
(1), which
prevents
and
P, qi , to
qb2 iinstantiate
which
identical
the
corresponding
2 = hS,
i t = 1, · · · , Nt
egative
rewards
onLeach
action
this
efficiency
goal,
but
they
retain
the
flexibility
119 spaces,
compositionality.
compositionality.
3 each task, which can also be hformed into thei Ni -by120
The
transition dynamics,
and internal3reward strucfor
instance, prefer
energetically
inefficient
trajectories
by placing
positive rewards on each
tures,
but
differ
in
their
exponentiated
boundary
reNt desirability basis matrix Zi = zi1 zi2 · · · ziN for the
120
The
The
The drawback wards
of this
flexibility
unstructured
maximization
of Eqn. (1), which prevents
qb1 and
qb2 . These is
maythe
be solved
independently
to
multitask module.
3
1
2
sitionality.
yield desirability functions z and z . The desirability
1
2
3
When
function of the LMDP L1+2 = hS, P, qi , αqb + βqb i,
3 we encounter a new task defined by a novel instant

3

whose instantaneous rewards are a weighted sum of
the first two, is simply z 1+2 = αz 1 + βz 2 . This property follows from the linearity of Eqn. 4, and is the
foundation of our hierarchical scheme.
3

2.3. The Multitask LMDP
To build a multitask action module, we exploit this compositionality to build a basis set of tasks (Foster & Dayan,
2002; Ruvolo & Eaton, 2013; Schaul et al., 2015; Pan et al.,
2015; Borsa et al., 2016). Suppose that we learn a set of
LMDPs Lt = hS, P, qi , qbt i, t = 1, · · · , Nt which all share
the same state space, passive dynamics, and internal rewards,
but differ in their instantaneous exponentiated boundary reward structure qbt , t = 1, · · · , Nt . Each of these LMDPs
corresponds to a different task, defined by its boundary reward structure, in the same overall state space (see Taylor
& Stone (2009) for an overview of this and other notions
of transfer and multitask learning). We denote the Multitask LMDP module M formed from these Nt LMDPs
as M = hS,
h P, qi , Qb i. Here
i the Nb -by-Nt task basis matrix Qb = qb1 qb2 · · · qbNt encodes a library of component
tasks in the MLMDP. Solving each component LMDP yields

taneous exponentiated reward structure q, if we can express it as a linear combination of previously learned tasks,
q = Qb w, where w ∈ RNt is a vector of task blend weights,
then we can instantaneously derive its optimal desirability
function as zi = Zi w. This immediately yields the optimal
action through Eqn. 6. Hence an MLMDP agent can act
optimally even in never-before-seen tasks, provided that
the new task lies in the subspace spanned by previously
learned tasks. This approach is an off-policy variant on
the successor representation method of Dayan (1993). It
differs by yielding the optimal value function, rather than
the value function under a particular policy π; and allows
arbitrary boundary rewards for the component tasks (the
SR implicitly takes these to be a positive reward on just
one state). Also related is the successor feature approach of
Barreto et al. (2016), which permits performance guarantees
for transfer to new tasks. With successor features, new tasks
must have rewards that are close in Euclidean distance to
prior tasks, whereas here we require only that they lie in the
span of prior tasks.
More generally, if the target task q is not an exact linear
combination of previously learned tasks, an approximate

Hierarchy Through Composition with Multitask LMDPs
(c)

task weighting w can be found as
argminw kq − Qb wk

subject to Qb w ≥ 0.

M4

(7)

The technical requirement Qb w ≥ 0 is due to the relationship q = exp(r/λ), such that negative values of q are not
possible. In practice this can be approximately solved as
w = Q†b q, where † denotes the pseudoinverse, and negative
elements of w are projected back to zero.
Here the coefficients of the task blend w constitute a distributed representation of the current task to be performed.
Although the set of basis tasks Qb is fixed and finite, they
permit an infinite space of tasks to be performed through
their concurrent linear composition. Figure 2.2 demonstrates this ability of the Multitask LMDP module in the
context of a 2D robot arm reaching task. From knowledge
of how to reach individual points in space, the module can
instantly act optimally to reach to a rectangular region.

3. Stacking the module: Concurrent
Hierarchical LMDPs
To build a hierarchy out of this Multitask LMDP module, we
construct a stack of MLMDPs in which higher levels select
the instantaneous reward structure that defines the current
task for lower levels. To take a navigation example, a high
level module might specify that the lower level module
should reach room A but not B by placing instantaneous
rewards in room A but no rewards in room B. Crucially,
the fine details of achieving this subgoal can be left to the
low-level module. Critical to the success of this hierarchical
scheme is the flexible, optimal composition afforded by
the Multitask LMDP module: the specific reward structure
commanded by the higher layer will often be novel for the
lower level, but will still be performed optimally provided it
lies in the basis of learned tasks.
3.1. Constructing a hierarchy of MLMDPs



We start with the MLMDP M 1 = S 1 , P 1 , qi1 , Q1b that we
must solve, where here the superscript denotes the hierarchy
level (Fig. 2). This serves as the base case for our recursive
scheme for generating a hierarchy.
For the inductive step,


given an MLMDP M l = S l , P l , qil , Qlb at level l, we augment the state space S̃ l = S l ∪ Stl with a set of Nt terminal
boundary states Stl that we call subtask states. Semantically,
entering one of these states will correspond to a decision by
the layer l MLMDP to access the next level of the hierarchy.
The transitions to subtask states are governed by a new Ntl by-Nil passive dynamics matrix Ptl , which is chosen by the
designer to encode the structure of the domain. Choosing
fewer subtask states than interior states will yield a higher
level which operates in a smaller state space, yielding state
abstraction. In the augmented MLMDP, the passive dynam-

(b)
M3
(a)
M2
M1
States
(a)

(c)

(b)

Figure 2. Stacking Multitask LMDPs. Top: Deep hierarchy for
navigation through a 1D corridor. Lower-level MLMDPs are abstracted to form higher-level MLMDPs by choosing a set of ‘subtask’ states which can be accessed by the lower level (grey lines
between levels depict passive subtask transitions Ptl ). Lower levels
access these subtask states to indicate completion of a subgoal
and to request more information from higher levels; higher levels
communicate new subtask state instantaneous rewards, and hence
the concurrent task blend, to the lower levels. Red lines indicate
higher level access points for one sample trajectory starting from
leftmost state and terminating at rightmost state. Bottom: Panels (a-c) depict distributed task blends arising from accessing the
hierarchy at points denoted in left panel. The higher layer states
accessed are indicated by filled circles. (a) Just the second layer
of hierarchy is accessed, resulting in higher weight on the task to
achieve the next subgoal and zero weights on already achieved
subgoals. (b) The second and third levels are accessed, yielding
new task blends for both. (c) All levels are accessed yielding task
blends at a range of scales.



ics become P̃ l = N ( Pil ; Pbl ; Ptl ) where the operation
N (·) renormalizes each column to sum to one.

It remains to specify the matrix of subtask-state instantaneous rewards Rtl for this augmented MLMDP. Often hierarchical schemes require designing a pseudoreward function
to encourage successful completion of a subtask. Here
we also pick a set of reward functions over subtask states;
however, the performance of our scheme is only weakly
dependent on this choice: we require only that our chosen
reward functions form a good basis for the set of subtasks
that the higher layer will command. Any set of tasks which
can linearly express the required space of reward structures
specified by the higher level is suitable. In our experiments,
we define Ntl tasks, one for each subtask state, and set each
instantaneous reward to negative values on all but a single ‘goal’ subtask state. Then the augmented MLMDP is

Hierarchy Through Composition with Multitask LMDPs

D

E
M̃ l = S̃ l , P̃ l , qil , Qlb 0; 0 Qlt .

the higher level l + 1,

The
level is itself an MLMDP M l+1 =

 l higher
l+1 l+1
St , P , qi , Ql+1
, defined not over the entire state
b
space but just over the Ntl subtask states of the layer below. To construct this, we must compute an appropriate
passive dynamics and reward structure. A natural definition
for the passive dynamics is the probability of starting at one
subtask state and terminating at another under the lower
layer’s passive dynamics,
Pil+1
Pbl+1

T

= P̃tl (I − P̃il )−1 P̃tl ,
=

P̃bl (I

−

T
P̃il )−1 P̃tl .

(8)
(9)

In this way, the higher-level LMDP will incorporate the
transition constraints from the layer below. The interiorstate reward structure can be similarly defined, as the reward
accrued under the passive dynamics from the layer below.
However for simplicity in our implementation, we simply
set small negative rewards on all internal states.
Hence, from a base MLMDP M l and subtask transition matrix Ptl , the above construction yields an augmented MLDMP M̃ l at the same layer and unaugmented
MLDMP M l+1 at the next higher layer. This procedure
may be iterated to form
n
o a deep stack of MLMDPs
1
2
D−1
D
M̃ , M̃ , · · · , M̃
,M
, where all but the highest is
augmented with subtask states. The key choice for the designer is Ptl , the transition structure from internal states to
subtask states for each layer. Through Eqns. (8)-(9), this
matrix specifies the state abstraction used in the next higher
layer. Fig. 2 illustrates this scheme for an example of navigation through a 1D corridor.
3.2. Instantaneous rewards and task blends:
Communication between layers
Bidirectional communication between layers happens via
subtask states and their instantaneous rewards. The higher
layer sets the instantaneous boundary rewards over subtask
states for the lower layer; and the lower layer signals that it
has completed a subtask and needs new guidance from the
higher layer by transitioning to a subtask state.
In particular, suppose we have solved a higher-level
MLMDP using any method we like, yielding the optimal
action al+1 . This will make transitions to some states more
likely than they would be under the passive dynamics, indicating that they are more attractive than usual for the current
task. It will make other transitions less likely than the passive dynamics, indicating that transitions to these states
should be avoided. We therefore define the instantaneous
rewards for the subtask states at level l to be proportional to
the difference between controlled and passive dynamics at

l+1
rtl ∝ al+1
i (·|s) − pi (·|s).

(10)

This effectively inpaints extra rewards for the lower layer,
indicating which subtask states are desirable from the perspective of the higher layer.
The lower layer MLMDP then uses its basis of tasks to determine a task weighting wl which will optimally achieve
the reward structure rtl specified by the higher layer by solving (7). The reward structure specified by the higher layer
may not correspond to any one task learned by the lower
layer, but it will nonetheless be performed well by forming
a concurrent blend of many different tasks and leveraging
the compositionality afforded by the LMDP framework.
This scheme may also be interpreted as implementing a
parametrized option, but with performance guarantees for
any parameter setting (Masson et al., 2016).
We now describe the execution model (see Supplementary
Material for pseudocode listing). The true state of the agent
is represented in the base level M̃ 1 , and next states are
drawn from the controlled transition distribution. If the next
state is an interior state, one unit of time passes and the state
is updated as usual. If the next state is a subtask state, the
next layer of the hierarchy is accessed at its corresponding
state; no ‘real’ time passes during this transition. The higher
level then draws its next state, and in so doing can access
the next level of hierarchy by transitioning to one of its
subtask states, and so on. At some point, a level will elect
not to access a subtask state; it then transmits its desired
rewards from Eqn. (10) to the layer below it. The lower
layer then solves its multitask LMDP problem to compute
its own optimal actions, and the process continues down
to the lowest layer M̃ 1 which, after updating its optimal
actions, again draws a transition. Lastly, if the next state is
a terminal boundary state, the layer terminates itself. This
corresponds to a level of the hierarchy determining that it
no longer has useful information to convey. Terminating
a layer disallows future transitions from the lower layer
to its subtask states, and corresponds to inpainting infinite
negative rewards onto the lower level subtask states.

4. Computational complexity advantages of
hierarchical decomposition
To concretely illustrate the value of hierarchy, consider navigation through a 1D ring of N states, where the agent must
perform N different tasks corresponding to navigating to
each particular state. We take the passive dynamics to be
local (a nonzero probability of transitioning just to adjacent
states in the ring, or remaining still). In one step of Z iteration (Eqn. 5), the optimal value function progresses at best
O(1) states per iteration because of the local passive dynamics (see Precup et al. (1998) for a similar argument). It

Hierarchy Through Composition with Multitask LMDPs

Instead suppose that we construct a hierarchy by placing a
subtask every M = log N states, and do this recursively to
form D layers. The recursion terminates when N/M D ≈ 1,
yielding D ≈ logM N . With the correct higher level policy
sequencing subtasks, each policy at a given layer only needs
to learn to navigate between adjacent subtasks, which are
no more than M states apart. Hence Z iteration can be
terminated after O(M ) iterations. At level l = 1, 2, · · · , D
of the hierarchy, there are N/M l subtasks, and N/M l−1
boundary reward tasks to learn. Overall this yields
D
X
l=1

M



N
N
+ l−1
Ml
M



≈ O(N log N )

total iterations (see Supplementary Material for derivation
and numerical verification). A similar analysis shows that
this advantage holds for memory requirements as well. The
flat scheme requires O(N 2 ) nonzero elements of Z to encode all tasks, while the hierarchical scheme requires only
O(N log N ). Hence hierarchical decomposition can yield
qualitatively more efficient solutions and resource requirements, reminiscent of theoretical results obtained for perceptual deep learning (Bengio, 2009; Bengio & LeCun, 2007).
We note that this advantage only occurs in the multitask
setting: the flat scheme can learn one specific task in time
O(N ). Hence hierarchy is beneficial when performing an
ensemble of tasks, due to the reuse of component policies
across many tasks (see also Solway et al. (2014)).

(a)

(b)
200

Trajectory length

FLAT
HIERARCHICAL

0
0

(c)

Epochs

100

(d)

B

Task blend weights

therefore requires O(N ) iterations in a flat implementation
for a useful value function signal to arrive at the furthest
point in the ring for each task. As there are N tasks, the
flat implementation requires O(N 2 ) iterations to learn all
of them.

A
B
C
D
E
F

Steps

Figure 3. Rooms domain. (a) Four room domain with subtask locations marked as red dots, free space in white and obstacles in black,
and derived higher-level passive dynamics shown as weighted links
between subtasks. (b) Convergence as trajectory length over learning epochs with and without the help of the hierarchy. (c) Sample
trajectory (gray line) from upper left to goal location in bottom
right. (d) Evolution of distributed task weights on each subtask
location over the course of the trajectory in panel (c).

one preinitialized using Z-iteration). From the start of learning, the hierarchy is able to drive the agent to the vicinity
of the goal. Fig. 3(c-d) illustrates the evolving distributed
task representation commanded by the higher layer to the
lower layer over the course of a trajectory. At each time
point, several subtasks have nonzero weight, highlighting
the concurrent execution in the system.
(a)

5. Experiments
5.1. Conceptual Demonstration
To illustrate the operation of our scheme, we apply it to a
2D grid-world ‘rooms’ domain (Fig. 3(a)). The agent is
required to navigate through an environment consisting of
four rooms with obstacles to a goal location in one of the
rooms. The agent can move in the four cardinal directions or
remain still. To build a hierarchy, we place six higher layer
subtask goal locations throughout the domain (Fig. 3(a), red
dots). The inferred passive dynamics for the higher layer
MLMDP is shown in Fig. 3(a) as weighted lines between
these subtask states. The higher layer passive dynamics conform to the structure of the problem, with the probability of
transition between higher layer states roughly proportional
to the distance between those states at the lower layer.
As a basic demonstration that the hierarchy conveys useful
information, Fig. 3(b) shows Z-learning curves for the base
layer policy with and without an omniscient hierarchy (i.e.,

(b)

Figure 4. Desirability functions. (a) Desirability function over
states as agent moves through environment, showing the effect
of reward inpainting from the hierarchy. (b) Desirability function
over states as goal location moves. Higher layers inpaint rewards
into subtasks that will move the agent nearer the goal.

Fig. 4(a) shows the composite desirability function resulting
from the concurrent task blend for different agent locations.
Fig. 4(b) highlights the multitasking ability of the system,

Hierarchy Through Composition with Multitask LMDPs

showing the composite desirability function as the goal
location is moved. The leftmost panel, for instance, shows
that rewards are painted concurrently into the upper left and
bottom right rooms, as these are both equidistant to the goal.

In order to quantitatively demonstrate the performance improvements possible with our method we consider the problem of a mobile robot navigating through an office block in
search of a charging station. This domain is shown in Fig. 5,
and is taken from previous experiments in transfer learning
(Fernández & Veloso, 2006).
Although the agent again moves in one of the four cardinal
directions at each time step, the agent is additionally provided with a policy to navigate to each room in the office
block, with goal locations marked by an ‘S’ in Fig. 5. The
goal of the agent is to learn a policy to navigate to the nearest charging station from any location in the office block,
given that there are a number of different charging stations
available (one per room, but only in some of the rooms).
This corresponds to an ‘OR’ navigation problem: navigating
to location A OR B, while knowing separately a policy to
get to A, and a policy to get to B. Concretely, the problem is
to navigate to the nearest of two unknown subtask locations.
The agent is randomly initialized at interior states, and the
trajectory lengths are capped at 500 steps.

Trajectory Length

5.2. Quantitative Comparison

Options
Our Method

Epochs

Figure 6. Learning rates. Our method jump-starts performance
in the OR task, while the options agent must learn a state-action
mapping.

Fig. 6 shows that our agent receives a significant jump-start
in learning. By leveraging the distributed representation
provided by our scheme, the agent is required only to learn
when to request information from the higher layer. Although the task is novel, the higher layer can express it as a
combination of prior tasks. Conversely, the options agent
must learn a unique mapping between all states and actions.
This issue is exacerbated as the number of available options
grows (Rosman & Ramamoorthy, 2012).

6. Conclusion

Figure 5. The office building domain. Each ‘subtask’ or ‘option’
policy navigates the agent to one of the rooms.

We compare a one-layer deep implementation of our method
to a simple implementation of the options framework (Sutton et al., 1999). In the options framework the agent’s action
space is augmented with the full set of option policies. The
initialization set for these options is the full state space, so
that any option may be executed at any time. The termination condition is defined such that the option terminates
only when it reaches its goal state. To minimize the action
space for the options agent, we remove the primitive actions, reducing the learning problem to simply choosing the
single correct option from each state. The options learning
problem is solved using Q-learning with sigmoidal learning
rate decrease and -greedy exploitation. These parameters
were optimized on a coarse grid to yield the fastest learning
curves. Results are averaged over 20 runs.

The Multitask LMDP module provides a novel approach
to control hierarchies, based on a distributed representation of tasks and parallel execution. Rather than learn to
perform one task or a fixed library of tasks, it exploits the
compositionality provided by linearly solvable Markov decision processes to perform an infinite space of task blends
optimally. Stacking the module yields a deep hierarchy
abstracted in state space and time, with the potential for
qualitative efficiency improvements.
Experimentally, we have shown that the distributed representation provided by our framework can speed up learning
in a simple navigation task, by representing a new task as a
combination of prior tasks.
While a variety of sophisticated reinforcement learning
methods have made use of deep networks as capable function approximators (Mnih et al., 2015; Lillicrap et al., 2015;
Levine et al., 2016), in this work we have sought to transfer some of the underlying intuitions, such as parallel distributed representations and stackable modules, to the control setting. In the future this may allow other elements
of the deep learning toolkit to be brought to bear in this
setting, most notably gradient-based learning of the subtask
structure itself.

Hierarchy Through Composition with Multitask LMDPs

Acknowledgements
We thank our reviewers for thoughtful comments. AMS
acknowledges support from the Swartz Program in Theoretical Neuroscience at Harvard. AE and BR acknowledge
support from the National Research Foundation of South
Africa.

References
Barreto, A., Munos, R., Schaul, T., and Silver, D. Successor
Features for Transfer in Reinforcement Learning. arXiv, 2016.
Barto, A.G. and Madadevan, S. Recent Advances in Hierarchical
Reinforcement Learning. Discrete Event Dynamic Systems:
Theory and Applications, (13):41–77, 2003.
Bellman, R.E. Dynamic Programming. Princeton University Press,
Princeton, NJ, 1957.
Bengio, Y. Learning Deep Architectures for AI. Foundations and
Trends in Machine Learning, 2(1):1–127, 2009.
Bengio, Y. and LeCun, Y. Scaling learning algorithms towards AI.
In Bottou, L., Chapelle, O., DeCoste, D., and Weston, J. (eds.),
Large-Scale Kernel Machines. MIT Press, 2007.

Dvijotham, K. and Todorov, E. A unified theory of linearly solvable
optimal control. Uncertainty in Artificial Intelligence, 2011.
Fernández, F. and Veloso, M. Probabilistic policy reuse in a reinforcement learning agent. Proceedings of the fifth international
joint conference on Autonomous agents and multiagent systems,
pp. 720, 2006.
Foster, D. and Dayan, P. Structure in the Space of Value Functions.
Machine Learning, (49):325–346, 2002.
Hinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality
of data with neural networks. Science, 313(5786):504–7, 7
2006.
Hinton, G.E., Osindero, S., and Teh, Y.-W. A Fast Learning
Algorithm for Deep Belief Nets. Neural Computation, 18:1527–
1554, 2006.
Howard, R.A. Dynamic Programming and Markov Processes.
MIT Press, Cambridge, MA, 1960.
Jonsson, A. and Gómez, V. Hierarchical Linearly-Solvable Markov
Decision Problems. In ICAPS, 2016.
Kappen, H.J. Linear Theory for Control of Nonlinear Stochastic
Systems. Physical Review Letters, 95(20):200201, 11 2005.

Bonarini, A., Lazaric, A., and Restelli, M. Incremental Skill
Acquisition for Self-motivated Learning Animats. In Nolfi,
S., Baldassare, G., Calabretta, R., Hallam, J., Marocco, D.,
Miglino, O., Meyer, J.-A., and Parisi, D. (eds.), Proceedings of
the Ninth International Conference on Simulation of Adaptive
Behavior (SAB-06), volume 4095, pp. 357–368, Heidelberg,
2006. Springer Berlin.

Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-End
Training of Deep Visuomotor Policies. Journal of Machine
Learning Research, 17:1–40, 2016.

Borsa, D., Graepel, T., and Shawe-Taylor, J. Learning Shared
Representations in Multi-task Reinforcement Learning. arXiv,
2016.

Masson, W., Ranchod, P., and Konidaris, G.D. Reinforcement
Learning with Parameterized Actions. In Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence, pp. 1934–
1940, 9 2016.

Botvinick, M.M., Niv, Y., and Barto, A.C. Hierarchically organized
behavior and its neural foundations: a reinforcement learning
perspective. Cognition, 113(3):262–80, 12 2009.
Burridge, R. R., Rizzi, A. A., and Koditschek, D.E. Sequential
Composition of Dynamically Dexterous Robot Behaviors. The
International Journal of Robotics Research, 18(6):534–555, 6
1999.

Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa,
Y., Silver, D., and Wierstra, D. Continuous control with deep
reinforcement learning. arXiv, 2015.

Mausam and Weld, D.S. Planning with Durative Actions in
Stochastic Domains. Journal of Artificial Intelligence Research,
31:33–82, 2008.

Dayan, P. Improving Generalization for Temporal Difference
Learning: The Successor Representation. Neural Computation,
5(4):613–624, 7 1993.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J.,
Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K.,
Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou,
I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis,
D. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.

Dayan, P. and Hinton, G. Feudal Reinforcement Learning. In
NIPS, 1993.

Pan, Y., Theodorou, E.A., and Kontitsis, M. Sample Efficient Path
Integral Control under Uncertainty. In NIPS, 2015.

Dietterich, T.G. Hierarchical Reinforcement Learning with the
MAXQ Value Function Decomposition. Journal of Artificial
Intelligence Research, 13:227–303, 2000.

Parr, R. and Russell, S. Reinforcement learning with hierarchies
of machines. In NIPS, 1998.

Drummond, C. Composing functions to speed up reinforcement
learning in a changing world. In Nédellec, C. and Rouveirol, C.
(eds.), Machine Learning: ECML-98., pp. 370–381, Heidelberg,
1998. Springer Berlin.
Dvijotham, K. and Todorov, E. Inverse Optimal Control with
Linearly-Solvable MDPs. In ICML, 2010.

Precup, D., Sutton, R., and Singh, S. Theoretical results on reinforcement learning with temporally abstract options. In ECML,
1998.
Ribas-Fernandes, J.J.F., Solway, A., Diuk, C., McGuire, J.T., Barto,
A.G., Niv, Y., and Botvinick, M.M. A neural signature of
hierarchical reinforcement learning. Neuron, 71(2):370–9, 7
2011.

Hierarchy Through Composition with Multitask LMDPs
Rosman, B. and Ramamoorthy, S. What good are actions? Accelerating learning using learned action priors. In 2012 IEEE
International Conference on Development and Learning and
Epigenetic Robotics (ICDL), number November. IEEE, 11 2012.
Ruvolo, P. and Eaton, E. ELLA: An efficient lifelong learning
algorithm. Proceedings of the 30th International Conference
on Machine Learning, 28(1):507–515, 2013.
Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal Value
Function Approximators. Proceedings of The 32nd International Conference on Machine Learning, pp. 1312–1320, 2015.
Solway, A., Diuk, C., Córdova, N., Yee, D., Barto, A.G., Niv, Y.,
and Botvinick, M.M. Optimal Behavioral Hierarchy. PLoS
Computational Biology, 10(8):e1003779, 8 2014.
Sutton, R.S. and Barto, A.G. Reinforcement Learning: An Introduction. The MIT Press, 1998.
Sutton, R.S., Precup, D., and Singh, S. Between MDPs and semiMDPs: A framework for temporal abstraction in reinforcement
learning. Artificial Intelligence, 112(1-2):181–211, 8 1999.
Taylor, M.E. and Stone, P. Transfer Learning for Reinforcement
Learning Domains: A Survey. Journal of Machine Learning
Research, 10:1633–1685, 2009.
Todorov, E. Linearly-solvable Markov decision problems. In NIPS,
2006.
Todorov, E. Efficient computation of optimal actions. Proceedings
of the National Academy of Sciences, 106(28):11478–11483, 7
2009a.
Todorov, E. Compositionality of optimal control laws. In NIPS,
2009b.

