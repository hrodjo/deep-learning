Source-Target Similarity Modelings for Multi-Source Transfer Gaussian
Process Regression

Pengfei Wei 1 2 Ramon Sagarna 1 2 Yiping Ke 1 2 Yew-Soon Ong 1 2 Chi-Keong Goh 3 2

Abstract
A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities. In this paper, we study different approaches based on Gaussian process models to
solve the multi-source transfer regression problem. Precisely, we first investigate the feasibility
and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We
theoretically show that using such a transfer covariance function for general Gaussian process
modelling can only capture the same similarity
coefficient for all the sources, and thus may result in unsatisfactory transfer performance. This
leads us to propose TCM S Stack, an integrated
strategy incorporating the benefits of the transfer covariance function and stacking. Extensive experiments on one synthetic and two realworld datasets, with learning settings of up to 11
sources for the latter, demonstrate the effectiveness of our proposed TCM S Stack.

1. Introduction
Transfer learning (TL) methods show specially appealing
for real-world applications where the data from the target
domain is scarce but a good amount of data from another
source domain is available. With research efforts largely
confined to the single-source setting (Pan et al., 2011; Wei
et al., 2016; Zhou et al., 2016), an increasing amount of
studies are contributing to a realistic applicability of TL by
addressing the multi-source scenario, mainly for classifica1
School of Computer Science and Engineering, Nanyang
Technological University, Singapore 2 Rolls-Royce@Nanyang
Technological University Corporate Lab 3 Rolls-Royce Advanced Technology Centre, Singapore. Correspondence to:
Pengfei Wei <Pwei001@e.ntu.edu.sg>, Ramon Sagarna <saramon@ntu.edu.sg>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tion (Tommasi et al., 2014; Fang et al., 2015; Bhatt et al.,
2016). The problem of regression, however, has been much
less studied, despite of the variety of real-world domains
in which it arises; for instance wifi or indoor signal location (Pan et al., 2008), biological data analysis (Lam et al.,
2016), or mechanical system design (Ghosh et al., 2015).
In this work, we concentrate on multi-source transfer regression (MSTR) based on Gaussian process (GP) models.
All the way through, the TL community has been paying
attention to modeling the similarity between different domains so that only the source knowledge that is helpful
for the target domain is transferred. This is because designing a TL method based on the assumption that domains
are mutually relevant may lead to negative transfer (Pan &
Yang, 2010). Similarity capture is particularly crucial in
multi-source TL as the transfer capacity to the target task
may differ considerably across the diverse source domains.
Thus, TL methods that are capable of tuning the strength of
the knowledge transfer to the similarity of the domains are
attracting increasing interest (Luo et al., 2008; Wang et al.,
2014; Al-Stouhi & Reddy, 2011).
As regards to MSTR, a key issue is to capture the diverse
Source-Target (S-T) similarities. The relatively few efforts
to date have focused on ensemble methods. Particularly, an
amount of works rely on the boosting strategy due to its capability to capture fine-grained S-T similarities be weighting the contribution of train instances individually (Dai
et al., 2007; Pardoe & Stone, 2010; Yao & Doretto, 2010).
However, as outlined in (Al-Stouhi & Reddy, 2011), such
an instance-based similarity strategy in boosting has shown
issues with slow/premature weights convergence that have
seriously penalized the computational cost or the transfer
performance. Another type of ensemble strategy for multisource transfer is stacking (Wolpert, 1992). Pardoe and
Stone propose a meta-model that aggregates the predictions
of several base models previously learned with each source
in isolation (Pardoe & Stone, 2010) . The aggregation is
done by assigning each base model a model importance. In
this case, the S-T similarities can be captured through the
model importance. However, in such stacking-based methods, the base models suffer from a lack of consideration of
the dependencies between the different source domains.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Another popular idea to model the S-T similarity is to
construct a transfer covariance function that relates two
data points from distinct domains through the similarity
coefficients (Bonilla et al., 2008; Williams et al., 2009).
Such idea has been proposed in multi-task learning (Bonilla
et al., 2008), where each task pair is assigned a particular
similarity coefficient. Note, however, that multi-task learning differs from the TL problem in that the former aims at
improving performance across all the domains while the
objective of the latter focuses on the target domain only.
Nevertheless, the idea of transfer covariance function is referential for the TL problem. In (Cao et al., 2010), a single
source transfer covariance function (TCSS ) was proposed.
In the corresponding transfer covariance matrix, one similarity coefficient was assigned to the S-T block to model
the inter-domain similarity. A GP with such TCSS (called
GP-TCSS ) was then trained for the transfer task.
When generalizing to MSTR, one may naturally consider
a multi-source transfer covariance function (TCM S ) with
different similarity coefficients attached to distinct S-T
blocks in the corresponding transfer covariance matrix. In
this work, we investigate the feasibility of such covariance
function. We theoretically prove that a general GP with
TCM S (GP-TCM S ) fails to capture the similarity diversities of various S-T domain pairs. Although TCM S intends
to utilize different similarity coefficients, the learnt GPTCM S would give the same similarity coefficient for all the
S-T domain pairs. The generalization error bounds of the
learnt GP-TCM S show that this coefficient is taking effect
in every source domain. Considering the diverse S-T similarities between the sources and the target, this may jeopardize the transfer performance, especially when the number of sources increases. Moreover, the learning of GPTCM S rapidly poses a computational issue with increasing amounts of source domains as usually O(m3 ) computations are required to evaluate a model for m data points.
The unsatisfactory performance of GP-TCM S leads us to
exploit the transfer covariance function in another way.
Considering that both the stacking strategy and the transfer covariance function can model the S-T similarity and
using the transfer covariance function at the base models
would therefore add flexibility to the similarity capture capability of the stacking approach, we propose to integrate
them into one unified model. Specifically, we first discuss TCSS Stack, a method that simply stacks GP-TCSS
base models. TCSS Stack alleviates the computational issue of GP since it allows to stretch the number of sources
due to its O(N n3 ) cost for N sources with n points each.
However, TCSS Stack still suffers from the aforementioned
limitation of conventional stacking. Thus, we propose a
more involved TCM S Stack. Two salient features make
TCM S Stack significantly different from TCSS Stack: (i) it
associates the similarity coefficients in the base GP-TCSS

with the model importance during learning, and (ii) it learns
the model importance and the base GP-TCSS jointly. By
doing so, on the one hand, TCM S Stack further reduces the
computational cost by lowering the number of optimization
variables. On the other hand, although the similarity coefficient in TCM S Stack represents bivariate S-T similarity relations, they are elicited by pondering all the inter-domain
dependencies. In the experiments, we show the superiority of TCM S Stack on the transfer performance compared
to TCSS Stack, GP-TCM S , and other MSTR methods.

2. Related Work
A main challenge in MSTR is to precisely capture the diverse S-T transfer capacities across the different sources.
Ensemble approaches (Dai et al., 2007), which can provide an explicit, fine-grained similarity capture, are widely
used to handle the MSTR problems. In (Pardoe & Stone,
2010), TrAdaBoost.R2 was proposed, a boosting based algorithm that weights the contribution of train instances individually, and thus delivers a model accounting for the S-T
similarities for every instance. However, such boostinglike methods suffer from slow/premature convergence issues that tremendously jeopardize the transfer performance
(Al-Stouhi & Reddy, 2011). Pardoe and Stone also introduced a multi-source transfer stacking in which base models are pretrained in different source domains separately,
and a meta-model is trained by aggregating the outputs of
the base models (Pardoe & Stone, 2010). By doing so, the
S-T similarities are captured at meta-model level by the
learnt model importance. Although the stacking methods
show success in some MSTR problems, they have the limitation that inter-domain dependencies between sources are
ignored at the base models.
At the other end of the spectrum are transfer covariance
function representing a multivariate similarity relation over
sources and target domains. A popular representative of
this family is the work by Bonilla et al. (Bonilla et al.,
2008) on multi-task learning, where a free-form kernel relates each pair of tasks. Apart from the difference of the application domain (multi-task learning versus TL), this kind
of models often imply fitting an increasingly large number of hyperparameters; e.g. in the free-form kernel, this
number grows as (N 2 − N )/2, where N is the number of
sources. Motivated by (Bonilla et al., 2008), (Cao et al.,
2010) develops another transfer covariance function for the
single source transfer.
In this work, we first describe a family of transfer covariance functions, and investigate their feasibility for MSTR.
With the theoretical analysis showing the unsatisfactory
performance of such transfer covariance function, we propose to unify the S-T similarity capture of stacking and the
transfer covariance function. To the best of our knowledge,

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

this is the first work that analyzes the feasibility and performance of such family of transfer covariance functions for
MSTR, and further combines them with stacking.

3. Problem Statement
We denote a domain set for MSTR as D = S ∪ T where
S = {Si : 1 ≤ i ≤ N } is a set of source domains and
T is the target domain. All source domain data and few
target domain data are labeled. Denote the data matrix
and its corresponding label vector in each source domain
Si as X(Si ) ∈ RnSi ×d and y(Si ) ∈ RnSi . Likewise, we
represent the target data set with X(T ) = {X(Tl ) , X(Tu ) }
where X(Tl ) ∈ RnTl ×d is the labeled target data matrix and
X(Tu ) ∈ RnTu ×d is the unlabeled one. We further define
y(Tl ) ∈ RnTl as the label vector for X(Tl ) . Moreover, we
assume nTl  min(nS1 , ..., nSN , nTu ). Our objective is
(Tl )
to utilize {X(Si ) , y(Si ) }N
, y(Tl ) } to predict
i=1 and {X
(Tu )
labels for X
.
We use the GP model for this regression task. We denote
the underlying latent function between the inputs x and the
outputs y as f , and the noise variance as σ 2 . Thus, f denotes the function vector over X. A GP model defines a
Gaussian distribution over the functions, f ∼ N (µ, K) in
which µ is the mean vector and K is the covariance matrix which is positive semi-definite (PSD, or equivalently
denoted as K  0). Usually µ is assumed to be 0, and
thus the GP model is completely specified by K given a
covariance function which is parameterized by Ω.

important role in transfer, while those completely targetunrelated sources will not be considered. However, to guarantee the GP model is always valid, any covariance matrix
K∗ constructed by k∗ (·, ·) should be PSD. Theorem 1 gives
the sufficient and necessary condition for a PSD K∗ .
Theorem 1. Let KDi Dj (Di , Dj ∈ D) denote a covariance
matrix for points in Di and Dj . A Gram matrix


KS1 S1 ...
KS1 SN
λ1 KS1 T


...
...
...
...

K∗ = 
 KSN S1 ... KSN SN
λN KSN T 
λ1 KT S1 ... λN KT SN
KT T
is PSD for any covariance matrix K in the form

KS1 S1 ... KS1 SN KS1 T

...
...
...
...
K=
 KSN S1 ... KSN SN KSN T
KT S1 ... KT SN
KT T

4.1. Transfer Covariance Function for Multi-Source
Since the GP model is specified by K, one straightforward way to achieve the knowledge transfer across multiple
source domains and the target domain is to design a transfer covariance function for multi-source. Different from a
classical GP which uses a fixed covariance function for the
data from different domains, we focus on the covariance
function of the form (TCM S ):

 λi k(x, x0 ), x ∈ X(Si ) & x0 ∈ X(T )
0
k∗ (x, x ) =
or x ∈ X(T ) & x0 ∈ X(Si ) ,

k(x, x0 ), otherwise.
(1)
where k(·, ·) is any valid covariance function, and λi is the
metric measuring the similarity between the source Si and
the target T . Through the learning, λi is expected to capture the different transfer strengths in different S-T domain
pairs. Those highly target-related sources will play a more





if and only if λ1 = ... = λN and |λi | ≤ 1.
Proof. Necessary condition: Let K∗ be a PSD matrix. We
use KSS to represent the sources-to-sources block matrix,
and KST , KST ∗ to represent the sources-to-target block
matrix in K and K∗ , respectively. Thus, we have:




KSS KST ∗
KSS KST
K=
, K∗ =
.
KT
KT T
KT T
KT
ST ∗
ST
Since K is PSD, according to the Schur complement theorem (Zhang, 2006), we have:

4. GP with Transfer Covariance Function
In this section, we analyze the transfer performance of GP
using a specific family of transfer covariance function.



e SS )KST = 0,
(I − KSS K

(2)

e
KT T − KT
ST KSS KST  0,

(3)

e SS is the generalized inverse of KSS . By rewritwhere K
e SS as a block matrix using K
e S S as the element, we
ing K
i j
further derive eq. (2) and eq. (3) as:


N P
N
P
e
KS1 T −
KS1 Si KSi Sj KSj T


i=1 j=1



 = 0, (4)
...


N P
N


P
e
KS T −
KS S KS S KS T
N

N

i

i

j

j

i=1 j=1

KT T −

N X
N
X

e S S KS T  0.
KT Si K
i j
j

(5)

i=1 j=1

Likewise, for the PSD matrix K∗ , we have the
two Schur complement derivations:

N P
N
P
e
λ K
−
λ K
K
K
 1 S1 T i=1 j=1 j S1 Si Si Sj Sj T


...

N P
N

P
e S S KS T
λN KS T −
λj KS S K
N

N

i

i

j

following



 = 0.



j

i=1 j=1

(6)

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

KT T −

N X
N
X

e S S KS T  0.
λi λj KT Si K
i j
j

(7)

i=1 j=1

Combining eq. (4) and eq. (6), we get:
 P
N P
N
e S S KS T
(λ − λj )KS1 Si K
i j
j
 i=1 j=1 1


...

N
N P
 P
e S S KS T
(λN − λj )KSN Si K
i j
j




 = 0.



(8)

i=1 j=1

Since Eq. (8) must hold for all PSD K, we induce λ1 =
... = λN = λ. Based on such conclusion, we combine eq.
(5) and eq. (7):
(1 − λ2 )KT T + λ2 M  0,
where M = KT T −

N P
N
P

(9)

e S S KS T . Since eq.
KT Si K
i j
j

i=1 j=1

(9) must hold for all PSD KT T and PSD M, we resolve
that |λ| ≤ 1.
Sufficient condition: Let λ1 = ... = λN = λ, and |λ| ≤ 1.
According to the Theorem 1 in (Cao et al., 2010), we obtain
that K∗ is a PSD matrix.
To sum up, we conclude that if K∗ is a PSD matrix, λi
should satisfy λ1 = ... = λN and |λi | ≤ 1.
From Theorem 1, we can see that |λi | ≤ 1, which means
a highly target-related source results in a full transfer of
KSi ,T , but a completely target-unrelated source results in
a zero block matrix. This indicates the adaptiveness of λi .
However, Theorem 1 also shows that k∗ (·, ·) can just give
one similarity coefficient for all S-T domain pairs to ensure
the validity of GP-TCM S . Such single similarity coefficient compromises the diverse similarities between different S-T domain pairs. This violates the original intention of
λi which is to distinguish the similarity diversity between
different S-T domain pairs.
4.2. Generalization Bounds of GP-TCM S
To investigate the effect of a single compromised similarity coefficient on the performance of GP-TCM S , we derive its generalization error bounds. In (Chai, 2009), an
earlier analysis can be found for the generalization errors
and learning curves in multi-task learning (specifically, two
learning tasks with the same noise variance). Our investigation is different from that work however as we are working
on a TL setting, and more importantly, on multiple sources
with different noise variances.
We denote the single compromised similarity coefficient as
λ, and the noise variance for different domains as σd2 , d ∈
{S1 , ..., SN , T }. Thus, the transfer covariance matrix of

the noisy training data is C∗ = K∗ + Σ where




KSS λKST
ΣS
0
K∗ =
,
Σ
=
0 σT2 IT T
λKT
KT T
ST
and ΣS is a diagonal block matrix with the diagonal block
elements {σS2 1 IS1 S1 , ..., σS2 N ISN SN }. According to (Rasmussen, 2006), if the GP prior is correctly specified, the
generalization error at a point is also the posterior variance
at such point. Specifically, for GP-TCM S , the posterior
variance at the target point xt is:
2
δT2 (xt , λ, {X(Si ) , σS2 i }N
i=1 , XT , σT )
−1
= ktt − kT
∗t C∗ k∗t ,

(10)

T
T
where kT
∗t = (λkSt , kT t ), kSt (kT t ) is the vector of co(Si ) N
variances between {X }i=1 (X(T ) ) and xt , and ktt is
the prior variance at xt . Wherever it is not misleading,
we will simplify the posterior variance expression using
2
δT2 (λ, {σS2 i }N
i=1 , σT ). The generalization error for the target domain can be obtained by averaging eq. (10) over xt :
2
T (λ, {X(Si ) , σS2 i }N
i=1 , XT , σT )
Z
2
= δT2 (xt , λ, {σS2 i }N
i=1 , σT )p(xt )dxt .

(11)

To derive the generalization error bounds for GP-TCM S ,
we first rewrite

 −2
λ (KSS + ΣS )
KST
Λ,
C∗ = Λ
KT T + σT2 IT T
KT
ST


λISS
0
where Λ =
. Thus, the posterior variance
0
IT T
at point xt becomes:
2
T
2 N
2 −1
δT2 (λ, {σS2 i }N
kt ,
i=1 , σT ) = ktt − kt Φ(λ, {σSi }i=1 , σT )
(12)
T
T
where kT
t = (kSt , kT t ) and
2
Φ(λ, {σS2 i }N
i=1 , σT )
 −2

λ (KSS + ΣS )
KST
=
.
KT
KT T + σT2 IT T
ST

Note that the above derivation excludes the situation where
λ = 0. When λ = 0, all the source domains are unrelated
to the target domain, and thus no knowledge is transferred.
This is easy to verify by plugging λ = 0 into eq. (12).
Further, we observe that δT2 is equal for λ and −λ, so we
only investigate the case λ ∈ (0, 1]. For eq. (12), we further
decompose it as:
2
Φ(λ, {σS2 i }N
i=1 , σT )
  2


KSS KST
σT ISS
0
+
=
+
0
σT2 IT T
KT
KT T
ST

 
KSS + ΣS 0
ΣS − σT2 ISS
(λ−2 − 1)
+
0
0
0

0
0



= Φ(1, {σT2 , ..., σT2 }N , σT2 ) + E1 + E2 ,
(13)

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression
−2



KSS + ΣS
0



0
0

where E1 = (λ − 1)
and E2 =


ΣS − σT2 ISS 0
. Eq. (13) unveils that the posterior
0
0
variance of having instances from different source domains
is equivalent to the posterior variance of having those instances from target domain with two additional correlated
noise terms, E1 and E2 . This shows us the two main factors that matter in the transfer performance; namely, the
S-T similarity and the noise variances. To further analyze
how the S-T similarity affects the transfer performance, we
focus on one factor and fix the other. Assuming that all the
sources are totally related to the target, i.e. λ = 1, and consequently, the noise variance for each source becomes ξS2 i ,
we define the difference:
2
2
2 N
2
∆ = δT2 (1, {ξS2 i }N
i=1 , σT ) − δT (λ, {σSi }i=1 , σT ).
2
To obtain the upper (lower) bound of δT2 (λ, {σS2 i }N
i=1 , σT ),
2

we are interested in those ξ Si (ξ 2S ) that make ∆ ≥ 0 (∆ <
i
0) for all the target points.
Proposition 1. Let δ and δ be the maximum and mini2
mum eigenvalues of KSS , ξ Si = λ−2 σS2 i − (1 − λ−2 )δ
and ξ 2S = λ−2 σS2 i − (1 − λ−2 )δ for every source Si .
i

2
Then, for all the target data points, δT2 (1, {ξ 2S }N
i=1 , σT ) ≤
2

Proof. By applying eq. (12), we have:
2
2
2 N
2
∆ = δT2 (1, {ξS2 i }N
i=1 , σT ) − δT (λ, {σSi }i=1 , σT )
2 N
2 −1
2 −1
= kT
− Φ(1, {ξS2 i }N
]kt
t [Φ(λ, {σSi }i=1 , σT )
i=1 , σT )

To make ∆ ≥ 0 for all the target data points, we need
2 −1
2 −1
to prove Φ(λ, {σS2 i }N
− Φ(1, {ξS2 i }N
is
i=1 , σT )
i=1 , σT )
PSD, which means:
2 −1
2 −1
Φ(λ, {σS2 i }N
− Φ(1, {ξS2 i }N
0
i=1 , σT )
i=1 , σT )

⇐⇒

2
2 N
2
Φ(λ, {σS2 i }N
i=1 , σT )  Φ(1, {ξSi }i=1 , σT )


(1 − λ−2 )KSS + (Σ0S − λ−2 ΣS ) 0
0
0
0

where Σ0S is a diagonal block matrix with the diagonal
block elements {ξS2 1 IS1 S1 , ..., ξS2 N ISN SN }
⇐⇒
⇐⇒

(1 − λ−2 )KSS + (Σ0S − λ−2 ΣS )  0
KSS 

2

thus we take the minimum λ−2 σS2 i − (1 − λ−2 )δ as ξ Si to
2
be the smallest upper bound of σT2 (λ, {σS2 i }N
i=1 , σT ). Sim2
−2 2
−2
ilarly, we have ξ S = λ σSi − (1 − λ )δ to construct
i
2
the largest lower bound of σT2 (λ, {σS2 i }N
i=1 , σT ).
Proposition 1 gives the lower and upper bounds of the pos2
terior variance δT2 (λ, {σS2 i }N
i=1 , σT ). By applying eq. (11),
we readily obtain the generalization error bounds.
Corollary 1. Let
2

2
N
2
T (λ, {σS2 i }N
i=1 , σT ) = T (1, {ξ Si }i=1 , σT )
2 N
2
2
T (λ, {σS2 i }N
i=1 , σT ) = T (1, {ξ S }i=1 , σT )
i

2
2 N
2
Then, T (λ, {σS2 i }N
i=1 , σT ) ≤ T (λ, {σSi }i=1 , σT ) ≤
2 N
2
T (λ, {σSi }i=1 , σT ).

Proposition 1 serves to demonstrate that λ takes effect in
every source on the final transfer performance. With the
assumption that different source domains have different ST similarities with the target domain, a single λ that works
for every sources has a great difficulty capturing such ST similarity diversity. This leads us to exploit the transfer
covariance function in another way.

i

2
2
N
2
δT2 (λ, {σS2 i }N
i=1 , σT ) ≤ δT (1, {ξ Si }i=1 , σT ).

⇐⇒

Note that ∆ is a monotonically increasing function of ξS2 i ,

(λ−2 ΣS − Σ0S )
(1 − λ−2 )

⇐⇒

(λ−2 σS2 i − ξS2 i )
δ≤
f or every Si
(1 − λ−2 )

⇐⇒

ξS2 i ≥ λ−2 σS2 i − (1 − λ−2 )δ f or every Si

5. Transfer Covariance Function Stacking
Considering the effectiveness showed by the stacking strategy for MSTR (Pardoe & Stone, 2010), we propose a framework that can integrate the capability for S-T similarity capture of both the transfer covariance function and stacking.
We first introduce TCSS Stack, a conventional way of stacking the transfer covariance function. Then, we design a
more involved stacking-inspired approach that overcomes
some limitations of the conventional stacking method.
5.1. Conventional Transfer Stacking TCSS Stack
Motivated by the fact that both the stacking strategy and
the transfer covariance function can model the S-T similarity and using the transfer covariance function at the
base models would therefore add flexibility to the similarity capture capability of the stacking approach, we propose a TCSS Stack method. In TCSS Stack, we first train
multiple GP-TCSS models using each Si and T (denoted
as {f (Si ,T ) (·|Ωi , λi )}N
i=1 ) and then apply the conventional
stacking strategy to combine their predictions. Given a target point x, the final prediction is given by:
f (x) =

XN
i=1

ωi f (Si ,T ) (x|Ωi , λi ),

XN
i=1

ωi = 1 (14)

where ωi are coefficients learned by minimizing the least
square error on the target labeled data.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

There are two major issues for the above model. (1) Since
each f (Si ,T ) is pretrained separately, the parameters learnt
for each f (Si ,T ) do not take the inter-domain dependencies between different source domains into account. (2)
Both λi and ωi reflect the S-T domain similarity. However, TCSS Stack takes them as two different variables and
learns them separately. Intuitively, the model importance
ωi should be positively correlated with the similarity coefficient λi . For example, the prediction of a GP-TCSS using
an unrelated source is less trustful, and should be assigned
a smaller coefficient in the stacking.
5.2. Improved Transfer Stacking TCM S Stack
To overcome the above issues, we propose a new transfer
stacking model (TCM S Stack) as follows:
XN
f ∗ (x) =
(g(λi )/Z)f (Si ,T ) (x, Ωi , λi ).
(15)
i=1

where λi refers to the similarity
PN coefficient in the GP-TCSS
for the i-th source, Z = i=1 g(λi ) is the normalization
term, and g(λi ) is any function preserving the monotonicity
of |λi | so that it coordinates the model importance and the
similarity coefficient. This also reduces the search efforts
by lowering the number of free parameters to fit. Moreover, instead of pretraining f (Si ,T ) (·|Ωi , λi ) separately, we
jointly learn f (Si ,T ) (·, Ωi , λi ) for all the source domains.
By doing so, the multiple GP-TCSS models are learned
together with the dependencies between multiple sources
taken into account.
Notice that the model in eq. (15) allows for multiple options to characterize the relative importance of GP-TCSS
models through g(·). In this paper, we use a simple function g(λi ) = |λi |. However, the absolute value function is
not smooth at the origin. Thus, we use a smooth function
studied in (Yong, 2015) to approximate it as follows:
1 λi
1 λi
|λi | ≈ αLn( e α + e− α ).
2
2
We set α = 0.01 which is the best approximation stated in
(Yong, 2015). Since Theorem 1 also tells us −1 ≤ λi ≤ 1,
we propose to define λi = 2(1/(1 + µi ))bi − 1 (µi ≥ 0
and bi ≥ 0), as in (Cao et al., 2010). Then, we conduct the
learning by minimizing the squared errors:
XnTl (T )
(T ) 2
min
(yj l − f ∗ (xj l )) .
(16)
{Ωi ,µi ,bi }N
i=1

j=1

In the optimization, we propose to use the conjugate gradient method. Other optimization methods can also be applied to solve this objective function.
5.3. Complexity Analysis
As in usual GP model training, the computational time
complexity of each f (Si ,T ) is dominated by the calculation

Table 1. Amazon review products dataset 15.
Top category
Beauty Health Grocery
Electronics
Home Garden Tool
Movies Music Game

Source domains
Beauty, Grocery Food, Health
Electronic, Office Product, Kindle Store
Kitchen, Pet Supplies
CD Vinyl, Digital Music, Video Games

Target domain
Clothing Shoes Jewelry
Cellphone Accessory
Tools Home Improvements
Movies TV

of the inverse of its covariance matrix, i.e. O((nTl +nSi )3 ).
Considering nTl  nSi and assuming nS1 = ... =
nSN = nS , the evaluation of a TCM S Stack model takes
then O(N n3S ). Notice that by following the stacking strategy of eq. (14) the training involves the steps of learning
each f (Si ,T ) and subsequently learning the ωi coefficients.
The latter calls for some cross-validation approach to evaluate a meta-model, as the f (Si ,T ) from the previous step
have been induced using the target data (Pardoe & Stone,
2010). In the extreme case of leave-one-out, this would
take O(nTl N n3S ). Even if we also choose a leave-one-out
validation to solve eq. (16) the cost of a TCM S Stack model
evaluation would be lower, since the first step of stacking
is not required. On the other side, by following TrAdaBoost.R2 or the GP-TCM S approach, a GP model evaluation requires O((N nS )3 ), which even exceeds the cost
√
for TCM S Stack using leave-one-out whenever N > nTl .

6. Experimental Study
In the following experimental study we aim at two main
goals: (i) to assess the ability of TCM S Stack in capturing
inter-domain similarity, and (ii) to evaluate its predictive
effectiveness compared to other approaches.
6.1. Experiment Setting
All the GPs herein build upon a standard squared exponential covariance function. The hyperparameters of each
method are optimized using the conjugate gradient implementation from the gpml package (Rasmussen & Nickisch,
2010). For each search, we allow a maximum of 200 evaluations. The reported results correspond to the model providing the best objective function value over 5 independent
runs with random initial solutions each. We use one synthetic dataset and two real-world datasets.
Synthetic dataset. We consider a linear function f (x) =
w0T x + , where w0 ∈ R100 and  is a zero-mean Gaussian
noise term, as the target. We use this function to generate
100 points as target test data, and 20 points as target train
data. For the source task, we use g(x) = (w0T +δ∆w)x+,
where ∆w is a random fluctuation vector and δ is the variable controlling the similarity between f and g (higher δ
indicates lower similarity), to generate 380 points for each
source with different δ.
Amazon reviews. We extract the raw data containing 15

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

product reviews from (McAuley et al., 2015), and categorize the products into four top categories according to the
Amazon website. Products in the same category are conceptually similar. Each product is taken as a domain, and
we select one as target from each category (see Table 1).
Reviews in each domain are represented by the count features and are labeled using stars in the set {1, 2, 3, 4, 5}.
UJIIndoorLoc. The building location dataset covers three
buildings of Universitat Jaume I with four floors each
(Torres-Sospedra et al., 2014). We build 12 domains by
taking the location data from each floor of each building as
a domain. The first floor of each building is taken as the
target. Domains from the same building are taken as similar. The received signal strength intensity from 520 wireless access points is used as the features, and the location
represented by the latitude and longitude is taken as label.

2 Sources

5 Sources

6.2. Domain Similarity Capture
10 Sources

We first elucidate the ability of TCM S Stack in capturing
the diverse S-T similarities through the λi coefficients. To
rationalize the assessment, we use the synthetic dataset
and consider a variety of problems covering a broad spectrum of TL settings. Precisely, we build four scenarios
of N = 2, 5, 10, 15 sources. In each scenario, we specify six problems, each given by a different combination of
sources. Three problems represent settings in which all the
sources are equally similar to the target, with high (δ = 0),
medium (δ = 15) and low (δ = 35) similarity strength.
The other three problems reflect diverse S-T similarities.
Each source is given by a δ randomly sampled from the set
{0, 4, 7, 10, 15, 20, 25, 30, 35} and with replacement. We
enforce the three problems to be different and avoid all the
sources to be equal. We show the results in Figure 1.
In the figure, the first three problems of each scenario are
the cases with equal S-T similarities. It can be observed in
the bar plots on the left hand side that the λ values learnt
by TCM S Stack are strictly reverse-correlated with the predefined δ values, which indicates an accurate capture of the
high, medium and low strengths of S-T similarity. We further observe from the black dots that GP-TCM S is also able
to strongly coordinate δ with a single compromised λ. This
is because all the sources share the same δ with the target,
and thus can be regarded as a single larger source.
The remaining three problems of each scenario reflect diverse similarities across source domains. In this case, Figure 1 shows that the λ values of TCM S Stack reflect the relative differences of δ across different sources fairly well
in general. The learnt λ is generally reverse-correlated
with the predefined δ values, but it is not strict and tight.
For instance, in problem 6 of the 5-sources scenario or
in the problem 5 of the 15-sources scenario, such reversecorrelated relations do not hold in all the sources. This is

15 Sources

Figure 1. Results on six problem settings for each scenario of 2, 5,
10 and 15 sources. Bar plots on the left show the correspondence
between each problem similarity (δ) and the similarity captured
by the model (λ); bars for TCM S Stack and black dots for GPTCM S . Bar plots on the right show the RMSE for TCM S Stack
(red) and GP-TCM S (green).

because, although the learnt λs only represent the bivariate
S-T similarities, each of them is specified during learning
by considering its influence relative to the rest of similarity coefficients, i.e. the inter-domain dependencies between
different sources are taken into account during the learning
of the λs. Thus, in some cases, the learnt λs may not strictly
approximate the real S-T similarities. However, λs are always learnt to guarantee the outcome of the best transfer
performance. That is the reason why the above two cases

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

the different amounts of source domains. This showcases
the capability of TCM S Stack to transfer knowledge from
various sources with different S-T similarities.

Figure 2. Comparison results on the two datasets.

still achieve satisfactory transfer performance in terms of
RMSE. By contrast, we find that GP-TCM S only gives a
trade-off value of λ over the diverse δ values. The right
hand side of the figure shows a consistently lower RMSE
for TCM S Stack than for GP-TCM S in all the problems. In
particular, a dramatic improvement is observed by utilizing TCM S Stack for the diverse problems 4-6. These results
indicate the superiority of TCM S Stack over GP-TCM S for
MSTR. We further verify this conclusion on the two realworld datasets in the next section.
6.3. Performance on Real-World Datasets
We compare TCM S Stack with several MSTR approaches,
namely: Tradaboost.R2, GP-TCM S , TCSS Stack, a variant
of TCSS Stack with joint learning of model importance coefficient and λ which we call TCSS Stack-Joint, and a variant of TCSS Stack using the learnt λ as the model importance which we call λ-Stacking. The evaluation comprises
both the Amazon and the UJIIndoorLoc datasets. For each
source domain we sample 500 points uniformly at random
for training. Likewise, train and test data from each target domain are obtained by sampling 25 points and 1000
points, respectively. For the Amazon dataset, we generated a set of problems by using each target domain in Table
1, and by randomly choosing a number of source domains
subsets. More precisely, for each scenario of 2, 3 and 5
sources, ten different source combinations were randomly
constructed. In addition, a scenario of 11 sources with all
the source domains was selected. Thus, we construct 40
transfer problems for each scenario of 2, 3 and 5 sources,
and 4 problems for the scenario of 11 sources. For the UJIIndoorLoc dataset, we generate the transfer problems in a
similar way to the Amazon dataset described above.
In Figure 2, we show the average RMSE results over all
the problems in each scenario for the two datasets. Overall, TCM S Stack is the winner among all the baselines on
the two datasets, improving the transfer performance across

For the other baselines, we observe that TrAdaBoost.R2
gives the poorest results due to the premature weights convergence issue. If, in addition, we consider the high computational cost for the models involved, TrAdaBoost.R2
does not seem to be a good choice for MSTR, especially
when the number of source domains is large. As for GPTCM S , it presents a steadily inferior performance than
TCM S Stack. Overall, the outcomes are in line with those
in the synthetic dataset, offering further support to the superiority of TCM S Stack to GP-TCM S . Notice that, since
the current benchmark was generated randomly, it is likely
a scenario to comprise diverse problem settings. Therefore,
capturing the diverse similarities through a single λ coefficient may compromise the performance of GP-TCM S . As
opposed to GP-TCM S , TCM S Stack offers more robust performance improvements.
Finally, the comparison with the other stacking-based
methods exposes the benefits of the two salient features of
TCM S Stack. Both TCSS Stack and λ-Stacking are beaten
by TCSS Stack-Joint and TCM S Stack. Since these two sets
of methods only differ in the joint learning of the parameters, the outcomes point at the benefits of bringing in
the inter-domain dependencies of the other sources during the learning. On the other side, the results for λStacking and TCM S Stack are better or comparable to those
by TCSS Stack and TCSS Stack-Joint, repectively. This provides support to the correlation of the model importance
with the similarity coefficients, which allows to specify
the model by estimating fewer hyper-parameters while preserving the similarity capture capability.

7. Conclusions
We investigate a family of transfer covariance functions
that represent the pairwise similarity between each source
and the target domain for the MSTR problem. We prove
that, GP-TCM S , a Gaussian process with such a transfer
covariance function can only capture the same similarity
coefficient for all the sources. By further analyzing the generalization errors of GP-TCM S , we conclude the bounds
depend on the single similarity coefficient, which may penalize the transfer performance. As an alternative, we propose TCM S Stack, an approach that integrates the transfer
covariance function and the stacking strategy into one unified model. TCM S Stack aligns the S-T similarity coefficients with the model importance and jointly learns the
base models. Extensive experiments on one synthetic and
two real-world datasets, with learning settings of up to 11
sources for the latter, show the superiority of TCM S Stack
to other MSTR methods.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Acknowledgments
This work was conducted within the Rolls-Royce@Nanyang Technological University Corporate Lab with support
from the National Research Foundation (NRF) Singapore
under the Corp Lab@University Scheme. It is also partially supported by the School of Computer Science and
Engineering at Nanyang Technological University.

References
Al-Stouhi, Samir and Reddy, Chandan. Adaptive boosting
for transfer learning using dynamic updates. Machine
Learning and Knowledge Discovery in Databases, pp.
60–75, 2011.
Bhatt, Himanshu Sharad, Rajkumar, Arun, and Roy, Shourya. Multi-source iterative adaptation for cross-domain
classification. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, pp.
3691–3697. AAAI Press, 2016.
Bonilla, Edwin V., Chai, Kian M., and Williams, Christopher. Multi-task gaussian process prediction. In Platt,
J. C., Koller, D., Singer, Y., and Roweis, S. T. (eds.),
Advances in Neural Information Processing Systems 20,
pp. 153–160. Curran Associates, Inc., 2008.
Cao, Bin, Pan, Sinno Jialin, Zhang, Yu, Yeung, Dit-Yan,
and Yang, Qiang. Adaptive transfer learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI’10, pp. 407–712. AAAI Press,
2010.
Chai, Kian M. Generalization errors and learning curves
for regression with multi-task gaussian processes. In
Advances in neural information processing systems, pp.
279–287, 2009.
Dai, Wenyuan, Yang, Qiang, Xue, Gui-Rong, and Yu,
Yong. Boosting for transfer learning. In Proceedings of
the 24th International Conference on Machine Learning,
ICML ’07, pp. 193–200, New York, NY, USA, 2007.
ACM. ISBN 978-1-59593-793-3.
Fang, Min, Guo, Yong, Zhang, Xiaosong, and Li, Xiao.
Multi-source transfer learning based on label shared subspace. Pattern Recognition Letters, 51:101–106, 2015.
Ghosh, Sayan, Jacobs, Ryan, and Mavris, Dimitri N. Multisource surrogate modeling with bayesian hierarchical regression. In 17th AIAA Non-Deterministic Approaches
Conference, pp. 1817–1829, 2015.
Lam, Kari Y, Westrick, Zachary M, Müller, Christian L,
Christiaen, Lionel, and Bonneau, Richard. Fused regression for multi-source gene regulatory network inference.
PLOS Computational Biology, 12:1–23, 2016.

Luo, Ping, Zhuang, Fuzhen, Xiong, Hui, Xiong, Yuhong,
and He, Qing. Transfer learning from multiple source
domains via consensus regularization. In Proceedings of
the 17th ACM conference on Information and knowledge
management, pp. 103–112. ACM, 2008.
McAuley, Julian, Targett, Christopher, Shi, Qinfeng, and
van den Hengel, Anton. Image-based recommendations
on styles and substitutes. In Proceedings of the 38th
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pp. 43–52. ACM,
2015.
Pan, Sinno Jialin and Yang, Qiang. A survey on transfer learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345–1359, 2010.
Pan, Sinno Jialin, Kwok, James T, and Yang, Qiang. Transfer learning via dimensionality reduction. In AAAI, volume 8, pp. 677–682, 2008.
Pan, Sinno Jialin, Tsang, Ivor W, Kwok, James T, and
Yang, Qiang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks,
22(2):199–210, 2011.
Pardoe, David and Stone, Peter. Boosting for regression
transfer. In Frnkranz, Johannes and Joachims, Thorsten
(eds.), Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pp. 863–870. Omnipress, 2010.
Rasmussen, Carl Edward. Gaussian processes for machine
learning. Citeseer, 2006.
Rasmussen, Carl Edward and Nickisch, Hannes. Gaussian
processes for machine learning (gpml) toolbox. Journal
of Machine Learning Research, 11:3011–3015, December 2010. ISSN 1532-4435.
Tommasi, Tatiana, Orabona, Francesco, and Caputo, Barbara. Learning categories from few examples with multi
model knowledge transfer. IEEE transactions on pattern analysis and machine intelligence, 36(5):928–941,
2014.
Torres-Sospedra, Joaquı́n, Montoliu, Raúl, Usó, Adolfo Martı́nez, Avariento, Joan P., Arnau, Tomas J.,
Benedito-Bordonau, Mauri, and Huerta, Joaquı́n. Ujiindoorloc: A new multi-building and multi-floor database
for wlan fingerprint-based indoor localization problems.
In International Conference on Indoor Positioning and
Indoor Navigation, pp. 261–270. IEEE, 2014.
Wang, Qifan, Ruan, Lingyun, and Si, Luo. Adaptive
knowledge transfer for multiple instance learning in image classification. In AAAI, pp. 1334–1340, 2014.

Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression

Wei, Pengfei, Ke, Yiping, and Goh, Chi Keong. Deep nonlinear feature coding for unsupervised domain adaptation. In In Proceedings of the Twenty-Fifth International
Joint Conference on Artificial Intelligence, pp. 2189–
2195. AAAI Press, 2016.
Williams, Christopher, Klanke, Stefan, Vijayakumar, Sethu, and Chai, Kian M. Multi-task gaussian process
learning of robot inverse dynamics. In Advances in Neural Information Processing Systems, pp. 265–272, 2009.
Wolpert, David H. Stacked generalization. Neural Networks, 5(2):241–259, February 1992. ISSN 0893-6080.
Yao, Yi and Doretto, Gianfranco. Boosting for transfer
learning with multiple sources. In Computer vision and
pattern recognition (CVPR), 2010 IEEE conference on,
pp. 1855–1862. IEEE, 2010.
Yong, Longquan. Uniform smooth approximation functions for absolute value function. Mathematics in practice and theory, pp. 250–255, 2015.
Zhang, Fuzhen. The Schur complement and its applications, volume 4. Springer Science & Business Media,
2006.
Zhou, Joey Tianyi, Pan, Sinno Jialin, Tsang, Ivor W, and
Ho, Shen-Shyang. Transfer learning for cross-language
text categorization through active correspondences construction. In AAAI, pp. 2400–2406, 2016.

