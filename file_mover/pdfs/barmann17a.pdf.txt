Emulating the Expert: Inverse Optimization through Online Learning
Andreas Bärmann * 1 Sebastian Pokutta * 2 Oskar Schneider * 1

Abstract
In this paper, we demonstrate how to learn the
objective function of a decision maker while only
observing the problem input data and the decision maker’s corresponding decisions over multiple rounds. Our approach is based on online
learning techniques and works for linear objectives over arbitrary sets for which we have a linear optimization oracle and as such generalizes
previous work based on KKT-system decomposition and dualization approaches. The applicability of our framework for learning linear constraints is also discussed briefly. Our algorithm
converges at a rate of O( p1T ), and we demonstrate its effectiveness and applications in preliminary computational results.

1. Introduction
Human decision makers are very good at making decisions
under rather imprecise specification of the decision-making
problem both in terms of constraints as well as objective.
One might argue that the human decision maker can pretty
reliably learn from observed previous decisions – a traditional learning-by-example setup. At the same time, when
we try to turn these decision-making problems into actual
optimization problems, we often run into all types of issues
in terms of specifying the model. In an optimal world, we
would be able to infer or learn the optimization problem
from previously observed decisions taken by an expert.
This problem naturally occurs in many settings where we
do not have direct access to the decision maker’s preference
or objective function but we can observe her behaviour and
the learner as well as the decision maker have access to
the same information. Natural examples are as diverse as
*

1
Equal contribution
Friedrich-Alexander-Universität
Erlangen-Nürnberg, Erlangen, Germany 2 Georgia Institute of
Technology, Atlanta, USA. Correspondence to: Andreas Bärmann <Andreas.Baermann@math.uni-erlangen.de>, Sebastian
Pokutta <Sebastian.Pokutta@isye.gatech.edu>, Oskar Schneider
<Oskar.Schneider@fau.de>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

making recommendations based on user history and strategic planning problems, where the agent’s preferences are
unknown but the system is observable. Other examples
include knowledge transfer from a human planner into a
decision-support system: often human operators have arrived at finely-tuned ‘objective functions’ through many
years of experience, and in many cases it is desirable to
replicate the decision-making process both for scaling up
and also for potentially including it in large-scale scenario
analysis and simulation to explore responses under varying
conditions.
Here we consider the learning of preferences or objectives
from an expert by means of observing her actions. More
precisely, we observe a set of input parameters and corresponding decisions of the form {(p1 , x1 ), . . . , (pT , xT )}.
They are such that pt 2 P with t = 1, . . . , T is a certain
realization of problem parameters from a given set P ✓ k
and xt is an optimal solution to the optimization problem
max cTtrue x
s.t. x 2 X(pt ),
where ctrue 2 n is the expert’s true but unknown objective
and X(pt ) ✓ n for some (fixed) n. We assume that we
have full information on the feasible set X(pt ) and that we
can compute argmax {cT x | x 2 X(pt )} for any candidate
objective c 2 n and t = 1, . . . , T . We present an online
learning algorithm based on the multiplicative weights update method that allows us to learn a strategy (c1 , . . . , cT )
of subsequent objective function choices with the following guarantee: if we optimize according to the surrogate
objective function ct instead of the actual unknown objective function ctrue in response to parameter realization pt ,
we obtain a sequence of optimal decisions (w.r.t. to each ct )
given by
x̄t = argmax {cTt x | x 2 X(pt )},
that are essentially as good as the decisions xt taken by the
expert on average. To this end, we interpret the observations of parameters and expert solutions as revealed over
multiple rounds such that in each round t we are shown the
parameters pt first, then take our optimal decision x̄t according to our objective function ct , then we are shown the
solution xt chosen by the expert and finally we are allowed

Emulating the Expert: Inverse Optimization through Online Learning

to update ct for the next round. For this setup, we will be
able to show that our algorithm attains an error bound of
r
T
1X
ln n
T
0
(ct ctrue ) (x̄t xt )  2K
,
T t=1
T

where K 0 is an upper bound on the `1 -diameter of the
feasible regions X(pt ) with t = 1, . . . , T . This implies that
both the deviations in true cost cTtrue (xt x̄t )
0 as well
as the deviations in surrogate cost cTt (x̄t xt ) 0 can be
made arbitrarily small on average. In other words, the average regret for having decided optimally according to the
surrogate objectives ct vs. having decided optimally for the
true objective ctrue vanishes at a rate of O( p1T ). This result shows that linear objective functions over general feasible sets can be learned from relatively few observations of
historical optimal parameter-solutions pairs. We will also
briefly discuss the case where the objective ctrue is known,
but some linear constraints are unknown in this paper.
Literature Overview
The idea of learning or inferring parts of an optimization
model from data is a reasonably well-studied problem under many different assumptions and applications and has
gained significant attention in the optimization community over the last few years, as discussed for example in
(den Hertog & Postek, 2016), (Lodi, 2016), (Simchi-Levi,
2014). These papers argue that there would be significant
benefits in combining traditional optimization models with
data-derived components. Most approaches in the literature focus on deriving the objective function of an expert decision maker based on past observations of input
data and the decisions she took in each instance. In almost all cases, the objective functions are learned by considering the KKT-conditions or the dual of the (parameterized) optimization problem, and as such convexity for
both the feasible region and the objective function is inherently assumed. Examples of this approach include (Keshavarz et al., 2011), (Li, 2016) and (Thai & Bayen, 2016),
where the latter also consider the derivation of variational
inequalities from data. Sometimes also distributional assumptions regarding the observations are made. Applications of such approaches have been heavily studied in the
context of energy systems (Ratliff et al., 2014; Konstantakopoulos et al., 2016), robot motion (Papadopoulos et al.,
2016), (Yang et al., 2014), medicine (Sayre & Ruan, 2014)
and revenue management (Kallus & Udell, 2015; Qiang
& Bayati, 2016; Chen et al., 2015; Kallus & Udell, 2016;
Bertsimas & Kallus, 2016); also in the situation where the
observed decisions were not necessarily optimal (Nielsen
& Jensen, 2004). Very closely related to our learning approach in terms of the problem formulation is (Troutt et al.,
2005) which was later extended in (Troutt et al., 2006),
where an optimization model is defined that searches for

a linear optimization problem that minimizes the total difference between the observed solutions and solutions found
by optimizing according to that optimization problem. In
the latter case, the models are solved using LP duality and
cutting planes respectively. In their follow-up work (Troutt
et al., 2008), a genetic algorithm is used to solve the problem heuristically under rather general assumptions, but inherently without any quality guarantees, and (Troutt et al.,
2011) study experimental setups for learning objectives under various stochastic assumptions, focussing on maximal
likelihood estimation, which is generally the case for their
line of work; we make no such assumptions.
Closely related to learning optimization models from observed data is the subject of inverse optimization. Here
the goal is to find an objective function that renders the
observed solutions optimal with respect to the concurrently observed parameter realizations. Here approaches
mostly from convex optimization are used for inverse optimal control (Iyengar & Kang, 2005; Panchea & Ramdani, 2015; Molloy et al., 2016), inverse combinatorial optimization (D. Burton, 1997; Burton & Toint, 1994; 1992;
Sokkalingam et al., 1999; Ahuja & Orlin, 2000), integer inverse optimization (Schaefer, 2009) and inverse optimization in the presence of noisy data, such as observed decisions that were suboptimal (Aswani et al., 2016; Chan
et al., 2015).
All these approaches heavily rely on duality and thus require convexity assumptions both for the feasible region as
well as the objectives. As such, they cannot deal with more
complex, possibly non-convex decision domains. This in
particular includes the important case of integer-valued decisions (such as ‘yes/no’-decisions) and also many other
non-convex setups (several of which admit efficient linear
optimization algorithms). Previously, this was only possible when the structure of the feasible set could be benefically exploited. In contrast, our approach does not make
any such assumptions and only requires access to a linear
optimization oracle (in short: LP oracle) for the feasible
region.
Also related to our work is inverse reinforcement learning
and apprenticeship learning, where the reward function is
the target to be learned. However, in this case the underlying problem is modeled as a Markov decision process
(MDP); see e.g. (Syed & Schapire, 2007) and (Ratia et al.,
2012). Typically, the guarantees are of a different form
though. Similarly, our work is not to be confused with that
of (Taskar et al., 2005) and (III et al., 2005), who develop
online algorithms for learning aggregation vectors for edge
features in graphs that use inverse optimization as a subroutine to define the update rule. In contrast, we do inverse
optimization by means of an online learning algorithm; basically the reverse setup.

Emulating the Expert: Inverse Optimization through Online Learning

Our approach is based on online learning, and we use the
simple EXP algorithm here to attain the stated regret bound.
The EXP algorithm is commonly also called Multiplicative
Weights Update (MWU) algorithm and was developed in
the works of (Littlestone & Warmuth, 1994), (Vovk, 1990)
and (Freund & Schapire, 1997) (see (Arora et al., 2012;
Hazan, 2016) for a comprehensive introduction; see also
(Audibert et al., 2013)). A similar algorithm was used by
(Plotkin et al., 1995) for solving fractional packing and
covering problems. We also note that our feedback is
stronger than bandit feedback. This requirement is not unexpected as the costs chosen by the ‘adversary’ depend on
our decision; as such the bandit model (see e.g. (Dani et al.,
2008), (Abbasi-Yadkori et al., 2011)) does not readily apply.
Contribution
To the best of the authors’ knowledge, this paper makes the
first attempt to learn the objective function of an optimization model from data using an online learning approach.
Online learning of optimization problems. Based on samples for the input-output relationship of an optimization
problem solved by a decision maker, our aim is to learn
an objective function which is consistent with the observed
input-output relationship; this is the best one can hope for.
In our setup, the expert solves the decision-making problem repeatedly for different input parameter realizations.
From these observations, we are able to learn a strategy of
objective functions that emulate the expert’s unknown objective function such that the difference in solution quality
between the solutions converges to zero on average.
While previous methods based on dualization or KKTsystem-based approaches can lead to similar or even
stronger results in the continuous/convex case, online
learning allows us to relax this convexity requirement and
to work with arbitrary decision domains as long as we are
able to optimize a linear function over them. Thus, we do
not explicitly analyze the KKT-system or the dual program
(in the case of LPs; see Remark 3.1). In particular, one
might consider our algorithm as an algorithmic analogue
of the KKT-system (or dual program) in the convex case.
To summarize, we stress that (a) we do not make any
assumptions regarding distribution of the observations,
(b) the observations can be chosen by a fully-adaptive adversary, and (c) we do not require any convexity assumptions regarding the feasible regions and only rely on access to an LP oracle. We would also like to mention that
our approach can be extended to work with slowly changing objectives using appropriate online learning algorithms
such as, for example, (Jadbabaie et al., 2015) or (Zinkevich,
2003); the regret bounds will depend on the rate of change.

Preliminary Computational Tests. While a full computational study is beyond the scope of this paper and left for
future work, we implemented a first preliminary version of
our algorithm, and we report computational results for a
few select problems.

2. Problem Setting
We consider the following family of optimization problems (OPT(p))p , which depend on a parameter p 2 P ✓
k
for some k 2 :
max cTtrue x
s.t. x

2

X(p),

where ctrue 2 n is the objective function and X(p) ✓
n
is the feasible region, where the latter depends on the
parameter p. Of particular interest to us will be feasible
regions that arise as polyhedra defined by linear constraints
and their intersections with integer lattices:
X(p) = {x 2

n l

⇥

l

| A(p)x  b(p)}

with A(p) 2 m⇥n and b(p) 2 m . However, our approach can also readily by applied in the case of more complex feasible regions, such as mixed-integer sets bounded
by convex functions:
X(p) = {x 2

n l

⇥

l

| G(p, x)  0}

with G : P ⇥ n l ⇥ l !
convex – or even more
general settings. In fact, for any possible choice of model
for the set of feasible decisions, we only require the availability of a linear optimization oracle capable of optimizing
linear functions over X(p) for any p 2 P . We call a decision x 2 n optimal for p if it is an optimal solution to
OPT(p).
We assume that Problem OPT(p) models a parameterized
optimization problem which has to be solved repeatedly for
various input parameters p. Our task is to learn the fixed objective function ctrue from given observations of the parameter p and a corresponding optimal solution x to OPT(p).
To this end, we further assume that we are given a series of
observations ((pt , xt ))t of parameter realizations pt 2 P
together with an optimal solution xt to OPT(pt ) computed
by the expert for t = 1, . . . , T ; these observations are revealed over time in an online fashion: in round t, we obtain a parameter setting pt and compute an optimal solution x̄t 2 X(pt ) with respect to an objective function ct
based on what we have learned about ctrue so far. Then
we are shown the solution xt the expert with knowledge of
ctrue would have taken and can use this information to update our inferred objective function for the next round. In
the end, we would like to be able to use our inferred objective function to take decisions that are essentially as good

Emulating the Expert: Inverse Optimization through Online Learning

as those chosen by the expert in an appropriate aggregation
measure such as, for example, ‘on average’ or ‘with high
probability’. The quality of the inferred objective is measured in terms of cost deviation between our solutions x̄t
and the solutions xt obtained by the expert. Details will
be given in the next section, where we will derive an algorithm based on multiplicative weights updates (MWU) to
solve the above task.
To fix some useful notations, let v(i) denote the i-th component of a vector v throughout and let [n] := {1, . . . , n}
for any natural number n. Further, let n := (1, . . . , 1)
denote the all-ones vector in n .
As a technical assumption, we further demand kctrue k1 =
1 and ctrue
0. Both requirements are without loss of
generality and are motivated by our choice of MWU as the
online learning algorithm to simplify the exposition. We
can drop these assumptions by employing, for example, the
Online Gradient Descent algorithm of (Zinkevich, 2003),
which requires an explicit projection step.

3. Learning Objectives
Ideally, we would like to find the true objective function ctrue as a solution to the following optimization problem:
◆
◆
X ✓✓
T
T
min
max c x
c xt ,
(1)
n
c2 + :
t2[T ]
kck1 =1

x2X(pt )

where xt 2 X(p) is the optimal decision taken by the
expert in round t. The (normalized) true objective function ctrue
0 is an optimal solution to Problem (1) with
objective value 0. This is because any solution ĉ with
kĉk1 = 1 is feasible and produces non-negative summands
✓
◆
T
max ĉ x
ĉT xt , t 2 [T ],
x2X(pt )

as we assume xt 2 X(pt ) to be optimal for pt with respect
to ctrue . Thus, the optimal value of (1) is bounded from
below by 0.
When solving Problem (1) we are interested in an objective
function vector c 2 n that delivers a consistent explanation for why the expert chose xt as his response to the parameters pt in round t = 1, . . . , T . To be more precise, we
want to minimize the deviation between the optimal value
obtained when optimizing over X(pt ) with our guess for
the objective function c and the value of the expert’s decision evaluated according to our guess c, averaged over all
observations. Our algorithm presented here will provide
even stronger guarantees in some cases, such as the one described in Section 3.1, showing that we can replicate the
decision-making behavior of the expert.

Problem (1) contains T instances of the following maximization subproblem:
max cT x

(2a)

s.t. x 2 X(pt )

(2b)

For each t = 1, . . . , T , the corresponding Subproblem (2)
asks for an optimal solution x̄t when optimizing over the
feasible set X(pt ) with our guess c as the objective function.
Remark 3.1. Note that in the case of polyhedral feasible
regions, i.e. pt = (At , bt ) 2 m⇥n ⇥ m and X(pt ) =
{x 2 n+ | At x  bt } for t = 1, . . . , T , Problem (1)
can be reformulated as a linear program by dualizing the
T instances of Subproblem (2). This yields
min

T
X

(bTt yt

c T xt )

t=1

s.t.

ATt yt

c

(8t = 1, . . . , T )

yt

0

(8t = 1, . . . , T )

n
X

ci

=

1

i=1

c

0,

where the yt are the corresponding dual variables and the
xt are the observed decisions from the expert (i.e. the latter
are part of the input data). This problem asks for a primal
objective function vector c that minimizes the total duality gap summed over all primal-dual pairs (xt , yt ) while
all yt ’s shall be dual feasible, which makes the xt ’s the respective primal optimal solutions. Thus, Problem (1) can
be seen as a direct generalization of the linear primal-dual
optimization problem. In fact, our approach also covers
non-convex cases, e.g. mixed-integer linear programs.
Problem (1) can be interpreted as a game over T rounds
between a player who chooses an objective function ct in
round t 2 [T ] and a player who knows the true objective
function ctrue and chooses the observations (pt , xt ) in a potentially adversarial way. The payoff of the latter player in
each round t is equal to cTt (x̄t xt )
0, i.e. the difference in cost between our solution and the expert’s solution
as given by our guessed objective function ct .
As Problem (1) is hard to solve in general, we will
design an algorithm that, rather than finding an optimal objective c, finds a strategy of objective functions (c1 , c2 , . . . , cT ) to play in each round whose error in
solution quality as compared to the true objective function
is as small as possible. Our aim will then be to give a quality guarantee for this strategy in terms of the number of
observations.

Emulating the Expert: Inverse Optimization through Online Learning

To allow for approximation guarantees, it is necessary that
the observed feasible sets have a common upper bound on
their `1 -diameter.

Definition 3.2. The `1 -diameter of a set S ✓ n , denoted by diam1 (S) is the largest distance between any
two points x1 , x2 2 S, measured in the infinity-norm, i.e.
diam1 (S) := maxx1 ,x2 2S kx1 x2 k1 .
In the following, we assume that there exists a K 0 with
diam1 (X(pt ))  K for all t = 1, . . . , T . With these
prerequisites, our application of multiplicative weights updates (MWU) to learn the objective function of an optimization problem proceeds as outlined in Algorithm 1.
Algorithm 1 Online Objective Function Learning
input observations (pt , xt ) for t = 1, . . . , T
output sequence
of objectives c1 , c2 , . . . , cT
q

{set learning rate}
w1
{initialize weights}
for t = 1, . . . , T do
wt
ct
kwt k1 {normalize weights}
x̄t
argmax {cTt x | X(pt )} {solve Subproblem (2)}
if x̄t = xt then
yt
0
else
x̄t xt
yt
kx̄t xt k1
end if
wt+1 (i)
wt (i)(1 ⌘yt (i)) {update weights}
end for
return (c1 , c2 , . . . , cT ).
ln n
T
n

1: ⌘
2:
3:
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:

For the series of objectives functions (ct )t that our algorithm produces over rounds t = 1, . . . , T , we can establish
the following guarantee:
Theorem 3.3. Let K 0 with diam1 X(pt )  K for all
t = 1, . . . , T . Then we have
T
1X
0
(ct
T t=1

T

ctrue ) (x̄t

xt )  2K

r

ln n
,
T

1. 0 

1
T

2. 0 

1
T

T
t=1 ct (x̄t

PT

T
t=1 ctrue (xt

xt )  2K

T
X
t=1

cTt yt 

T
X
t=1

q

x̄t )  2K

ln n
T ,

q

ln n
T .

Proof. According to the standard performance guarantee
of MWU (see, e.g., (Arora et al., 2012), Corollary 2.2),
Algorithm 1 attains the following solution quality for the

cTtrue (yt + ⌘|yt |) +

ln n
,
⌘

where the |yt | is to be understood component-wise. Using
that each each entry of |yt | is at most 1 and dividing by T ,
we can conclude
T
1X T
c yt
T t=1 t

and further

T
n
X
1X T
ln n
ctrue yt  ⌘
ctrue (i) +
T t=1
⌘T
i=1

T
1X T
c yt
T t=1 t

T
1X T
ln n
ctrue yt = ⌘ +
.
T t=1
⌘T

The right-hand side attains its minimum for ⌘ =
which yields the bound
r
T
T
1X T
1X T
ln n
c yt
c yt  2
.
T t=1 t
T t=1 true
T

q

ln n
T ,

Substituting back for the yt ’s and using
max kx̄t

t=1,...,T

xt k1  max diam1 (X(pt ))  K,
t=1,...,T

we obtain
T
1X T
c (x̄t
T t=1 t

T
1X T
xt ) +
c (xt
T t=1 true

x̄t )  2K

r

ln n
.
T

Observe that for each summand t 2 [T ] we have cTt (x̄t
xt ) 0 as x̄t , xt 2 X(pt ) and x̄t is the maximum over this
set with respect to ct . With a similar argument, we see that
cTtrue (xt x̄t ) 0 for all t 2 [T ]. Thus, we have
r
T
1X
ln n
T
0
(ct ctrue ) (x̄t xt )  2K
,
(3)
T t=1
T
and similarly for the separate terms with analogue argumentation. This establishes the claim.
Note that by using exponential updates of the form
wt+1 (i)

and in particular it also holds:
PT

comparison between the objective function ct chosen in
each round t with the unknown true objective function ctrue :

wt (i)e

⌘yt (i)

in line 11 of the algorithm, we could attain the same bound,
cf. (Arora et al., 2012, Theorem 2.3). Secondly, we remark
that our choice of the learning rate ⌘ requires the number
of rounds T to be known beforehand.
From the above theorem, we can conclude that the average
error over all observations (pt , xt ) for t = 1, . . . , T when
choosing objective function ct in iteration t of Algorithm 1
instead of ctrue converges to 0 with an increasing number of
observations T at a rate of roughly O( p1T ):

Emulating the Expert: Inverse Optimization through Online Learning

Corollary 3.4. Let K 0 with diam1 X(pt )  K for all
t = 1, . . . , T . Then we have
1.

limT !1 T1

2. limT !1

1
T

PT

T
t=1 ct (x̄t

PT

T
t=1 ctrue (xt

xt ) = 0

and

x̄t ) = 0.

In other words, both the average error incurred from replacing the actual objective function ctrue by the estimation ct
as well as the average error in solution quality with respect
to ctrue tend to 0 as T grows.
Moreover, using Markov’s inequality we also obtain the
following quantitative bound on the deviation by more than
" > 0 from the average cost:
Corollary 3.5. Let " > 0. Then theqfraction of observations xt with cTtrue (xt x¯t )
2K lnTn + " is at most
p"
1
. In particular, for any 0 < p < 1 after
2K lnTn +"
⇣
⌘2
p)2K
T
ln n (1 p"
observations the fraction of obserq
vations xt with cost cTtrue (xt x̄t ) 1 " p 2K lnTn + "
is at most p.
Proof. The first part is an obvious application of Markov’s
inequality. The second part follows from solving 1
p"ln n
 p for T and plugging in values.
2K

T

argmin {cT x | x 2 X(pt )} so that for xt 6= x̄ we have
cTtrue (xt

x̄t )

,

i.e. either the two optimal solutions coincide or they differ
by at least with respect to ctrue . In particular, optimizing
ctrue over X(pt ) leads to a unique optimal solution for all
pt with t 2 [T ]. While this condition sounds unnatural at
first, for example it is trivially satisfied for the important
case where X(pt ) with t 2 [T ] is a polytope with vertices in {0, 1} and ctrue is a rational vector. In this case,
d
write ctrue = kdk
with d 2 n+ and observe that the min1
imum change in objective value between any two vertices
x, y of the 0/1-polytope with cTtrue x 6= cTtrue y is bounded by
1
1
:= kdk
|cTtrue (x y)|
-stability with
kdk1 , so that
1
holds in this case. The same argument works for more general polytopes via bounding the minimum non-zero change
in objective function value via the encoding length.
We obtain the following simple corollary of Theorem 3.3.
Corollary 3.6. Let K 0 with diam1 X(pt )  K for all
t = 1, . . . , T , let (X(pt ))t be -stable for some
> 0,
and let NT := {t 2 [T ] | x̄t 6= xt }. Then
|NT |  2K

r

T ln n

.

+"

3.1. The Stable Case
Note that limT !1 (ct
ctrue )T (x̄t
xt ) = 0, as derived from Equation (3) does not necessarily imply that we
can approximate ctrue itself. A counterexample is the case
where X(pt ) ✓ {x 2 n | x(1) = 0}, which means that
any two objective functions c1 , c2 6= 0 with c2 (i) = c1 (i)
for i = 2, . . . , n and 0 <   1 are equivalent if
c1 (1), c2 (1) are chosen such that kc1 k1 = kc2 k1 = 1.
Using this construction, we can easily find examples for
which kc1 c2 k1 = 2(1 ), but where the two objective
functions are equivalent in terms of optimal solutions.
While in most applications it is sufficient to be able to produce solutions via the surrogate objectives that are essentially equivalent to those for the true objective, we will
show now that under slightly strengthened assumptions we
can obtain significantly stronger guarantees for the convergence of the solutions: we will show that in the long run
we learn to emulate the true optimal solutions provided that
the problems have unique solutions as we will make precise
now.
We say that the sequence of feasible regions (X(pt ))t
is -stable for ctrue for some
> 0 if for any t 2
n
[T ], c 2
with kck1 = 1, c 6= ctrue and x̄t :=

Proof. We start with the guarantee from theq
proof of TheoPT T
1
rem 3.3: 0  T t=1 ctrue (xt x̄t )  2K lnTn . Now let
P
NT be as above so that 0  T1 t2NT cTtrue (xt x̄t ) 
q
2K lnTn . Observe that
 cTtrue (xt x̄t ) as xt was
optimal for ctrue together
with -stability. We thus obq
tain T1 |NT |  2K
q
2K T ln n .

ln n
T ,

which is equivalent to |NT | 

From the above corollary, we obtain in particular
that in
q
1
ln n
the -stable case we have T |NT |  2K T , i.e. the
average number of times that x̄t deviates from xt tends to
0 in the long run. We hasten to stress, however, that the
convergence implied by this bound can potentially be slow
as it is exponential in the actual encoding length of ctrue ;
this is to be expected given the convergence rates of our
algorithm and online learning algorithms in general.
3.2. Learning Constraints
We will only very briefly address the case of learning constraints due to space limitations. We consider the family of
optimization problems (OPT2(p))p , p 2 P ✓ k , given

Emulating the Expert: Inverse Optimization through Online Learning

by
max c(p)T x
s.t. Ax
x



btrue
0,

where c(p) 2 n is the objective function, A 2 m⇥n is
the constraint matrix and btrue 2 m is the right-hand side.

We assume that the ct ’s are known to both the learner and
the expert. The same can be assumed for A without loss of
generality by standard arguments. The right-hand side btrue
is only known to the expert and to be learned from observing pt as well as an optimal solution xt for OPT2(pt ) in
round t.
The most natural approach for solving this learning problem is to apply Algorithm 1 to the dual of OPT2(pt ):
min bTtrue y
s.t. AT y
y

c(pt )
0,

where y are the dual variables for the linear constraints. In
the dual problem, btrue is the unknown objective function
(btrue 0 without loss of generality), while the constraints
to be optimized over in each round are known – the same
setting as before. It is important to note though that the
learner has to observe the dual optimal solutions yt and
the guarantee will be that the dual regret is tending to 0.
It remains open whether this can be also achieved when
receiving only the primal optimal solutions xt as feedback;
we suspect the answer to be in the negative in general.

4. Applications
We will now sketch two select applications of our framework for learning objective functions. These are the learning of customer preferences from observed purchases and
the learning of travel times in a road network.
Our preliminary computational experiments have been obtained on a Mac Book Pro (2016) with an Intel Core i5 CPU
with two 2.00 GHz cores. We have implemented our framework using python and Gurobi 7.0.1 (Gurobi Optimization,
Inc., 2016).
4.1. Learning Customer Preferences
We consider a market, where different goods G can be
bought by its customers. The prices for the goods can vary
over different days t 2 [T ]. We assume that the goods are
chosen by the customer to maximize utility given their budget constraints. Each sample (pt , xt ) corresponds to a day
t 2 [T ] where pt = (pt0 , ptG ) with pt0 is the budget of
the buyer and pG
t contains the prices ptg for each good g

at time t. The customer solves the following optimization
problem OPT(pt ) on day t:
X
max
ug xg
g2G

s.t.

X

ptg xg



pt0

x

2

{0, 1}|G| ,

g2G

where the utility ug of good g of the customer is unknown
(and kept constant over time).
We consider two different setups: in the first setup, we assume that the goods are divisible, which means that the
condition x 2 {0, 1}|G| is relaxed to x 2 [0, 1]|G| ; this
is the Linear Knapsack Problem. In the second setup, the
goods are not divisible, so that we solve the problem with
the original constraint x 2 {0, 1}|G| as an integer program;
this is the Integer Knapsack Problem.
We generated random instances for our computational results, considering T = 1000 observations for a varying
number of goods n 2 {100, 500, 1000}. The customer’s
unknown utility vector is chosen at random as (arbitrary)
integer numbers from the interval [1, 1000] from a uniform
distribution and then normalized to have `1 -norm 1. The
prices on day t are chosen to be ptg = ug + 100 + rtg ,
where rtg is an integer uniformly chosen at random from
the interval [ 10, 10]. Choosing utilities and weights similar to each other typically leads to harder (integer) knapsack problems, cf. (Pisinger, 2005). The right-hand side
pt0 is then
Pagain an integer drawn uniformly from the interval [1, g2G ptg 1].
Table 1 Errors for Integer Knapsack with n = 1000 items
Error / T
10
100
1000
Average objective error
0.1511 0.0185 0.0036
Average solution error
0.0492 0.0087 0.0013
Average cumulative error 0.2003 0.0272 0.0049

In Table 1, we show the computational results for the Integer Knapsack Problem with n = 1000 items. We report
errors for 10, 100 and 1000 iterations of our algorithm.
The objective error for each round t 2 [T ] is defined by
cTt (x¯t xt ) and describes the deviation between the solution x̄t found by the oracle in that round and the solution
xt observed from the expert as evaluated with our guess for
the objective function ct . Accordingly, the solution error
in each round t is defined as cTtrue (x¯t xt ) and evaluates
the deviation between the two solutions in the true objective function. Together they yield the total error, given by
(ct ctrue )T (x̄t xt ), which is the total deviation between
choosing ct and ctrue in Problem (1) in round t. Each of
these error types is shown in our plots over time, depicting both the error in a given round t and the average error

Emulating the Expert: Inverse Optimization through Online Learning

Figure 1. Linear Knapsack problem
P with n = 100 items over
T = 1000 iterations. We plot T1 Tt=1 (ct ctrue )T (x̄t xt ) over
T on the x-axis in blue. In red we plot the cost (ct ctrue )T (x̄t
xT ) of round t. As can be seen, after few iterations most solutions
reside on the x-axis and only few deviate beyond the average.

over rounds t0 2 [t]. We depict a representative knapsack
instance in Figure 1.
4.2. Learning Travel Times
While the first example explored learning over a temporal
horizon, in this example the various observations t 2 [T ]
arise from different drivers in a road network. More precisely, we consider a resource-constrained shortest path
problem, where we are given a graph G = (V, E), where
drivers have to find cost-minimal s-t-paths subject to a resource or budget constraint. For each arc e 2 E, we denote
the arc length with ae . The observations t 2 [T ] represent drivers in the network and each observation (pt , xt )
consists of pt = (p1t , p2t , p3t ), where p1t is the starting point
and p2t is the ending point of the journey of driver t. It
is assumed that driver t takes the path with the shortest
travel time with respect to the unknown travel times ce with
e 2 E while at the same time being subject to a limit p3t of
total distance that can be traveled. The values of xt indicate the traversed edges of the graph that driver t takes. The
optimization problem OPT(pt ) solved by driver t is then:
X
min
c e xe
e2E

s.t.

X

e2

xe

(v)

X

e2

X

+

xe

=

(v)

8
< 1,
1,
:
0,

if v = p1t
if v = p2t (8v 2 V )
otherwise

ae xe



p3t

x

2

{0, 1}|E| ,

e2E

Figure 2. Resource-constrained shortest path problem on a grid
graph with m = 15 rows and n = 30 columns as described above
for T = 1000 iterations. Total error as in Figure 1. Convergence
is slower here (although with an error that is several orders of
magnitude smaller) as the problem is much more complex. Still,
in most rounds we have an error close to 0.

and we want to learn the values ce for e 2 E corresponding
to the travel time to traverse arc e.
We created instances of the problem based on grid graphs
with m rows and n columns. The unknown driving times
vector of the network and the resource value for each arc
(in our case the length of the arc) were chosen at random
in the same fashion as for the knapsack problems. For each
sample, we chose a random pair of an origin and a destination node. The resource limit for the sample is calculated as
the length of the shortest path between the selected nodes
multiplied by 1.25. In other words: find the fastest path
while driving at most 25% more than length of the shortest
path. See Figure 2 for our result over T = 1000 samples.
Additional computations are included in the Appendix.

5. Final Remarks
In its current form, we explicitly use the optimality of the
observed actions to learn the objective function. While beyond the scope of this paper, it would be interesting to analyse to what extent this optimality requirement can be relaxed to approximate solutions. Clearly, one can simply redefine the underlying feasible regions X(pt ) to correspond
to the approximate feasible regions, however this can lead
to unwanted effects of the summands in our regret bound
not being non-negative anymore if the solutions obtained
for the surrogate objective is better than the approximately
optimal observed solution. Another important question is
to what extent our framework can be extended to the learning of constraints (and objective functions simultaneously).

Emulating the Expert: Inverse Optimization through Online Learning

Acknowledgements
We would like to thank the reviewers for the helpful comments. Research reported in this paper was partially supported by NSF CAREER award CMMI-1452463.

References
Abbasi-Yadkori, Yasin, Pál, Dávid, and Szepevári, Csaba.
Improved algorithms for linear stochastic bandits. In
Conference on Neural Information Processing System
(NIPS), 2011.
Ahuja, Ravindra K. and Orlin, James B. A faster algorithm
for the inverse spanning tree problem. Journal of Algorithms, pp. 177–193, 2000.
Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative weights update method: A meta-algorithm and
applications. Theory of Computing, 8:121–164, 2012.
Aswani, Anil, Shen, Zuo-Jun Max, and Siddiq, Auyon. Inverse optimization with noisy data. Technical report,
University of California, Berkeley, 2016. available at:
https://arxiv.org/abs/1507.03266.
Audibert, Jean-Yves, Bubeck, Sébastien, and Lugosi, Gábor. Regret in online combinatorial optimization. Mathematics of Operations Research, 39(1):31–45, 2013.
Bertsimas, Dimitris and Kallus, Nathan. Pricing from observational data. Technical report, Massachusetts Institute of Technology, 2016. available at:https://
arxiv.org/pdf/1605.02347.
Burton, D. and Toint, Ph. L. On an instance of the inverse
shortest paths problem. Mathematical Programming, 53
(1):45–61, 1992.
Burton, D. and Toint, Ph. L. On the use of an inverse
shortest paths algorithm for recovering linearly correlated costs. Mathematical Programming, 63(1):1–22,
1994.

Dani, Varsha, Hayes, Thomas P., and Kakade, Sham. M.
Stochastic linear optimization under bandit feedback. In
Conference on Learning Theory (COLT), 2008.
den Hertog, Dick and Postek, Krzysztof. Bridging the
gap between predictive and prescriptive analytics –
new optimization methodology needed. Technical report, Tilburg University, Netherlands, 2016. Available
at: http://www.optimization-online.org/
DB_HTML/2016/12/5779.html.
Freund, Yoav and Schapire, Robert E. Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29:79–103, 1997.
Gurobi Optimization, Inc. Gurobi optimizer reference
manual. http://www.gurobi.com, 2016. URL
http://www.gurobi.com.
Hazan, Elad. Introduction to online convex optimization.
Foundations and Trends in Optimization, 2(3–4):157–
325, 2016. doi: 10.1561/2400000013. URL http:
//ocobook.cs.princeton.edu/.
III, Hal Daumé, Khuller, Samir, Purohit, Manish, and
Sanders, Gregory. On correcting inputs: Inverse optimization for online structured prediction. In Proceedings of the IARCS Annual Conference on Foundations of
Software Technology and Theoretical Computer Science
(FSTTCS), 2005.
Iyengar, Garud and Kang, Wanmo. Inverse conic programming with applications. Operations Research Letters, 33
(3):319–330, 2005.
Jadbabaie, Ali, Rakhlin, Alexander, Shahrampour, Shahin,
and Sridharan, Karthik. Online optimization: Competing
with dynamic comparators. In AISTATS, 2015.

Chan, Timothy C. Y., Lee, Taewoo, and Terekhov, Daria.
Goodness of fit in inverse optimization. Technical report, University of Toronto, Canada, 2015. available at:
https://arxiv.org/abs/1511.04650.

Kallus, Nathan and Udell, Madeleine. Learning preferences from assortment choices in a heterogeneous
population.
Technical report, Massechusetts Institute of Technology, 2015. available at: https:
//pdfs.semanticscholar.org/b29d/
026b6776e94a00d1ea15f83518ebbbd14d85.
pdf.

Chen, Xi, Owen, Zachary, Pixton, Clark, and Simchi-Levi,
David. A statistical learning approach to personalization
in revenue management. Technical report, New York
University, 2015.

Kallus, Nathan and Udell, Madeleine.
Dynamic
assortment personalization in high dimensions.
Technical report,
Cornell University,
2016.
https://arxiv.org/pdf/1610.05604.

D. Burton, W. R. Pulleyblank, Ph. L. Toint. Network Optimization, chapter The Inverse Shortest Paths Problem
with Upper Bounds on Shortest Paths Costs, pp. 156–
171. Springer, 1997.

Keshavarz, Arezou, Wang, Yang, and Boyd, Stephen. Imputing a convex objective function. In Proceedings of
the 2011 IEEE International Symposium on Intelligent
Control (ISIC), pp. 613–619, 2011.

Emulating the Expert: Inverse Optimization through Online Learning

Konstantakopoulos, Ionnias C., Ratliff, Lillian J., Jin,
Ming, Spanos, Costas, and Sastry, S. Shankar. Smart
building energy efficiency via social game: A robust
utility learning framework for closing-the-loop. In
2016 1st International Workshop on Science of Smart
City Operations and Platforms Engineering (SCOPE) in
partnership with Global City Teams Challenge (GCTC)
(SCOPE - GCTC), pp. 1–6, 2016.
Li, Jonathan Yu-Meng. Inverse optimization of convex
risk function. Technical report, University of Ottawa,
Canada, 2016. available at: https://arxiv.org/
abs/1607.07099.
Littlestone, Nick and Warmuth, Manfred K. The weighted
majority algorithm. Information and Computation, 108:
212–261, 1994.
Lodi, Andrea.
Big data & mixed-integer (nonlinear) programming.
Presentation, available at:
https://atienergyworkshop.files.
wordpress.com/2015/11/andrealodi.pdf,
2016.
Molloy, Timothy L., Tsai, Dorian, Ford, Jason J., and
Perez, Tristan. Discrete-time inverse optimal control
with partial-state information: A soft-optimality approach with constrained state estimation. In 2016 IEEE
55th Conference on Decision and Control (CDC), pp.
1926–1932, 2016.
Nielsen, Thomas D. and Jensen, Finn V. Learning a decision makers’s utility function from (possibly) inconsistent behavior. Artificial Intelligence, 160:53–78, 2004.
Panchea, Adina M. and Ramdani, Nacim. Towards solving
inverse optimal control in a bounded-error framework.
In 2015 American Control Conference (ACC), pp. 4910–
4915, 2015.
Papadopoulos, Alessandro Vittorio, Bascetta, Luca, and
Ferretti, Gianni. Generation of human walking paths.
Autonomous Robots, 40(1):55–75, 2016.

Ratia, Héctor, Montesano, Luis, and Martinez-Cantin,
Ruben.
On the performance of maximum likelihood inverse reinforcement learning. arXiv preprint
arXiv:1202.1558, 2012.
Ratliff, Lillian J., Dong, Roy, Ohlsson, Henrik, and Sastry,
S. Shankar. Incentive design and utility learning via energy disaggregation. In Proceedings of the 19th World
Congress of the International Federation of Automatic
Control, pp. 3158–3163, 2014.
Sayre, G. A. and Ruan, D. Automatic treatment planning
with convex imputing. Journal of Physics: Conference
Series, 489(1), 2014.
Schaefer, Andrew. Inverse integer programming. Optimization Letters, 3(4):483–489, 2009.
Simchi-Levi, David. OM research: From problem-driven
to data-driven research. Manufacturing & Service Operations Management, 16(1):2–10, 2014.
Sokkalingam, P. T., Ahuja, Ravindra K., and Orlin,
James B. Solving inverse spanning tree problems
through network flow techniques. Operations Research,
47(2):291–298, 1999.
Syed, Umar and Schapire, Robert E. A game-theoretic
approach to apprenticeship learning. In Conference on
Neural Information Processing System (NIPS), 2007.
Taskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and
Guestrin, Carlos. Learning structured prediction models:
A large margin approach. In Proceedings of the International Conference on Machine Learning (ICML), 2005.
Thai, Jérôme and Bayen, Alexandre M. Imputing a
variational inequality function or a convex objective
function: A robust approach.
Journal of Mathematical Analysis and Applications, 2016. to appear,
available at: http://www.sciencedirect.com/
science/article/pii/S0022247X16305340.

Pisinger, David. Where are the hard knapsack problems?
Computers & Operations Research, 32(9):2271–2284,
2005.

Troutt, Marvin D., Tadisina, Suresh K., Sohn, Changsoo,
and Brandyberry, Alan A. Linear programming system
identification. European Journal on Operational Research, 161:663–672, 2005.

Plotkin, Serge A., Shmoys, David B., and Éva Tardos.
Fast approximation algorithms for fractional packing
and covering problems. Mathematics of Operations Research, 20(2):257–301, 1995.

Troutt, Marvin D., Pang, Wan-Kai, and Hung-Huo, Shui.
Behavioral estimation of mathematical programming objective function coefficients. Management Science, 52
(3):422–434, 2006.

Qiang, Sheng and Bayati, Mohsen.
Dynamic
pricing with demand covariates.
Technical report, Stanford University, 2016.
available at:
https://papers.ssrn.com/sol3/papers2.
cfm?abstract_id=2765257.

Troutt, Marvin D., Brandybarry, Alan A., Sohn, Changsoo, and Tadisina, Suresh K. Linear programming system identification: The general nonnegative parameters
case. European Journal on Operational Research, 185:
63–75, 2008.

Emulating the Expert: Inverse Optimization through Online Learning

Troutt, Marvin D, Gwebu, Kholekile L, Wang, Jing, and
Brandyberry, Alan A. Some experiments on subjective
optimisation. International Journal of Operational Research, 12(1):79–103, 2011.
Vovk, Volodimir G. Aggregating strategies. In Conference
on Learning Theory (COLT), 1990.
Yang, Insoon, Zeilinger, Melanie N., and Tomlin, Claire J.
Utility learning model predictive control for personal
electric loads. In 53rd IEEE Conference on Decision
and Control, pp. 4868–4874, 2014.
Zinkevich, Martin. Online convex programming and
generalized infinitesimal gradient ascent. Technical
report, School of Computer Science, Carnegie Mellon
University, 2003. available at: http://www-cgi.
cs.cmu.edu/afs/cs.cmu.edu/Web/People/
maz/publications/techconvex.pdf.

