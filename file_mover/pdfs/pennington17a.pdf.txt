Geometry of Neural Network Loss Surfaces via Random Matrix Theory

Jeffrey Pennington 1 Yasaman Bahri 1

Abstract
Understanding the geometry of neural network
loss surfaces is important for the development of
improved optimization algorithms and for building a theoretical understanding of why deep
learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues
of the Hessian matrix at critical points of varying
energy. We introduce an analytical framework
and a set of tools from random matrix theory that
allow us to compute an approximation of this distribution under a set of simplifying assumptions.
The shape of the spectrum depends strongly on
the energy and another key parameter, œÜ, which
measures the ratio of parameters to data points.
Our analysis predicts and numerical simulations
support that for critical points of small index, the
number of negative eigenvalues scales like the 3/2
power of the energy. We leave as an open problem an explanation for our observation that, in
the context of a certain memorization task, the
energy of minimizers is well-approximated by
the function 21 (1 ‚àí œÜ)2 .

2014; Choromanska et al., 2015; Neyshabur et al., 2015),
architecture design, and generalization (Keskar et al.).
1.1. Related work
There is no shortage of prior work focused on the loss
surfaces of neural networks. Choromanska et al. (2015)
and Dauphin et al. (2014) highlighted the prevalence of
saddle points as dominant critical points that plague optimization, as well as the existence of many local minima
at low loss values. Dauphin et al. (2014) studied the distribution of critical points as a function of the loss value
empirically and found a trend which is qualitatively similar to predictions for random Gaussian landscapes (Bray
& Dean, 2007). Choromanska et al. (2015) argued that the
loss function is well-approximated by a spin-glass model
studied in statistical physics, thereby predicting the existence of local minima at low loss values and saddle points
at high loss values as the network increases in size. Goodfellow et al. observed that loss surfaces arising in practice tend to be smooth and seemingly convex along lowdimensional slices. Subsequent works (Kawaguchi, 2016;
Safran & Shamir, 2016; Freeman & Bruna, 2016) have furthered these and related ideas empirically or analytically,
but it remains safe to say that we have a long way to go
before a full understanding is achieved.

1. Introduction
1.2. Our contributions
Neural networks have witnessed a resurgence in recent
years, with a smorgasbord of architectures and configurations designed to accomplish ever more impressive tasks.
Yet for all the successes won with these methods, we have
managed only a rudimentary understanding of why and in
what contexts they work well. One difficulty in extending our understanding stems from the fact that the neural
network objectives are generically non-convex functions
in high-dimensional parameter spaces, and understanding
their loss surfaces is a challenging task. Nevertheless, an
improved understanding of the loss surface could have a
large impact on optimization (Saxe et al.; Dauphin et al.,
1
Google Brain. Correspondence to: Jeffrey Pennington <jpennin@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

One shortcoming of prior theoretical results is that they are
often derived in contexts far removed from practical neural network settings ‚Äì for example, some work relies on
results for generic random landscapes unrelated to neural
networks, and other work draws on rather tenuous connections to spin-glass models. While there is a lot to be gained
from this type of analysis, it leaves open the possibility that
characteristics of loss surfaces specific to neural networks
may be lost in the more general setting. In this paper, we
focus narrowly on the setting of neural network loss surfaces and propose an analytical framework for studying the
spectral density of the Hessian matrix in this context.
Our bottom-up construction assembles an approximation
of the Hessian in terms of blocks derived from the weights,
the data, and the error signals, all of which we assume to

Geometry of Neural Networks

be random variables1 . From this viewpoint, the Hessian
may be understood as a structured random matrix and we
study its eigenvalues in the context of random matrix theory, using tools from free probability. We focus on singlehidden-layer networks, but in principle the framework can
accommodate any network architecture. After establishing
our methodology, we compute approximations to the Hessian at several levels of refinement. One result is a prediction that for critical points of small index, the index scales
like the energy to the 3/2 power.

2. Preliminaries
Let W (1) ‚àà Rn1 √ón0 and W (2) ‚àà Rn2 √ón1 be weight matrices of a single-hidden-layer network without biases. Denote by x ‚àà Rn0 √óm the input data and by y ‚àà Rn2 √óm the
targets. We will write z (1) = W (1) x for the pre-activations.
We use [¬∑]+ = max(¬∑, 0) to denote the ReLU activation,
which will be our primary focus. The network output is
yÃÇi¬µ =

n1
X

(2)

(1)

Wik [zk¬µ ]+ ,

(1)

k=1

and the residuals are ei¬µ = yÃÇi¬µ ‚àí yi¬µ . We use Latin indices
for features, hidden units, and outputs, and ¬µ to index examples. We consider mean squared error, so that the loss
(or energy) can be written as,
n2 ,m
1 X 2
e .
L = n2  =
2m i,¬µ=1 i¬µ

1
1
‚àÇ yÃÇi¬µ ‚àÇ yÃÇi¬µ
‚â° [JJ T ]Œ±Œ≤ ,
m i,¬µ=1 ‚àÇŒ∏Œ± ‚àÇŒ∏Œ≤
m

(3)

where we have introduced the Jacobian matrix, J, and,
 2

n2 ,m
1 X
‚àÇ yÃÇi¬µ
[H1 ]Œ±Œ≤ ‚â°
ei¬µ
.
(4)
m i,¬µ=1
‚àÇŒ∏Œ± ‚àÇŒ∏Œ≤
We will almost exclusively consider square networks with
n ‚â° n0 = n1 = n2 . We are interested in the limit of
large networks and datasets, and in practice they are typically of the same order of magnitude. A useful charac1

1. The matrices H0 and H1 are freely independent, a
property we discuss in sec. 3.
2. The residuals are i.i.d. normal random variables with
tunable variance governed by , ei¬µ ‚àº N (0, 2). This
assumption allows the gradient to vanish in the large
m limit, specifying our analysis to critical points.
3. The data features are i.i.d. normal random variables.
4. The weights are i.i.d. normal random variables.
We note that normality may not be strictly necessary. We
will discuss these assumptions and their validity in sec. 5.

3. Primer on Random Matrix Theory

where Œ∏Œ± ‚àà {W (1) , W (2) }. It decomposes into two pieces,
H = H0 + H1 , where H0 is positive semi-definite and
where H1 comes from second derivatives and contains all
of the explicit dependence on the residuals. Specifically,
[H0 ]Œ±Œ≤ ‚â°

We will be making a variety of assumptions throughout this
work. We distinguish primary assumptions, which we use
to establish the foundations of our analytical framework,
from secondary assumptions, which we invoke to simplify
computations. To begin with, we establish our primary assumptions:

(2)

The Hessian matrix is the matrix of second derivatives of
2
L
the loss with respect to the parameters, i.e. HŒ±Œ≤ = ‚àÇŒ∏‚àÇŒ± ‚àÇŒ∏
,
Œ≤

nX
2 ,m

terization of the network capacity is the ratio of the number of parameters to the effective number of examples2 ,
œÜ ‚â° 2n2 /mn = 2n/m. As we will see, œÜ is a critical
parameter which governs the shape of the distribution. For
instance, eqn. (3) shows that H0 has the form of a covariance matrix computed from the Jacobian, with œÜ governing
the rank of the result.

Although we additionally assume the random variables are
independent, our framework does not explicitly require this assumption, and in principle it could be relaxed in exchange for
more technical computations.

In this section we highlight a selection of results from the
theory of random matrices that will be useful for our analysis. We only aim to convey the main ideas and do not
attempt a rigorous exposition. For a more thorough introduction, see, e.g., (Tao, 2012).
3.1. Random matrix ensembles
The theory of random matrices is concerned with properties of matrices M whose entries Mij are random variables.
The degree of independence and the manner in which the
Mij are distributed determine the type of random matrix
ensemble to which M belongs. Here we are interested
primarily in two ensembles: the real Wigner ensemble for
which M is symmetric but otherwise the Mij are i.i.d.; and
the real Wishart ensemble for which M = XX T where
Xij are i.i.d.. Moreover, we will restrict our attention to
studying the limiting spectral density of M. For a random
matrix Mn ‚àà Rn√ón , the empirical spectral density is defined as,
n

œÅMn (Œª) =

1X
Œ¥(Œª ‚àí Œªj (Mn )) ,
n j=1

(5)

2
In our context of squared error, each of the n2 targets may be
considered an effective example.

Geometry of Neural Networks

where the Œªj (Mn ), j = 1, . . . , n, denote the n eigenvalues
of Mn , including multiplicity, and Œ¥(z) is the Dirac delta
function centered at z. The limiting spectral density is
defined as the limit of eqn. (5) as n ‚Üí ‚àû, if it exists.

from which œÅ can be recovered using the inversion formula,
1
œÅ(Œª) = ‚àí lim Im G(Œª + i) .
(12)
œÄ ‚Üí0+
3.2.2. T HE R TRANSFORM

For a matrix M of the real Wigner matrix ensemble
whose entries Mij ‚àº N (0, œÉ 2 ), Wigner (1955) computed
its limiting spectral density and found the semi-circle law,
(
‚àö
1
4œÉ 2 ‚àí Œª2 if |Œª| ‚â§ 2œÉ
2
2œÄœÉ
œÅSC (Œª; œÉ, œÜ) =
. (6)
0
otherwise
Similarly, if M = XX T is a real Wishart matrix with
X ‚àà Rn√óp and Xij ‚àº N (0, œÉ 2 /p), then the limiting spectral density can be shown to be the Marchenko-Pastur distribution (MarcÃåenko & Pastur, 1967),
(
œÅ(Œª)
if œÜ < 1
œÅMP (Œª; œÉ, œÜ) =
,
‚àí1
(1 ‚àí œÜ )Œ¥(Œª) + œÅ(Œª) otherwise
(7)
where œÜ = n/p and,
1 p
(Œª ‚àí Œª‚àí )(Œª+ ‚àí Œª)
œÅ(Œª) =
2œÄŒªœÉœÜ
(8)
p 2
Œª¬± = œÉ(1 ¬± œÜ) .
The Wishart matrix M is low rank if œÜ > 1, which explains
the delta function density at 0 in that case. Notice that there
is an eigenvalue gap equal to Œª‚àí , which depends on œÜ.
3.2. Free probability theory
Suppose A and B are two random matrices. Under what
conditions can we use information about their individual
spectra to deduce the spectrum of A + B? One way of
analyzing this question is with free probability theory, and
the answer turns out to be exactly when A and B are freely
independent. Two matrices are freely independent if
Ef1 (A)g1 (B) ¬∑ ¬∑ ¬∑ fk (A)gk (B) = 0

(9)

whenever fi and gi are such that
Efi (A) = 0 ,

Egi (B) = 0 .

(10)

Notice that when k = 1, this is equivalent to the definition of classical independence. Intuitively, the eigenspaces
of two freely independent matrices are in ‚Äúgeneric position‚Äù (Speicher, 2009), i.e. they are not aligned in any special way. Before we can describe how to compute the spectrum of A + B, we must first introduce two new concepts,
the Stieltjes transform and the R transform.
3.2.1. T HE S TIELTJES TRANSFORM
For z ‚àà C \ R the Stieltjes transform G of a probability
distribution œÅ is defined as,
Z
œÅ(t)
G(z) =
dt ,
(11)
z
‚àít
R

Given the Stieltjes transform G of a probability distribution œÅ, the R transform is defined as the solution to the
functional equation,

1
= z.
(13)
R G(z) +
G(z)
The benefit of the R transform is that it linearizes free convolution, in the sense that,
RA+B = RA + RB ,

(14)

if A and B are freely independent. It plays a role in free
probability analogous to that of the log of the Fourier
transform in commutative probability theory.
The prescription for computing the spectrum of A + B
is as follows: 1) Compute the Stieltjes transforms of œÅA
and œÅB ; 2) From the Stieltjes transforms, deduce the R
transforms RA and RB ; 3) From RA+B = RA + RB ,
compute the Stieltjes transform GA+B ; and 4) Invert the
Stieltjes transform to obtain œÅA+B .

4. Warm Up: Wishart plus Wigner
Having established some basic tools from random matrix
theory, let us now turn to applying them to computing the
limiting spectral density of the Hessian of a neural network
at critical points. Recall from above that we can decompose the Hessian into two parts, H = H0 + H1 and that
H0 = JJ T /m. Let us make the secondary assumption
that at critical points, the elements of J and H1 are i.i.d.
normal random variables. In this case, we may take H0 to
be a real Wishart matrix and H1 to be a real Wigner matrix.
We acknowledge that these are strong requirements, and
we will discuss the validity of these and other assumptions
in sec. 5.
4.1. Spectral distribution
We now turn our attention to the spectral distribution of H
and how that distribution evolves as the energy changes.
For this purpose, only the relative scaling between H0 and
H1 is relevant
and we may for simplicity take œÉH0 = 1 and
‚àö
œÉH1 = 2. Then we have,
‚àö
œÅH0 (Œª) = œÅMP (Œª; 1, œÜ), œÅH1 (Œª) = œÅSC (Œª; 2, œÜ) ,
(15)
which can be integrated using eqn. (11) to obtain GH0 and
GH1 . Solving eqn. (13) for the R transforms then gives,
1
RH0 (z) =
, RH1 (z) = 2z .
(16)
1 ‚àí zœÜ

Geometry of Neural Networks

œÅ(Œª)

-2

-1

1.0

1.0

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

0

1

2

3

4

-2

-1

1.4
1.0
0.8
0.6
0.4
0.2

0

Œª

1

2

3

4

-2

-1

0

Œª

(a) œÜ = 1/4

œµ = 0.0
œµ = 0.1
œµ = 0.5
œµ = 1.0

1.2

1

2

3

4

5

Œª

(b) œÜ = 1/2

(c) œÜ = 3/4

Figure 1. Spectral distributions of the Wishart + Wigner approximation of the Hessian for three different ratios of parameters to data
points, œÜ. As the energy  of the critical point increases, the spectrum becomes more semicircular and negative eigenvalues emerge.

We proceed by computing the R transform of H,
RH = RH0 + RH1 =

1
+ 2z ,
1 ‚àí zœÜ

(17)

so that, using eqn. (13), we find that its Stieltjes transform
GH solves the following cubic equation,
2œÜ G3H ‚àí (2 + zœÜ)G2H + (z + œÜ ‚àí 1)GH ‚àí 1 = 0 . (18)
The correct root of this equation is determined by the
asymptotic behavior GH ‚àº 1/z as z ‚Üí ‚àû (Tao, 2012).
From this root, the spectral density can be derived through
the Stieltjes inversion formula, eqn. (12). The result is illustrated in fig. 1. For small , the spectral density resembles
the Marchenko-Pastur distribution, and for small enough
œÜ there is an eigenvalue gap. As  increases past a critical value c , the eigenvalue gap disappears and negative
eigenvalues begin to appear. As  gets large, the spectrum
becomes semicircular.
4.2. Normalized index
Because it measures the number of descent directions3 ,
one quantity that is of importance in optimization and in
the analysis of critical points is the fraction of negative
eigenvalues of the Hessian, or the index of a critical point,
Œ±. Prior work (Dauphin et al., 2014; Choromanska et al.,
2015) has observed that the index of critical points typically grows rapidly with energy, so that critical points with
many descent directions have large loss values. The precise
form of this relationship is important for characterizing the
geometry of loss surfaces, and in our framework it can be
readily computed from the spectral density via integration,
Z 0
Z ‚àû
Œ±(, œÜ) ‚â°
œÅ(Œª; , œÜ) dŒª = 1 ‚àí
œÅ(Œª; , œÜ) dŒª .
‚àí‚àû

0

(19)
Given that the spectral density is defined implicitly through
equation eqn. (18), performing this integration analytically
3

Here we ignore degenerate critical points for simplicity.

is nontrivial. We discuss a method for doing so in the supplementary material. The full result is too long to present
here, but we find that for small Œ±,


  ‚àí c 3/2


Œ±(, œÜ) ‚âà Œ±0 (œÜ) 
,
(20)
c 
where the critical value of ,
c =

1
(1 ‚àí 20œÜ ‚àí 8œÜ2 + (1 + 8œÜ)3/2 ) ,
16

(21)

is the value of the energy below which all critical points are
minimizers. We note that c can be found in a simpler way:
it is the value of  below which eqn. (18) has only real roots
at z = 0, i.e. it is the value of  for which the discriminant
of the polynomial in eqn. (18) vanishes at z = 0. Also, we
observe that c vanishes cubically as œÜ approaches 1,
c ‚âà

2
(1 ‚àí œÜ)3 + O(1 ‚àí œÜ)4 .
27

(22)

The 3/2 scaling in eqn. (20) is the same behavior that was
found in (Bray & Dean, 2007) in the context of the field
theory of Gaussian random functions. As we will see later,
the 3/2 scaling persists in a more refined version of this calculation and in numerical simulations.

5. Analysis of Assumptions
5.1. Universality
There is a wealth of literature establishing that many properties of large random matrices do not depend on the details
of how their entries are distributed, i.e. many results are
universal. For instance, Tao & Vu (2012) show that the
spectrum of Wishart matrices asymptotically approaches
the Marcenko-Pastur law regardless of the distribution of
the individual entries, so long as they are independent, have
mean zero, unit variance, and finite k-th moment for k > 2.
Analogous results can be found for Wigner matrices (Aggarwal). Although we are unaware of any existing analy-

Geometry of Neural Networks

5.2.2. R ESIDUALS ARE I . I . D . RANDOM NORMAL
1.5

H = H0 + H1
H = H0 + QH1QT

œÅ(Œª)

1.0

0.5

2

4

6

8

Œª
Figure 2. Testing the validity of the free independence assumption by comparing the eigenvalue distribution of H0 + H1 (in
blue) and H0 + QH1 QT (in orange) for randomly generated orthogonal Q. The discrepancies are small, providing support for
the assumption. Data is for a trained single-hidden-layer ReLU
autoencoding network with 20 hidden units and no biases on 150
4 √ó 4 downsampled, grayscaled, whitened CIFAR-10 images.

ses relevant for the specific matrices studied here, we believe that our conclusions are likely to exhibit some universal properties ‚Äì for example, as in the Wishart and Wigner
cases, normality is probably not necessary.
On the other hand, most universality results and the tools
we are using from random matrix theory are only exact
in the limit of infinite matrix size. Finite-size corrections
are actually largest for small matrices, which (counterintuitively) means that the most conservative setting in
which to test our results is for small networks. So we
will investigate our assumptions in this regime. We expect
agreement with theory only to improve for larger systems.
5.2. Primary assumptions
In sec. 2 we introduced a number of assumptions we
dubbed primary and we now discuss their validity.
5.2.1. F REE INDEPENDENCE OF H0 AND H1
Our use of free probability relies on the free independence
of H0 and H1 . Generically we may expect some alignment
between the eigenspaces of H0 and H1 so that free independence is violated; however, we find that this violation
is often quite small in practice. To perform this analysis,
it suffices to examine the discrepancy between the distribution of H0 + H1 and that of H0 + QH1 QT , where Q is an
orthogonal random matrix of Haar measure (Chen et al.,
2016). Fig. 2 show minimal discrepancy for a network
trained on autoencoding task; see the supplementary material for further details and additional experiments. More
precise methods for quantifying partial freeness exist (Chen
et al., 2016), but we leave this analysis for future work.

First, we note that ei¬µ ‚àº N (0, 2) is consistent with the
definition of the energy  in eqn. (2). Furthermore, because
the gradient of the loss is proportional to the residuals, it
vanishes in expectation (i.e. as m ‚Üí ‚àû), which specializes
our analysis to critical points. So this assumptions seems
necessary for our analysis. It is also consistent with the priors leading to the choice of the squared error loss function.
Altogether we believe this assumption is fairly mild.
5.2.3. DATA ARE I . I . D . RANDOM NORMAL
This assumption is almost never strictly satisfied, but it is
approximately enforced by common preprocessing methods, such as whitening and random projections.
5.2.4. W EIGHTS ARE I . I . D . RANDOM NORMAL
Although the i.i.d. assumption is clearly violated for a network that has learned any useful information, the weight
distributions of trained networks often appear random, and
sometimes appear normal (see, e.g., fig. S1 of the supplementary material).
5.3. Secondary assumptions
In sec. 4 we introduced two assumptions we dubbed secondary, and we discuss their validity here.
5.3.1. J AND H1 ARE I . I . D . RANDOM NORMAL
Given that J and H1 are constructed from the residuals, the
data, and the weights ‚Äì variables that we assume are i.i.d.
random normal ‚Äì it is not unreasonable to suppose that their
entries satisfy the universality conditions mentioned above.
In fact, if this were the case, it would go a long way to validate the approximations made in sec. 4. As it turns out, the
situation is more complicated: both J and H1 have substructure which violates the independence requirement for
the universality results to apply. We must understand and
take into account this substructure if we wish to improve
our approximation further. We examine one way of doing
so in the next section.

6. Beyond the Simple Case
In sec. 4 we illustrated our techniques by examining the
simple case of Wishart plus Wigner matrices. This analysis
shed light on several qualitative properties of the spectrum
of the Hessian matrix, but, as discussed in the previous section, some of the assumptions were unrealistic. We believe
that it is possible to relax many of these assumptions to produce results more representative of practical networks. In
this section, we take one step in that direction and focus on
the specific case of a single-hidden-layer ReLU network.

Geometry of Neural Networks

6.1. Product Wishart distribution and H1

Probability density H1 over 10 runs, output + noise
phi value = 0.0625

0.30

In the single-hidden-layer ReLU case, we can write H1 as,


0
H12
H1 =
,
H12 T
0

0.20

Probability density H1 over 10 runs, output + noise
phi value = 0.25

0.25
0.15
0.20

0.15

0.10

0.10
0.05
0.05

where its off-diagonal blocks are given by the matrix H12 ,

0.00
‚àí 15

‚àí 10

‚àí5

0

5

10

15

0.00
‚àí 15

‚àí 10

‚àí5

Eigenvalue

[H12 ]ab;cd

1 X
‚àÇ yÃÇi¬µ
‚àÇ
=
ei¬µ
(2)
(1)
m i¬µ
‚àÇW ‚àÇW
cd

ab

m
1 X
(1)
=
ec¬µ Œ¥ad Œ∏(za¬µ
)xb¬µ .
m ¬µ=1

(23)

Here it is understood that the matrix H12 is obtained from
the tensor [H12 ]ab;cd by independently vectorizing the first
two and last two dimensions, Œ¥ad is the Kronecker delta
function, and Œ∏ is the Heaviside theta function. As discussed above, we take the error to be normally distributed,
X (2) (1)

ec¬µ =
Wcj [zj¬µ ]+ ‚àí yc¬µ (x) ‚àº N (0, 2) . (24)
j

We are interested in the situation in which the layer width
n and the number of examples m are large, in which case
(1)
Œ∏(za¬µ ) can be interpreted as a mask that eliminates half of
the terms in the sum over ¬µ. If we reorder the indices so
that the surviving ones appear first, we can write,
m/2

H12

‚âà

¬µÃÇ=1

where we have written eÃÇ and xÃÇ to denote the n√ó m
2 matrices
derived from e and x by removing the vanishing half of
their columns.
Owing to the block off-diagonal structure of H1 , the
squares of its eigenvalues are determined by the eigenvalues of the product of the blocks,
1
In ‚äó (eÃÇxÃÇT )(eÃÇxÃÇT )T .
m2

(25)

The Kronecker product gives an n-fold degeneracy to each
eigenvalue, but the spectral density is the same as that of
1
M ‚â° 2 (eÃÇxÃÇT )(eÃÇxÃÇT )T .
m

(26)

It follows that the spectral density of H1 is related to that
of M ,
œÅH1 (Œª) = |Œª|œÅM (Œª2 ) .

5

10

15

Figure 3. Spectrum of H1 at initialization, using 4 √ó 4 downsampled, grayscaled, whitened CIFAR-10 in a single layer ReLU autoencoder with 16 hidden units. Error signals have been replaced
by noise distributed as N (0, 100), i.e.  = 50. Theoretical prediction (red) matches well. Left œÜ = 1/16, right œÜ = 1/4.

the cavity method (Dupic & Castillo, 2014). As the specific
form of the result is not particularly enlightening, we defer
its presentation to the supplementary material. The result
may also be derived using free probability theory (Muller,
2002; Burda et al., 2010). From either perspective it is possible to derive the the R transform, which reads,
RH1 (z) =

œÜz
.
2 ‚àí œÜ2 z 2

(28)

See fig. 3 for a comparison of our theoretical prediction for
the spectral density to numerical simulations4 .
6.2. H0 and the Wishart assumption

X
1
1
Œ¥ad
ec¬µÃÇ xb¬µÃÇ = In ‚äó eÃÇxÃÇT ,
m
m

H12 H12 T =

0

Eigenvalue

(27)

Notice that M is a Wishart matrix where each factor is itself
a product of real Gaussian random matrices. The spectral
density for matrices of this type has been computed using

Unlike the situation for H1 , to the best of our knowledge
the limiting spectral density of H0 cannot easily be
deduced from known results in the literature. In principle
it can be computed using diagrammatics and the method
of moments (Feinberg & Zee, 1997; Tao, 2012), but this
approach is complicated by serious technical difficulties
‚Äì for example, the matrix has both block structure and
Kronecker structure, and there are nonlinear functions
applied to the elements. Nevertheless, we may make
some progress by focusing our attention on the smallest
eigenvalues of H0 , which we expect to be the most relevant
for computing the smallest eigenvalues of H.
The empirical spectral density of H0 for an autoencoding network is shown in fig. 4. At first glance, this
distribution does not closely resemble the MarchenkoPastur distribution (see, e.g., the  = 0 curve in fig. 1)
owing to its heavy tail and small eigenvalue gap. On the
other hand, we are not concerned with the large eigenvalues, and even though the gap is small, its scaling with
œÜ appears to be well-approximated by Œª‚àí from eqn. (8)
4

As the derivation requires that the error signals are random, in
the simulations we manually overwrite the network‚Äôs error signals
with random noise.

Geometry of Neural Networks
0.045

40
35

0.014

25

spectrum

Value

0.035

30

0.030
0.025
0.020

20

0.010

15

0.012
0.010
0.008

0.015

Gap in

Hist ogram

0.016

0.040

0

5

10

15

20

25

30

35

kth smallest non-zero eigenvalue

k

10

0.006
0.004
0.002

5

0.000

0
0.0

0.5

1.0

1.5

2.0

2.5

0

3.0

1

2

3

4

5

6

7

8

Value of eigenvalue

Figure 4. The spectrum of H0 at initialization of a single layer
ReLU autoencoder with 16 hidden units and 256 4 √ó 4 downsampled, grayscaled, whitened CIFAR-10 images. There are 16
null directions, and the corresponding zero eigenvalues have been
removed. The inset shows the values of the smallest 35 nonzero
eigenvalues. The positive value of the first datapoint reflects the
existence of a nonzero gap.

Figure 5. Evolution of the H0 spectral gap as a function of 1/œÜ,
comparing the Marcenko-Pastur (red) against empirical results for
a 16-hidden unit single-layer ReLU autoencoder at initialization
(black dots, each averaged over 30 independent runs). Dataset
was taken from 4√ó4 downsampled, grayscaled, whitened CIFAR10 images. A single fit parameter, characterizing the variance of
the matrix elements in the Wishart ensemble, is used.

for appropriate œÉ. See fig. 5. This observation suggests
that as a first approximation, it is sensible to continue to
represent the limiting spectral distribution of H0 with the
Marchenko-Pastur distribution.

Despite the apparent pole at œÜ = 1, c actually vanishes
there,
8 2
c ‚âà
œÉ (1 ‚àí œÜ)3 + O(1 ‚àí œÜ)4 .
(33)
27

6.3. Improved approximation to the Hessian
The above analysis motivates us to propose an improved
approximation to RH (z),
RH (z) =

œÉ
œÜz
+
,
1 ‚àí œÉzœÜ 2 ‚àí œÜ2 z 2

(29)

where the first term is the R transform of the MarchenkoPastur distribution with the œÉ parameter restored. As before, an algebraic equation defining the Stieltjes transform
of œÅH can be deduced from this expression,


2 = 2 z ‚àí œÉ(1 ‚àí œÜ) G ‚àí œÜ 2œÉz + (1 ‚àí œÜ) G2H
(30)

‚àí œÜ2 z + œÉ(œÜ ‚àí 2) G3H + zœÉœÜ3 G4H .
Using the same techniques as in sec. 4, we can use this
equation to obtain the function Œ±(, œÜ). We again find the
same 3/2 scaling, i.e.,


  ‚àí c 3/2


Œ±(, œÜ) ‚âà Œ±ÃÉ0 (œÜ) 
.
c 

(31)

Compared with eqn. (20), the coefficient function Œ±ÃÉ0 (œÜ)
differs from Œ±0 (œÜ) and,

œÉ 2 27 ‚àí 18œá ‚àí œá2 + 8œá3/2
,
c =
32œÜ(1 ‚àí œÜ)3

Curiously, for œÉ = 1/2, this is precisely the same behavior
we found for the behavior of c near 1 in the Wishart
plus Wigner approximation in sec. 4. This observation,
combined with the fact that the 3/2 scaling in eqn. (31) is
also what we found in sec. 4, suggest that it is H0 , rather
than H1 , that is governing the behavior near c .

œá = 1+16œÜ‚àí8œÜ2 .
(32)

6.4. Empirical distribution of critical points
We conduct large-scale experiments to examine the distribution of critical points and compare with our theoretical
predictions. Uniformly sampling critical points of varying
energy is a difficult problem. Instead, we take more of a
brute force approach: for each possible value of the index,
we aim to collect many measurements of the energy and
compute the mean value. Because we cannot control the
index of the obtained critical point, we run a very large
number of experiments (‚àº50k) in order to obtain sufficient
data for each value of Œ±. This procedure appears to be a
fairly robust method for inferring the Œ±() curve.
We adopt the following heuristic for finding critical
points. First we optimize the network with standard gradient descent until the loss reaches a random value between
0 and the initial loss. From that point on, we switch
to minimizing a new objective, Jg = |‚àáŒ∏ L|2 , which,
unlike the primary objective, is attracted to saddle points.
Gradient descent on Jg only requires the computation of
Hessian-vector products and can be executed efficiently.

Geometry of Neural Networks
0.30

0.5

0.08

1
(1-œï)
2
1
(1-œï)2
2

0.06

0.25
Œ±

0.20

0.02

0.00
0.36

Œ±

0.4

0.04

0.38

0.40

œµ

0.42

0.44

œï =2/3
œï =1/2
œï =1/3
œï =1/4
œï =1/8
œï =1/16

0.46

0.15
0.10

œµc

Benefit of nonlinearity

0.2

0.1

0.05
0.00
0.0

0.3

0.1

0.2

œµ

0.3

0.4

0.5

(a) Index of critical points versus energy

0.0
0.0

0.2

0.4

0.6

0.8

1.0

œï

(b) Energy of minimizers versus parameters/data points

Figure 6. Empirical observations of the distribution of critical points in single-hidden-layer tanh networks with varying ratios of parameters to data points, œÜ. (a) Each point represents the mean energy of critical points with index Œ±, averaged over ‚àº200 training runs. Solid
lines are best fit curves for small Œ± ‚âà Œ±0 | ‚àí c |3/2 . The good agreement (emphasized in the inset, which shows the behavior for small
Œ±) provides support for our theoretical prediction of the 3/2 scaling. (b) The best fit value of c from (a) versus œÜ. A surprisingly good fit
is obtained with c = 21 (1 ‚àí œÜ)2 . Linear networks obey c = 21 (1 ‚àí œÜ). The difference between the curves shows the benefit obtained
from using a nonlinear activation function.

We discard any run for which the final Jg > 10‚àí6 ;
otherwise we record the final energy and index.
We consider relatively small networks and datasets in
order to run a large number of experiments. We train
single-hidden-layer tanh networks of size n = 16, which
also equals the input and output dimensionality. For each
training run, the data and targets are randomly sampled
from standard normal distributions, which makes this
a kind of memorization task. The results are summarized in fig. 6. We observe that for small Œ±, the scaling
Œ± ‚âà | ‚àí c |3/2 is a good approximation, especially for
smaller œÜ. This agreement with our theoretical predictions
provides support for our analytical framework and for the
validity of our assumptions.
As a byproduct of our experiments, we observe that the
energy of minimizers is well described by a simple function, c = 12 (1 ‚àí œÜ)2 . Curiously, a similar functional form
was derived for linear networks (Advani & Ganguli, 2016),
c = 12 (1 ‚àí œÜ). In both cases, the value at œÜ = 0 and
œÜ = 1 is understood simply: at œÜ = 0, the network has
zero effective capacity and the variance of the target distribution determines the loss; at œÜ = 1, the matrix of hidden
units is no longer rank constrained and can store the entire
input-output map. For intermediate values of œÜ, the fact
that the exponent of (1 ‚àí œÜ) is larger for tanh networks
than for linear networks is the mathematical manifestation
of the nonlinear network‚Äôs better performance for the same
number of parameters. Inspired by these observations and
by the analysis of Zhang et al. (2016), we speculate that
this result may have a simple information-theoretic explanation, but we leave a quantitative analysis to future work.

7. Conclusions
We introduced a new analytical framework for studying
the Hessian matrix of neural networks based on free
probability and random matrix theory. By decomposing
the Hessian into two pieces H = H0 + H1 one can
systematically study the behavior of the spectrum and
associated quantities as a function of the energy  of a
critical point. The approximations invoked are on H0 and
H1 separately, which enables the analysis to move beyond
the simple representation of the Hessian as a member of
the Gaussian Orthogonal Ensemble of random matrices.
We derived explicit predictions for the spectrum and
the index under a set of simplifying assumptions. We
found empirical evidence in support of our prediction
that that small Œ± ‚âà | ‚àí c |3/2 , raising the question of
how universal the 3/2 scaling may be, especially given the
results of (Bray & Dean, 2007). We also showed how
some of our assumptions can be relaxed at the expense of
reduced generality of network architecture and increased
technical calculations. An interesting result of our numerical simulations of a memorization task is that the energy of
minimizers appears to be well-approximated by a simple
function of œÜ. We leave the explanation of this observation
as an open problem for future work.

Acknowledgements
We thank Dylan J. Foster, Justin Gilmer, Daniel HoltmannRice, Ben Poole, Maithra Raghu, Samuel S. Schoenholz,
Jascha Sohl-Dickstein, and Felix X. Yu for useful comments and discussions.

Geometry of Neural Networks

References
Advani, Madhu and Ganguli, Surya. Statistical mechanics
of optimal convex inference in high dimensions. Physical Review X, 6(3):031034, 2016.

Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal,
Jorge, Smelyanskiy, Mikhail, and Tang, Ping Tak Peter. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR 2017. URL
http://arxiv.org/abs/1609.04836.

Aggarwal, Amol.
Bulk universality for generalized
wigner matrices with few moments. arXiv preprint
arXiv:1612.00421.

Laisant, C-A. InteÃÅgration des fonctions inverses. Nouvelles annales de matheÃÅmatiques, journal des candidats
aux eÃÅcoles polytechnique et normale, 5:253‚Äì257, 1905.

Bray, Alan J. and Dean, David S. Statistics of critical
points of gaussian fields on large-dimensional spaces.
Phys. Rev. Lett., 98:150201, Apr 2007. doi: 10.1103/
PhysRevLett.98.150201. URL http://link.aps.
org/doi/10.1103/PhysRevLett.98.150201.

MarcÃåenko, Vladimir A and Pastur, Leonid Andreevich.
Distribution of eigenvalues for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):457,
1967.

Burda, Z, Jarosz, A, Livan, G, Nowak, MA, and Swiech,
A. Eigenvalues and singular values of products of rectangular gaussian random matrices. Physical Review E,
82(6):061114, 2010.
Chen, Jiahao, Van Voorhis, Troy, and Edelman, Alan.
Partial freeness of random matrices. arXiv preprint
arXiv:1204.2257, 2016.
Choromanska, Anna, Henaff, Mikael, Mathieu, MichaeÃàl,
Arous, GeÃÅrard Ben, and LeCun, Yann. The loss surface
of multilayer networks. JMLR, 38, 2015.
Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar,
Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua.
Identifying and attacking the saddle point problem in
high-dimensional non-convex optimization. Advances
in Neural Information Processing Systems 27, pp. 2933‚Äì
2941, 2014.
Dupic, Thomas and Castillo, Isaac PeÃÅrez. Spectral density
of products of wishart dilute random matrices. part i: the
dense case. arXiv preprint arXiv:1401.7802, 2014.

Muller, Ralf R. On the asymptotic eigenvalue distribution
of concatenated vector-valued fading channels. IEEE
Transactions on Information Theory, 48(7):2086‚Äì2091,
2002.
Neyshabur, Behnam, Salakhutdinov, Ruslan, and Srebro,
Nathan. Path-sgd: Path-normalized optimization in deep
neural networks. pp. 2413‚Äì2421, 2015.
Safran, Itay and Shamir, Ohad. On the quality of the initial
basin in overspecified neural networks. JMLR, 48, 2016.
URL http://arxiv.org/abs/1511.04210.
Saxe, Andrew M., McClelland, James L., and Ganguli,
Surya. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. ICLR.
Speicher, Roland. Free probability theory. arXiv preprint
arXiv:0911.0087, 2009.
Tao, T. and Vu, V. Random covariance matrices: universality of local statistics of eigenvalues. Annals of Probability, 40(3):1285‚Äì1315, 2012.
Tao, Terence. Topics in random matrix theory, volume 132.
American Mathematical Society Providence, RI, 2012.

Feinberg, Joshua and Zee, Anthony. Renormalizing rectangles and other topics in random matrix theory. Journal
of statistical physics, 87(3-4):473‚Äì504, 1997.

Wigner, Eugene P. Characteristic vectors of bordered matrices with infinite dimensions. Annals of Mathematics,
pp. 548‚Äì564, 1955.

Freeman, C. Daniel and Bruna, Joan. Topology and geometry of half-rectified network optimization. 2016. URL
http://arxiv.org/abs/1611.01540.

Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals, Oriol. Understanding deep learning requires rethinking generalization. arXiv preprint
arXiv:1611.03530, 2016.

Goodfellow, Ian J., Vinyals, Oriol, and Saxe, Andrew M.
Qualitatively characterizing neural network optimization
problems. ICLR 2015. URL http://arxiv.org/
abs/1412.6544.
Kawaguchi, Kenji. Deep learning without poor local minima. Advances in Neural Information Processing Systems 29, pp. 586‚Äì594, 2016.

