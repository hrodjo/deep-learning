Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo

Matthew D. Hoffman 1

Abstract
Deep latent Gaussian models are powerful
and popular probabilistic models of highdimensional data. These models are almost
always fit using variational expectationmaximization, an approximation to true
maximum-marginal-likelihood estimation. In
this paper, we propose a different approach:
rather than use a variational approximation
(which produces biased gradient signals), we
use Markov chain Monte Carlo (MCMC, which
allows us to trade bias for computation). We
find that our MCMC-based approach has several
advantages: it yields higher held-out likelihoods, produces sharper images, and does not
suffer from the variational overpruning effect.
MCMC’s additional computational overhead
proves to be significant, but not prohibitive.

1. Introduction
Deep latent Gaussian models (DLGMs; Kingma &
Welling, 2014; Rezende et al., 2014) are powerful generative models that can be efficiently fit to very large datasets
of complicated, high-dimensional data. These models assume that the observed data were generated by sampling
some latent variables, feeding them into a deep neural network, and adding some noise to that network’s output.
The central computational challenge in fitting these models
is approximate posterior inference; to make the model better fit an observation x, we need to estimate the posterior
distribution p(z | x) over the latent variables z that generated x. Exact posterior inference is intractable, so these
models are almost always fit with variational inference
(Jordan et al., 1999), which approximates the intractable
posterior with a distribution q from some tractable family.
The choice of family for q matters—more expressive families will better approximate the posterior, leading to better
1

Google, San Francisco, California, USA. Correspondence to:
Matthew D. Hoffman <mhoffman@google.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

learning signals. There has been a flurry of papers recently
proposing more flexible variational families, all of which
show that the quality of the variational approximation matters (Salimans et al., 2015; Rezende & Mohamed, 2015;
Burda et al., 2016; Tran et al., 2016; Sønderby et al., 2016;
Kingma & Salimans, 2016).
The other workhorse of approximate posterior inference is
Markov chain Monte Carlo (MCMC; Neal, 1993), which
sets up and simulates a random process whose stationary
distribution is the posterior of interest. Compared to variational methods, MCMC methods have a reputation for being slow, but accurate. Perhaps surprisingly, there is little
work exploring the use of MCMC to fit DLGMs (Salimans
et al., 2015; Wolf et al., 2016; Han et al., 2017).
In this paper, we explore the advantages of the MCMC approach, and find that, as promised, it allows us to meaningfully trade off bias and computation. By initializing our
chains with a sample from a variational approximation, we
are able to both speed up convergence and guarantee that
our posterior approximation is strictly better than that of
the underlying variational approximation. Some minor additional refinements dramatically speed up the algorithm.
In section 5, we show that the MCMC approach eliminates overpruning and blurriness (two known issues with
variational autoencoders), and achieves good held-out
log-likelihood on the dynamically binarized permutationinvariant MNIST dataset.

2. Background
2.1. Variational Autoencoders and DLGMs
A deep latent Gaussian model (Kingma & Welling, 2014;
Rezende et al., 2014, DLGM; ) assumes the following generative process for each observation x:
1. Sample a vector z ∈ RK variables independently from
the standard normal distribution.
2. Compute a vector-valued nonlinear function gθ (z). g
is typically a deep neural network with some parameters θ.
3. Sample x from some distribution f (g(z)) that takes
the output of g as a parameter.

Learning DLGMs with MCMC

For example, if x is a vector of real-valued numbers, a relatively simple DLGM would assume g(z) =
W max{0, V z + c} + b and xd ∼ N (gd (z), σ); g(z) is the
output of a one-hidden-layer ReLU network, and the model
assumes that g is further corrupted by Gaussian noise with
known variance σ 2 . In general, g could be more complex
(e.g., it might be a deep convolutional network), and different elements of z could be injected at different layers of
computation.
This procedure defines a joint probability distribution
p(z, x) = N (z; 0, I)f (x; gθ (z)).

(1)

We can easily sample from and compute this joint distribution for any given parameters θ, latent vector z, and observation x.
We want to fit the model to data by maximizing the
marginal likelihood of a dataset x1,...,N under the model.
This is an optimization problem; the goal is to choose
1 X
log pθ (xn )
θ N
n
Z
1 X
= arg max
log pθ (z, xn )dz.
θ N
z
n

θ? = arg max

(2)

In VAEs the first expectation Eq [log pθ (x | z)] is usually intractable, since it is the integral of a complicated nonlinear
function. But we can estimate it by Monte Carlo sampling
from q. Usually a single sample suffices. Depending on
h, the KLD term may or may not be tractable to compute
exactly, but again it can be approximated by Monte Carlo
as long as h can be computed.
This architecture has a lot to offer: if the variational distribution q is inexpensive to compute, then we can quickly
and easily estimate and optimize the ELBO using stochastic optimization.
But this speed and ease comes at the price of accuracy.
To the extent that q(z | x) diverges from p(z | x), the
ELBO may be a poor approximation of the true marginal
likelihood, and the VEM estimate of θ may be systematically different than the true maximum-likelihood estimate
(MLE). In particular, there are two ways to adjust pθ to
increase the ELBO: increase the true marginal likelihood
log pθ (x), or make the typical posterior pθ (z | x) close in
KLD to qφ (z | x).
2.2. Variational pruning

Unfortunately, the integral over z is usually intractable.
The typical solution is to apply variational expectationmaximization (VEM; Bishop, 2006), which approximates
the intractable marginal log p(x) with the evidence lower
bound (ELBO) L(φ, θ):
L(φ, θ, x) , Eq [log pθ (x | z)] − KL(qz|x || pz )

generative model and “VAE” to refer to the paired encoder/decoder architecture for inference/generation.

(3)

= log pθ (x) − KL(qz|x || pz|x ) ≤ log pθ (x).
qφ (z | x) is any family of distributions that we choose;
it is indexed by some parameters φ and an observation x.
Eq denotes expectations with respect to q. KL(q || p) denotes the Kullback-Leibler divergence (KLD) between two
distributions q and p. The inequality follows from the nonnegativity of the KLD, and since KL(pz|x || pz|x ) = 0, the
bound is tight when qφ (z | x) = pθ (z | x).
The standard choice in DLGMs is to choose qφ (z | x) ,
h(z; rφ (x)), where rφ (x) is the vector-valued output of an
additional neural network and h(z; r) is a tractable distribution with parameters r. A common form
Q for q is a multivariate Gaussian; when q(z | x) = k q(zk | x), q is
called a “mean-field” approximation (Jordan et al., 1999).
In this framework, sometimes r is called an “encoder” network and g is called a “decoder” network by analogy to
classical neural autoencoders. This analogy suggested the
name “variational autoencoder” (VAE; Kingma & Welling,
2014); in this paper we will use “DLGM” to refer to the

In VAEs this issue tends to manifest itself most dramatically as “pruning”—a phenomenon where the optimizer
severs the connections between most of the latent variables
z and the data (MacKay, 2001). Pruning occurs because of
the KL(qz|x || pz|x ) term in the ELBO. Imagine that we
sever the connections between z1 and x, so that z1 and x
are independent. (This can be easily done by zeroing out
some weights in the neural network gθ (z).) Then
KL(qz|x || pz|x ) = KL(qz1 |x || pz1 ) + KL(qz2:K |x || pz2:K |x ).
(4)
The first KLD term can be set to zero by setting q(z1 |
x) = N (z1 ; 0, 1). To the extent that it is easier for the
optimizer to improve the ELBO by such a strategy, it will
find a (possibly bad) local optimum that uses only a few
latent dimensions. Hoffman & Blei (2015); Theis & Hoffman (2015) found that for classic mixture models and latent
factor models such as latent Dirichlet allocation (Blei et al.,
2003) this sort of pruning was mostly a local optimum
problem that could be remedied by improved variational
approximations or optimization schemes. Bowman et al.
(2016) and Sønderby et al. (2016) found improved local
optima in VAEs by initially ignoring and then gradually introducing the KL(qz|x || pz ) term, and Burda et al. (2016)
found that initializing a mean-field VAE with a model that
had been trained with a more flexible q distribution yielded
less pruning and better local optima. But unlike the results
of Hoffman & Blei (2015) for LDA, Burda et al. (2016)

Learning DLGMs with MCMC

still found significant pruning even with the more flexible
posterior approximation, and that pruning increased when
moving from the flexible approximation to the mean-field
approximation.

What makes the VAE ELBO so susceptible to pruning compared to shallow mixture and factor models? One explanation is that the expressive power of shallow models is
tightly bound to latent dimensionality—e.g., fewer mixture
components makes for a less-powerful mixture model. So
if a shallow model is fit to a large, high-dimensional dataset
drawn from a difficult-to-model population, it may be possible to justify using many latent dimensions, since there is
no other way to explain the observed variation in the data.
By contrast, the expressive power of DLGMs is a function
of both the latent dimensionality and the complexity of the
nonlinear function gθ (z). Even if z is one-dimensional,
a DLGM can in principle approximate arbitrary smooth
densities in RD if g approximates a space-filling curve in
RD and f adds some low-variance noise to g. An example of a one-latent-dimension DLGM approximating a twodimensional uniform distribution is given in figure 1. This
example is contrived, but it could be easily implemented
by a neural network (though it might be hard to fit), and it
demonstrates the decoupling of latent dimensionality from
expressive power.

Samples from p(x)
1.0

0.8

0.8

0.6

0.6
x2

g2 (z)

The results of Burda et al. (2016) suggest that this pruning phenomenon is not solely due to local optima. Adding
another latent variable to the model should make it possible to improve the average true marginal log-likelihood
log pθ (x), but it will also make the typical posterior p(z |
x) more difficult to approximate, increasing the average
KL(qz|x || pz|x ) term. At some point the marginal cost in
KLD will be greater than the marginal benefit in log pθ (x),
even at the global optimum.

Samples from p(g(z))
1.0

0.4

0.4

0.2

0.2

0.0

0.0
0.0 0.2 0.4 0.6 0.8 1.0
g1 (z)

0.0 0.2 0.4 0.6 0.8 1.0
x1

Figure 1. Approximating a 2-d distributionP
with a 1-d DLGM.
n−1
σ(τ (Φ(z)n −
Here z ∼ N (0, 1), g1 (z) =P
(Φ(z) − 1)n + i=0
n−1
1 1
i − 1)), g2 (z) = n ( 2 + i=0 σ(τ (Φ(z)n − i)), and xd ∼
1
1
Uniform(gd (z) − 2n
, gd (z) + 2n
). Φ(·) denotes the standard
normal CDF, and σ(·) denotes the sigmoid function. Here n = 20
and τ = 1000. As n and τ get large, the model converges to a
2-d uniform distribution.

Algorithm 1 Hamiltonian Monte Carlo
Input: previous latent variable z, data x,
step size vector , number of steps L.
Sample r ∼ N (0, I).
Initialize z 0 ← z, r0 ← r.
for l = 1 to L do
Update r0 ← r0 + 2 ◦ ∇z log p(z 0 , x).
Update z 0 ← z 0 +  ◦ r0
Update r0 ← r0 + 2 ◦ ∇z log p(z 0 , x).
end for
Sample u ∼ Uniform(0, 1).
0
,x)
if u < exp{− 12 ||r0 ||2 + 21 ||r||2 } p(z
p(z,x) then
Return z 0
else
Return z
end if

2.3. Markov chain Monte Carlo and Hamiltonian
Monte Carlo
Markov chain Monte Carlo (MCMC; Neal, 1993) is the
classic alternative to variational methods for approximate
posterior inference. MCMC methods proceed by designing
a Markov chain whose stationary distribution is the distribution of interest, then simulating that chain to draw samples from that distribution. For our purposes here, MCMC
methods have many weaknesses compared to variational
methods: they may require many “burn in” iterations to
forget their initial state, successive samples from the chain
may be highly correlated, it is sometimes challenging to
diagnose when a chain has burned in (Gelman & Shirley,
2011), and getting an estimate of marginal probabilities using MCMC is relatively expensive and complicated (Neal,
2001).

MCMC’s great advantage is that it allows us to trade computation for accuracy without limit. We can estimate any
expectation with respect to the posterior arbitrarily precisely if we run the chain long enough. In contrast, even
very powerful variational methods may be limited by the
form of the variational approximation.
A particularly powerful MCMC algorithm is Hamiltonian
Monte Carlo (HMC, sometimes called hybrid Monte Carlo;
Duane et al., 1987; Neal, 1993; 2011). Suppose the goal
is to sample from p(z | x) ∝ p(z, x); HMC augments
this model to p(r, z | x) , N (r; 0, I)p(z | x), where the
“momentum” variable r has the same dimensionality as z.
HMC transforms the problem of posterior sampling into
the problem of simulating the time-evolution of a fictitious
physical system. The algorithm interprets the latent vari-

Learning DLGMs with MCMC

ables z as a position vector, log p(z, x) as a negative potential energy function, and the Gaussian log-likelihood
− 12 ||r||2 − D
2 log 2π as a negative kinetic energy function. The sum of the potential and kinetic energy functions defines a Hamiltonian, which in turn defines a set of
dynamics that are time-reversible, conserve volume, and
conserve energy. Those dynamics can be simulated with
the “leapfrog” integrator, which is also time-reversible and
volume-preserving, and for small enough integration time
steps is approximately energy-conserving over long trajectories (Leimkuhler & Reich, 2004). Since the leapfrog integrator is reversible and volume-preserving, we can use
it to define a deterministic Metropolis proposal. The ac0 0
,z ,x)
ceptance probability of this proposal is min{1, p(r
p(r,z,x) },
which depends only on the difference in energy between
the time-evolved state r0 , z 0 and the original state r, z; if
the simulation is accurate, that difference will be small and
the probability of acceptance will be high.
Algorithm 1 outlines the procedure for a single iteration of
HMC. Each iteration, we resample the momentum r, apply L iterations of the symplectic “leapfrog” integrator, and
then apply a Metropolis correction to accept or reject the
proposal z 0 . Each call requires L calls to the gradient function ∇z log p(z 0 , x)—each leapfrog update begins with the
gradient from the previous update, and if the algorithm is
applied repeatedly then we can cache the initial gradient
∇z log p(z, x).
The algorithm can be used with separate step sizes for each
dimension of z and r (Neal, 2011), which is analogous to
using a diagonal preconditioner in gradient descent. The
optimal step size vector will depend on the problem, but
we can
Qget intuitions from the idealized case where p(z |
x) = k N (zk ; µk , σk ). In this case, the optimal setting is
k ∝ σk ; this allows each dimension to be explored equally
quickly. We will use this intuition below.

3. Practical MCMC for DLGMs
We are interested in optimizing the average marginal loglikelihood
1
N

P

n

log pθ (xn ) =

1
N

P

n

log

R
z

pθ (z, xn )dz.

(5)

Its gradient with respect to θ is
1
N

P

n

R
∇θ log z pθ (z, xn )dz
P R
= N1 n z pθ (z | xn )∇θ log pθ (z, xn )dz, (6)

since this is the gradient of the ELBO when q(z | x) =
p(z | x) and the ELBO is tight (Dempster et al., 1977).
The standard VAE training recipe typically approximates
this expectation by replacing p(z | x) with a more tractable

distribution
q(z | x) and computing a Monte Carlo estimate
P
of N1 n Eq [∇θ log p(z, xn )]:
γ VAE =

1
|S|

P

s∈S

∇θ log pθ (zs , xs ),

(7)

where S is a randomly (or cyclically) chosen set of integers
and zs is drawn from q(z | xs ). That is, the expectation
with respect to q is approximated with a single sample, and
the expectation with respect to the training set is approximated with a minibatch. These stochastic approximations
introduce noise, but no bias.
However, replacing the true posterior p(z | x) with q(z |
x) does introduce bias that cannot be averaged away. We
would like to reduce or eliminate this bias.
A naive approach would be to simply run HMC from some
generic initialization to estimate ∇θ log p(x). However, if
we do not run the chain for long enough, this approach may
actually produce a more biased estimate, since it may not
have had time to forget its initialization. Running it for
“long enough” to compete with a variational approximation
may be quite expensive. Instead, we propose using HMC
to improve on an initial variational approximation.
The core idea is this: initialize an HMC sampler with a
sample from a variational approximation, run it for a small
number of iterations, and use the last sample to get a (hopefully nearly unbiased) estimate of ∇θ log p(x). More formally, we define the refined distribution q 0 as
Z
0
q,L,M
(z | x) , q(z̃ | x)HMC,L,M (z | z̃, x)dz̃, (8)
z̃

where HMC,L,M (z | z̃, x) is the distribution of the last
sample from an M -step HMC chain with step-size vector 
and L leapfrog steps per iteration. In this approach, we substitute q 0 for q when estimating the gradient of log pθ (x).
Regardless of how many HMC refinement steps we run, q 0
is guaranteed to have lower KL divergence to the posterior
p(z | x) than q does (Cover & Thomas, 1991, section 2.9 ).
In fact, in some cases the KLD from q 0 (z | x) to p(z | x)
may go down exponentially fast in M (Ottobre & Pavliotis,
2011). This is an appealing property. The bias in the gradient of the ELBO comes precisely from the KL(qz|x || pz|x )
term, and as that term approaches 0 so too must its gradient
(since 0 is the minimum possible KLD).
This observation suggests that we should still care about
making the initial variational distribution q(z | x) as close
as possible to the posterior p(z | x), since that should reduce the number of HMC steps needed to bring q 0 tolerably
close to the posterior. To that end, we use a standard meanfield inference network approach as proposed by Kingma &
Welling (2014) and Rezende et al. (2014). This inference
network’s parameters φ are trained as usual to maximize

Learning DLGMs with MCMC

Eq [log qpφθ (z,x)
(z|x) ]. But, unlike in VAEs, we will not optimize
θ using this objective.
3.1. Refinements
In practice, two refinements to the approach outlined above
make it possible to achieve good results with far less computation.
Learning a shared shearing matrix to rotate q(z | x).
DLGMs are unidentifiable up to a rotation. For example,
suppose we have two simple DLGMs defined by
p(z) = N (z; 0, I); g(z) = V max{0, W z + b} + c;
p(x | z) = f (x; g(z));
p0 (z 0 ) = N (z 0 ; 0, I); g 0 (z 0 ) = V max{0, W U z 0 + b} + c;
p0 (x | z 0 ) = f (x; g(z 0 )),

(9)

where U is an arbitrary rotation matrix. These models define the same marginal distribution, i.e., p(x) = p0 (x). We
can see this by substituting z , U z 0 ; the rotation does
not change the marginal distribution of z, and it makes
g(z) = g 0 (z 0 ), and therefore p(x | z) = p0 (x | z 0 ).
Such a rotation does, however, rotate the posteriors; that is,
p(z | x) = p0 (U > z | x). This is bad, because mean-field
variational inference is not rotationally invariant—it works
best when the posterior exhibits no correlations. In normal
VAE training, the generative network is optimized to be as
friendly as possible to mean-field variational inference, so
it learns to rotate its weight matrices to break this rotational
symmetry. In our setup, this does not naturally happen; our
whole goal is in a sense to avoid letting information about
the variational approximation leak through to the generative network.
To address this issue, we introduce an extra lowertriangular matrix A in the generative network that can partially correct for such rotations, so that our model is
z ∼ N (0, I);

z 0 , Az;

p(x | z) , f (g(z 0 )).

(10)

A is optimized with φ to maximize the variational ELBO
Eq [log qpφθ (z,x)
(z|x) ]. A is constrained to have all of its diagonal
elements equal to 1 so that it cannot prune out any latent
dimensions. If q(z | x) = N (z; µ(x), diag(σ(x)2 )), then
q(z 0 | x) = N (z 0 ; Aµ(x), Adiag(σ(x)2 )A> ), so adding A
allows q(z 0 | x) to have correlation structure. We found
that adding this correlation structure significantly improves
the quality of the initial mean-field approximation. In conjunction with the per-variable step size technique outlined
below, we found that using this shearing matrix speeds up
the algorithm by 2–3x.
Setting per-variable step sizes and the number of
leapfrog steps. HMC has two critical tuning parameters:

the number of leapfrog steps L, and the step size vector .
As in gradient descent, large step sizes allow for fast
progress, but step sizes that are too large lead to unstable results (and therefore Metropolis rejections). It makes sense
to use smaller step sizes in the dimensions with larger gradients and more oscillatory dynamics.
If L is too small, then the algorithm will make small updates each iteration, leading to random-walk behavior and
slow mixing (Neal, 2011). If L is too large, eventually
the trajectory will turn back and retrace its steps, wasting
computation. Hoffman & Gelman (2014) proposed the noU-turn sampler (NUTS), a variant that automatically determines an appropriate number of steps, but NUTS is a
complicated recursive algorithm that is quite difficult to implement in computational graph languages such as TensorFlow (Abadi et al., 2016).
We adopt a simple heuristic for setting the step sizes and
leapfrog steps based on intuitions from the case where the
posterior is Gaussian with diagonal covariance. In this
case, the continuous-time Hamiltonian dynamics in each
dimension k are sinusoidal with period 2πσk , where σk is
the scale of dimension k. If we use a step size vector 
such that k = 0 σk , then after L steps of leapfrog integration the dynamics in each dimension will have advanced
0L
periods. We therefore set L = d 10 e, so that each
by 2π
HMC iteration simulates approximately (2π)−1 ≈ 0.16 periods. This number is somewhat arbitrary; it is chosen to be
slightly less than 0.25 periods. We found that using larger
integration times sometimes caused undesirable resonances
and aliasing (Neal, 2011).
We tune 0 throughout training to give a reasonable worstcase acceptance rate. In each minibatch, we compute the
average acceptance rate over the M HMC iterations for
each example in the minibatch. If the smallest average acceptance rate is less than 0.25, we decrease 0 by 0.5%;
otherwise we increase 0 by 0.5%. This worst-case procedure ensures that 0 is small enough to allow the chains
for all training examples to mix; simply tuning the average
acceptance rate would not guarantee this. Although this resembles an adaptive MCMC algorithm (Andrieu & Thoms,
2008), note that adaptation is done between MCMC runs,
so the usual caveats and requirements of vanishing adaptation do not apply.
We summarize our HMC-based approach to DLGM training in algorithm 2. Figure 2 sketches a specific training
architecture.
3.2. Evaluating held-out likelihood
A benefit of variational methods over MCMC is that they
immediately provide easy-to-estimate lower bounds on the
marginal likelihood. A relatively reliable (but expensive)

Learning DLGMs with MCMC

Algorithm 2 Hamiltonian Monte Carlo for DLGMs
Input: dataset D, SGD step size schedule γ(t), minibatch size S, initial HMC step size 0 , number of HMC
steps M , number of iterations T .
Randomly initialize the inference network parameters φ,
the generative network parameters θ, and the shearing
matrix A.
for t = 1 to T do
Compute L = d0 (t)−1 e.
Select a minibatch S from the dataset D.
for xs ∈ S do
Sample zs,0 ∼ N (µφ (xs ), σφ (xs )).
pθ (zs,0 ,xs )
.
Compute γφs = ∇φ log N (zs,0 ;µ
φ (xs ),σφ (xs ))
p (z

,x )

s,0 s
θ
s
Compute γA
= ∇A log N (zs,0 ;µ
.
φ (xs ),σφ (xs ))
Compute s = 0 (t)σφ (xs ).
for m = 1 to M do
sample zs,m from HMC(zs,m−1 , xs , s , L).
end for
Compute γθs = ∇θ log pθ (zs,M , xs ).
end for
P
Apply gradient update to φ using γφ = S1 s γφs .
P
Apply gradient update to θ using γθ = S1 Ps γθs .
s
, set
Apply gradient update to A using γA = S1 s γA
Ak,k = 1, Ak,l = 0 for all k ∈ {1, . . . , K} and l > k.
end for

way to use MCMC to estimate marginal likelihoods is annealed importance sampling (AIS; Neal, 2001), and we
follow Wu et al. (2016) in using AIS to evaluate our trained
models on held-out data. We use 20 samples per test example, 1000 annealing steps, and the same HMC step size and
number of leapfrog steps as during training.

4. Comparison to Related Work
The idea of using MCMC to improve variational approximations has come up a number of times (e.g., De Freitas
et al., 2001; Mimno et al., 2012).
Our approach is most closely related to the Hamiltonian
variational inference (HVI) approach proposed by Salimans et al. (2015). HVI also initializes an HMC algorithm
with a sample from an inference network. But HVI learns
to explicitly lower-bound the entropy of the marginal distribution of the final sample from the MCMC chain, making it possible to directly optimize and estimate a bound
on the ELBO obtained by using this marginal distribution
as a variational distribution. By contrast, we make no attempt to estimate that ELBO, and instead optimize our variational parameters φ to maximize the much looser ELBO
defined by the initial variational distribution. This is a relative strength of HVI.

Our approach also has several relative strengths over HVI.
First, it is simpler—HVI requires training an auxiliary inverse inference network to reverse the HMC Markov chain
that approximates the posterior. Second, to the extent that
this inverse network cannot exactly reverse the Markov
chain, the HVI lower bound on the ELBO will not be tight,
reintroducing some bias in the gradient estimates. Finally,
using HVI with Markov chains that involve multiple accept/reject steps is difficult and/or expensive, since the inverse inference network must then learn to infer the binary accept/reject random variables. This last issue is highlighted by our results in section 5, where (like Wolf et al.,
2016) we find that using multiple HMC steps is important.
More recently, Han et al. (2017) proposed using
Metropolis-adjusted Langevin (i.e., HMC with one
leapfrog step) to fit DLGMs in a stochastic EM framework.
Their approach is effective for small datasets, but it is inherently a batch method that involves maintaining a Markov
chain for each example throughout training. This limits its
ability to scale to very large datasets.
There has been a flurry of work in the last few years developing more and more accurate variational approximations
for VAEs (Salimans et al., 2015; Rezende & Mohamed,
2015; Burda et al., 2016; Tran et al., 2016; Sønderby et al.,
2016; Kingma & Salimans, 2016). This work has demonstrated the benefits of using accurate posterior approximations, which originally motivated us to explore using highly
accurate MCMC approximations.

5. Experiments
Below, we compare the results of our proposed method to a
baseline mean-field VAE and to other published results on
the binarized MNIST dataset (LeCun et al., 1998). In all
experiments, we used a simple fully connected architecture
with one stochastic layer of 64 latent variables and two deterministic hidden layers with 1024 hidden units each and
softplus nonlinearities. The architecture is sketched in figure 2. For MNIST, we used 60,000 images for training
and held out 10,000 for testing. The training images were
re-binarized each epoch to prevent overfitting (as done by,
e.g., Burda et al., 2016). All models were optimized using Adam (Kingma & Ba, 2015) with default parameters
and a minibatch size of 250. We trained all models for 500
epochs with a learning rate of 0.001, then for another 100
epochs with a learning rate of 0.0001.
5.1. Held-out likelihoods
We estimate held-out likelihood using annealed importance
sampling (AIS; Neal, 2001). Performance as a function of
number of steps of HMC is summarized in figure 3.
Using more HMC steps during training clearly leads to

Learning DLGMs with MCMC
Performance versus Computation

-82

μ(x), σ(x)
softplus
fc (1024)

HMC

z0

zM

shear (K)

shear (K)

fc (1024)

fc (1024)

-83

Held-out log-likelihood

Normal(0, I)

-84
-85
-86
-87
-88
-89
-90

softplus

softplus

softplus

fc (1024)

fc (1024)

fc (1024)

softplus

softplus

fc (D)

fc (D)

inference
log-loss [φ, A]

generation
log-loss [θ]

x

Figure 2. Architecture used in the experiments. Dotted lines denote weight sharing. “fc (N )” denotes a dense matrix multiply
with output dimension N . “shear (N )” denotes a unit-diagonal
lower-triangular matrix multiply with output dimension N . Parameters to cyan nodes are optimized with respect to the inference
log-loss, parameters to purple nodes are optimized with respect to
the generation log-loss.

better models. Our best log-likelihood of −82.53 compares favorably with other reported values in the literature for permutation-invariant (i.e., non-convolutional) binarized MNIST models (see table 1). These number could
doubtless be improved—we did very little experimentation
with architectural choices. For example, all models in table 1 that outperform ours (and some that don’t) use 2–5
stochastic layers, whereas we use only one.
The held-out likelihood with two HMC steps is very close
to that obtained by Salimans et al. (2015), who used only
one HMC update due to the computational difficulty of using many Metropolis accept/reject steps in their framework.
This suggests that the improved performance of our approach is indeed due to the use of many HMC steps (rather
than due to architecture, hyperparameters, etc.).
The per-minibatch cost of our approach goes up linearly
with M , the number of HMC steps. Training an MNIST
model with M = 20 took about 8 hours on an NVIDIA

-91

0

5

10

15

20

Number of HMC steps M

Figure 3. Held-out likelihood as a function of number of HMC
steps M used during training. More computation leads to better
results, but even a few HMC steps provide a major advantage over
standard VAE training (M = 0).
Table 1. Reported held-out log-likelihoods on dynamically binarized permutation-invariant MNIST, with number of stochastic
layers. HMC-DLGM-2 and HMC-DLGM-20 denote our approach with 2 and 20 HMC steps, respectively. Other approaches
are due to [1] Salimans et al. (2015), [2] Burda et al. (2016), [3]
Tran et al. (2016), [4] Sønderby et al. (2016).
M ODEL

# L AYERS

≤ log p(x)

1
1
1
1
2
1
2
5

−90.14
−85.51
−85.15
−84.78
−82.90
−82.53
−81.90
−81.74

VAE BASELINE
HVI [1]
HMC-DLGM-2
IWAE [2]
IWAE [2]
HMC-DLGM-20
VGP [3]
LVAE [4]

K20 GPU, 21 times longer than with M = 11 . It may be
instructive to compare this number with the cost of training an IWAE (Burda et al., 2016), which scales linearly in
the number of samples used during training—Burda et al.
(2016) used 50 samples in their experiments.
5.2. Pruning and blurriness
In this section, we evaluate some qualitative features of the
MNIST model trained with our HMC-based procedure.
Figure 4 shows some samples from the VAE and from the
DLGM trained with M = 20. The VAE samples are blurry,
while the HMC-trained model generates sharp samples.
Some of the HMC-trained model’s samples look strange,
1
This is less than might be expected, given that each HMC
iteration applies several leapfrog steps (typically 2–3). The cost
of a leapfrog step is less than the cost of computing gradients in a
VAE, since we need not backprop through the inference network.

Learning DLGMs with MCMC

dimensions to estimate
R
E[x|z1:C ] = z0 N (z 0 ; 0, I)E[x|z = (z1:C , z 0 )]dz 0 , (12)
where z 0 is a vector of dimension K − C, and (·, ·) denotes
concatenation. If we first rotate z by V , then z1:C will in
a sense denote the C most-important directions in z-space,
and information that is encoded by the less-important directions zC+1:K will be blurred out in E[x | z1:C ].
Figure 4. Left: Samples from a VAE. Right: Samples from a
DLGM trained with HMC. More examples are in the supplement.

but many look good, which is consistent with the observation of Theis et al. (2016) that maximum-likelihood training places a relatively small penalty on generating many
bad samples (e.g., generating 95% garbage costs only 3
nats of log-likelihood).

C = 10

C = 20

C = 30

C = 40

Log-Singular-Value Spectrum of J

103

C = 50

Singular value

102

C = 64
1

10

Figure 6. Effects of keeping only the first C most important dimensions of z, marginalizing out the remaining dimensions. More
examples are in the supplement.

100

10-1
VAE
HMC
10-2

0

10

20

30

40

50

60

Singular value index

Figure 5. Influence of latent directions on output space.

Following Krishnan & Hoffman (2016), to determine
which directions in z-space are most and least important, we sample 10,000 random vectors z1:10000 from their
N (0, I) marginal distribution and compute the singular
value decomposition of the average Jacobian matrix J of
the expected observation vector with respect to z:
P10000 ∂gθ (z) 
1
>
J , 10000
(11)
i=1
∂z  = U SV .
zi

Figure 5 shows the singular value spectra given by the diagonal of S for the VAE and the HMC-trained DLGM. The
VAE’s spectrum falls off sharply at about 10, showing that
the remaining 118 dimensions are not used to explain the
data. The spectrum for the DLGM trained with HMC falls
off much more gradually.
To get a qualitative idea of what is encoded in which dimensions, we can marginalize out all but the first C latent

Figure 6 displays E[x | z1:C ] for various values of C for
five z vectors2 for the model trained with M = 20. Identity and broad structure is indeed encoded by the lowerorder principal components, but the images remain somewhat blurry until many higher-order components are added.
This suggests that variational pruning is at least partially responsible for some of the blurriness associated with VAEs.

6. Conclusion
We have proposed a practical approach to using HMC to
train DLGMs. This approach yields less-biased gradients
of the true marginal likelihood of the data than mean-field
VAEs, and thereby learns better generative models. It is
more expensive than standard VAE training, but not prohibitively so.

References
Abadi, Martı́n, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Gregory S., Davis, Andy, Dean, Jeffrey, Devin, Matthieu,
2
The z vectors are randomly sampled, but curated due to space
constraints. Uncurated examples are in the supplement.

Learning DLGMs with MCMC

Ghemawat, Sanjay, Goodfellow, Ian J., Harp, Andrew, Irving, Geoffrey, Isard, Michael, Jia, Yangqing,
Józefowicz, Rafal, Kaiser, Lukasz, Kudlur, Manjunath,
Levenberg, Josh, Mané, Dan, Monga, Rajat, Moore,
Sherry, Murray, Derek Gordon, Olah, Chris, Schuster,
Mike, Shlens, Jonathon, Steiner, Benoit, Sutskever, Ilya,
Talwar, Kunal, Tucker, Paul A., Vanhoucke, Vincent,
Vasudevan, Vijay, Viégas, Fernanda B., Vinyals, Oriol,
Warden, Pete, Wattenberg, Martin, Wicke, Martin, Yu,
Yuan, and Zheng, Xiaoqiang. TensorFlow: Large-scale
machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.
Andrieu, Christophe and Thoms, Johannes. A tutorial on
adaptive MCMC. Statistics and Computing, 18(4):343–
373, 2008.
Bishop, C. Pattern Recognition and Machine Learning.
Springer New York., 2006.
Blei, D., Ng, A., and Jordan, M. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–
1022, January 2003.
Bowman, Samuel R, Vilnis, Luke, Vinyals, Oriol, Dai, Andrew M, Jozefowicz, Rafal, and Bengio, Samy. Generating sentences from a continuous space. CoNLL, pp. 10,
2016.
Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan.
Importance weighted autoencoders. In International
Conference on Learning Representations, 2016.
Cover, Thomas M and Thomas, Joy A. Elements of information theory. John Wiley & Sons, 1991.
De Freitas, Nando, Højen-Sørensen, Pedro, Jordan,
Michael I, and Russell, Stuart. Variational MCMC.
In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 120–127. Morgan
Kaufmann Publishers Inc., 2001.
Dempster, A., Laird, N., and Rubin, D. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38,
1977.
Duane, Simon, Kennedy, Anthony D, Pendleton, Brian J,
and Roweth, Duncan. Hybrid monte carlo. Physics letters B, 195(2):216–222, 1987.
Gelman, Andrew and Shirley, Kenneth. Inference from
simulations and monitoring convergence. In Handbook
of Markov chain Monte Carlo, pp. 163–174. Chapman
and Hall/CRC, 2011.
Han, Tian, Lu, Yang, Zhu, Song-Chun, and Wu, Ying Nian.
Alternating back-propagation for generator network. In
AAAI Conference on Artificial Intelligence, 2017.

Hoffman, Matthew D. and Blei, David M. Structured
stochastic variational inference. In International Conference on Artificial Intelligence and Statistics, pp. 361–
369, 2015.
Hoffman, Matthew D. and Gelman, Andrew. The no-u-turn
sampler: adaptively setting path lengths in Hamiltonian
Monte Carlo. Journal of Machine Learning Research,
15(1):1593–1623, 2014.
Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. Introduction to variational methods for graphical models.
Machine Learning, 37:183–233, 1999.
Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. In International conference on
learning representations, 2015.
Kingma, Diederik P and Salimans, Tim. Improving variational inference with inverse autoregressive flow. In Advances in Neural Information Processing Systems, 2016.
Kingma, D.P. and Welling, Max. Auto-encoding variational Bayes. In International Conference on Learning
Representations, 2014.
Krishnan, Rahul G and Hoffman, Matthew D. Inference
& introspection in deep generative models of sparse
data. In NIPS Workshop on Advances in Approximate
Bayesian Inference, 2016.
LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The mnist database of handwritten digits, 1998.
Leimkuhler, Benedict and Reich, Sebastian. Simulating
hamiltonian dynamics, volume 14. Cambridge University Press, 2004.
MacKay, David J. C. Local minima, symmetry-breaking,
and model pruning in variational free energy minimization. Technical report, 2001.
Mimno, D., Hoffman, M., and Blei, D. Sparse stochastic
inference for latent Dirichlet allocation. In International
Conference on Machine Learning, 2012.
Neal, R. Probabilistic inference using Markov chain Monte
Carlo methods. Technical Report CRG-TR-93-1, Department of Computer Science, University of Toronto,
1993.
Neal, Radford M. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.
Neal, Radford M. MCMC using Hamiltonian dynamics. In
Handbook of Markov Chain Monte Carlo, pp. 113–162.
Chapman and Hall/CRC, 2011.

Learning DLGMs with MCMC

Ottobre, M and Pavliotis, GA. Asymptotic analysis for
the generalized Langevin equation. Nonlinearity, 24(5):
1629, 2011.
Rezende, Danilo and Mohamed, Shakir. Variational inference with normalizing flows. In International Conference on Machine Learning, pp. 1530–1538, 2015.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, 2014.
Salimans, Tim, Kingma, Diederik P, Welling, Max, et al.
Markov chain Monte Carlo and variational inference:
Bridging the gap. In International Conference on Machine Learning, volume 37, pp. 1218–1226, 2015.
Sønderby, Casper Kaae, Raiko, Tapani, Maaløe, Lars,
Sønderby, Søren Kaae, and Winther, Ole. Ladder variational autoencoders. In Advances in Neural Information
Processing Systems, pp. 3738–3746, 2016.
Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. In International Conference on Learning Representations, Apr 2016. URL
http://arxiv.org/abs/1511.01844.
Theis, Lucas and Hoffman, Matthew D. A trust-region
method for stochastic variational inference with applications to streaming data. In International Conference
on Machine Learning, pp. 2503–2511, 2015.
Tran, D., Ranganath, R., and Blei, D. The variational Gaussian process. In International Conference on Learning
Representations, 2016.
Wolf, Christopher, Karl, Maximilian, and van der Smagt,
Patrick. Variational inference with Hamiltonian Monte
Carlo. arXiv preprint arXiv:1609.08203, 2016.
Wu, Yuhuai, Burda, Yuri, Salakhutdinov, Ruslan, and
Grosse, Roger.
On the quantitative analysis of
decoder-based generative models.
arXiv preprint
arXiv:1611.04273, 2016.

