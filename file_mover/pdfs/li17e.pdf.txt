Forest-type Regression with General Losses
and Robust Forest
Alexander Hanbo Li 1 Andrew Martin 2

Abstract
This paper introduces a new general framework
for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions.
In particular, when plugged in the squared error
and quantile losses, it will recover the classical
random forest (Breiman, 2001) and quantile random forest (Meinshausen, 2006). We then use robust loss functions to develop more robust foresttype regression algorithms. In the experiments,
we show by simulation and real data that our robust forests are indeed much more insensitive to
outliers, and choosing the right number of nearest
neighbors can quickly improve the generalization
performance of random forest.

1. Introduction
Since its development by Breiman (2001), random forest
has proven to be both accurate and efficient for classification and regression problems. In regression setting, random forest will predict the conditional mean of a response
variable by averaging predictions of a large number of regression trees. Later then, many other machine learning
algorithms were developed upon random forest. Among
them, robust versions of random forest have also been proposed using various methodologies. Besides the sampling
idea (Breiman, 2001) which adds extra randomness, the
other variations are mainly based on two ideas: (1) use
more robust criterion to construct regression trees (Galimberti et al., 2007; Brence & Brown, 2006; Roy & Larocque,
2012); (2) choose more robust aggregation method (Meinshausen, 2006; Roy & Larocque, 2012; Tsymbal et al.,
2006).
Meinshausen (2006) generalized random forest to pre1

University of California at San Diego, San Diego, California, USA 2 Zillow, Seattle, Washington, USA. Correspondence to:
Alexander Hanbo Li <alexanderhanboli@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

dict quantiles by discovering that besides calculating the
weighted mean of the observed response variables, one
could also get information for the weighted distribution of
observed response variables using the sets of local weights
generated by random forest. This method is strongly connected to the adaptive nearest neighbors procedure (Lin &
Jeon, 2006) which we will briefly review in section 1.2.
Different from classical k-NN methods that rely on predefined distance metrics, the dissimilarities generated by
random forest are data dependent and scale-invariant.
Another state-of-the-art algorithm AdaBoost (Freund &
Schapire, 1995; Freund et al., 1996) has been generalized
to be applicable to a large family of loss functions (Friedman, 2001; Mason et al., 1999; Li & Bradic, 2016). Recent
development of more flexible boosting algorithms such as
xgboost (Chen & Guestrin, 2016) have become the go-to
forest estimators with tabular or matrix data. One way in
which recent boosting algorithms have an advantage over
the random forest is the ability to customize the loss function used to reduce the influence of outliers or optimize a
metric more suited to the specific problem other than the
mean squared error.
In this paper, we will propose a general framework for
forest-type regression which can also be applied to a broad
family of loss functions. It is claimed in (Meinshausen,
2006) that quantile random forest is another nonparametric approach which does not minimize an empirical loss.
However, we will show in fact both random forest and
quantile random forest estimators can be re-derived as regression methods using the squared error or quantile loss
respectively in our framework. Inspired by the adaptive
nearest neighbor viewpoint, we explore how random forest
makes predictions using the local weights generated by ensemble of trees, and connect that with locally weighted regression (Fan & Gijbels, 1996; Tibshirani & Hastie, 1987;
Staniswalis, 1989; Newey, 1994; Loader, 2006; Hastie &
Loader, 1993). The intuition is that when predicting the
target value (e.g. E[Y |X = x]) at point x, the observations
closer to x should receive larger weights. Different from
predefining a kernel, random forest assigns the weights
data dependently and adaptively. After we illustrate the relation between random forest and local regression, we will
use random forest weights to design other regression algo-

Forest-type Regression with General Losses and Robust Forest

rithms. By plugging robust loss functions like Huber loss
and Tukey‚Äôs redescending loss, we get forest-type regression methods that are more robust to outliers. Finally, motivated from the truncated squared error loss example, we
will show that decreasing the number of nearest neighbors
in random forest will also immediately improve its generalization performance.
The layout of this paper is as follows. In Section 1.1 and 1.2
we review random forest and adaptive nearest neighbors.
Section 2 introduces the general framework of forest-type
regression. In Section 3 we plug in robust regression loss
functions to get robust forest algorithms. In Section 4 we
motivate from the truncated squared error loss and investigate the importance of choosing right number of nearest
neighbors. Finally, we test our robust forests in Section
5 and show that they are always superior to the traditional
formulation in the presence of outliers in both synthetic and
real data set.
1.1. Random forest
Following the notation of Breiman (2001), let Œ∏ be the random parameter determining how a tree is grown, and data
(X, Y ) ‚àà X √ó Y. For each tree T (Œ∏), let L be the total
number of leaves, and Rl denotes the rectangular subspace
in X corresponding to the l-th leaf. Then for every x ‚àà X ,
there is exactly one leaf l such that x ‚àà Rl . Denote this
leaf by l(x, Œ∏).
For each tree T (Œ∏), the prediction of a new data point
X = x is the average of data values in leaf l(x, Œ∏), that
Pn
is, Yb (x, Œ∏) = j=1 w(Xi , x, Œ∏)Yi , where
w(Xi , x, Œ∏) =

1I{Xi ‚ààRl(x,Œ∏) }
#{j : Xj ‚àà Rl(x,Œ∏) }

.

(1)

Finally, the conditional mean E[Y |X = x] is approximated by the averaged prediction of m trees, Yb (x) =
Pm
m‚àí1 t=1 Yb (x, Œ∏t ). After rearranging the terms, we can
write the prediction of random forest as
Yb (x) =

n
X

w(Xi , x)Yi ,

(2)

i=1

where the averaged weight w(Xi , x) is defined as
1
w(Xi , x) =
m

m
X

1.2. Adaptive nearest neighbors
Lin and Jeon (2006) studies the connection between random forest and adaptive nearest neighbor. They introduced
the so-called potential nearest neighbors (PNN): A sample
point xi is called a k-PNN to a target point x if there exists
a monotone distance metric under which xi is among the k
closest to x among all the sample points.
Therefore, any k-NN method can be viewed as choosing
k points from the k-PNNs according to some monotone
metric. For example, under Euclidean metric, the classical
k-NN algorithm sorts the observations by their Euclidean
distances to the target point and outputs the k closest ones.
This is equivalent to weighting the k-PNNs using inverse
L2 distance.
More interestingly, they prove that those observations with
positive weights (3) all belong to the k-PNNs (Lin & Jeon,
2006). Therefore, random forests is another weighted kPNN method, but it assigns weights to the observations different from any k-NN method under a pre-defined monotonic distance metric. In fact, the random forest weights
are adaptive to the data if the splitting scheme is adaptive.

2. General framework for forest-type
regression
In this section, we generalize the classical random forest to
a general forest-type regression (FTR) framework which is
applicable to a broad family of loss functions. In Section
2.1, we motivate the framework by connecting random forest predictor with locally weighted regression. Then in Section 2.2, we formally propose the new forest-type regression framework. In Section 2.3, we rediscover the quantile
random forest estimator by plugging the quantile loss function into our framework.
2.1. Squared error and random forest
Classical random forest can be understood as an estimator
of conditional mean E[Y |X]. As shown in (2), the estimator Yb (x) is weighted average of all response Yi ‚Äôs. This
special form reminds us of the classical least squares regression, where the estimator is the sample mean. To be
more precise, we rewrite (2) as
n
X

w(Xi , x)(Yi ‚àí Yb (x)) = 0.

(4)

i=1

w(Xi , x, Œ∏t ).

(3)

t=1

From equation (2), the prediction of the conditional expectation E[Y |X = x] is the weighted average of the response
values
Pnof all observations. Furthermore, it is easy to show
that i=1 w(Xi , x) = 1.

Equation (4) is the estimating equation (first order condition) of the locally weighted least squares regression (Ruppert & Wand, 1994):
Yb (x) = argmin
Œª‚ààR

n
X
i=1

w(Xi , x)(Yi ‚àí Œª)2

(5)

Forest-type Regression with General Losses and Robust Forest

In classical local regression, the weight w(Xi , x) serves
as a local metric between the target point x and observation Xi . Intuitively, observations closer to target x should
be given more weights when predicting the response at
x. One common choice of such local metric is kernel
Kh (Xi , x) = K((Xi ‚àí x)/h). For example, the tricube
kernel K(u) = (1 ‚àí |u|3 )3 1I(|u| ‚â§ 1) will ignore the impact of observations outside a window centered at x and increase the weight of an observation when it is getting closer
to x. The form of kernel-type local regression is as follows:
argmin
Œª‚ààR

n
X

2.3. Quantile loss and quantile random forest
Kh (Xi ‚àí x)(Yi ‚àí Œª)2 ,

i=1

The random forest weight w(Xi , x) (3) defines a similar
data dependent metric, which is constructed using the ensemble of regression trees. Using an adaptive splitting
scheme, each tree chooses the most informative predictors
from those at its disposal. The averaging process then assigns positive weights to these training responses, which
are called voting points in (Lin & Jeon, 2006). Hence
via the random forest voting mechanism, those observations close to the target point get assigned positive weights
equivalent to a kernel functionality (Friedman et al., 2001).
2.2. Extension to general loss
Note that the formation (5) is just a special case when using
squared error loss œÜ(a, b) = (a‚àíb)2 . In more general form,
we have the following local regression problem:
Yb (x) = argmin
s‚ààF

uses ensemble of trees to recursively partition the covariate
space X . However, there are many other data dependent
dissimilarity measures that can potentially be used, such as
k-NN, mp -dissimilarity (Aryal et al., 2014), shared nearest neighbors (Jarvis & Patrick, 1973), information-based
similarity (Lin et al., 1998), mass-based dissimilarity (Ting
et al., 2016), etc. And there are many other domain specific
dissimilarity measures. To avoid distraction, we will only
use random forest weights throughout the rest of this paper.

n
X

(6)

where w(Xi , x) is a local weight, F is a family of functions, and œÜ(¬∑) is a general loss. For example, when local
weight is a kernel and F stands for polynomials of a certain degree, it reduces to local polynomial regression (Fan
& Gijbels, 1996). Random forest falls into this framework
with squared error loss, a family of constant functions and
local weights (3) constructed from ensemble of trees.
Algorithm 1 Forest-type regression
Step 1: Calculate local weights w(Xi , x) using ensemble or trees.
Step 2: Choose a loss œÜ(¬∑, ¬∑) and a family F of function.
Then do the locally weighted regression

s‚ààF

YbœÑ (x) = argmin
Œª‚ààR

n
X

w(Xi , x)œÅœÑ (Yi ‚àí Œª),

i=1

we get the corresponding first order condition
w(Xi , x)œÜ(s(Xi ), Yi )

i=1

Yb (x) = argmin

Meinshausen (2006) proposed the quantile random forest
which can extract the information of different quantiles
rather than just predicting the average. It has been shown
that quantile random forest is more robust than the classical random forest (Meinshausen, 2006; Roy & Larocque,
2012). In this section, we show quantile random forest estimator is also a special case of Algorithm 1. It is well
known that the œÑ -th quantile of an (empirical) distribution
is the constant that minimizes the (empirical) risk using œÑ th quantile loss function œÅœÑ (z) = z(œÑ ‚àí 1I{z<0} ) (Koenker,
2005). Now let the loss function in Algorithm 1 be the
quantile loss œÅœÑ (¬∑), F be the family of constant functions,
and w(Xi , x) be random forest weights (3). Solving the
optimization problem

n
X

n
X

w(Xi , x)(œÑ ‚àí 1I {Yi ‚àí YbœÑ (x) < 0}) = 0.

i=1

Recall that

Pn

n
X

i=1

w(Xi , x) = 1, hence, we have

w(Xi , x) 1I {Yi < YbœÑ (x)} = œÑ.

(7)

i=1

The estimator YbœÑ (x) in (7) is exactly the same estimator
proposed in (Meinshausen,
2006). In particular, when œÑ =
Pn
0.5, the equation i=1 w(Xi , x) 1I {Yi < Yb0.5 (x)} = 0.5
will give us the median estimator Yb0.5 (x). Therefore, we
have rediscovered quantile random forest from a totally different point of view as a local regression estimator with
quantile loss function and random forest weights.

w(Xi , x)œÜ(Yi , s(Xi )).

i=1

In Algorithm 1, we summarize the forest-type regression
as a general two-step method. Note that here we only focus on local weights generated by random forest, which

3. Robust forest
From the framework 1, quantile random forest is insensitive to outliers because of the more robust loss function. In
this section, we test our framework on other robust losses
and proposed fixed-point method to solve the estimating

Forest-type Regression with General Losses and Robust Forest

equation. In Section 3.1 we choose the famous robust loss
‚Äì (pseudo) Huber loss, and in Section 3.2, we further investigate a non-convex loss ‚Äì Tukey‚Äôs biweight.
3.1. Huber loss
The Huber loss (Huber et al., 1964)
(
1 2
y
for |y| ‚â§ Œ¥,
HŒ¥ (y) = 2
Œ¥(|y| ‚àí 12 Œ¥) elsewhere
is a well-known loss function used in robust regression.
The penalty acts like squared error loss when the error is
within [‚àíŒ¥, Œ¥] but becomes linear outside this range. In this
way, it will penalize the outliers more lightly but still preserves more efficiency than absolute deviation when data
is concentrated in the center and has light tails (e.g. Normal). By plugging Huber loss into the FTR framework 1,
we get a robust counterpart of random forest. The estimating equation is
n
X

wi (x) sign(Yb (x) ‚àí Yi ) min(Yb (x) ‚àí Yi , Œ¥) = 0.

(8)

i=1

Direct optimization of (8) with local weights is hard, hence
instead we will investigate the pseudo-Huber loss (see Figure 1),
!
r
 y 2
1+
‚àí1
LŒ¥ (y) = Œ¥ 2
Œ¥
which is a smooth approximation of Huber loss (Charbonnier et al., 1997). The estimating equation
n
X



wipH (x) YbpH (x) ‚àí Yi = 0.

(9)

Figure 1. In the first row, we compare squared error loss 12 x2 and
pseudo-Huber loss with different Œ¥. In the second row, we plot
the scaling factor (12) of Huber loss. We observe that as Œ¥ decreases to zero, the Huber loss becomes more linear and flat, and
the scaling factor shrinks more quickly as the input deviates from
zero.

i=1

is very similar to that of square error loss if we define a new
weight
wipH (x) = r
1+

wi (x)
b

YpH (x)‚àíYi
Œ¥

2 .

(10)

Then the (pseudo) Huber estimator can be expressed as
Pn
wipH (x)Yi
YbpH (x) = Pi=1
.
n
pH
i=1 wi (x)

(11)

Informally, the estimator (11) can be viewed as a weighted
average of all the responses Yi ‚Äôs. From (10), we know the
new weight for pseudo-Huber loss has an extra scaling factor
p
‚àí1
1 + (Œ¥ ‚àí1 u)2
(12)

and hence will shrink more to zero whenever Œ¥ ‚àí1 |YbpH (x)‚àí
Yi | is large. The tuning parameter Œ¥ acts like a control of the
level of robustness. A smaller Œ¥ will lead to more shrinkage
on the weights of data that have responses far away from
the estimator.
The estimating equation (9) can be solved by fix-point
method which we propose in Algorithm 2. For notation
simplicity, we will use wi,j to denote w(Xi , xj ), where Xi
is the i-th training point and xj is the j-th testing point. The
convergence to the unique solution (if exists) is guaranteed
by Lemma 1.
Lemma 1. Define
Pn

i=1

KŒ¥ (y) = Pn

i=1

w i Yi
q

y‚àíYi

1+( Œ¥
wi
q
1+(

y‚àíYi
Œ¥

2

)

2

)

,

Forest-type Regression with General Losses and Robust Forest

Algorithm 2 pseudo-Huber loss (Œ¥)
b (0) (xj )},
Input: Test points {xj }m
j=1 , initial guess {Y
n
local weights wi,j , training responses {Yi }i=1 , and error
tolerance 0 .
while  > 0 do
(a) Update the weights
(k)
wi,j = r

1+

wi,j
 b (k‚àí1)
Y

(xj )‚àíYi
Œ¥

2

(b) Update the estimator
Pn
(k)
i=1 wi,j Yi
(k)
b
Y (xj ) = Pn
(k)
i=1 wi,j
(c) Calculate error
m
2
1 X bk
Y (xj ) ‚àí Yb (k‚àí1) (xj )
=
m j=1

(d) k ‚Üê k + 1
end while
Output the pseudo-Huber estimator:
YbpH (xj ) = Yb (k) (xj )

Pn
where i=1 wi = 1. Let K = maxi=1,¬∑¬∑¬∑ ,n |Yi |. Then
Algorithm 2 can be written as Yb (k) (x) = KŒ¥ (Yb (k‚àí1) ),
and converges exponentially to a unique solution as long
as Œ¥ > 2K.

Figure 2. We plot the scaling factor (13) of Tukey‚Äôs biweight.
Compared to Huber scaling factor (see (12)), it has a hard threshold at Œ¥.

of redescending loss whose derivative will vanish to zero
as the input goes outside the interval [‚àíŒ¥, Œ¥]. It is defined
in the following way:
Ô£± 
2
2
Ô£≤
d
for |y| ‚â§ Œ¥,
y 1 ‚àí yŒ¥2
TŒ¥ (y) =
Ô£≥
dy
0
elsewhere.
Similarly, by rearranging the estimating equation, we have
Pn
wtukey (Xi , x)Yi
b
Ytukey (x) = Pi=1
n
tukey (X , x)
i
i=1 w
where

From Lemma 1, we know it is important to standardize the
responses Yi so that Œ¥ will be of the same scale for different
problems. In practice, we observe that one will not need to
choose Œ¥ that satisfies the worst-case condition Œ¥ > K in
order for convergence, but making Œ¥ too small does lead
to slow convergence rate. For assigning the initial guess
Yb (0) , two simplest ways are to either take the random forest
estimator we got or a constant vector equaling to the sample
mean. Throughout the rest of this paper, we will choose the
weights to be random forest weights (3).
3.2. Tukey‚Äôs biweight
Non-convex function has played an important role in the
context of robust regression (Huber, 2011; Hampel et al.,
2011). Unlike convex losses, the penalization on the errors can be bounded and hence the contribution of outliers in the estimating equation will eventually vanish. Our
forest regression framework 1 also incorporates the nonconvex losses which will show through the Tukey‚Äôs biweight function TŒ¥ (¬∑) (Huber, 2011), which is an example

Ô£±
Ô£≤
wtukey (Xi , x) = w(Xi , x) max 1 ‚àí
Ô£≥

Ybtukey ‚àí Yi
Œ¥

with an extra scaling factor (see Figure 2)

 u 2 
max 1 ‚àí
,0 .
Œ¥

!2
,0

Ô£º
Ô£Ω
Ô£æ

(13)

In another word, the final estimator actually only depends on data with responses inside [‚àíŒ¥, Œ¥], and the importance of any data (Xi , Yi ) will be shrinking to zero when
|Ybtukey (x) ‚àí Yi | gets closer to the boundary value Œ¥.

4. Truncated squared loss and nearest
neighbors
In this section, we will further use the framework 1 to investigate truncated squared error loss, and use this example to
motivate the relation between random forest generalization
performance and the number of adaptive nearest neighbors.

Forest-type Regression with General Losses and Robust Forest

Then we can define the k random forest nearest neighbors
(k-RFNN) of x to be {XœÉ(1) , ¬∑ ¬∑ ¬∑ , XœÉ(k) }, k ‚â§ n0 , and get
predictor

4.1. Truncated squared error
For the truncated squared error loss
(
1 2
y for |y| ‚â§ Œ¥,
SŒ¥ (y) = 12 2
elsewhere
2Œ¥

Ybk (x) =

w(X
e œÉ(i) , x)YœÉ(i) ,

(17)

i=1

the corresponding estimating equation is
X
w(Xi , x)(Ybtrunc (x) ‚àí Yi ) = 0.
btrunc (x)‚àíYi |‚â§Œ¥
|Y

If we define a new weight
wtrunc (Xi , x) = w(Xi , x) 1I{|Ybtrunc (x) ‚àí Yi | ‚â§ Œ¥}, (14)
then the estimator for truncated squared loss is
Pn
wtrunc (Xi , x)Yi
.
Ybtrunc (x) = Pi=1
n
trunc (X , x)
i
i=1 w

k
X

(15)

The estimator (15) is like a trimmed version of the random
forest estimator (2). We first sort {Yi }ni=1 and trim off the
responses where |Ybtrunc (x) ‚àí Yi | > Œ¥. Therefore, for any
truncation level Œ¥, the estimator Ybtrunc (x) only depends on
data satisfying |Ybtrunc (x) ‚àí Yi | ‚â§ Œ¥ with the same local
random forest weights (1).

Pk
where w(X
e œÉ(i) , x) = w(XœÉ(i) , x)/ j=1 w(XœÉ(i) , x). In
the numerical experiments (Section 5.3), we will test the
performance of the estimator (17) with different k, and
show that by merely choosing the right number of nearest neighbors, one can largely improve the performance of
classical random forest.
Shi and Horvath (2006) proposed a similar ensemble tree
based nearest neighbor method. In their approach, if the
observations Xi and Xj lie in the same leaf, then the similarity between them is increased by one. At the end, the
similarities are normalized by dividing the total number
of trees in the forest. Therefore,
Pm their weights (similarities) w(Xi , x) will be m‚àí1 t=1 1I{Xi ‚ààRl(x,Œ∏) } contrast
to (3). So different from their approach, for random forest, the similarity between Xi and Xj will be increased by
1/#{p : Xp ‚àà Rl(Xi ,Œ∏) } if they both lie in the same leaf
l(Xi , Œ∏). This means the increment in the similarity also
depends on the number of data points in the leaf.

4.2. Random Forest Nearest Neighbors
In classical random forest, all the data with positive weights
(3) are included when calculating the final estimator Yb (x).
However, from section 4.1, we know in order to achieve
robustness, some of the data should be dropped out of consideration. For example, using the truncated squared error loss, we will only consider the data satisfying |Yi ‚àí
Ybtrunc (x)| ‚â§ Œ¥. In classical random forest, the criterion of tree split is to reduce the mean squared error, then
in most cases, data points inside one terminal node will
tend to have more similar responses. So informally larger
|Ybtrim (x)‚àíYi | will indicate smaller local weight w(Xi , x).
Therefore, instead of solving for (15), we investigate a related estimator
P
w(X ,x)‚â• w(Xi , x)Yi
b
Ywt (x) = P i
(16)
w(Xi ,x)‚â• w(Xi , x)
where  > 0 is a constant in (0, 1). Recall that in (Lin
& Jeon, 2006), they show all the observations with positive weights are considered voting points for random forest
estimator. However, (16) implies that we should drop observations with weights smaller than a threshold in order
for the robustness. More formally, let œÉ be a permutation
such that w(XœÉ(1) , x) ‚â• ¬∑ ¬∑ ¬∑ ‚â• w(XœÉ(n0 ) , x) > 0, then (2)
is equivalent to
Yb (x) =

n0
X
i=1

w(XœÉ(i) , x)YœÉ(i) .

5. Experiments
In this section, we plug in the quantile loss, Huber loss and
Tukey‚Äôs biweight loss into the general forest framework
and compare these algorithms with random forest. Unless
otherwise stated, for both Huber and Tukey forest, the error
tolerance is set to be 10‚àí6 , and every forest is an ensemble
of 1000 trees with maximum terminal node size 10. The
robust parameter Œ¥ are set to be 0.005 and 0.8 for Huber
and Tukey forest, respectively.
5.1. One dimensional toy example
We generate 1000 training data points from a Uniform distribution on [‚àí5, 5] and another 1000 testing points from
the same distribution. The true underlying model is Y =
X 2 + ,  ‚àº N (0, 1). But on the training samples, we
choose 20% of the data and add noise 2T2 to the responses,
where T2 follows t-distribution with degree of freedom 2.
In Figure 3, we plot the true squared curve and different
forest predictions. It is clear that Huber and Tukey forest
achieve competitive robustness as quantile random forest,
and can almost recover the true underlying distribution, but
random forest is largely impacted by the outliers. We also
repeat the experiments for 20 times, and report the average mean squared error (MSE), mean absolute deviation
(MAD) and median absolute percentage error (MAPE) in
Table 1.

Forest-type Regression with General Losses and Robust Forest
Table 2. Comparison of the four methods in the setting (1). The
average MSE is reported in first row, and average MAD in second
row.
MSE

0%

5%

10%

15%

20%

8.19
9.80
9.02
10.56

12.14
11.63
9.86
12.41

20.32
13.30
10.40
18.16

22.61
13.83
10.49
12.34

25.23
14.71
10.88
16.62

MAD

0%

5%

10%

15%

20%

RF
QRF
H UBER
T UKEY

2.10
2.23
2.20
2.37

2.49
2.37
2.28
2.45

2.73
2.66
2.36
2.54

2.89
2.75
2.38
2.52

3.02
2.84
2.43
2.66

RF
QRF
H UBER
T UKEY

Figure 3. One dimensional comparison of random forest, quantile random forest, Huber forest and Tukey forest. All forests are
ensemble of 500 regression trees and the maximum number of
points in terminal nodes is 20.

Table 1. Comparison of random forest (RF), quantile random forest (QRF), Huber forest (Huber) and Tukey forest (Tukey) on one
dimensional example.
M EASURE
MSE
MAD
MAPE

RF

QRF

H UBER

2.56
1.20
0.16

1.88
1.07
0.13

1.85
1.06
0.12

T UKEY
1.82
1.07
0.12

Table 3. Comparison of the four methods in the setting (2).
MSE

0%

5%

10%

15%

20%

9.21
11.47
11.19
12.84

13.00
12.07
12.08
13.09

13.69
12.21
12.15
13.31

14.92
12.29
12.20
14.52

17.78
13.16
12.74
14.60

MAD

0%

5%

10%

15%

20%

RF
QRF
H UBER
T UKEY

1.88
2.06
2.04
2.26

2.19
2.13
2.15
2.34

2.74
2.28
2.17
2.39

2.80
2.32
2.17
2.35

2.83
2.41
2.22
2.41

RF
QRF
H UBER
T UKEY

5.3. Nearest neighbors
5.2. Multivariate example
We generate data from 10 dimensional Normal distribution,
i.e. X ‚àº N10 (~0, Œ£). Then we test out algorithms on following models.
P10
(1) Y = i=1 Xi2 +  and  ‚àº N (0, 1), Œ£ = I.
P10
(2) Y = i=1 Xi2 +  and  ‚àº N (0, 1), Œ£ = Toeplitz(œÅ =
0.7).
Then for each model, we randomly choose Œ∑ proportion of
the training samples and add noise 15T2 where T2 follows
t-distribution with degree of freedom 2. The noise level
Œ∑ ‚àà {0, 0.05, 0.1, 0.15, 0.2}. The results are summarized
in Table 2 and 3. On the clean data, random forest still play
the best, however, Huber forest‚Äôs performance is also competitive and lose less efficiency than QRF and Tukey forest.
On the noisy data, all three robust methods outperform random forest. Among them, Huber forest is most robust and
stable.

In this section, we check how the number of adaptive nearest neighbors k in (17) will have impact on the performance
of k-RFNN. We consider the same two models (1) and
(2), and keep both training sample size and testing sample size to be 1000. The relations between MSE, MAD and
the number of adaptive nearest neighbors are illustrated in
Figure 4. Recall that k-RFNN with all 1000 neighbors is
equivalent to random forest. From the figures, we clearly
observe a kink at k = 15, which is much less than 1000.
5.4. Real data
We take two regression datasets from UCI machine learning repository (Lichman, 2013), and one real estate dataset
from OpenIntro. For each dataset, we randomly choose 2/3
observations for training and the rest for testing. MSE and
MAD are reported by averaging over 20 trials. The results
are presented in Table 4. To further test the robustness, we
then repeat the experiment but add extra T2 noise to 20%

Forest-type Regression with General Losses and Robust Forest
Table 5. Test on real data sets with extra noise.
MSE
CCS
A IRFOIL
A MES (√ó108 )
MAD
CCS
A IRFOIL
A MES (√ó104 )

RF

QRF

H UBER

T UKEY

68.51
18.22
5.77

39.21
10.04
18.20

39.05
14.28
5.28

40.27
16.55
5.39

RF

QRF

H UBER

T UKEY

5.46
3.45
1.64

4.53
2.30
3.23

4.57
3.08
1.47

4.80
3.17
1.55

expect even better performance after carefully tuning the
parameter.

Figure 4. The performance of k-RFNN against the number of
nearest neighbors.

of the standardized training data response variables everytime. The results are in Table 5. Robust forests outperform
random forest in most of the cases except for Ames data
sets, on which quantile random forest behaves poorly.
Table 4. Comparison of the four methods on two UCI repository
datasets: (1) concrete compressive strength (CCS) (Yeh, 1998);
(2) airfoil self-noise (Airfoil); and one OpenIntro dataset: Ames
residential home sales (Ames).
MSE

RF

QRF

H UBER

T UKEY

Besides random forest weights, other data dependent similarities could also be used in Algorithm 1. We could also
design loss functions which optimizes a metric for specific
problems. The fixed-point method could be replaced by
other more efficient algorithms. The framework could be
easily extended to classification problems. All these will
be potential future work.

7. Appendix
7.1. Proof of Lemma 1
Proof. Because Yb (k) (x) = KŒ¥ (Yb (k‚àí1)
 is a fixed )0 which


point method, we only need to show KŒ¥ (y) < 1 in order
for the existence and uniqueness of the solution. Define the
normalized weight
X
n
wi
wi
r
r
w
ei =



 ,
1+

CCS
A IRFOIL
A MES (√ó108 )
MAD
CCS
A IRFOIL
A MES (√ó104 )

37.22
18.22
4.51

34.79
10.04
12.21

32.98
14.28
5.22

34.42
16.55
5.91

RF

QRF

H UBER

T UKEY

4.62
3.45
1.34

4.25
2.30
2.44

4.17
3.08
1.31

4.30
3.17
1.36

6. Conclusion and discussion
The experimental results show that Huber forest, Tukey forest and quantile random forest are all much more robust
than random forest in the presence of outliers. However,
without outliers, Huber forest preserves more efficiency
than the other two robust methods. We did not cross validate the parameter Œ¥ for different noise levels, so one would

we have

Pn

i=1

y‚àíYi
Œ¥

2

i=1

1+

y‚àíYi
Œ¥

2

 0 


w
ei = 1, and KŒ¥ (y)


Ô£∂
Ô£´

X
n
X

 n
y
‚àí
Y
j


Ô£≠
Ô£∏
‚â§ 
w
ei Yi
(1I(i = j) ‚àí w
ej ) 2
2 
Œ¥
+
(y
‚àí
Y
)
j

 i=1
j=1


n
X
|y ‚àí Yi |
‚â§ 2
w
ei |Yi | max
i=1,¬∑¬∑¬∑ ,n Œ¥ 2 + (y ‚àí Yi )2
i=1
=

2

n
X

w
ei |Yi |

i=1

1
mini=1,¬∑¬∑¬∑ ,n

1
max |Yi | .
Œ¥
 0 


Therefore, KŒ¥ (y) <
‚â§



Œ¥2
|y‚àíYi |


+ |y ‚àí Yi |

i=1,¬∑¬∑¬∑ ,n

1
2

if Œ¥ > 2 maxi=1,¬∑¬∑¬∑ ,n |Yi | = 2K.

Forest-type Regression with General Losses and Robust Forest

Acknowledgements
We would like to thank Stan Humphrys and Zillow for supporting this research, as well as three anonymous referees
for their insightful comments. Part of the implementation
in this paper is based on Zillow code library.

References
Aryal, Sunil, Ting, Kai Ming, Haffari, Gholamreza, and
Washio, Takashi. mp-dissimilarity: A data dependent
dissimilarity measure. In Data Mining (ICDM), 2014
IEEE International Conference on, pp. 707‚Äì712. IEEE,
2014.
Breiman, Leo. Random forests. Machine learning, 45(1):
5‚Äì32, 2001.
Brence, MAJ John R and Brown, Donald E. Improving the
robust random forest regression algorithm. Systems and
Information Engineering Technical Papers, Department
of Systems and Information Engineering, University of
Virginia, 2006.
Charbonnier, Pierre, Blanc-FeÃÅraud, Laure, Aubert, Gilles,
and Barlaud, Michel. Deterministic edge-preserving regularization in computed imaging. IEEE Transactions on
image processing, 6(2):298‚Äì311, 1997.

Hampel, Frank R, Ronchetti, Elvezio M, Rousseeuw, Peter J, and Stahel, Werner A. Robust statistics: the approach based on influence functions, volume 114. John
Wiley & Sons, 2011.
Hastie, Trevor and Loader, Clive. Local regression: Automatic kernel carpentry. Statistical Science, pp. 120‚Äì129,
1993.
Huber, Peter J. Robust statistics. Springer, 2011.
Huber, Peter J et al. Robust estimation of a location parameter. The Annals of Mathematical Statistics, 35(1):
73‚Äì101, 1964.
Jarvis, Raymond Austin and Patrick, Edward A. Clustering
using a similarity measure based on shared near neighbors. IEEE Transactions on computers, 100(11):1025‚Äì
1034, 1973.
Koenker, Roger. Quantile regression. Number 38. Cambridge university press, 2005.
Li, Alexander Hanbo and Bradic, Jelena. Boosting in the
presence of outliers: adaptive classification with nonconvex loss functions. Journal of the American Statistical Association, (just-accepted), 2016.
Lichman, M. UCI machine learning repository, 2013. URL
http://archive.ics.uci.edu/ml.

Chen, Tianqi and Guestrin, Carlos. Xgboost: A scalable
tree boosting system. In Proceedings of the 22Nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785‚Äì794. ACM, 2016.

Lin, Dekang et al. An information-theoretic definition of
similarity. In ICML, volume 98, pp. 296‚Äì304. Citeseer,
1998.

Fan, Jianqing and Gijbels, Irene. Local polynomial modelling and its applications: monographs on statistics and
applied probability 66, volume 66. CRC Press, 1996.

Lin, Yi and Jeon, Yongho. Random forests and adaptive
nearest neighbors. Journal of the American Statistical
Association, 101(474):578‚Äì590, 2006.

Freund, Yoav and Schapire, Robert E. A desicion-theoretic
generalization of on-line learning and an application to
boosting. In European conference on computational
learning theory, pp. 23‚Äì37. Springer, 1995.

Loader, Clive. Local regression and likelihood. Springer
Science & Business Media, 2006.

Freund, Yoav, Schapire, Robert E, et al. Experiments with
a new boosting algorithm. In icml, volume 96, pp. 148‚Äì
156, 1996.

Mason, Llew, Baxter, Jonathan, Bartlett, Peter L, and
Frean, Marcus R. Boosting algorithms as gradient descent. In NIPS, pp. 512‚Äì518, 1999.
Meinshausen, Nicolai. Quantile regression forests. Journal
of Machine Learning Research, 7(Jun):983‚Äì999, 2006.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics Springer, Berlin, 2001.

Newey, Whitney K. Kernel estimation of partial means and
a general variance estimator. Econometric Theory, 10
(02):1‚Äì21, 1994.

Friedman, Jerome H. Greedy function approximation: a
gradient boosting machine. Annals of statistics, pp.
1189‚Äì1232, 2001.

Roy, Marie-HeÃÅleÃÄne and Larocque, Denis. Robustness of
random forests for regression. Journal of Nonparametric
Statistics, 24(4):993‚Äì1006, 2012.

Galimberti, Giuliano, Pillati, Marilena, and Soffritti,
Gabriele. Robust regression trees based on m-estimators.
Statistica, 67(2):173‚Äì190, 2007.

Ruppert, David and Wand, Matthew P. Multivariate locally
weighted least squares regression. The annals of statistics, pp. 1346‚Äì1370, 1994.

Forest-type Regression with General Losses and Robust Forest

Shi, Tao and Horvath, Steve. Unsupervised learning with
random forest predictors. Journal of Computational and
Graphical Statistics, 15(1):118‚Äì138, 2006.
Staniswalis, Joan G. The kernel estimate of a regression function in likelihood-based models. Journal of
the American Statistical Association, 84(405):276‚Äì283,
1989.
Tibshirani, Robert and Hastie, Trevor. Local likelihood estimation. Journal of the American Statistical Association, 82(398):559‚Äì567, 1987.
Ting, Kai Ming, Zhu, Ye, Carman, Mark, Zhu, Yue,
and Zhou, Zhi-Hua. Overcoming key weaknesses of
distance-based neighbourhood methods using a data
dependent dissimilarity measure. In Proceedings of
the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 1205‚Äì1214.
ACM, 2016.
Tsymbal, Alexey, Pechenizkiy, Mykola, and Cunningham,
PaÃÅdraig. Dynamic integration with random forests. In
European conference on machine learning, pp. 801‚Äì808.
Springer, 2006.
Yeh, I-C. Modeling of strength of high-performance concrete using artificial neural networks. Cement and Concrete research, 28(12):1797‚Äì1808, 1998.

