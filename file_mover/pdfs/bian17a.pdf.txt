Guarantees for Greedy Maximization of
Non-submodular Functions with Applications
Andrew An Bian 1 Joachim M. Buhmann 1 Andreas Krause 1 Sebastian Tschiatschek 1

Abstract

where V = {v1 , . . . , vn } is the ground set. Specifically,
in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized. This problem arises naturally in domains where performing experiments is costly. In sparse modeling, the task
is to identify sparse representations of signals, enabling interpretability and robustness in high-dimensional statistical
problems—properties that are crucial in modern data analysis.

We investigate the performance of the standard
G REEDY algorithm for cardinality constrained
maximization of non-submodular nondecreasing
set functions. While there are strong theoretical
guarantees on the performance of G REEDY for
maximizing submodular functions, there are few
guarantees for non-submodular ones. However,
G REEDY enjoys strong empirical performance
for many important non-submodular functions,
e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our
guarantees are characterized by a combination
of the (generalized) curvature ↵ and the submodularity ratio . In particular, we prove that
G REEDY enjoys a tight approximation guarantee
of ↵1 (1 e ↵ ) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important
real-world objectives, including the Bayesian Aoptimality objective, the determinantal function
of a square submatrix and certain linear programs
with combinatorial constraints. We experimentally validate our theoretical findings for both
synthetic and real-world applications.

Frequently, the standard G REEDY algorithm (Alg. 1) is
used to (approximately) solve (P). For the case that F (S)
Algorithm 1: The G REEDY Algorithm
Input: Ground set V, set function F : 2V ! R+ , budget K
S0
;
for t = 1, . . . , K do
v⇤
arg maxv2V\S t 1 F (S t 1 [ {v}) F (S t 1 )
t
S
S t 1 [ {v ⇤ }
Output: S K

1. Introduction
Many important problems, such as experimental design
and sparse modeling, are naturally formulated as a subset
selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e.,
max

S✓V,|S|K

F (S),

(P)

1

Department of Computer Science, ETH Zurich, Zurich,
Switzerland. Correspondence to: Joachim M. Buhmann <jbuhmann@inf.ethz.ch>, Andreas Krause <krausea@ethz.ch>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

is a monotone nondecreasing submodular set function1 ,
the G REEDY algorithm enjoys the multiplicative approximation guarantee of (1 1/e) (Nemhauser et al., 1978;
Vondrák, 2008; Krause & Golovin, 2014). This constant
factor can be improved by refining the characterization of
the objective using the curvature (Conforti & Cornuéjols,
1984; Vondrák, 2010; Iyer et al., 2013), which informally
quantifies how close a submodular function is to being
modular (i.e., F (S) and F (S) are submodular).
However, for many applications, including experimental
design and sparse Gaussian processes (Lawrence et al.,
2003), F (S) is in general not submodular (Krause et al.,
2008) and the above guarantee does not hold. In practice,
however, the standard G REEDY algorithm often achieves
very good performance on these applications, e.g., in subset selection with the R2 (squared multiple correlation) ob1

F (·) is monotone nondecreasing if 8A ✓ V, v 2 V, F (A [
{v}) F (A). F (·) is submodular iff it satisfies the diminishing
returns property F (A [ {v}) F (A)
F (B [ {v}) F (B)
for all A ✓ B ✓ V \ {v}. Assume wlog. that F (·) is normalized,
i.e., F (;) = 0.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

jective (Das & Kempe, 2011). To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set
function is to being submodular.
Another important class of non-submodular set functions
comes as the auxiliary function when optimizing a continuous function f (x) s.t. combinatorial constraints, i.e.,
minx2C,supp(x)2I f (x), where supp(x) := {i | xi 6= 0}
is the support set of x, C is a convex set, and I is the
independent sets of the combinatorial structure. One of
the most popular ways to solve this problem is to use
the G REEDY algorithm to maximize the auxiliary function
F (S) := maxx2C,supp(x)✓S f (x). This setting covers
various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation
(Das & Kempe, 2008; Krause & Cevher, 2010), sparse
recovery (Candes et al., 2006), sparse M-estimation (Jain
et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler
et al., 2016). Recently, Elenberg et al. (2016) proved that if
f (x) has L-restricted smoothness and m-restricted strong
convexity, then the submodularity ratio of F (S) is lower
bounded by m/L. This result significantly enlarges the domain where the G REEDY algorithm can be applied.
In this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant
factor approximation guarantees of the G REEDY algorithm.
Our guarantees allow us to better characterize the empirical success of applying G REEDY on a significantly larger
class of non-submodular functions. Furthermore, we bound
these characteristics for important applications, rendering
the usage of G REEDY a principled choice rather than a mere
heuristic. Our main contributions are:
- We prove the first tight constant-factor approximation guarantees for G REEDY on maximizing nonsubmodular nondecreasing set functions s.t. a cardinality constraint, characterized by a novel combination of
the (generalized) notions of submodularity ratio and
curvature ↵.
- By theoretically bounding parameters ( , ↵) for several
important objectives, including Bayesian A-optimality
in experimental design, the determinantal function of a
square submatrix and maximization of LPs with combinatorial constraints, our theory implies the first guarantees for them.
- Lastly, we experimentally validate our theory on several real-world applications. It is worth noting that
for the Bayesian A-optimality objective, G REEDY generates comparable solutions as the classically used
semidefinite programming (SDP) based method, but is
usually two orders of magnitude faster.

Notation. We use boldface letters, e.g., x, to represent vectors, and capital boldface letters, e.g., A, to denote matrices. xi is the ith entry of the vector x. We refer to
V = {v1 , ..., vn } as the ground set. We use f (·) to denote
a continuous function, and F (·) to represent a set function.
supp(x) := {i 2 V | xi 6= 0} is the support set of the
vector x, and [n] := {1, ..., n} for an integer n
1. We
denote the marginal gain of a set ⌦ ✓ V in context of a set
S ✓ V as ⇢⌦ (S) := F (⌦ [ S) F (S). For v 2 V, we use
the shorthand ⇢v (S) for ⇢{v} (S).

2. Submodularity Ratio and Curvature
In this section we provide the submodularity ratio and
curvature for general, not necessarily submodular functions2 , they are natural extensions of the classical ones. Let
S 0 = ;, S t = {j1 , ..., jt }, t = 1, ..., K be the successive
sets chosen by G REEDY. For brevity, let ⇢t := ⇢jt (S t 1 )
be the marginal gain of G REEDY in step t.
Definition 1 (Submodularity ratio (Das & Kempe, 2011)).
The submodularity ratio of a non-negative set function F (·)
is the largest scalar s.t.
X
⇢! (S)
⇢⌦ (S), 8 ⌦, S ✓ V.
!2⌦\S

The greedy submodularity ratio is the largest scalar G s.t.
X
G
⇢! (S t )
⇢⌦ (S t ), 8|⌦| = K, t = 0, . . . , K 1.
!2⌦\S t

It is easy to see that G
. The submodularity ratio
measures to what extent F (·) has submodular properties.
We make the following observations:
Remark 1. For a nondecreasing function F (·), it holds a)
, G 2 [0, 1]; b) F (·) is submodular iff = 1.

Definition 2 (Generalized curvature). The curvature of a
non-negative function F (·) is the smallest scalar ↵ s.t.
⇢i (S \ {i} [ ⌦)

(1

8 ⌦, S ✓ V, i 2 S\⌦.

↵)⇢i (S \ {i}),

The greedy curvature is the smallest scalar ↵G
⇢ji (S i
2

1

[ ⌦)

(1

↵G )⇢ji (S i

8 ⌦ : |⌦| = K, i : ji 2 S

K 1

\⌦.

1

0 s.t.
),

Curvature is commonly defined for submodular functions.
Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions. We show in Appendix C the
details of these notions and the relations to ours. Additionally,
we prove in Remark 3 of Appendix C.2 that our combination of
curvature and submodularity ratio is more expressive than that
of Sviridenko et al. (2013) in characterizing the maximization of
problem (P) using standard G REEDY.

When K = n or 1, S K 1 \⌦ = ;, it is natural to define
↵G = 0. It is easy to observe that ↵G  ↵. Note that the
classical total curvature is ↵total := 1 mini2V ⇢i (V\{i})
⇢i (;) .
Remark 2. For a nondecreasing function F (·), it holds:
a) ↵, ↵G 2 [0, 1]; b) F (·) is supermodular iff ↵ = 0; c) If
F (·) is submodular, then ↵G  ↵ = ↵total .
So for a submodular function, our notion of curvature is
consistent with ↵total . Notably, ↵G usually characterizes the
problem better than ↵total , as will be validated in Section 5.

3. Approximation Guarantee
We present approximation guarantee of G REEDY in Theorem 1. Note that both versions of the submodularity ratio
and curvature apply in the proof. For brevity, we use
and ↵ to refer to any of these versions in the sequel. In
Section 3.3 we prove tightness of the approximation guarantees. All omitted proofs are given in Appendix B.
Theorem 1. Let F (·) be a non-negative nondecreasing set
function with submodularity ratio 2 [0, 1] and curvature
↵ 2 [0, 1]. The G REEDY algorithm enjoys the following
approximation guarantee for solving problem (P):
"
✓
◆K #
1
K ↵
K
F (S )
1
F (⌦⇤ )
↵
K
1
(1
↵

e

↵

)F (⌦⇤ ),

(1)

where ⌦⇤ is the optimal solution of (P) and S K the output
of the G REEDY algorithm.3
3.1. Interpreting Theorem 1
Before proving the theorem, we want to give the reader an
intuition of the results and show how our results recover
and extend several classical guarantees for the G REEDY algorithm. For the case ↵ = 0 (i.e., F (·) is supermodular),
the approximation guarantee is lim ↵1 (1 e ↵ ) = ,
↵!0
which gives the first guarantee of greedily maximizing
a nondecreasing supermodular function with bounded .
When = 1, (i.e., F (·) is submodular), we recover the
guarantee of ↵ 1 (1 e ↵ ) (Conforti & Cornuéjols, 1984).
For the case ↵ = 1, we have a guarantee of (1 e )
(Das & Kempe, 2011). For the case ↵ = 1, = 1, we
recover the classical guarantee of (1 1/e) (Nemhauser
et al., 1978). We plot the constant-factor approximation
guarantees for different values of and ↵ in Fig. 1. One
interesting phenomenon is that and ↵ play different roles:
Looking at = 0, the approximation factor is always 0, independent of the value ↵ takes. In contrast, for ↵ = 0, the
3

For the setting that G REEDY is allowed to pick more than K
elements, e.g., pick K 0 > K elements, our theory can be easily
0
0
extended to show that F (S K ) ↵ 1 (1 e ↵ K /K )F (⌦⇤ ).

approx. guarantee

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

1

ar
ul

od
m

0.5
0
1

1

0.5

0.5
0

Figure 1: Approximation guarantee ↵1 (1 e ↵ ). The blue
cross marks the classical (1 1/e)-guarantee of G REEDY.
The red line illustrates the influence of the curvature on the
guarantees for submodular functions, and the black line illustrates the influence of on the guarantees for the worstcase curvature ↵ = 1. The green line is the guarantees for
K-cardinality constrained supermodular maximization.
approximation guarantee is (1 e ). This can be interpreted as the curvature boosting the guarantees.
3.2. Proof of Theorem 1
The high-level proof framework is based on Conforti &
Cornuéjols (1984) (where they derive the approximation
guarantee for maximizing a nondecreasing submodular
function with bounded curvature). However, adapting
the proof to non-submodular functions requires several
changes detailed in Section 6.
Proof overview. Let us denote all problem instances
of maximizing a non-negative nondecreasing function
F (·) s.t. K-cardinality constraint (max|S|K F (S)) to be
PK,↵, , where F (·) is parametrized by submodularity ratio and curvature ↵. Let P⌦⇤ ,S K 2 PK,↵, denote those
problem instances with optimal solution ⌦⇤ and greedy solution S K . We group all problem instances PK,↵, according to the set ⌦⇤ \ S K := {l1 = jm1 , l2 = jm2 , . . . , ls =
jms }, where jm1 , . . . , jms are consistent with the order of
greedy selection. Let us denote the problem instances with
⌦⇤ \ S K = {l1 , . . . , ls } as the group PK,↵, ({l1 , . . . , ls }).
The main idea of the proof is to investigate the worst-case
approximation ratio of each group of the problem instances
PK,↵, ({l1 , . . . , ls }), 8{l1 , . . . , ls } ✓ S K . We do this by
constructing LPs based on the properties of the problem instances. By studying the structures of these LPs, we will
prove that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \ S K = ;. Thus the desired
approximation guarantee corresponds to the worst-case approximation ratio of PK,↵, (;).

The proof. When = 0 or F (⌦⇤ ) = 0, (1) holds naturally.
In the following, let 2 (0, 1] and F (⌦⇤ ) > 0. First, we
present Lemma 1, which will be used to construct the LPs.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Lemma 1. For any ⌦ ✓ V with |⌦| = K and any t 2
{0, . . . , K 1}, let wt := |S t \ ⌦|. It holds that
X
X
↵
⇢i +
⇢i + 1 (K wt )⇢t+1 F (⌦).
i:ji 2S t \⌦

i:ji 2S t \⌦

We now specify the constructing of the LPs: For any problem instance P⌦⇤ ,S K 2 PK,↵, ({l1 , . . . , ls }), we know
PK
that F (S K ) = i=1 ⇢i (telescoping sum). Hence, the apK
P ⇢i
)
proximation ratio is FF(S
=
, which we denote
(⌦
P⇤ ) ⇢i i F (⌦⇤ )
⇢i
as R({l1 , . . . , ls }) = i F (⌦⇤ ) . Define xi := F (⌦
⇤) , i 2
[K]. Since F is nondecreasing, xi 0. Plugging ⌦ = ⌦⇤
into Lemma 1, and considering t = 0, . . . , K 1, we have
in total K constraints over the variables xi , which constitute the constraints of the LP. So the worst-case approximation ratio of the group PK,↵, ({l1 , . . . , ls }) is:
R({l1 , . . . , ls }) = min
row (0)
row (1)
..
.
row (l1 1)
row (l2 1)
row (q = lr )
..
.
row (ls 1)
..
.
row (K 1)

2K

6↵
6
6.
6.
6.
6
6↵
6
6↵
6
6↵
6
6.
6.
6.
6
6↵
6
6 ..
4.
↵

XK

i=1

xi , s.t. xi

0 and,

After that the change of the LP objective is,
LP

Claim 2. For all K
1, 1  r  q < K, it holds that

0,
8
2
(0,
1].
Equality is achieved when r = q
LP
and = 1.
Therefore we reach the contradiction that x⇤ is an optimal
solution of the constructed LP.
Given Lemma 2, we prove in the following Lemma, which
states that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \ S K = ;.
Lemma 3. For all {l1 , . . . ,ls } ✓ S K , it holds that
⇣
⌘K
K ↵
R({l1 , . . . , ls }) R(;) = ↵1 1
.
K

So the greedy solution has objective F (S K )
⇣
⌘K
K ↵
1
1
1
F (⌦⇤ )
e ↵ )F (⌦⇤ ).
↵
K
↵ (1
3

..

.
···
···
···
···
···

0

K

1
1
..
.
1
..
.
1

✏ + ✏q+1 + ✏q+2 + . . . + ✏K .

One can prove that the LP objective decreases:

K

..
.
↵
↵
↵
..
.
↵
..
.
↵

=

K

1

1
..
.
1
..
.
1

K

..
.
↵
..
.
↵

r

..

.
···

K

s+1

..
.
1
(2)

···

The following Lemma presents the key structure of the constructed LPs, which will be used to deduce the relation between the LPs of different problem instance groups.
Lemma 2. Assume that the optimal solution of the con⇤
K
structed LP is x⇤ 2 RK
1. For
+ and that s = |⌦ \ S |
⇤
⇤
all 1  r  s it holds that xq  xq+1 , where q = lr .
Proof sketch of Lemma 2. Assume by virture of creating a
contradiction that x⇤q > x⇤q+1 . We can always create a
⇤
new feasible solution y ⇤ 2 RK
+ by decreasing xq by some
⇤
⇤
✏ > 0, while increasing all the xq+1 to xK by some proper
values, s.t. y ⇤ has smaller LP objective value. Specifically, we define y ⇤ as: for k = 1, . . . , q 1, yk⇤ := x⇤k ;
yq⇤ := x⇤q ✏; for k = q + 1, . . . , K, yk⇤ := x⇤k + ✏k where
✏k s are defined recursively as: ✏q+1 = ✏ K r , and

..

.
···

2 x 3
1
7
7 6 x2 7
7 6
7 6 .. 7
7 6 . 7
7
7 6
7 6 xl1 7
7
7 6
7 6 xl2 7
7
7 6
7 · 6xq+1 7
7
7 6
7 6 .. 7
7 6 . 7
7
7 6
7 6 xls 7
7
7 6
7 4 .. 7
5
7
.
5
xK
s

21 3
61 7
6.7
6.7
6.7
6 7
61 7
61 7
6 7
61 7
6 7
6.7
6.7
6.7
61 7
6 7
6.7
4.5
.
1

3.3. Tightness Result
We demonstrate that the
approximation guarantee in
Theorem 1 is tight, i.e., for
every submodularity ratio
and every curvature ↵,
there exist set functions that
achieve the bound exactly.

Assume the ground set V contains the elements in S :=
{j1 , . . . , jK } and the elements in ⌦ := {!1 , . . . , !K }
(S \ ⌦ = ;) and n 2K dummy elements. The objective
function we are going to construct will not depend on these
dummy elements, i.e., the objective value of a set does not
change if dummy elements are removed from or added to
that set. Consequently, the dummy elements will not affect
the submodularity ratio and the curvature. For the constants
↵ 2 [0, 1], 2 (0, 1], we define the objective function as,
K

F (T ) :=

f (|⌦ \ T |)
1
K

where ⇠i :=
1
K
K 1 x.

1
K

⇣

K

↵
K

⌘i

↵

X

i:ji 2S\T

1

⇠i +

X

⇠i , (3)

i:ji 2S\T

, i 2 [K]; f (x) =

1
1 2
K 1 x +

1.

Note that f (x) is convex nondecreasing over
[0, K], and that f (0) = 0, f (1) = 1, f (K) = K/ . It
is clear that F (;) = 0 and F (·) is monotone nondecreasing. The following lemma shows that it is generally nonsubmodular and non-supermodular.

Claim 1. a) The new solution y ⇤
0; b) All of the constraints in (2) are still feasible for y ⇤ .

Lemma 4. For the objective in (3): a) When ↵ = 0, it is
supermodular; b) When = 1, it is submodular; c) F (T )
has submodularity ratio and curvature ↵.

✏q+1+u = ✏q+u

K

r
K

u+1
r u

,1  u  K

q

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Considering the problem of max|T |K F (T ), we claim
that the G REEDY algorithm may output S. This can be
proved by induction. One can see that ⇢j1 (;) = ⇠1 =
⇢!1 (;), so G REEDY can choose j1 in the first step. Assume
in step t 1 G REEDY has chosen S t 1 = {j1 , . . . , jt 1 },
one can verify that the marginal gains coincide, i.e.,
⇢jt (S t 1 ) = ⇠t = ⇢!t (S t 1 ). However, the optimal solution is actually ⌦ with function value as F (⌦) = 1 . So the

⇣
⌘K
F (S)
K ↵
1
approximation ratio is F (⌦) = ↵ 1
, which
K
matches our approximation guarantee in Theorem 1.

4. Applications
We consider several important real-world applications and
their corresponding objective functions. We show that the
submodularity ratio and the curvature of these functions
can be bounded and, hence, the approximation guarantees
from our theoretical results are applicable. All the omitted
proofs are provided in Appendix D.
4.1. Bayesian A-optimality in Experimental Design
In Bayesian experimental design (Chaloner & Verdinelli,
1995), the goal is to select a set of experiments to perform
s.t. some statistical criterion is optimized, e.g., the variance of certain parameter estimates is minimized. Krause
et al. (2008) investigated several criteria for this purpose,
amongst others the Bayesian A-optimality criterion. This
criterion is used to maximally reduce the variance in the
posterior distribution over the parameters. In general, the
criterion is not submodular as shown in Krause et al. (2008,
Section 8.4).
Formally, assume there are n experimental stimuli
{x1 , . . . , xn }, each xi 2 Rd , which constitute the data
matrix X 2 Rd⇥n . Let us arrange a set S ✓ V of stimuli as
a matrix XS := [xv1 , . . . , xvs ] 2 Rd⇥|S| . Let ✓ 2 Rd be
the parameter vector in the linear model yS = X>
S ✓ + w,
where w is the Gaussian noise with zero mean and variance
2
, i.e., w ⇠ N (0, 2 I), and yS is the vector of dependent
variables. Suppose the prior takes the form of an isotropic
Gaussian, i.e., ✓ ⇠ N (0, ⇤ 1 ), ⇤ = 2 I. Then,

 2
1
1
yS
I + X>
XS X>
S⇤
S⇤
⇠ N (0, ⌃), ⌃ =
.
1
1
✓
⇤ XS
⇤
This implies that ⌃✓|yS = (⇤ +
optimality objective is defined as,
FA (S) := tr(⌃✓ )
= tr(⇤

1

)

2

XS X>
S)

1

. The A-

tr(⌃✓|yS )
tr((⇤ +

(4)
2

XS X>
S)

1

).

The following Proposition gives bounds on the submodularity ratio and curvature of (4).

Proposition 1. Assume normalized stimuli, i.e., kxi k =
1, 8i 2 V. Let the spectral norm of X be kXk.4
Then, a) The objective in (4) is monotone nondecreasing. b) Its submodularity ratio can be lower bounded
2
by kXk2 ( 2 + 2 kXk2 ) , and its curvature ↵ can be upper
bounded by 1

2

kXk2 (

2+

2 kXk2 )

.

4.2. The Determinantal Function
The determinantal function of a square submatrix is widely
used in many areas, e.g., in determinantal point processes
(Kulesza & Taskar, 2012) and active set selection for sparse
Gaussian processes. Monotone nondecreasing determinantal functions appear in the second problem. Assume ⌃ is
the covariance matrix parameterized by a positive definite
kernel. In the Informative Vector Machine (Lawrence et al.,
2003), the information gain of a subset of points S ✓ V is
1
2 log F (S), where
F (S) := det(I +

2

(5)

⌃S ),

where
is the noise variance in the Gaussian process
model, ⌃S is the square submatrix with both its rows and
columns indexed by S. Although log F (S) is submodular, F (S) is in general not submodular. The approximation
guarantee of G REEDY for maximizing log F (S) does not
translate to a guarantee for maximizing F (S). The following Proposition characterizes (5).
Proposition 2. a) F (S) in (5) is supermodular, its curva2
ture is 0; b) Let the eigenvalues of A := I +
⌃ be
·
·
·
>
1.
The
greedy
submodularity
ratio
of
1
n
K( n 1)
Q
F (S) can be lower bounded by ( K
.
) 1
j=1

j

4.3. LPs with Combinatorial Constraints

LPs with combinatorial constraints appear frequently in
practice. Consider the following example: Suppose that
V is the set of all products a company can produce. Given
budget constraints on the raw materials needed, companies
consider the LP maxx2P hd, xi, where d is the vector of
profits for the individual products and where P is a polytope representing the continuous constraints. The above
LP can be used to assess the profit maximizing production
plan. Usually the company needs to consider combinatorial constraints as well. For instance, the company has at
most K production lines, thus they have to select a subset
of K products to produce. Often this kind of problems can
be formalized as maxx2P,supp(x)2I hd, xi, where I is the
independent set of the combinatorial structure. Hence, a
natural auxiliary set function is,
F (S) := maxsupp(x)✓S, x2P hd, xi, 8S ✓ V.
4

By Weyl’s inequality, a naive upper bound is kXk 

(6)
p

n.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

1.01
1

1

0.99

Objective/OPT

Objective/OPT

0.98
0.97
0.96
Greedy
SDP

0.95
2

4.4. More Applications

0.99

0.95

Parameters

Objective/OPT

1

0.98
Greedy
SDP

0.97

0.9
total
G

0.85

G

0.96

0.8
6

8

10

12

8

10

14

2

4

K

(a) Objective values/OPT

6

8

10

12

K

(b) Parameters

Figure 2: Results on the Boston Housing data.

5. Experimental Results
We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for
several applications. Since it is too time consuming to calculate the full versions of ↵ and using exhaustive search,
we only calculated the greedy versions (↵G , G ). All averaged results are from 20 repeated experiments. Source
code is available at https://github.com/bianan/
non-submodular-max.5 More results are put in Appendix H.
5.1. Bayesian Experimental Design
We considered the Bayesian A-optimality objective for
both synthetic and real-world data. In all experiments, we
normalized the data points to have unit `2 -norm.

0.96
Greedy
SDP

12

2

4

6

8

10

12

K
1

0.8

0.8

0.6
0.4

total

0.2

G

G

0.6
0.4

total

0.2

G

G

0
2

1

4

6

0.97

1

0

Many real-world applications can benefit from the theory in
this work, for instance: subset selection using the R2 objective, sparse modeling and the budget allocation problem
with combinatorial constraints. Details on these applications are deferred to Appendix G.

2

4

0.98

K

Parameters

Proposition 3. a) F (S) in (6) is a normalized nondecreasing set function. b) With regular non-degenerancy assumptions (details in Appendix D.3.2), its submodularity ratio
can be lower bounded by 0 > 0.

0.99

0.95

Parameters

Let P = {x 2 Rn | 0  x  ū, Ax  b, ū 2
Rn+ , A 2 Rm⇥n
, b 2 Rm
+ }. In general F (S) in (6)
+
is non-submodular as illustrated by two examples in Appendix D.3. Upper bounding the curvature is equivalent to
F (S\{i}[⌦)
lower bounding F (S[⌦)
, which can be 0 in the
F (S) F (S\{i})
worst case. However, the submodularity ratio can be lower
bounded by a non-zero scalar.

4

6

8

10

K

(a) Correlation: 0.2

12

2

4

6

8

10

12

K

(b) Correlation: 0.6

Figure 3: Results for A-optimality on synthetic data.
The dataset6 has 14 features (e.g., crime rate, property tax
rates, etc.) and 516 samples. To be able to quickly calculate
the parameters and optimal solution by exhaustive search,
the first n = 14 samples were used. As a baseline, we used
an SDP-based algorithm (abbreviated as SDP, details are
available in Appendix E). Results are shown in Fig. 2 for
varying values of K. In Fig. 2a we can observe that both
G REEDY and SDP compute near-optimal solutions. From
Fig. 2b we can see that the greedy submodularity ratio G
is close to 1, and that the greedy curvature ↵G is less than
1, while the classical curvature ↵total is always 1 (the worstcase value). This implies that the classical total curvature
↵total characterizes the considered maximization problems
less accurate than the greedy curvature.
Synthetic results: We generated random observations
from a multivariate Gaussian distribution with different
correlations. To be able to assess the ground truth, we used
n = 12 samples with d = 6 features. Fig. 3 shows the
results with correlation 0.2 (first column) and 0.6 (second
column), respectively: The first row shows the average objective values over the optimal value with error bars, and
the second row shows the parameters. One can observe that
G REEDY always obtains near-optimal solutions and that
these solutions are roughly comparable with those obtained
by the SDP. The classical curvature ↵total is always close to
1, while ↵G take smaller values, and G takes values close
to 1, thus characterize the performance of G REEDY better.

Real-world results: We used the Boston Housing Data.

Medium-scale synthetic experiments: To compare the
runtime of SDP and G REEDY, we considered mediumscale datasets (we cannot report results on larger datasets
because of the huge computational demands of the SDP).

5
All experiments were implemented using Matlab. We used
the SDP solver provided by CVX (Version 2.1).

6
https://archive.ics.uci.edu/ml/datasets/
Housing

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

7

30
25
20
15
10

SDP
Greedy

5
20

40

60

80

6
5
4
SDP
Greedy

3
2
1

100

10

20

30

K

40

K

(a) n = 112, d = 40, corr. = .5 (b) n = 48, d = 24, corr. : .99

Figure 4: A-optimality on medium-scale problems

5.3. Determinantal Functions Maximization

Fig. 4 shows the objective value achieved by G REEDY and
SDP for different numbers of features d and numbers of
samples n, as well as the correlations. We can observe
that G REEDY computes solutions that are on par or superior
to those of SDP. In Table 1 we summarize the runtime of
G REEDY and SDP for different values of d and n, for correlation 0.5. Furthermore, we show the ratio of runtimes of
the two algorithms. We can observe that G REEDY is usually two orders of magnitude faster than SDP.

We experimented with synthetic and real-world data: For
synthetic data, we generated random covariance matrices
⌃ 2 Rn⇥n with uniformly distributed eigenvalues in [0, 1].
We set n = 10, = 2. In Fig. 6 (left) we plot the optimal determinantal objective value and the value achieved
by G REEDY. Fig. 6 (right) traces the greedy submodularity ratio G . Since the determinantal objective is supermodular, so the approximation guarantee equals to G . We
can see that G can reasonably predict the performance of
G REEDY.

Table 1: Runtime in seconds of G REEDY and SDP. The
last row shows the ratio of runtimes of SDP and G REEDY.

G REEDY
SDP
SDP

G REEDY

d: 40
n: 112
0.360
115.2
319.9

d: 64
n: 128
0.765
205.4
268.7

d: 100
n: 200
4.666
1741.2
373.2

d: 120
n: 250
10.56
3883.5
367.7

OPT
Greedy

4

Det. objective

d: 60
n: 80
0.278
95.2
341.7

1

4.5

0.8

3.5

Parameters

A-optimality objective

A-optimality objective

35

[0, 1]. We set b = d = 1, and set ū as 1. The first
row of Fig. 5 plots the optimal LP objective (calculated
using exhaustive search) and the LP objective returned by
G REEDY. The second row shows the curvature and submodularity ratio. The first column (Fig. 5a) presents the results for n = 6, m = 20, while the second column (Fig. 5b)
presents that for n = 8, m = 30. Note the greedy submodularity ratio takes values between ⇠ 0.15 and 1, and that
the curvature is close to the worst-case value of 1. These
observations are consistent with the theory in Section 4.3.

3
2.5
2

0.6
0.4
0.2

1.5

G

0
2

4

6

8

10

2

4

6

K

8

10

K

Figure 6: Synthetic result. Left: objective value, right:

G

5.2. LPs with Combinatorial Constraints

LP objective

0.993
0.992
OPT
Greedy

0.991
1

2

3

4

K

total

5

G

6

OPT
Greedy

2

G

4

6

K

total

G

8

For real-world data, we considered an active set selection
task on the CIFAR-107 dataset. The first n = 12 images in the test set were used to calculate the covariance
matrix with an squared exponential kernel (k(xi , xj ) =
exp( kxi xj k2 /h2 ), h was set to be 1). The results in
Fig. 7 shows similar results as with the synthetic data.

G

1

0.5

0

1
6
0.8

0.5

0
1

2

3

4

5

K

(a) n = 6, m = 20

6

2

4

6

5

Parameters

Parameters

1

Parameters

0.64
0.62
0.6
0.58
0.56
0.54
0.52

Det. objective

LP objective

0.994

4
3

G

K

0
2

(b) n = 8, m = 30

Figure 5: Results for LPs with K-cardinality constraints.
We generated synthetic LPs as follows: Firstly, we generated the matrix A 2 Rm⇥n
, Aij 2 [0, 1] by drawing
+
all entries independently from a uniform distribution on

0.4
0.2

OPT
Greedy

2

8

0.6

4

6

8

K

10

12

2

4

6

8

10

12

K

Figure 7: CIFAR-10 result. Left: objective value, right:
7

https://www.cs.toronto.edu/˜kriz/cifar.
html

G

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

6. Related Work
In this section we briefly discuss related work on various
notions of non-submodularity and the optimization of nonsubmodular functions (Further details in Appendix F).
Relation to Conforti & Cornuéjols (1984) in deriving
approximation guarantees. In proving Theorem 1 we
use the similar proof framework (i.e., utilizing LP formulations to analyze the worst-case approximation ratios
of different groups of problem instances) as that in Conforti & Cornuéjols (1984), where they derive guarantees
for maximizing submodular functions. However, since we
are proving guarantees for non-submodular functions, the
specific techniques on how to manipulate these LPs are
different. Specifically, 1) The building block to construct
LPs (Lemma 1) is different; 2) The technique to prove the
structure of the LPs (which corresponds to Lemma 2) is
significantly different for a submodular function and a nonsubmodular function, and Lemma 2 is the key to investigate
the worst-case approximation ratios of different groups of
problem instances. 3) The specific way to prove Lemma 3
is also different since the constraints of the LPs are different for submodular and non-submodular functions.
Submodularity ratio and curvature. Curvature is typically defined for submodular functions. Sviridenko et al.
(2013) present a notion of curvature for monotone nonsubmodular functions. Appendix C provides details of that
notion and relates it to our definition. Yoshida (2016) prove
an improved approximation ratio for knapsack-constrained
maximization of submodular functions with bounded curvature. Submodularity ratio (Das & Kempe, 2011) is a
quantity characterizing how close a function is to being
submodular.
Approximate submodularity. Krause et al. (2008) define approximately submodular functions with parameter
✏
0 as those functions F that satisfy an approximate
diminishing returns property, i.e., 8A ✓ B ✓ V \ v it
holds that ⇢v (A)
⇢v (B) ✏. G REEDY yields a solution with objective F (S K ) (1 e 1 )F (⌦⇤ ) K✏, for
maximizing a monotone F s.t. a K-cardinality constraint.
Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity. Restricted submodularity
refers to functions which are submodular only over some
collection of subsets of V, and shifted submodularity can
be viewed as a special case of the approximate diminishing
returns as defined above. Recently, Horel & Singer (2016)
study ✏-approximately submodular functions, which arised
from their research on “noisy” submodular functions. A
function F (·) is ✏-approximately submodular if there exists a submodular function G s.t. (1 ✏)G(S)  F (S) 
(1 + ✏)G(S), 8S ✓ V.

Weak submodularity. Borodin et al. (2014) study weakly
submodular functions, i.e., montone, nomalized functions
F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T )
|S \ T |F (S [ T ) + |S [ T |F (S \ T ). For a function F (·),
we show in Remark 4 that the following two facts do not
imply each other: i) F (·) is weakly submodular; ii) The
submodularity ratio of F (·) is strictly larger than 0, and its
curvature is strictly smaller than 1.
Other notions of non-submodularity. Feige & Izsak
(2013) introduce the supermodular degree as a complexity
measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the R AN DOM G REEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.
Optimization of non-submodular functions.
The
submodular-supermodular procedure has been proposed
to minimize the difference of two submodular functions
(Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012).
Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in
general, and propose efficient algorithms for optimization. Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function. Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation
factor.

7. Conclusion
We analyzed the guarantees for greedy maximization of
non-submodular nondecreasing set functions. By combining the (generalized) curvature ↵ and submodularity ratio
for generic set functions, we prove the first tight approximation bounds in terms of these definitions for greedily maximizing nondecreasing set functions. These approximation bounds significantly enlarge the domain where
G REEDY has guarantees. Furthermore, we theoretically
bounded the parameters ↵ and for several non-trivial applications, and validate our theory in various experiments.
ACKNOWLEDGEMENTS
The authors would like to thank Adish Singla, Kfir Y. Levy
and Aurelien Lucchi for valuable discussions. This research was partially supported by ERC StG 307036 and
the Max Planck ETH Center for Learning Systems. This
work was done in part while Andreas Krause was visiting
the Simons Institute for the Theory of Computing.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

References
Altschuler, Jason, Bhaskara, Aditya, Fu, Gang, Mirrokni,
Vahab, Rostamizadeh, Afshin, and Zadimoghaddam,
Morteza. Greedy column subset selection: New bounds
and distributed algorithms. In ICML, pp. 2539–2548,
2016.
Bach, Francis. Learning with submodular functions:
A convex optimization perspective. Foundations and
Trends R in Machine Learning, 6(2-3):145–373, 2013.
Bai, Wenruo, Iyer, Rishabh, Wei, Kai, and Bilmes, Jeff.
Algorithms for optimizing the ratio of submodular functions. In ICML, pp. 2751–2759, 2016.
Bertsimas, Dimitris and Tsitsiklis, John. Introduction to
Linear Optimization. Athena Scientific, 1st edition,
1997.
Bian, Andrew An, Mirzasoleiman, Baharan, Buhmann,
Joachim M., and Krause, Andreas. Guaranteed nonconvex optimization: Submodular maximization over
continuous domains. In AISTATS, pp. 111–120, 2017.
Borodin, Allan, Le, Dai Tri Man, and Ye, Yuli. Weakly
submodular functions. arXiv preprint arXiv:1401.6697,
2014.
Boyd, Stephen and Vandenberghe, Lieven. Convex optimization. Cambridge university press, 2004.
Buchbinder, Niv, Feldman, Moran, Naor, Joseph, and
Schwartz, Roy. Submodular maximization with cardinality constraints. In SODA, pp. 1433–1452, 2014.
Candes, Emmanuel J, Romberg, Justin K, and Tao, Terence. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and
Applied Mathematics, 59(8):1207–1223, 2006.
Chaloner, Kathryn and Verdinelli, Isabella. Bayesian experimental design: A review. Statistical Science, 10(3):
273–304, 1995.
Conforti, Michele and Cornuéjols, Gérard. Submodular
set functions, matroids and the greedy algorithm: tight
worst-case bounds and some generalizations of the radoedmonds theorem. Discrete Applied Mathematics, 7(3):
251–274, 1984.

Du, Ding-Zhu, Graham, Ronald L, Pardalos, Panos M,
Wan, Peng-Jun, Wu, Weili, and Zhao, Wenbo. Analysis
of greedy approximations with nonsubmodular potential
functions. In SODA, pp. 167–175, 2008.
Elenberg, Ethan R, Khanna, Rajiv, Dimakis, Alexandros G, and Negahban, Sahand. Restricted strong convexity implies weak submodularity. arXiv preprint
arXiv:1612.00804, 2016.
Feige, Uriel and Izsak, Rani. Welfare maximization and the
supermodular degree. In Proceedings of the Fourth Conference on Innovations in Theoretical Computer Science,
pp. 247–256, 2013.
Guyon, Isabelle and Elisseeff, André. An introduction to
variable and feature selection. Journal of machine learning research, 3(Mar):1157–1182, 2003.
Horel, Thibaut and Singer, Yaron. Maximization of approximately submodular functions. In NIPS, pp. 3045–
3053. 2016.
Iyer, Rishabh and Bilmes, Jeff. Algorithms for approximate minimization of the difference between submodular functions, with applications. In Proceedings of the
Twenty-Eighth Conference on Uncertainty in Artificial
Intelligence, pp. 407–417, 2012.
Iyer, Rishabh K, Jegelka, Stefanie, and Bilmes, Jeff A. Curvature and optimal algorithms for learning and minimizing submodular functions. NIPS, pp. 2742–2750, 2013.
Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On iterative hard thresholding methods for high-dimensional
m-estimation. In NIPS, pp. 685–693, 2014.
Jegelka, Stefanie and Bilmes, Jeff. Submodularity beyond submodular energies: coupling edges in graph cuts.
In IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1897–1904, 2011.
Kawahara, Yoshinobu, Iyer, Rishabh K, and Bilmes, Jeff A.
On approximate non-submodular minimization via treestructured supermodularity. In AISTATS, pp. 444–452,
2015.
Krause, Andreas and Cevher, Volkan. Submodular dictionary selection for sparse representation. In ICML, pp.
567–574, 2010.

Das, Abhimanyu and Kempe, David. Algorithms for subset
selection in linear regression. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,
pp. 45–54, 2008.

Krause, Andreas and Golovin, Daniel.
Submodular
function maximization.
In Tractability: Practical
Approaches to Hard Problems. Cambridge University
Press, February 2014.

Das, Abhimanyu and Kempe, David. Submodular meets
spectral: Greedy algorithms for subset selection, sparse
approximation and dictionary selection. In ICML, pp.
1057–1064, 2011.

Krause, Andreas, Singh, Ajit, and Guestrin, Carlos. Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal
of Machine Learning Research, 9(Feb):235–284, 2008.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Kulesza, Alex and Taskar, Ben. Determinantal point processes for machine learning. Foundations and Trends R
in Machine Learning, 5(2–3):123–286, 2012.
Lawrence, Neil, Seeger, Matthias, and Herbrich, Ralf. Fast
sparse gaussian process methods: The informative vector machine. NIPS, pp. 625–632, 2003.
Narasimhan, Mukund and Bilmes, Jeff. A submodularsupermodular procedure with applications to discriminative structure learning. In Proceedings of the TwentyFirst Conference on Uncertainty in Artificial Intelligence, pp. 404–412, 2005.
Nemhauser, George L, Wolsey, Laurence A, and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functions–i. Mathematical Programming, 14(1):265–294, 1978.
Soma, Tasuku, Kakimura, Naonori, Inaba, Kazuhiro, and
Kawarabayashi, Ken-ichi. Optimal budget allocation:
Theoretical guarantee and efficient algorithm. In ICML,
pp. 351–359, 2014.
Sviridenko, Maxim, Vondrák, Jan, and Ward, Justin. Optimal approximation for submodular and supermodular
optimization with bounded curvature. arXiv preprint
arXiv:1311.4728, 2013.
Vondrák, Jan. Optimal approximation for the submodular
welfare problem in the value oracle model. In Proceedings of the Fortieth Annual ACM Symposium on Theory
of Computing, pp. 67–74, 2008.
Vondrák, Jan. Submodularity and curvature: the optimal
algorithm. RIMS Kokyuroku Bessatsu B, 23:253–266,
2010.
Yoshida, Yuichi. Maximizing a monotone submodular
function with a bounded curvature under a knapsack
constraint. arXiv preprint arXiv:1607.04527, 2016.
Zhou, Yuxun and Spanos, Costas J. Causal meets submodular: Subset selection with directed information. In
NIPS, pp. 2649–2657. 2016.

