Improving Stochastic Policy Gradients in Continuous Control with Deep
Reinforcement Learning using the Beta Distribution
Po-Wei Chou 1 Daniel Maturana 1 Sebastian Scherer 1

Abstract
Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D
locomotion and robotic manipulation. However,
in real-world control problems, the actions one
can take are bounded by physical constraints,
which introduces a bias when the standard Gaussian distribution is used as the stochastic policy.
In this work, we propose to use the Beta distribution as an alternative and analyze the bias
and variance of the policy gradients of both policies. We show that the Beta policy is bias-free
and provides signiﬁcantly faster convergence and
higher scores over the Gaussian policy when
both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and offpolicy stochastic methods respectively, on OpenAI Gym’s and MuJoCo’s continuous control environments.

1. Introduction
Over the past years, reinforcement learning with deep feature representations (Hinton et al., 2012; Krizhevsky et al.,
2012) has achieved unprecedented (or even super-human
level) successes in many tasks, including playing Go (Silver et al., 2016) and playing Atari games (Mnih et al., 2013;
2015; Guo et al., 2014; Schulman et al., 2015a).
In reinforcement learning tasks, the agent’s action space
may be discrete, continuous, or some combination of both.
Continuous action spaces are generally more challenging
(Lillicrap et al., 2015). A naive approach to adapting deep
reinforcement learning methods, such as deep Q-learning
(Mnih et al., 2013), to continuous domains is simply dis1

Robotics Institute, Carnegie Mellon University, USA. Correspondence to: Sebastian Scherer <basti@andrew.cmu.edu>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. An example of continuous control with bounded action
space. In most real-world continuous control problems, the actions can only take on values within some bounded interval (ﬁnite
support). For example, the steering angle of most Ackermannsteered vehicles can only range from −30◦ to +30◦ .

cretizing the action space. However, this method has several drawbacks. If the discretization is coarse, the resulting output will not be smooth; if it is ﬁne, the number
of discretized actions may be intractably high. This issue
is compounded in scenarios with high degrees of freedom
(e.g., robotic manipulators and humanoid robots), due to
the curse of dimensionality (Bellman, 1956).
There has been much recent progress in model-free continuous control with reinforcement learning. Asynchronous
Advantage Actor-Critic (A3C) (Mnih et al., 2016) allows
neural network policies to be trained and updated asynchronously with multiple CPU cores in parallel. Value
Iteration Networks (Tamar et al., 2016), provide a differentiable module that can learn to plan. Exciting results
have been shown on highly challenging 3D locomotion and
manipulation tasks (Heess et al., 2015; Schulman et al.,
2015b;a), including real-world robotics problems where
the inputs is raw visual data (Watter et al., 2015; Lillicrap et al., 2015; Levine et al., 2016). Derivative-free black
box optimization like evolution strategies (Salimans et al.,
2017) have also been proven to be very successful in wide
variety of tasks.
Despite recent successes, most reinforcement learning algorithms still require large amounts of training episodes
and huge computational resources. This limits their applicability to richer, more complex, and higher dimensional

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

continuous control real-world problems.
In stochastic continuous control problems, it is standard
to represent their distribution with a Normal distribution
N (µ, σ 2 ), and predict the mean (and sometimes the variance) of it with a function approximator such as deep neural
networks (Williams, 1992; Duan et al., 2016; Mnih et al.,
2016). This is called a Gaussian Policy.
By computing the gradients of the policy with respect to µ
and σ, backpropagation (Rumelhart et al., 1988) and minibatch stochastic gradient descent (or ascent) can be used to
train the network efﬁciently.
However, a little-studied issue in recent approaches is that
for many applications, the action spaces are bounded: action can only take on values within a bounded (ﬁnite) interval due to physical constraints. Examples include the
joint torque of a robot arm manipulator and the steering angle and acceleration limits of Ackermann-steered vehicles.
In these scenarios, any probability distribution with inﬁnite support like the Gaussian will unavoidably introduce
an estimation bias due to boundary effects (as in Figure 1),
which may slow down the training progress and make these
problems even harder to solve.
In this work, we focus on continuous state-action deep
reinforcement learning. We address the shortcomings of
the Gaussian distribution with a ﬁnite support distribution.
Speciﬁcally, we use the Beta distribution with shape parameters α, β as in (9) and call this the Beta policy. It
has several advantages. First, the Beta distrbution is ﬁnitesupport and does not suffer from the same boundary effects as the Gaussian does. Thus it is bias-free and converges faster, which means a faster training process and
a higher score. Second, since we only change the underlying distribution, it is compatible with all state-of-the-art
stochastic continuous control on- and off-policy algorithms
such as trust region policy optimization (TRPO) (Schulman et al., 2015a) and actor-critic with experience replay
(ACER) (Wang et al., 2017).
We show that the Beta policy provides substantial gains in
scores and training speed over the Gaussian policy on several continuous control environments, including two simple classical control problems in OpenAI Gym (Brockman
et al., 2016), three multi-joint dynamics and control problems in MuJoCo (Todorov et al., 2012), and one all-terrainvehicle (ATV) driving simulation in an off-road environment.

2. Background
2.1. Preliminaries
We model our continuous control reinforcement learning
as a Markov decision process (MDP). An MDP consists

of a state space S, an action space A, an initial state s0 ,
and the corresponding state distribution p0 (s0 ), a stationary
transition distribution describing the environment dynamics p(st+1 |st , at ) that satisﬁes the Markov property, and a
reward function r(s, a) : S × A → R for every state s
and action a. An agent selects actions to interact with the
environment based on a policy, which can be either deterministic or stochastic. In this paper, we focus on the latter. A stochastic policy can be described as a probability
distribution of taking an action a given a state s parameterized by a n-dimensional vector θ ∈ Rn , denoted as
πθ (a|s) : S → A.
At each timestep t, a policy distribution πθ (a|st ) is
constructed from the distribution parameters (e.g., from
µθ (s), σθ (s) if it’s a Normal distribution). An action at
is then sampled from this distribution to interact with the
environment, i.e. at ∼ πθ (·|st ). Starting from an initial state, an agent follows a policy to interact with the
MDP to generate a trajectory of states, actions, and rewards {s0 , a0 , r0 , . . . , sT , aT , rT }. The goal of an agent
is to maximize the return from
a state, deﬁned as the to�∞
i
tal discounted reward rtγ =
i=0 γ r(st+i , at+i ), where
γ ∈ (0, 1] is the discount factor describing how much we
favor current reward over those in the future.
To describe how good it is being in state s under the policy π, a state-value function V π (s) = Eπ [r0γ |s0 = s] is
deﬁned as the expected return starting from state s, following the policy π, interacting with environment dynamics, and repeating until the maximum number of episodes
is reached. An action-value function Qπ (s, a), which describes the value of taking a certain action, is deﬁned similarly, except it is the expected return starting from state s
after taking an action a under policy π.
The goal in reinforcement learning is to learn a policy maximizing the expected return from the start distribution
�
�
π
ρ (s)
πθ (s, a)r(s, a)da ds
(1)
J(πθ ) =
S

A

(2)
= Es∼ρπ ,a∼πθ [r(s, a)] ,
�
∞
t
where ρπ (s) =
t=0 γ p(st = s) is the unnormalized
discounted state visitation frequency in the limit (Sutton
et al., 1999).
2.2. Stochastic Policy Gradient

Policy gradient methods are featured heavily in the stateof-the-art model-free reinforcement learning algorithms
(Mnih et al., 2016; Duan et al., 2016; Lillicrap et al., 2015;
Wang et al., 2017). In these methods, training of the policy is performed by following the gradient of the performance with respect to the parameters, ∇θ J(πθ ). This gradient can be computed from the Policy Gradient Theorem
(Sutton et al., 1999) by simply changing r(s, a) to Qπ (s, a)

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

in (2) and moving the gradient operator inside the integral:
�
�
ρπ (s)
∇θ πθ (a|s)Qπ (s, a)da ds
∇θ J(πθ ) =
�S
�A
π
=
ρ (s)
πθ (a|s)gq da ds
S

A

(3)

= Es∼ρπ ,a∼πθ [gq ] ,

where πθ (a|s) instead of πθ (s, a) is used to represent a
stochastic policy and gq is the policy gradient estimator using Qπ (s, a) as the target
gq = ∇θ log πθ (a|s)Qπ (s, a) .

(4)

However, exact computation of the double integral in (3) is
generally intractable. Instead, we can estimate it by sampling: given enough samples of gq , the sample mean g¯q ,
will converge to its expectation, ∇θ J(πθ ), by the law of
large numbers
n

g¯q =

1�
P
gq −
→ E[gq ] = ∇θ J(πθ ),
n i=1

as n → ∞ . (5)

Estimating the policy gradient is one of the most important
issues in reinforcement learning. We want gq in (4) to be
bias-free so that it converges to the true policy gradient. As
we will show in the following section, this is not always
true. At the same time, we also want to reduce the sample variance, so that the gradient is less noisy and stable, as
this improves the convergence rate and speeds up the training progress. The action-value function Qπ (s, a) can be
estimated by a variety of sample-based algorithms such as
Monte-Carlo (MC) or temporal-difference (TD) learning.
A lookup table is usually used to store Qπ (s, a) for each
state s and action a.
2.3. Stochastic Actor-Critic
For an MDP with intractably large state space, using a
lookup table is no longer practical. Instead, function
approximation methods are more common. Deep QNetworks (DQN) (Mnih et al., 2013) use a deep neural network parameterized by θv to approximate the action-value
function, denoted as Qθv (s, a) ≈ Qπ (s, a). This is appealing since deep learning has been shown to be very powerful
and successful in computer vision, speech recognition and
many other domains (LeCun et al., 2015).
Unfortunately, direct application of DQN to continuous action spaces is difﬁcult. First, as mentioned earlier, if we
discretize the action space, it is hampered by the curse
of dimensionality. Second, in the Q-learning algorithm,
one needs to ﬁnd the (greedy) action that maximizes the
action-value function, i.e. a = arg maxa Qθv (s, a). This
means an additional optimization procedure is required at

every step inside the stochastic gradient descent optimization, which makes it impractical.
The solution to this is the Actor-Critic methods (Sutton &
Barto, 1998; Peters & Schaal, 2008; Degris et al., 2012;
Munos et al., 2016). In these methods an actor learns a
policy to select actions and a critic estimates the value function, and criticizes decisions made by the actor. The actor
with policy πθ (a|s) and the critic with Qθv (s, a) are trained
simultaneously.
Replacing the true action-value function Qπ (s, a) by
a function approximator Qθv (s, a) may introduce bias.
Nonetheless, in practice, with the help of experience replay
(Lin, 1993) and target networks (Mnih et al., 2013) actorcritic methods still converge to good policies, even with
deep neural networks (Lillicrap et al., 2015; Silver et al.,
2016).
One of the best known variance reduction technique for
actor-critic without introducing any bias is to substract a
baseline function B(s) from Qπ (s, a) in (4) (Greensmith
et al., 2004). A natural choice for B(s) is V π (s), since it is
the expected action-value function Qπ (s, a), i.e. V π (s) =
Ea∼πθ [Qπ (s, a)]. This gives us the deﬁnition of advantage
function Aπ (s, a) and the following stochastic policy gradient estimates:
Δ
Aπ (s, a) =
Qπ (s, a) − V π (s) ,
π

ga = ∇θ log πθ (a|s)A (s, a) .

(6)
(7)

The advantage function Aπ (s, a) measures how much better than the average it is to take an action a. With this
method, the policy gradient in (4) is shifted in a way such
that it is the relative difference, rather than the absolute
value Qπ (s, a), that determines the gradient.

3. Inﬁnite/Finite Support Distribution for
Stochastic Policy in Continuous Control
Using the Gaussian distribution as a stochastic policy
in continous control has been well-studied and commonly used in the reinforcement learning community since
(Williams, 1992). This is most likely because the Gaussian distribution is easy to sample and has gradients that
are easy to compute, which makes it the ﬁrst choice of the
probability distribution.
However, we argue that this is not always a good choice.
In most continuous control reinforcement learning applications, actions can only take on values within some ﬁnite interval due to physical constraints, which introduces a nonnegligible bias caused by boundary effects, as we show below.
This motivates us to use a distribution that can solve this
problem. Among continuous distributions with ﬁnite sup-

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

deﬁnition of inﬁnite support, every action a is assigned
with a probability density πθ (a|s) that is greater than 0.
Nonetheless, in reality, all actions outside the ﬁnite support
have probability exactly equal to 0 (see Figure 1).

Reward

reward
over estimated reward
policy distribution
biased policy distribution
biased
toward
boundary
-h

0
Action

h

Figure 2. An example of over estimation of rewards outside the
boundary.

port, the well-known Beta distribution emerges as a natural
candidate, as it is expressive yet simple, with two easily
interpretable parameters.
In Bayesian statistics, the Beta distribution is often used
as the conjugate prior probability distribution for the
Bernoulli and binomial distributions, describing the initial belief about the probability of the success of each trial
(Bernardo & Smith, 1994). One loose inspiration behind
our use of the Beta function is spike-rate coding, as seen
in biological neurons (Gerstner et al., 1997), or pulse density modulation, as used in artiﬁcial systems; here, the Beta
could be seen as modeling the probability of a neuron ﬁring, or a pulse being emitted, over a small time interval.
In the following, we show that the Beta policy is bias-free
and a better choice than the Gaussian. We compare the
variance of the policy gradient of both policies and show
that as with the Gaussian policy, Natural Policy Gradient is
also necessary for the Beta policy to achieve a good performance.
3.1. Gaussian Policy
To employ a Gaussian policy, we can deﬁne the policy as
�
�
(a − µ)2
1
,
(8)
exp −
πθ (a|s) = √
2σ 2
2πσ
where the mean µ = µθ (s) and the standard deviation
σ = σθ (s) are given by a function approximator parameterized by θ. To enable the use of backpropagation, we
can reparameterize (Heess et al., 2015) action a ∼ πθ (·|s)
as a = µθ (s) + σθ (s)ξ, where ξ ∼ N (0, 1). The policy gradient with respective to µ, σ can be computed exand ∇σ log πθ (a|s) =
plicitly as ∇µ log πθ (a|s) = (a−µ)
σ2
(a−µ)2
σ3

− σ1 . In general, for problem with higher degrees of
freedom, all action dimensions are assumed to be mutually
independent.
3.2. Bias due to Boundary Effect

Modeling a ﬁnite support stochastic policy with an inﬁnite
support probability distribution may introduce bias. By the

To simplify the analysis, we consider the phased update
framework (Kearns & Singh, 2000): in each phase, we are
given n samples of Qπθ (s, a) from environments under a
ﬁxed πθ . In other words, we focus mainly on the inner expectation of (2). Without loss of generality, let us consider
an one-dimensional action space A = [−h, h], where 2h is
the width of the closed interval. For any action space that
is not symmetric around 0, we can always map it to [−h, h]
by scaling and shifting.
So far we have seen two main approaches to employ the
Gaussian policy in this bounded action scenario in the existing RL implementations:
1. Send the action to the environment without capping
(truncating) it ﬁrst, let the environment cap it for us,
and use the uncapped action to compute the policy
gradient.
2. Cap the action to the limit, send it to the environment,
and use the capped action to compute the policy gradient.
In the ﬁrst approach, by letting the environment capping
the actions for us, we simply pretend there are no action
bounds. In other words, all actions outside the bounds just
happen to have the same effect as the actions at the limits.
The policy gradient estimator in (4) now becomes gq� =
∇θ log πθ (a|s)Qπ (s, a� ), where a� is the truncated action.
The bias of the estimator gq� is
E[gq� ] − ∇θ J(πθ )
�� ∞
�
πθ (a|s)∇θ log πθ (a|s)Qπ (s, a� )da − ∇θ J(πθ )
= Es
= Es
+

��

�

−∞
−h

πθ (a|s)∇θ log πθ (a|s) [Qπ (s, −h) − Qπ (s, a)] da
�
π
π
πθ (a|s)∇θ log πθ (a|s) [Q (s, h) − Q (s, a)] da .

−∞
∞

h

We can see that as long as the action space A covers the
support of the policy distribution (i.e. supp(πθ (a|s)) ⊂ A
or as h → ∞) the last two integrals immediately evaluate to
zero. Otherwise, there is a bias due to the boundary effect.
The boundary effect can be better illustrated by the example in Figure 2 where the reward function peaks (assuming a single mode) at a good action close to the boundary.
This effectively extends the domain of reward (or value)
function to previously undeﬁned region by extrapolating, or
more precisely, the “replicated” padding, which results in
artiﬁcially higher rewards outside the bounds and therefore

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

bias the estimated policy distribution toward the boundary.
As for multimodal reward functions, one might need to
consider the use of a mixture model or other density estimation methods since neither the Gaussian nor the Beta
sufﬁces under this scenario. However, this is beyond the
scope of our discussion.
To make things worse, as σ grows, bias also increases. This
makes sense intuitively, because as σ grows, more probability density falls outside the boundary. Note that this is
not an unusual case: to encourage the actor to explore the
state space in the early stage of training, larger σ is needed.
In the second approach, the policy gradient estimator is even more biased because the truncated action
a� is used both in the state-value function Qπ and in
the gradient of log probability ∇θ log πθ , i.e. gq�� =
∇θ log πθ (a� |s)Qπ (s, a� ). In this case, the commonly
used variance reduction techique is less useful since
Ea∼πθ [∇θ log πθ (a� |s)V π (s)] no longer integrates to 0 as
it should be if a instead of a� was used. Not only does it
suffer from the same bias problem we saw earlier, another
bias is also introduced through the substraction of the baseline function.

Let us now consider the Beta distribution
Γ(α + β) α−1
x
(1 − x)β−1 ,
Γ(α)Γ(β)

in (4) does not always yield the steepest direction (Amari,
1998), but the natural policy gradient (Kakade, 2002; Peters & Schaal, 2006) does. The natural policy gradient is
given by
(10)
gqnat = I −1 (θ)gq ,
where I(θ) is the Fisher information matrix deﬁned as
�
�
I(θ) = Ea∼πθ ∇θ log πθ (a|s) ∇θ log πθ (a|s)T

3.3. Beta Policy

f (x; α, β) =

Figure 3. Probability density function of Beta distributions with
different α and β.

(11)

and the variance of the policy gradient is
(9)

where α and β are the shape parameters and Γ(·) is the
Gamma function that extends factorial to real numbers, i.e.
Γ(n) = (n−1)! for positive integer n. The beta distribution
has a support x ∈ [0, 1] (as shown in Figure 3) and it is
often used to describe the probability of success, where α−
1 and β − 1 can be thought of as the counts of successes
and failures from the prior knowledge respectively.
We use πθ (a|s) = f ( a+h
2h ; α, β) to represent the stochastic
policy and call it the Beta Policy. Since the beta distribution has ﬁnite support and no probability density falls outside the boundary, the Beta policy is bias-free. The shape
parameters α = αθ (s), β = βθ (s) are also modeled by
neural networks with parameter θ. In this paper, we only
consider the case where α, β > 1, in which the Beta distribution is concave and unimodal.
3.3.1. VARIANCE C OMPARED TO G AUSSIAN P OLICY
One unfortunate property of the Gaussian policy is that the
variance of policy gradient estimator is inversely proportional to σ 2 . As the policy improves and becomes more
deterministic (σ → 0), the variance of (4) goes to inﬁnity
(Sehnke et al., 2008; Zhao et al., 2011; Silver et al., 2014).
This is mainly because the ordinary policy gradient deﬁned

Va [gq ] = Ea [gq2 ] − E2a [gq ]
�
�
= Ea ∇θ log πθ (a|s) ∇θ log πθ (a|s)T Qπ2 (s, a) − E2a [gq ] .

First note that it is often more useful (and informative) to
say X standard deviations rather than just Y points above
the average. In other words, one should consider the metric
deﬁned on the underlying statistical manifold instead of the
Euclidean distance. The Fisher information matrix I(θ) is
such metric (Jeffreys, 1946). A gradient vector consists of
direction and length. For a univariate Gaussian distribution, the ordinary policy gradient has the correct direction,
but not the correct length. As one moves in the parameter
space, the metric deﬁned on this space also changes, which
effectively changes the length of the ordinary gradient vector. The natural gradient adjusts the learning rate according
to the probability distribution, slowing down the learning
rate when the distance on the parameter space compresses,
and speeding it up as the distance expands.
For the Gaussian distribution, the Fisher information matrix has the form of 1/σ 2 (see Supplementary Section A).
The more deterministic the policy becomes, the smaller the
size of step (proportional to σ 2 ) is needed to take in order
to increase the same amount of objective function. As a result, a constant step of the ordinary gradient descent update
will overshoot, which results in higher variance of (4).

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution
Table 1. List of Environments
E NVIRONMENTS
M OUNTAIN C AR C ONTINUOUS - V 0
P ENDULUM - V 0
I NVERTED P ENDULUM - V 1
I NVERTED D OUBLE P ENDULUM - V 1
H UMANOID - V 1
O FF - ROAD D RIVING

Figure 4. Log-likelihood of Beta distributions. For each curve,
we sample N data points xi ∼ f (x; α, β)
�and plot the the averaged log-likelihood curve by evaluating N
i=1 log f (xi ; α, β) at
α = β ranging from 0 to 20. The maximum log-likehood will
peak at the same α, β where the samples were drawn from initially if N is large enough. Unlike the Normal distribution, the
more deterministic the Beta distribution becomes, the ﬂatter the
log-likelihood curve (from blue, orange . . . to cyan).

�S�

2
3
4
11
376
400 + 6

�A�
1
1
1
1
17
2

modeled by softplus, except a constant 1 is added to the
output to make sure α, β ≥ 1 (see Section 3).

For both policy distributions, we add the entropy of policy πθ (a|s) with a constant multiplier 0.001 encouraging
exploration in order to prevent premature convergence to
sub-optimal policies (Mnih et al., 2016). A discount factor
γ is set to 0.995 across all tasks.
4.1. Classical Control

As for the Beta policy, the Fisher information matrix goes
to zero as policy becomes deterministic, as does the variance of the policy gradient (see Supplementary Section B).
However, this is not a desirable property. This can be better
illustrated by the example in Figure 4, where the curvature
ﬂattens out at a rate so high that it is impossible for the
ordinary policy gradient to catch up with, making the estimation of α and β increasingly hard without the use of
the natural policy gradient. In this case, not just the length
has to be adjusted, but also the off-diagonal terms in the
information matrix.

4. Experiments
We evaluate our proposed methods in a variety of environments, including the classical control problems in OpenAI
Gym, the physical control and locomotion tasks in MultiJoint dynamics with Contact (MuJoCo) physical simulator,
and a setup intended to simulate an autonomous driving in
an off-road environment.
In all experiments, inputs are processed using neural networks with architectures depending on the observation and
action spaces. For both distributions, we assume the action dimensions are independent and thus have zero covariance. For all architectures, the last two layers output two
�A�-dimensional real vectors: either (a) the mean µ and
the variance σ 2 for a multivariate normal distribution with
a spherical covariance, or (b) the shape vectors α, β for a
Beta distribution.
Speciﬁcally, for the Normal distribution, µ is modeled by
a linear layer and σ 2 by a softplus element-wise operation,
log(1 + exp(x)). For the Beta distribution, α, β are also

First, as a proof of concept, we compare the Beta distribution with Normal distribution in two classical continuous control: MountainCarContinuous-v0 and Pendulum-v0
(see Figure 5(a) and 5(c)) using the simplest actor-critic
method: no asynchronuous updates (Mnih et al., 2016), experience replays, or natural policy gradient are used. For
the actor, we only use low-dimensional physical state like
joint velocities and vehicle speed. No visual input, such
as RGB pixel values, is used. We ﬁrst featurize the input state to 400-dimensional vectors using random Radial
Basis Functions (Rahimi et al.) and then pass it to a simple neural network where the only layer is the ﬁnal output
layer generating statistics for the policy distribution. This is
effectively a linear combination of state features: φ(s)T θ,
where φ is the featurizing function and θ is the weight vector to be learnt. For the critic, we use 1-step TD-error1 as
an unbiased estimation of the advantage function in (7).
In both tasks, we found that Beta policies consistently provide faster convergence than Gaussian policies (see Figure
5(b) and 5(d)).
4.2. MuJoCo
Next, we evaluate Beta policies on three OpenAI
Gym’s MuJoCo environments: InvertedPendulum-v1,
InvertedDoublePendulum-v1 and Humanoid-v1 (see Figure
5(e), 5(g), and 5(i)) using both on-policy and off-policy
methods. Results are shown in Figure 5(f), 5(h), and 5(j).
The goal for the ﬁrst two is to balance the inverted pendulum and stay upright as long as possible. For the humanoid
robot, the goal is to walk as fast as possible without falling
1

k-step TD error =

�
�k−1 � i
k
i=0 γ rt+i + γ Vθ (st+k ) − Vθ (st )

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

at the same time minimize actions to take and impacts of
each joint.

100
80
Beta
Gaussian

60
40

Score

20
0
-20
-40
-60
-80
-100

0

20

(a) Mountain Car

40
60
Training episodes

80

100

(b) Mountain Car
0
-200
-400
-600
Score

-800
-1000
-1200
-1400

Beta
Gaussian

-1600
-1800

0

(c) Pendulum

200

400
600
Training episodes

800

1000

(d) Pendulum
1000
TRPO+Beta
TRPO+Gaussian
ACER+Beta
ACER+Gaussian

800

Score

600
400
200
0

0

1K

2K

3K

4K

Training episodes

(e) Inverted Pendulum

(f) Inverted Pendulum
10K
TRPO+Beta
TRPO+Gaussian
ACER+Beta
ACER+Gaussian

8K

Score

6K
4K
2K
0K

0

10K

20K

30K

40K

Training episodes

(g) Double Pendulum

(h) Double Pendulum
6K
TRPO+Beta
TRPO+Gaussian
ACER+Beta
ACER+Gaussian

5K

Score

4K
3K
2K
1K
0K

0

50K

100K

150K

200K

Training episodes

(i) Humanoid

(j) Humanoid

Figure 5. Screenshots of the continuous control tasks on OpenAI
Gym (ﬁrst two) and MuJoCo (last three) and training summary
for Normal distribution and Beta distribution. The x-axis shows
the total number of training epochs. The y-axis shows the average
scores (also 1 standard deviation) over several trials.

In the on-policy experiments, we use the original implementation2 provided by the authors of TRPO (Schulman
et al., 2015a) with the same hyperparameters and conﬁguration that were used to generate their state-of-the-art training results. TRPO is similar to natural policy gradient
methods but more efﬁcient for optimizing large function
approximators such as neural networks.
By simply changing the policy distribution, we ﬁnd that
TRPO+Beta provides a signiﬁcant performance improvement (about 2x faster) over TRPO+Gaussian on the most
difﬁcult Humanoid environment. However, only a slight
improvement over the Gaussian policy is observed on the
less difﬁcult Inverted Double Pendulum. For the simplest
task, Inverted Pendulum, Gaussian+TRPO has a slight advantage over TRPO+Beta; however, since both methods
completely solve the Inverted Pendulum in a matter of minutes, the absolute difference is small.
For the off-policy experiments, we implement ACER in
TensorFlow according to Algorithm 3 in (Wang et al.,
2017). Asynchronous updates with four CPUs and nonprioritized experience replays of ratio 8 are used. The
learning rate is sampled log-uniformly from [10−4 , 5 ×
10−4 ]. The soft updating parameter for the average policy
network is set to 0.995 across all tasks. For the Gaussian
distribution, σ is squashed by a hyperbolic tangent function to prevent a variance that is too large (too unstable to
be compared) or too small (underﬂow). Speciﬁcally, we
only allow σ ranging from 10−4 to h (see Section 3.2).
Substantial improvements over Gaussian policies are also
observed in the off-policy experiments among all tasks.
Though sometimes Gaussian can ﬁnd a good policy faster
than the Beta, it plummets after tens of training episodes,
then repeats, which results in a lower average score and
higher variance (Figure 5(h)). The improvement over the
Gaussian policy on the Humanoid is the most prominent
and that on the Inverted Pendulum is less signiﬁcant. This
trend suggests that the bias introduced by constrained action spaces is compounded in systems with higher degrees
of freedom.
Note that these results are not directly comparable with the
previous on-policy TRPO. First, a fast and efﬁcient variant
of TRPO was proposed in ACER as a trade-off. Second,
we do not use the generalized advantage estimator (GAE)
(Schulman et al., 2015b), though it can be done by modifying the Retrace (Munos et al., 2016) target update rule in
ACER. Third, a smaller batch size is usually used during
the alternating on-policy and off-policy updates in ACER.
Similar unstable behaviors can also be observed when we
2

See https://github.com/joschu/modular_rl.

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

Beta
Gaussian

100
80

Score

60
40
20
0
-20
0

(a) Off-Road Driving

80

5K

6K

The vehicle’s dynamics, which are unknown to the agent
(thus model-free), are derived from a vehicle model obtained by system identiﬁcation. The data for the identiﬁcation was recorded by driving an ATV manually in an offroad environment. In all simulations, a constant timestep
of 0.025 seconds is used to integrate ẋ, ẏ, ω̇ for generation
of trajectories with a unicycle model.

Beta
Gaussian

100
80
60
Score

60
Score

2K
3K
4K
Training episodes

(b) replay ratio 0.25

Beta
Gaussian

100

1K

40
20

40
20

0

0

-20

-20
0

1K

2K
3K
4K
Training episodes

5K

(c) replay ratio 1

6K

0

1K

2K
3K
4K
Training episodes

5K

5 Hz: steering angle and forward speed. Steering angle
is constrained to [−30◦ , 30◦ ] and speed is constrained to
[6, 40] km/h. The vehicle’s state is (x, y, ω, ẋ, ẏ, ω̇), where
x, y are velocity in lateral and forward direction, ω is the
yaw rate, and ẋ, ẏ, ω̇ are the time derivatives. The vehicle
commands are related to ẏ and ω̇ by a second order vehicle
model.

6K

(d) replay ratio 4

Figure 6. Screenshots of off-road driving task and training summary for the Normal distribution and Beta distribution. The xaxis shows the total number of training epochs. The y-axis shows
the average scores (also 1 standard deviation) over 10 different
experiments with varying parameters (see text for details).

try to reduce the batch size of update in on-policy TRPO
experiments. We believe this is because a smaller batch
size means more frequent updates, which helps the agents
to explore faster in the early stage of training but starts to
hamper the performance in the later stage, when a larger
sample size is needed to reduce the sample variance in such
unstable robot arm (or locomotion) conﬁgurations.
Similar to the ﬁndings in evolution strategies (Salimans
et al., 2017), humanoid robots under different stochastic
policies also exhibit different gaits: those with Beta policies always walk sideways but those with Gaussian policies
always walk forwards. We believe this suggests a different
exploration behavior and could be an interesting research
direction in the future.
4.3. Off-Road Autonomous Driving in Local Maps
Last, we consider a simpliﬁed All Terrain Vehicle (ATV)
autonomous navigation problem. In this problem, the angent (an ATV vehicle) must navigate an off-road 2D map
where each position in the map has a scalar traversability
value. The vehicle is rewarded for driving on smoother terrain, while maintaining a minimum speed.
The map is 20 × 20 meters, represented as a 40 × 40 grid
(as shown in Figure 6(a)). The input of the agent is the vehicle’s physical state and top-down view of 20 × 20 grid in
front of the vehicle, rotated to the vehicle frame. The vehicle’s action space consists of two commands updated every

We follow the (convolutional) network architectures used
for 3D maze navigation in (Mirowski et al., 2017) and use
the same setup of ACER as in Section 4.2, except we use a
replay ratio sampled over the values {0.25, 1, 4}.
Results show the Beta policy consistently outperforms the
Gaussian policy signiﬁcantly under all different replay ratios. We found that higher replay ratio works better for the
Beta but not for the Gaussian. We suspect that despite offpolicy training being more sample efﬁcient (a sample can
be learnt several times using experience replay), it is generally noisier due to the use of importance sampling. Even
with the help of Retrace, off-policy training with high experience replay ratio still destabilizes the Gaussian policy
(Figure 6(d)).

5. Conclusions
We introduce a new stochastic policy based on the Beta
distribution for continuous control reinforcement learning. This method solves the bias problem due to boundary effects arising from the mismatch of inﬁnite support of the commonly used Gaussian distribution and the
bounded controls that can be found in most real-world
problems. Our approach outperforms the Gaussian policy when TRPO and ACER, the state-of-the-art on- and
off-policy methods, are used. It is also compatible with
all other continuous control reinforcement algorithms with
Gaussian policies. For future work, we aim to apply this
to more challenging real-world robotic learning tasks such
as autonomous driving and humanoid robots, and extend it
for more complex problems, e.g. by using mixtures of Beta
distributions for multimodal stochastic policies.

Acknowledgements
We thank Guan-Horng Liu, Ming Hsiao, Yen-Chi Chen,
Wen Sun and Nick Rhinehart for many helpful discussions,
suggestions and comments on the paper. This research was

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

funded under award by Yamaha Motor Corporation and
ONR under award N0014-14-1-0643.

References
Amari, Shun-Ichi. Natural gradient works efﬁciently in
learning. Neural computation, 10(2):251–276, 1998.
Bellman, Richard. Dynamic programming and lagrange
multipliers. Proceedings of the National Academy of Sciences, 42(10):767–769, 1956.
Bernardo, J. M. and Smith, A. F. M. Bayesian Theory. John
Wiley & Sons, New York, 1994.
Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig,
Schneider, Jonas, Schulman, John, Tang, Jie, and
Zaremba, Wojciech. Openai gym, 2016.
Degris, Thomas, White, Martha, and Sutton, Richard S.
Off-policy actor-critic. arXiv preprint arXiv:1205.4839,
2012.
Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John,
and Abbeel, Pieter. Benchmarking deep reinforcement
learning for continuous control. In Proceedings of The
33rd International Conference on Machine Learning,
pp. 1329–1338, 2016.
Gerstner, Wulfram, Kreiter, Andreas K., Markram, Henry,
and Herz, Andreas V. M. Neural codes: Firing rates andbeyond. Proceedings of the National Academy of Sciences, 94(24):12740–12741, 1997.
Greensmith, Evan, Bartlett, Peter L, and Baxter, Jonathan.
Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning
Research, 5(Nov):1471–1530, 2004.
Guo, Xiaoxiao, Singh, Satinder, Lee, Honglak, Lewis,
Richard L, and Wang, Xiaoshi. Deep learning for
real-time atari game play using ofﬂine monte-carlo tree
search planning. In Advances in neural information processing systems, pp. 3338–3346, 2014.
Heess, Nicolas, Wayne, Gregory, Silver, David, Lillicrap,
Tim, Erez, Tom, and Tassa, Yuval. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp.
2944–2952, 2015.
Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four
research groups. IEEE Signal Processing Magazine, 29
(6):82–97, 2012.

Jeffreys, Harold. An invariant form for the prior probability in estimation problems. In Proceedings of the Royal
Society of London a: mathematical, physical and engineering sciences, volume 186, pp. 453–461. The Royal
Society, 1946.
Kakade, Sham M. A natural policy gradient. In Advances in
Neural Information Processing Systems, pp. 1531–1538,
2002.
Kearns, Michael J and Singh, Satinder P. Bias-variance error bounds for temporal difference updates. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, pp. 142–147. Morgan Kaufmann Publishers Inc., 2000.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.
LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep
learning. Nature, 521(7553):436–444, 2015.
Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,
Pieter. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):
1334–1373, 2016.
Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,
Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,
and Wierstra, Daan. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,
2015.
Lin, Long-Ji. Reinforcement learning for robots using neural networks. PhD thesis, Fujitsu Laboratories Ltd, 1993.
Mirowski, Piotr, Pascanu, Razvan, Viola, Fabio, Soyer,
Hubert, Ballard, Andy, Banino, Andrea, Denil, Misha,
Goroshin, Ross, Sifre, Laurent, Kavukcuoglu, Koray,
et al. Learning to navigate in complex environments.
In The 5th International Conference on Learning Representations (ICLR), 2017.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and
Riedmiller, Martin. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop, 2013.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A, Veness, Joel, Bellemare, Marc G,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K,
Ostrovski, Georg, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–
533, 2015.

Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

Mnih, Volodymyr, Badia, Adrià Puigdomènech, Mirza,
Mehdi, Graves, Alex, Lillicrap, Timothy, Harley, Tim,
Silver, David, and Kavukcuoglu, Koray. Asynchronous
methods for deep reinforcement learning. In International Conference on Machine Learning, pp. 1928–1937,
2016.
Munos, Rémi, Stepleton, Tom, Harutyunyan, Anna, and
Bellemare, Marc. Safe and efﬁcient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1046–1054, 2016.
Peters, Jan and Schaal, Stefan. Policy gradient methods
for robotics. In Intelligent Robots and Systems, 2006
IEEE/RSJ International Conference on, pp. 2219–2225.
IEEE, 2006.
Peters, Jan and Schaal, Stefan. Natural actor-critic. Neurocomputing, 71(7):1180–1190, 2008.
Rahimi, Ali, Recht, Benjamin, et al. Random features for
large-scale kernel machines.
Rumelhart, David E, Hinton, Geoffrey E, and Williams,
Ronald J. Learning representations by back-propagating
errors. Cognitive modeling, 5(3):1, 1988.
Salimans, Tim, Ho, Jonathan, Chen, Xi, and Sutskever,
Ilya. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864,
2017.
Schulman, John, Levine, Sergey, Abbeel, Pieter, Jordan,
Michael, and Moritz, Philipp. Trust region policy optimization. In Proceedings of The 32nd International Conference on Machine Learning, pp. 1889–1897, 2015a.
Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan,
Michael, and Abbeel, Pieter. High-dimensional continuous control using generalized advantage estimation.
arXiv preprint arXiv:1506.02438, 2015b.
Sehnke, Frank, Osendorfer, Christian, Rückstieß, Thomas,
Graves, Alex, Peters, Jan, and Schmidhuber, Jürgen. Policy gradients with parameter-based exploration for control. In International Conference on Artiﬁcial Neural
Networks, pp. 387–396. Springer, 2008.
Silver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas,
Wierstra, Daan, and Riedmiller, Martin. Deterministic
policy gradient algorithms. In ICML, 2014.
Silver, David, Huang, Aja, Maddison, Chris J, Guez,
Arthur, Sifre, Laurent, Van Den Driessche, George,
Schrittwieser, Julian, Antonoglou, Ioannis, Panneershelvam, Veda, Lanctot, Marc, et al. Mastering the game of
go with deep neural networks and tree search. Nature,
529(7587):484–489, 2016.

Sutton, Richard S and Barto, Andrew G. Reinforcement
learning: An introduction, volume 1. MIT press Cambridge, 1998.
Sutton, Richard S, McAllester, David A, Singh, Satinder P,
Mansour, Yishay, et al. Policy gradient methods for reinforcement learning with function approximation. 1999.
Tamar, Aviv, Levine, Sergey, Abbeel, Pieter, WU, YI, and
Thomas, Garrett. Value iteration networks. In Advances
in Neural Information Processing Systems, pp. 2146–
2154, 2016.
Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. Mujoco:
A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–5033. IEEE, 2012.
Wang, Ziyu, Bapst, Victor, Heess, Nicolas, Mnih,
Volodymyr, Munos, Remi, Kavukcuoglu, Koray, and
de Freitas, Nando. Sample efﬁcient actor-critic with experience replay. In The 5th International Conference on
Learning Representations (ICLR), 2017.
Wasserman, Larry. All of statistics: a concise course in statistical inference. Springer Science & Business Media,
2013.
Watter, Manuel, Springenberg, Jost, Boedecker, Joschka,
and Riedmiller, Martin. Embed to control: A locally linear latent dynamics model for control from raw images.
In Advances in Neural Information Processing Systems,
pp. 2746–2754, 2015.
Williams, Ronald J. Simple statistical gradient-following
algorithms for connectionist reinforcement learning.
Machine learning, 8(3-4):229–256, 1992.
Zhao, Tingting, Hachiya, Hirotaka, Niu, Gang, and
Sugiyama, Masashi. Analysis and improvement of policy gradient estimation. In Advances in Neural Information Processing Systems, pp. 262–270, 2011.

