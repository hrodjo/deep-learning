Sparse + Group-Sparse Dirty Models: Statistical Guarantees without
Unreasonable Conditions and a Case for Non-Convexity

Eunho Yang 1 2 Aurélie C. Lozano 3

Abstract
Imposing sparse + group-sparse superposition
structures in high-dimensional parameter estimation is known to provide flexible regularization
that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task
learning, thereby striking the right balance between parameter overlap across tasks and task
specificity. Existing theoretical results on estimation consistency, however, are problematic as
they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the
gap between the practical success and suboptimal analysis of sparse + group-sparse models,
by providing the first consistency results that do
not require unrealistic assumptions. We also
study non-convex counterparts of sparse + groupsparse models. Interestingly, we show that these
are guaranteed to recover the true support set
under much milder conditions and with smaller
sample size than convex models, which might be
critical in practical applications as illustrated by
our experiments.

1. Introduction
We consider high-dimensional statistical models where the
ambient dimension p is much larger than the number of
observations n. Under such high-dimensional scaling, it
is still possible to obtain consistent estimators by imposing low-dimensional structural constraints upon the statistical models, such as sparsity (e.g. in compressed sens1

School of Computing, KAIST, Daejeon, South Korea 2 AItrics, Seoul, South Korea 3 IBM T.J. Watson Research Center, Yorktown Heights, NY, USA. Correspondence
to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano
<aclozano@us.ibm.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), lowrank structure (Recht et al., 2007; Negahban & Wainwright,
2010), sparse graphical model structure (Friedman et al.,
2007; Ravikumar et al., 2008), and sparse additive structure for non-parametric models (Ravikumar et al., 2009). A
widely used approach to structured learning is via specific
regularization functions. For instance, `1 -regularization
is employed for sparse models (Tibshirani, 1996), `1 /`q
norms for group sparsity (Yuan & Lin, 2006), and nuclear
norm for low-rank matrix-structured models (Candès &
Tao, 2010). Much attention has been devoted to the study
of these structured norms and their theoretical properties.
Such a “clean” regularization approach, however, might be
too stringent in practice. For instance in linear regression,
a blend of element-wise sparsity and group-sparsity might
be more appropriate than a purely sparse or purely groupsparse solution. In multitask learning, while some parameters might be shared across tasks, others might only be relevant to a subset of tasks or a single task. To overcome
this limitation, a line of work on so-called dirty models
has emerged, which addresses this caveat by “mixing and
matching” different structures. One basic approach consists in decomposing the model parameters as a sum of
two components, each penalized separately: one component captures the common structure across tasks and the
other task-specific characteristics (Jalali et al., 2010; Gong
et al., 2012). For instance the dirty model in Jalali et al.
(2010) employs `1,1 and `1,∞ regularizers to the two components. Chandrasekaran et al. (2011) consider the problem of recovering unknown low-rank and sparse matrices,
given the sum of their sum, with application such as optical imaging systems. Robust principal component analysis
and related extensions (Candès et al., 2011; Agarwal et al.,
2012; Hsu et al., 2011) estimate a covariance matrix that is
the sum of a low-rank matrix and a structured (e.g. sparse,
column sparse) matrix.
A general framework for studying dirty models was recently proposed in Yang & Ravikumar (2013), which
bridges and extends several analyses for specific pairs
of superposition structures and specific statistical models (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011);
Candès et al. (2011); Agarwal et al. (2012); Hsu et al.

Sparse + Group Sparse Dirty Models

(2011)) Specifically, this framework applies to a general
class of M -estimators employing a so-called hybrid regularization function, which is the infimal convolution of
weighted regularization functions, one for each structural
component. This formulation is equivalent to an M estimator that combines a loss function applied to the sum
of multiple parameter vectors (one per structural component) and a weighted sum of regularization functions (one
per parameter vector).
For the sparse + group sparse decomposition, however
existing analyses are highly problematic. The key weakness is that they require some form of structural incoherence condition which captures the interaction between the
different structured components. While such a structural
incoherence is a reasonable assumption for e.g. sparse
+ low rank superposition, it is what too stringent for the
sparse+group sparse case because the two structures are
completely coherent for this case! This yields a key motivating question for this paper: Under the sparse + group
sparse setting, can we bypass structural incoherence conditions and yet obtain tight error bounds?
In this paper we provide a positive answer by developing
a novel proof technique. Prior analyses require ‘local’ restricted strong convexity conditions (RSC): one condition
for the sparse component and one for the group sparse
component. The use of structural incoherence between
sparse and group sparse components in then needed to
show ‘global’ RSC for the vector concatenating sparse and
group sparse components. To avoid the need for structural
incoherence, we use RSC in the summed space directly
(namely for the summed sparse + group-sparse structure).
However, this brings in a new issue: in this case, the dirty
regularizer for the parameter vector is not decomposable.
To circumvent this issue, our key ingredient is to introduce
“surrogate” sparse and group sparse components depending on our estimators such that i) their sum equals the sum
of the true parameter components and ii) corresponding error vectors are decomposable even though the regularizer
itself is not decomposable. Using the decomposability of
error vectors, we are then able to show `2 consistency for
general loss functions.
As an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to
non-convex regularizers, and show their `∞ consistency.
Interestingly, these models are guaranteed to recover the
true support set under much milder conditions and with
smaller sample size than convex models. In particular,
our `∞ consistency results require neither incoherence in
the loss function nor structural incoherence between sparse
and group sparse parameters. We illustrate the practical
impact of this superior theoretical results with simulation
experiments.

The remainder of this paper is organized as follows. In Section 2 we review sparse+group-sparse dirty models with
convex penalties and introduce their non-convex counterparts. In Section 3 we discuss the incoherence assumption required by prior analyses and explain why such an
assumption is unreasonable. Section 4 introduces the key
ingredient of our novel proof technique. Section 5 presents
the convergence bounds for models with convex penalties.
Those for non-convex penalties are stated in Section 6. Finally, simulation experiments are provided in Section 7 to
illustrate the remarkable practical advantage of non-convex
penalties, agreeing with their superior convergence rates.

2. Sparse + Group-Sparse Dirty Models:
Setup and Formulations
Consider a data collection Z = {Z1 , . . . , Zn }, where each
element is drawn independently from distribution P, and a
loss function L(· ; Z) : Ω → R where L(θ ; Z) measures
the goodness of fit of parameter θ ∈ Ω to the given data
collection Z. Typically Ω = Rp (parameters are vectors)
or Rp×r (parameters are matrices). Assume there are some
known groups G = {G1 , . . . , Gq } that partition the parameter index set: Gi ∩ Gj = φ and ∪qg=1 Gg = {1, . . . , p}.
We aim at recovering parameter θ ∗ which is the
unique minimizer of the population risk: θ ∗ :=
argminθ∈Ω EZ [L(θ; Z)] in cases where
θ ∗ = α∗ + β ∗ ,

(1)

where α∗ is a sparse component and β ∗ is a group-sparse
component obeying the group structure G. For that purpose,
we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity. We
consider both convex and non-convex regularizers as follows.
2.1. Dirty models with convex regularizers
We focus on regularized M -estimators of the form
minimize L(α + β; Z) + λ1 kαk1 + λ2 kβk1,a ,
α,β

(2)

where the loss function L(· ; Z) is possibly nonconvex. Here, given known parameter groups G =
{G1 , G2 , . . .P
, Gq }, the group regularizer is defined as
q
kβk1,a := t=1 kβGt ka for a ≥ 2, where βGt denotes
the parameter subset in group Gt . The constant a determines how the elements within each group are combined.
We provide examples for the popular settings of linear regression and inverse covariance estimation.
Linear regression. Consider the standard linear model
y = Xθ ∗ + w where y ∈ Rn is the observation vector, θ ∗

Sparse + Group Sparse Dirty Models

is the true regression parameter which is the sum of sparse
α∗ and group sparse β ∗ , X ∈ Rn×p is the design matrix,
and w ∈ Rn is the observation noise. The “dirty” regularized least squares solves
minimize
p
α,β∈R

1
ky − X α + β)k22 + λ1 kαk1 + λ2 kβk1,a
2n
(3)

where groups are defined within a (single) parameter vector
space via β. The formulation can be seamlessly extended
to cover the dirty multitask learning setting of Jalali et al.
(2010):
minimize
p×m

α,β∈R

m
X

1 
y (k) − X (k) [α + β]( ,k) 2
·
2
2n

(4)

k=1

+λ1 kαk1 + λ2 kβk1,∞
where we have m related tasks in columns: α, β ∈ Rp×m ,
and the groups can be defined across tasks in rows. E.g. for
predictor j, β(j,1) , . . . , β(j,m) belong to the same group.
Here, [α + β](·,k) indicates k-th column of matrix input
α + β.
Graphical Model Estimation. Another key example is
a modified graphical Lasso where the goal is to estimate
the structure of the underlying graphs representing conditional independences across variables. Assume that there
are some known set of edge groups and that the true parameter Θ∗ has only a small number of active edge groups
plus some individual edges. To recover Θ∗ we solve

b − log det(S + B)
minimize trace (S + B) Σ
(5)
S+B0

+λ1 kSk1 + λ2 kBk1,a
b is the sample covariance matrix and regularizwhere Σ
ers are applied to off-diagonal entries of S and B. As
done in (4) for the linear model, the formulation (5) can be
seamlessly extended to the multitask setting where we wish
to estimate multiple precision matrices jointly, encouraging similar structure while allowing for some discrepancy
across them. This estimator is discussed in Hara & Washio
(2013).
Equivalent Program. As shown in Yang & Ravikumar
(2013), the formulation (2) can be rewritten as:
minimize L(θ; Z) + kθkλ
θ

(6)

where kθkλ is the infimal convolution of two regularizers
n
o
kθkλ := inf λ1 kαk1 + λ2 kβk1,a : α + β = θ . (7)
α,β

It is known that k · kλ is a norm and its dual is defined
as kθk∗λ := max{kθk∞ /λ1 , kθk∞,a∗ /λ2 } where 1/a +
1/a∗ = 1 so that k · k∞,a∗ is the dual norm of k · k1,a (see
Yang & Ravikumar (2013) for details).

2.2. Dirty models with non-convex regularizers
In this paper, we introduce and study estimators of the form

minimize L(α + β) + ρλ1 α + φλ2 ,a β).
(8)
α,β

Here ρλ1 (·) is any regularizer inducing sparsity beyond the
`1 -norm (note that the notation encapsulates the regularization parameter λ1 itself within the regularizer) satisfying
the following conditions (Loh & Wainwright, 2014):
(C1) ρλ1 (0) = 0 and is symmetric. For t > 0,
ρλ1 (t) is non-decreasing but ρλ1 (t)/t is non-increasing
in t. Besides, ρλ1 (t) is differentiable for t 6= 0 with
limt→0+ ρ0λ1 (t) = λ1 , and is ρλ1 (t) + µ2 t2 is convex for
some µ > 0.
(C2) There exists some scalar γ ∈ (0, ∞) such that
ρ0λ1 (t) = 0 when t ≥ γλ1 .
Following the notation of Loh & Wainwright (2014), we
call ρλ1 (·) µ-amenable if it satisfies (C1) and (µ, γ)amenable if it additionally satisfies (C2). The popular non-convex regularizers SCAD (Fan & Li, 2001) and
MCP (Zhang, 2010) are both (µ, γ)-amenable (Loh &
Wainwright, 2014).
The regularizer φλ2 ,a (·) a non-convex counterpart of the
group regularizer λ2 k · k1,a employed in (2) where we use
ρλ2 (·) instead of λ2 k · k1 , over groups:
φλ2 ,a (β) := ρλ2 (G(β))
where G(β) := (kβG1 ka , . . . , kβGq ka )> . Example of
non-convex regularizers include the Group-SCAD and
Group-MCP penalties where SCAD and MCP penalties are
respectively used on the norm of each group.
Remarkably, the proof techniques developed in this paper
make it possible to provide not only `2 -error bounds under
milder conditions than prior work on convex problem (2),
but also support set recovery guarantees for non-convex
one (8). In fact we shall see that dirty models with nonconvex regularizers (8) enjoy strictly better statistical guarantees than their convex counterpart (2), with practical consequences.

3. Structural Incoherence: essential in prior
work, yet an unreasonable assumption
As our starting point, we focus on the case of convex
dirty models in (2) or equivalently in (6). A key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer (Negahban
et al., 2012). However, considering the form of regularizer
in (6), it is not obvious to find the model space and its orthogonal complement with which we could directly derive

Sparse + Group Sparse Dirty Models

error bounds with optimal rates. To circumvent this problem, Yang & Ravikumar (2013) utilize the decomposability
of each component separately, but this requires restricted
strong convexity (RSC) to hold jointly for all component
parameters. In order to have the “joint” RSC property from
“local” RSC with respect to each individual component,
Yang & Ravikumar (2013) assume a structural incoherence
condition. Even if the loss function is strongly convex with
respect to each component, such incoherence across components is essential for the joint RSC due to the linearity
across components. To see this more clearly, suppose we
have the function z 2 for z ∈ R, which is obviously strongly
convex. If we assume, however, that z is the sum of two
components x, y ∈ R, then one can immediately see that
(x + y)2 is not strongly convex jointly in x and y because
x and y are completely coherent in this one dimensional
example.
The problem is that the structural incoherence condition for
the `1 +`1,a setting is way too restrictive because the sparse
and the group-sparse structures essentially share the same
model and its orthogonal spaces1 . In order to see this, we
consider the popular linear model setting (3) for example.
Let s∗ (and b∗ ) be the support set of true sparse (groupsparse) component and sc be its complement. Furthermore, [ n1 X > X](sc ∩b∗ ) denotes the projection of the sample covariance onto sc ∩ b∗ -coordinate space (j-th coordinate becomes zero if j ∈
/ sc ∩ b∗ ). Projections on other
spaces are defined similarly. Then, the structural incoherence condition for joint RSC can be reduced as: for all
(s, b) ∈ {(sc , b∗ ), (s∗ , bc ), (sc , bc )},

σmax [ n1 X > X](s∩b) ≤ Cκ1
(9)
where σmax (·) is the maximum singular value of a matrix, κ1 is the curvature of (restricted) eigenvalue condition, and C is some fixed constant. Informally, this condition requires the maximum singular value of sample covariance (modulo the projection onto the true model and
its orthogonal space) to be smaller than its minimum singular value (Note that for linear models, the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance). This condition can be easily shown to fail in many cases. For instance consider the popular setting where the design matrix X is a set of samples from Gaussian ensemble with
covariance Σ, and the true parameter is the sum of group
sparse + a single nonzero component as depicted in Figure 1. Then, the incoherence condition in (9) implies
maxi,j |[ n1 X > X]ij | ≤ 1/128σmin (Σ), which can be easily
violated in many natural setting of Σ because the minimum
eigenvalue of Σ is smaller than the maximum element of Σ
1

Note that the sparse + group sparse setting is outstanding.
The structural incohence assumption makes sense in other dirty
models settings, e.g. sparse + low rank dirty models.

Figure 1. Example illustrating why the incoherence condition required by previous work fails to hold.

by the Rayleigh quotient.
This naturally leads to the following question:
Can we provide tight error bounds for the problem (2) not
requiring the joint RSC across individual structures and
hence bypassing the incoherence condition?

4. Our key strategy: Constructing surrogate
components that are always decomposable.
In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components
dependent of our estimation. Consider arbitrary target parameter θ ∗ such that θ ∗ = α∗ + β ∗ . Note that we do not
impose additional constraints on defining the sparse component α∗ and the group sparse component β ∗ , hence the
possible combination of (α∗ , β ∗ ) is not unique. As we
will see later, we provide estimation error bounds that depend on the selection of (α∗ , β ∗ )–more precisely on the
sparsity level of α∗ and the group sparsity level of β ∗ . In
that sense our theorems provide sets of estimation bounds.
However, it is important to note that we still do not need to
worry about the identifiability between structures, because
we only care about the `2 and `∞ error rates of the final
or “summed” estimator (we do not recover (nor care about)
the individual components).
e and
Suppose we compute θe from the program (6) where α
βe are minimizing its dirty regularizer (7). Then, rather than
e − α∗ and
directly deriving error bounds of θe − θ ∗ from α
∗
e
β − β , which are not decomposable, we introduce an additional set of vectors, ᾱ, β̄ and θ̄ from the following rules:
1. If α∗j = βj∗ = 0, then ᾱj = β̄j := 0.
2. If α∗j 6= 0 and βj∗ = 0, then β̄j := βej , and ᾱj :=
θj∗ − βej .
e j and β̄j := θj∗ − α
ej.
3. If βj∗ 6= 0, then ᾱj := α
4. θ̄ is defined as the sum of ᾱ and β̄.

Theta^*

a^*

b^*

a_hat

b_hat

Sparse + Group Sparse Dirty Models

Del^*

Gam^*

3

1

1

0

0

0

3

1

1

1

2

1

0

0

0

1

2

0

0

1

0

0

1

0

0

0

1

0

0

1

0

0

0

0

0

0

0

0

0

0

(a) θ

0

a_bar

∗

b_bar
∗

(b) α

1.9

0

0.2

0.8

1

0

1.1

0

0

0.1

0

1

0

0

0.9

0.1

0

0

(c) β

Del^bar

∗

0.8

0.8

1.1

1.1

1.1

0.2

-0.2

0.2

0.2

0

0

0

0

0

0

0

Gam^bar

e
(e) β

e
(d) α

1.9

0

0.2

-2.2

-0.2

-0.2

1.9

0

0.2

1.1

1

0.8

0

0

0

-0.3

-0.2

0

0

1.1

0

0.1

-0.9

0.1

0

1.1

0

1

0.9

1

0

0

0

0.1

0.2

0.1

0.1

0

0

0.2

-0.2

0.2

0

0

0.8

0

0

0.2

0.1

0

0.2

0.2

-0.2

0

0

-0.1

0.2

0

0

0

0

1

0

0

0

0

0

-0.1

0.2

0

0

0

0.1

0

0

0

0

0

0

0

0

0

0

0

0.1

0

0

0

0

0

e − α∗
(f) α

e − β∗
(g) β

(h) ᾱ

(i) β̄

e − ᾱ
(j) α

e − β̄
(k) β

e via transformation T (·). Error
e and (e) β
Figure 2. Example of constructing surrogate target parameters given (b) α∗ , (c) β ∗ , (d) α
vectors based on surrogates are decomposable (see text for details).

By construction, θ̄ is same as θ ∗ , but ᾱ has different spare The same
sity pattern and values from α∗ , depending on α.
holds for β̄ as well. We denote the above transformation as
e
e β).
(ᾱ, β̄) := T (α∗ , β ∗ ; α,
It turns out that the error vectors computed based on the
surrogate ᾱ and β̄ are always decomposable as described
in the following proposition, and the consequence of this
decomposability plays a key role for showing i) `2 -error
bounds without incoherence condition and ii) support set
recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).
Proposition 1. Consider any local optimum θe of convex or non-convex dirty models, and corresponding θ̄ :=
e Then, the error vectors for individual
e β).
T (α∗ , β ∗ ; α,
e
e − ᾱ and Γ :=
components, ∆ := α
 β − β̄, are decom

posable in the sense that [∆ + Γ]j = |∆j | + |Γj | for all
j, and the overall `2 error kθe − θ ∗ k2 is lower bounded as
follows:
kθe − θ ∗ k22 ≥ k∆k22 + kΓk22 .

(10)

Moreover, let s∗ := supp(α∗ ) (the support set of α∗ ), s̄ :=
supp(α∗ ) ∪ supp(ᾱ) and U := supp(α∗ ) ∪ supp(β ∗ ).
Similarly, we also define b∗ := supp(β ∗ ) and b̄ :=
supp(β̄). Then, by construction of T , s∗ ⊆ s̄ ⊆ U and
b∗ ⊆ b̄ ⊆ U . However, it is always guaranteed that
∆s∗ = ∆s̄ = ∆U

and

Γb∗ = Γb̄ = ΓU

Illustrative example. Figure 2 describes an example:
consider a 5 × 3 matrix parameter with 5 known groups in
rows. Suppose (i) the target parameter is given by (a), (ii)
we define (b) and (c) as the sparse and group sparse components of θ ∗ , and (iii) the minimizer of program (6) are come Then, (f)
e and β.
puted as in (d) and (e), respectively for α
and (g) show the error vectors for sparse and group-sparse
components, which are not decomposable ((10) does not
hold for (f) and (g)). On the other hand, for ᾱ in (h) and β̄
in (i) derived from T (·), we can verify that surrogate error
vectors (shown in (j) and (k)) are decomposable; at every
e − ᾱ and βe − β̄ are sign consistent (or at least
position, α
one of them is zero).

5. Statistical Guarantees of Models with
Convex Regularizers
Throughout our analysis, we assume that the loss function
L(·) is twice differentiable and and satisfies the restricted
strong convexity condition
(RSC) For any vector θ1 , θ2 ∈ Rp , the loss function L(·)
satisfies



∇L(θ1 + θ2 ) − ∇L(θ1 ), θ2

≥

κ1 kθ2 k22 − τ1 kθ2 k2η ,

for all kθ2 k2 ≤ 1 , (12)

κ2 kθ2 k2 − τ2 kθ2 kη ,

for all kθ2 k2 ≥ 1 . (13)

(11)

where ∆s∗ represents the projection of ∆ onto the s∗ coordinate space; that is, [∆s∗ ]j is ∆j if j ∈ s∗ , and 0
otherwise.

RSC of the loss is also used to guarantee `2 -consistency
(Negahban et al., 2012; Loh & Wainwright, 2015) or `∞ consistency (Loh & Wainwright, 2014) of “clean” structurally constrained problems (i.e. problems with a single

Sparse + Group Sparse Dirty Models

structure). Note that there are slight variations in the definition of RSC conditions in the literature. Here we adopt
the form with tolerance terms in Loh & Wainwright (2014;
2015), to allow for a wide class of non quadratic and/or
non-convex loss functions. We will show that RSC with
tolerance in dirty norm holds with high probability under
the popular setting of Gaussian ensembles, as an example.
For the analysis, we consider a slight modification of the
program (6):
minimize L(θ) + kθkλ
kθkη ≤r

(14)

where L is possibly non-convex, but satisfies (RSC). The
additional constraint kθkη ≤ r also involves the dirty norm
(7) but with a different parameter vector η. This constraint
is a safety radius commonly used for analyzing non-convex
problems to ensure that the global minimum exists (see e.g.
Loh & Wainwright (2014; 2015)). In practice, we can disregard this additional constraint.
Theorem 1. Consider the dirty model for problem
(14) where L(·) is possibly non-convex but satisfies the restricted strong convexity (RSC). Suppose
that θ ∗ is feasible and the regularization parameters
are set so that λ1 ≥ 4k∇L(θ ∗ )k∞ and λ2 ≥
∗
4k∇L(θ
	 that r ≤
 κ2 )k∞,a∗ . κ2 Suppose λfurthermore
min 4τ2 , 5 max{λ1 /η1 ,λ2 /η2 } , 8τ11η1 , 8τλ12η2 . Then, any local optimum θe of (14) is guaranteed to be `2 consistent:


	
 √
θe − θ ∗  ≤ 3 max λ1 s , λ2 √sG
2
κ1

(15)

where s is the number of nonzero elements in α∗ and sG is
the number of nonzero groups in β ∗ .
Remarks. The error bound in (15) scales with
(n, p, s, sG ) at the same rate as previous analysis (Yang &
Ravikumar, 2013) for the sparse plus group sparse setting,
which required a much stringent incoherence condition,
as we already discussed. It is also instructive to note
that Theorem 1 holds for any combination of (α∗ , β ∗ )
such that α∗ + β ∗ = θ ∗ , but different views of (α∗ , β ∗ )
constructing θ ∗ give different bounds depending on
sparsity/group sparsity levels of (α∗ , β ∗ ) (i.e. s and sG ).
In this sense, Theorem 1 provides a set of `2 estimation
upper bounds.
Linear model and modified graphical Lasso. In the following corollaries, we apply Theorem 1 to the linear model
(3) and the modified graphical Lasso problem (5) and derive their corresponding `2 estimation bounds.
Corollary 1. Consider the linear model (3). Assume that
(i) each row Xi of the observation matrix X is independently sampled from N (0, Σ), (ii) X is (group) column normalized by scaling as in (Negahban et al., 2012), and (iii)

w is independent sub-Gaussian with parameter σ. Now
suppose that in (14) we set a := 2 (where a is the parameter for the group norm both for kθkη and kθkλ ), rpconstant
(r only depends on p
Σ and σ),p
λ1 = η1 := 8σ log p/n
and λ2 = η2 := 8σ( m/n + log q/n) for q groups and
maximum group size m (maxg=1,...,q |Gg |). Suppose that
θ ∗ is feasible to program (14) with these settings. Then
with probability at least 1 − c1 exp(−c2 nλ2s ) − c3 /q 2 , any
local optimum θe satisfies
r
r
r

s
log
p
s
m
sG log q
24σ
G
∗
max
,
+
.
kθe − θ k2 ≤
κ1
n
n
n
where κ1 is some constant depending on Σ.
Corollary 2. Consider the modified graphical Lasso (5) to
estimate inverse covariance Θ∗ . Suppose we set the pa

rameters of (14) as λ1 = η1 := 4 maxi6=j b
Σij − (Θ∗ )−1
ij ,


∗ −1 
b
λ2 = η2 := 4 maxg∈G Σg − (Θ )g a∗ and r ≤
1
5(|||Θ∗ |||2 +1)2 where ||| · |||2 is the spectral norm of the matrix and a ≥ 2. In addition, assume that Θ∗ is feasible for
e satisfies
this problem. Then, any local optimum Θ

	
√
e − Θ∗ kF ≤ 3(|||Θ∗ |||2 + 1)2 max λ1 s , λ2 √sG .
kΘ
(16)
Remark. Since kθkη scales with √1n for the specified
values of η, the constraint kθkη ≤ r gets milder as n increases. It is also important to note that this constraint is
no more stringent than those of non-convex analyses with
a single regularizer (Loh & Wainwright, 2015; 2014): their
constraints can be written as η1 kθk1 ≤ r (since η1 
p
log p/n for linear models for example.) in our notation,
which directly implies kθkη ≤ r since kθkη ≤ η1 kθk1 by
the definition of k · kη .

6. Statistical Guarantees of Models with
Non-convex Penalties
A natural extension of (14) is to incorporate non-convex
regularizers that have some advantages such as unbiasedness. For that purpose, in this section we consider the following formulation
minimize L(θ) + R(θ; λ)
kθkη ≤r

(17)



where R(θ; λ) = inf α,β {ρλ1 α + φλ2 ,a β : α + β =
θ}. While the `2 analysis in Theorem 1 can be extended
to non-convex regularizers following proof techniques recently developed in Loh & Wainwright (2015), using nonconvex unbiased regularizers has no benefit in terms of
asymptotic convergence rates of `2 estimation errors. Instead, we here investigate the `∞ -norm bound and related
support set recovery guarantees where non-convex unbiased regularizers help. To derive `∞ bounds, we use the

Sparse + Group Sparse Dirty Models

primal-dual witness method described in the supplementary materials.
Theorem 2. Consider the dirty program with nonconvex penalties in (17), under (RSC). Suppose 2r(τ2 +
2 max{ λη11 , λη22 }) ≤ 1. Also suppose that for some


δ ∈ max{ 4rτλ11η1 , 4rτλ12η2 }, 1 , the strict dual feasibility
of primal-dual witness holds. Then, any stationary point
θe of (17) is supported by U (recall U := supp(α∗ ) ∪
supp(β ∗ )) if the
of samples is large enough to
√ number
√
satisfy max{λ1 s, λ2 sG }2 < c for some constant c depending only on κ1 , τ1 and δ.

Corollary 4. Consider the multitask regression model.
Suppose that for each task, design matrix X (k) is a zeromean Gaussian ensemble and is column normalized, w(k)
is independent sub-Gaussian with parameter σ. Now suppose we set parameters of (18) as a :=
p ∞, r constant (only
log(pm)/n, λ2 :=
depends
on
Σ
and
σ.),
λ
:=
c
σ
1
1
p
c2 σ (log p + m log 2)/n, and Θ∗ is feasible to program
(14) with these settings. Then, for any local optimum
e with probability at least 1 − c1 exp(−c2 log(pm)) −
Θ,

c3 exp − c4 (log p + m log 2) (which is approaching to 1)
for some positive constants c1 − c4 ,

As in Theorem 1, the decomposability in (10) and (11) with
respect to the surrogates ᾱ and β̄, plays a crucial role in
establishing the support set recovery guarantee of any local
optimum in Theorem 2.

e ⊆ supp(Θ∗ ),
1. supp(Θ)

Based on Theorem 2, we can derive the `∞ bounds following the standard steps in (Loh & Wainwright, 2014):
Corollary 3. Suppose the assumptions in Theorem 2 hold.
Then,
2
√
√
1. If κ12−µ ≥ τ1 max{η1 s, η2 sG } holds for large
enough sample size n, the program (17) has a unique
b specified by the construction of the
stationary point θ,
primal dual witness.

R
b := 1 ∇2 L θ ∗ + t(θb − θ ∗ ) dt, it holds
2. Letting Q
0



b U U −1 ∇L(θ ∗ )U  +
that kθb − θ ∗ k∞ ≤  Q
∞





b U U −1  where ||| · |||∞ denotes a
min{λ1 , λ2 } Q
∞
matrix induced norm (maximum absolute row sum).
3. Moreover, if ρ is (µ, γ)-amenable, and the mini∗
mum absolute value θmin
:= minj |θj∗ | is lower


∗
b U U −1 ∇L(θ ∗ )U  +
bounded by θmin
≥  Q
∞

 
b U U −1 
min{λ1 , λ2 } Q
+
2
max{λ
,
λ
1
2 }γ.
∞
Then, the error bound in the statement 2 is
reduced to tighter bound as kθb − θ ∗ k∞ ≤



b U U −1 ∇L(θ ∗ )U  .
 Q
∞
Multi-task high-dimensional linear regression. We
consider the multi-task high-dimensional linear regression,
as a concrete example of using non-convex dirty regularizers. This is the counterpart of model (4) which uses convex
dirty regularizer. In the following corollary, we analyze the
sparsistency of dirty multi-task linear regression with nonconvex regularizers:
minimize
p×m

Θ∈R

s.t.kΘkη ≤r

m
X

1 
y (k) − X (k) Θ( ,k) 2 + R(Θ; λ)
· 2
2n

k=1

(18)




where R(Θ; λ) = inf α,β {ρλ1 α + φλ2 ,a β : α + β =
Θ}. Now, we derive a corollary for this particular nonconvex dirty model.

2. if additionally the regularizer ρλ is (µ, γ)-amenable
with µ < λmin (Σ) (the minimum eigenvalue of Σ)
(k) 
and Cmin := mink=1,...,m λmin ΣUk Uk > 0, then
e = supp(Θ∗ ) and the element-wise difference
supp(Θ)
is bounded as follows:
s
e − Θ∗ |||max := max |[Θ
e − Θ∗ ]i,j | ≤ σ 100 log(pm)
|||Θ
i,j
nCmin
q
log(pm)
∗
+ min{λ1 , λ2 }
provided that θmin
≥ σ 100nC
min
(k)

maxk=1,...,m |||(ΣUk Uk )−1 |||∞ + 2 max{λ1 , λ2 }γ.
Remark. In order to highlight the benefit of using
(µ, γ)-amenable regularizers, we briefly compare the result of Corollary 4 with that of `1 + `1,a case in
(Jalali et al., 2010). Not only the result in (Jalali
et al., 2010) requires the incoherence on X (specifically,
Pm  (k)
(k) −1 
 < 1), but it also
maxj∈U c k=1 Σj,Uk Σj,Uk
1
sλ1√
e − Θ∗ |||max bound.
has an additional
term in |||Θ
Cmin

n

Moreover, the required λ1 and √
λ2 there can converge
log(pm)
to zero more slowly: λ1  √ √
and λ2 
n− s log(pm)
√
m(m+log p)
√
.
√
n−

sm(m+log p)

7. Experiments
To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties,
we perform experiments on both simulated and real-world
data and compare convex and non-convex dirty models for
sparse + group-sparse structures.
Simulated data. We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings
of parameters (s, sG ) ∈ {(2p/10, p/20), (p/10, p/10)} with
respectively less / more support overlap across tasks (recall
s and sG are the number of nonzero elements in α∗ and
the number of nonzero groups in β ∗ , respectively). The

Sparse + Group Sparse Dirty Models

0.8

GLasso
GSCAD
GMCP
Convex DM (Lasso/GLasso)
Non−convex DM (SCAD/G−SCAD)
Non−convex DM (MCP/G−MCP)

0.2

0.2

0.3

0.4

0.4

error
0.5

error
0.6

0.6

0.7

Lasso
SCAD
MCP
Convex DM (Lasso/GLasso)
Non−convex DM (SCAD/G−SCAD)
Non−convex DM (MCP/G−MCP)

0.2

0.4

0.6
n/p

0.8

1.0

0.2

0.4

0.6
n/p

0.8

1.0

Figure 3. `∞ -error for comparison methods for varying sample size n. Left: less sharing across tasks. Right: more sharing across tasks

rows of the design matrices X are sampled i.i.d. from a
zero-mean Gaussian distribution with correlation of 0.2 between feature pairs. For each set of parameters (s, sG ),
we generate 100 instances of the problem where for each
instance the non-zero entries of the true model parameter matrix are i.i.d. zero-mean Gaussian to agree with s
and sG . Gaussian error with standard deviation of 4 is
added to each observation. For varying sample size n we
measure the `∞ error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty
model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty. We
also evaluate the following baselines: Lasso, MCP, SCAD,
Group-Lasso, Group-MCP and Group-SCAD 2 . The regularization parameters of each method are tuned via 5-folds
cross-validation. The results are presented in Figure 3 (To
avoid cluttering the graphs, we do not display standard errors as these are much lower than the gaps between the
pertinent groups of methods, and we only display the best
group of baselines for each setting). As can be seen from
the figure, dirty models with non-convex penalties enjoy
superior performance over their counterparts with convex
penalties as a function of the sample size. In terms of
computational cost, (Group) coordinate descent steps for
(group) lasso, (group) MCP and (group) SCAD all have
simple closed-form expressions (Huang et al., 2012), similarly for proximal-based approaches. We noticed that for
a wide range of (λ1 , λ2 ), non-convex procedures took less
time and converged faster (See supplements). As future
work it would be interesting to study their theoretical numerical convergence rates.
Real data analysis. We consider the problem of predicting biological activities of molecules given features
extracted from their chemical structures.
We analyze three biological activity datasets from the “molec2
Our theorem on `∞ consistency requires the sample size to
be larger than the maximum of two terms, which precludes from
presenting graphs with curve alignment across p (by rescaling the
x-axis with a control parameter as in Jalali et al. (2010)).

ular activity challenge” (http://www.kaggle.com/
c/MerckActivity). Specifically we consider multitask
regression with three tasks corresponding to predicting the
raw value (− log(IC50)) of three different types of biological activities : ‘binding to cannabinoid receptor 1’, ‘inhibition of dipeptidyl peptidase 4’ and ‘time dependent 3A4
inhibitions’. For each task we used n = 200 observations
with p = 3000 molecular features. We consider 20 random
data splits into training and validation sets, using 2/3 of the
data for tranining and 1/3 for validation, and report the average R2 over these random splits. As shown in table1,
dirty models outperformed “clean” models suggesting the
importance to strike a balanc e between task specificity and
sharing for this data. Non-convex dirty models achieved
the best R2 , which illustrate their capability as a valuable
tool for high-dimensional data analysis.
Table 1. Average R2 for comparison methods on molecular activity data
Method
R2
Lasso
0.36 ± 0.03
0.37 ± 0.03
SCAD
MCP
0.36 ± 0.04
GLasso
0.35 ± 0.03
GSCAD
0.37 ± 0.03
GMCP
0.38 ± 0.03
Convex DM (Lasso/GLasso)
0.43 ± 0.04
Nonconvex DM (SCAD/GSCAD) 0.49 ± 0.04
Nonconvex DM (MCP/GMCP)
0.49 ± 0.03

8. Concluding Remarks
This paper finally resolved the outstanding case of sparse +
group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not
require implausible assumptions, thereby fully justifying
their practical success. In addition we proposed and studied dirty models with non-convex penalties and showed that
they enjoy superior theoretical guarantees that translate into
significant practical impact. An interesting direction for
future work is to investigate whether our proof technique
might be applicable to other dirty models and beyond.

Sparse + Group Sparse Dirty Models

Acknowledgments
E.Y. acknowledges the support of MSIP/NRF (National Research Foundation of Korea) via NRF2016R1A5A1012966 and MSIP/IITP (Institute for
Information & Communications Technology Promotion of Korea) via ICT R&D program 2016-0-00563,
2017-0-00537.

References
Agarwal, A., Negahban, S., and Wainwright, M. J. Noisy
matrix decomposition via convex relaxation: Optimal
rates in high dimensions. The Annals of Statistics, 40
(2):1171–1197, 2012.
Baraniuk, R. Compressive sensing. IEEE Signal Processing Magazine, 24(4):118–121, 2007.
Candès, E. J. and Tao, T. The power of convex relaxation:
Near-optimal matrix completion. Information Theory,
IEEE Transactions on, 56(5):2053–2080, 2010.

Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A
dirty model for multi-task learning. In Neur. Info. Proc.
Sys. (NIPS), 23, 2010.
Loh, P. and Wainwright, M. J. Support recovery without
incoherence: A case for nonconvex regularization. Arxiv
preprint arXiv:1412.5632, 2014.
Loh, P. and Wainwright, M. J. Regularized m-estimators
with nonconvexity: Statistical and algorithmic theory for
local optima. Journal of Machine Learning Research
(JMLR), 16:559–616, 2015.
Negahban, S. and Wainwright, M. J. Estimation of (near)
low-rank matrices with noise and high-dimensional scaling. In Inter. Conf. on Machine learning (ICML), 2010.
Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A unified framework for high-dimensional analysis
of M-estimators with decomposable regularizers. Statistical Science, 27(4):538–557, 2012.

Candès, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the ACM, 58(3),
May 2011.

Raskutti, G., Wainwright, M. J., and Yu, B. Restricted
eigenvalue properties for correlated gaussian designs.
Journal of Machine Learning Research (JMLR), 99:
2241–2259, 2010.

Chandrasekaran, V., Sanghavi, S., Parrilo, P. A., and Willsky, A. S. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–
596, 2011.

Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu,
B. Model selection in gaussian graphical models: Highdimensional consistency of `1 -regularized mle. In Neur.
Info. Proc. Sys. (NIPS), 21, 2008.

Fan, J. and Li, R. Variable selection via non-concave penalized likelihood and its oracle properties. Jour. Amer.
Stat. Ass., 96(456):1348–1360, December 2001.

Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L.
Sparse additive models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB),
(5):1009–1030, 2009.

Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse
covariance estimation with the graphical Lasso. Biostatistics, 2007.
Gong, P., Ye, J., and Zhang, C. Multi-stage multi-task feature learning. In Pereira, F., Burges, C. J. C., Bottou, L.,
and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1988–1996. 2012.
Hara, S. and Washio, T. Learning a common substructure
of multiple graphical gaussian models. Neural Networks,
38:23–38, 2013.
Hsu, D., Kakade, S. M., and Zhang, T. Robust matrix decomposition with sparse corruptions. Information Theory, IEEE Transactions on, 57(11):7221–7234, 2011.
Huang, J., Breheny, P., and Ma, S. A selective review of group selection in high-dimensional models. Statist. Sci., 27(4):481–499, 11 2012. doi: 10.
1214/12-STS392. URL http://dx.doi.org/10.
1214/12-STS392.

Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed
minimum-rank solutions of linear matrix equations via
nuclear norm minimization. Allerton Conference 07,
Allerton House, Illinois, 2007.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58(1):267–288, 1996.
Wainwright, M. J. Sharp thresholds for high-dimensional
and noisy sparsity recovery using `1 -constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–2202, May 2009.
Yang, E. and Ravikumar, P. Dirty statistical models. In
Neur. Info. Proc. Sys. (NIPS), 26, 2013.
Yang, E., Ravikumar, P., Allen, G. I., and Liu, Z. Graphical models via univariate exponential family distributions. Journal of Machine Learning Research (JMLR),
16:3813–3847, 2015.

Sparse + Group Sparse Dirty Models

Yuan, M. and Lin, Y. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society B, 1(68):49, 2006.
Zhang, C. H. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(8):894–
942, 2010.

