Sparse + Group-Sparse Dirty Models: Statistical Guarantees without
Unreasonable Conditions and a Case for Non-Convexity

Eunho Yang 1 2 AureÌlie C. Lozano 3

Abstract
Imposing sparse + group-sparse superposition
structures in high-dimensional parameter estimation is known to provide flexible regularization
that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task
learning, thereby striking the right balance between parameter overlap across tasks and task
specificity. Existing theoretical results on estimation consistency, however, are problematic as
they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the
gap between the practical success and suboptimal analysis of sparse + group-sparse models,
by providing the first consistency results that do
not require unrealistic assumptions. We also
study non-convex counterparts of sparse + groupsparse models. Interestingly, we show that these
are guaranteed to recover the true support set
under much milder conditions and with smaller
sample size than convex models, which might be
critical in practical applications as illustrated by
our experiments.

1. Introduction
We consider high-dimensional statistical models where the
ambient dimension p is much larger than the number of
observations n. Under such high-dimensional scaling, it
is still possible to obtain consistent estimators by imposing low-dimensional structural constraints upon the statistical models, such as sparsity (e.g. in compressed sens1

School of Computing, KAIST, Daejeon, South Korea 2 AItrics, Seoul, South Korea 3 IBM T.J. Watson Research Center, Yorktown Heights, NY, USA. Correspondence
to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano
<aclozano@us.ibm.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ing (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), lowrank structure (Recht et al., 2007; Negahban & Wainwright,
2010), sparse graphical model structure (Friedman et al.,
2007; Ravikumar et al., 2008), and sparse additive structure for non-parametric models (Ravikumar et al., 2009). A
widely used approach to structured learning is via specific
regularization functions. For instance, `1 -regularization
is employed for sparse models (Tibshirani, 1996), `1 /`q
norms for group sparsity (Yuan & Lin, 2006), and nuclear
norm for low-rank matrix-structured models (CandeÌ€s &
Tao, 2010). Much attention has been devoted to the study
of these structured norms and their theoretical properties.
Such a â€œcleanâ€ regularization approach, however, might be
too stringent in practice. For instance in linear regression,
a blend of element-wise sparsity and group-sparsity might
be more appropriate than a purely sparse or purely groupsparse solution. In multitask learning, while some parameters might be shared across tasks, others might only be relevant to a subset of tasks or a single task. To overcome
this limitation, a line of work on so-called dirty models
has emerged, which addresses this caveat by â€œmixing and
matchingâ€ different structures. One basic approach consists in decomposing the model parameters as a sum of
two components, each penalized separately: one component captures the common structure across tasks and the
other task-specific characteristics (Jalali et al., 2010; Gong
et al., 2012). For instance the dirty model in Jalali et al.
(2010) employs `1,1 and `1,âˆ regularizers to the two components. Chandrasekaran et al. (2011) consider the problem of recovering unknown low-rank and sparse matrices,
given the sum of their sum, with application such as optical imaging systems. Robust principal component analysis
and related extensions (CandeÌ€s et al., 2011; Agarwal et al.,
2012; Hsu et al., 2011) estimate a covariance matrix that is
the sum of a low-rank matrix and a structured (e.g. sparse,
column sparse) matrix.
A general framework for studying dirty models was recently proposed in Yang & Ravikumar (2013), which
bridges and extends several analyses for specific pairs
of superposition structures and specific statistical models (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011);
CandeÌ€s et al. (2011); Agarwal et al. (2012); Hsu et al.

Sparse + Group Sparse Dirty Models

(2011)) Specifically, this framework applies to a general
class of M -estimators employing a so-called hybrid regularization function, which is the infimal convolution of
weighted regularization functions, one for each structural
component. This formulation is equivalent to an M estimator that combines a loss function applied to the sum
of multiple parameter vectors (one per structural component) and a weighted sum of regularization functions (one
per parameter vector).
For the sparse + group sparse decomposition, however
existing analyses are highly problematic. The key weakness is that they require some form of structural incoherence condition which captures the interaction between the
different structured components. While such a structural
incoherence is a reasonable assumption for e.g. sparse
+ low rank superposition, it is what too stringent for the
sparse+group sparse case because the two structures are
completely coherent for this case! This yields a key motivating question for this paper: Under the sparse + group
sparse setting, can we bypass structural incoherence conditions and yet obtain tight error bounds?
In this paper we provide a positive answer by developing
a novel proof technique. Prior analyses require â€˜localâ€™ restricted strong convexity conditions (RSC): one condition
for the sparse component and one for the group sparse
component. The use of structural incoherence between
sparse and group sparse components in then needed to
show â€˜globalâ€™ RSC for the vector concatenating sparse and
group sparse components. To avoid the need for structural
incoherence, we use RSC in the summed space directly
(namely for the summed sparse + group-sparse structure).
However, this brings in a new issue: in this case, the dirty
regularizer for the parameter vector is not decomposable.
To circumvent this issue, our key ingredient is to introduce
â€œsurrogateâ€ sparse and group sparse components depending on our estimators such that i) their sum equals the sum
of the true parameter components and ii) corresponding error vectors are decomposable even though the regularizer
itself is not decomposable. Using the decomposability of
error vectors, we are then able to show `2 consistency for
general loss functions.
As an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to
non-convex regularizers, and show their `âˆ consistency.
Interestingly, these models are guaranteed to recover the
true support set under much milder conditions and with
smaller sample size than convex models. In particular,
our `âˆ consistency results require neither incoherence in
the loss function nor structural incoherence between sparse
and group sparse parameters. We illustrate the practical
impact of this superior theoretical results with simulation
experiments.

The remainder of this paper is organized as follows. In Section 2 we review sparse+group-sparse dirty models with
convex penalties and introduce their non-convex counterparts. In Section 3 we discuss the incoherence assumption required by prior analyses and explain why such an
assumption is unreasonable. Section 4 introduces the key
ingredient of our novel proof technique. Section 5 presents
the convergence bounds for models with convex penalties.
Those for non-convex penalties are stated in Section 6. Finally, simulation experiments are provided in Section 7 to
illustrate the remarkable practical advantage of non-convex
penalties, agreeing with their superior convergence rates.

2. Sparse + Group-Sparse Dirty Models:
Setup and Formulations
Consider a data collection Z = {Z1 , . . . , Zn }, where each
element is drawn independently from distribution P, and a
loss function L(Â· ; Z) : â„¦ â†’ R where L(Î¸ ; Z) measures
the goodness of fit of parameter Î¸ âˆˆ â„¦ to the given data
collection Z. Typically â„¦ = Rp (parameters are vectors)
or RpÃ—r (parameters are matrices). Assume there are some
known groups G = {G1 , . . . , Gq } that partition the parameter index set: Gi âˆ© Gj = Ï† and âˆªqg=1 Gg = {1, . . . , p}.
We aim at recovering parameter Î¸ âˆ— which is the
unique minimizer of the population risk: Î¸ âˆ— :=
argminÎ¸âˆˆâ„¦ EZ [L(Î¸; Z)] in cases where
Î¸ âˆ— = Î±âˆ— + Î² âˆ— ,

(1)

where Î±âˆ— is a sparse component and Î² âˆ— is a group-sparse
component obeying the group structure G. For that purpose,
we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity. We
consider both convex and non-convex regularizers as follows.
2.1. Dirty models with convex regularizers
We focus on regularized M -estimators of the form
minimize L(Î± + Î²; Z) + Î»1 kÎ±k1 + Î»2 kÎ²k1,a ,
Î±,Î²

(2)

where the loss function L(Â· ; Z) is possibly nonconvex. Here, given known parameter groups G =
{G1 , G2 , . . .P
, Gq }, the group regularizer is defined as
q
kÎ²k1,a := t=1 kÎ²Gt ka for a â‰¥ 2, where Î²Gt denotes
the parameter subset in group Gt . The constant a determines how the elements within each group are combined.
We provide examples for the popular settings of linear regression and inverse covariance estimation.
Linear regression. Consider the standard linear model
y = XÎ¸ âˆ— + w where y âˆˆ Rn is the observation vector, Î¸ âˆ—

Sparse + Group Sparse Dirty Models

is the true regression parameter which is the sum of sparse
Î±âˆ— and group sparse Î² âˆ— , X âˆˆ RnÃ—p is the design matrix,
and w âˆˆ Rn is the observation noise. The â€œdirtyâ€ regularized least squares solves
minimize
p
Î±,Î²âˆˆR

1
ky âˆ’ X Î± + Î²)k22 + Î»1 kÎ±k1 + Î»2 kÎ²k1,a
2n
(3)

where groups are defined within a (single) parameter vector
space via Î². The formulation can be seamlessly extended
to cover the dirty multitask learning setting of Jalali et al.
(2010):
minimize
pÃ—m

Î±,Î²âˆˆR

m
X

1 
y (k) âˆ’ X (k) [Î± + Î²]( ,k) 2
Â·
2
2n

(4)

k=1

+Î»1 kÎ±k1 + Î»2 kÎ²k1,âˆ
where we have m related tasks in columns: Î±, Î² âˆˆ RpÃ—m ,
and the groups can be defined across tasks in rows. E.g. for
predictor j, Î²(j,1) , . . . , Î²(j,m) belong to the same group.
Here, [Î± + Î²](Â·,k) indicates k-th column of matrix input
Î± + Î².
Graphical Model Estimation. Another key example is
a modified graphical Lasso where the goal is to estimate
the structure of the underlying graphs representing conditional independences across variables. Assume that there
are some known set of edge groups and that the true parameter Î˜âˆ— has only a small number of active edge groups
plus some individual edges. To recover Î˜âˆ— we solve

b âˆ’ log det(S + B)
minimize trace (S + B) Î£
(5)
S+B0

+Î»1 kSk1 + Î»2 kBk1,a
b is the sample covariance matrix and regularizwhere Î£
ers are applied to off-diagonal entries of S and B. As
done in (4) for the linear model, the formulation (5) can be
seamlessly extended to the multitask setting where we wish
to estimate multiple precision matrices jointly, encouraging similar structure while allowing for some discrepancy
across them. This estimator is discussed in Hara & Washio
(2013).
Equivalent Program. As shown in Yang & Ravikumar
(2013), the formulation (2) can be rewritten as:
minimize L(Î¸; Z) + kÎ¸kÎ»
Î¸

(6)

where kÎ¸kÎ» is the infimal convolution of two regularizers
n
o
kÎ¸kÎ» := inf Î»1 kÎ±k1 + Î»2 kÎ²k1,a : Î± + Î² = Î¸ . (7)
Î±,Î²

It is known that k Â· kÎ» is a norm and its dual is defined
as kÎ¸kâˆ—Î» := max{kÎ¸kâˆ /Î»1 , kÎ¸kâˆ,aâˆ— /Î»2 } where 1/a +
1/aâˆ— = 1 so that k Â· kâˆ,aâˆ— is the dual norm of k Â· k1,a (see
Yang & Ravikumar (2013) for details).

2.2. Dirty models with non-convex regularizers
In this paper, we introduce and study estimators of the form

minimize L(Î± + Î²) + ÏÎ»1 Î± + Ï†Î»2 ,a Î²).
(8)
Î±,Î²

Here ÏÎ»1 (Â·) is any regularizer inducing sparsity beyond the
`1 -norm (note that the notation encapsulates the regularization parameter Î»1 itself within the regularizer) satisfying
the following conditions (Loh & Wainwright, 2014):
(C1) ÏÎ»1 (0) = 0 and is symmetric. For t > 0,
ÏÎ»1 (t) is non-decreasing but ÏÎ»1 (t)/t is non-increasing
in t. Besides, ÏÎ»1 (t) is differentiable for t 6= 0 with
limtâ†’0+ Ï0Î»1 (t) = Î»1 , and is ÏÎ»1 (t) + Âµ2 t2 is convex for
some Âµ > 0.
(C2) There exists some scalar Î³ âˆˆ (0, âˆ) such that
Ï0Î»1 (t) = 0 when t â‰¥ Î³Î»1 .
Following the notation of Loh & Wainwright (2014), we
call ÏÎ»1 (Â·) Âµ-amenable if it satisfies (C1) and (Âµ, Î³)amenable if it additionally satisfies (C2). The popular non-convex regularizers SCAD (Fan & Li, 2001) and
MCP (Zhang, 2010) are both (Âµ, Î³)-amenable (Loh &
Wainwright, 2014).
The regularizer Ï†Î»2 ,a (Â·) a non-convex counterpart of the
group regularizer Î»2 k Â· k1,a employed in (2) where we use
ÏÎ»2 (Â·) instead of Î»2 k Â· k1 , over groups:
Ï†Î»2 ,a (Î²) := ÏÎ»2 (G(Î²))
where G(Î²) := (kÎ²G1 ka , . . . , kÎ²Gq ka )> . Example of
non-convex regularizers include the Group-SCAD and
Group-MCP penalties where SCAD and MCP penalties are
respectively used on the norm of each group.
Remarkably, the proof techniques developed in this paper
make it possible to provide not only `2 -error bounds under
milder conditions than prior work on convex problem (2),
but also support set recovery guarantees for non-convex
one (8). In fact we shall see that dirty models with nonconvex regularizers (8) enjoy strictly better statistical guarantees than their convex counterpart (2), with practical consequences.

3. Structural Incoherence: essential in prior
work, yet an unreasonable assumption
As our starting point, we focus on the case of convex
dirty models in (2) or equivalently in (6). A key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer (Negahban
et al., 2012). However, considering the form of regularizer
in (6), it is not obvious to find the model space and its orthogonal complement with which we could directly derive

Sparse + Group Sparse Dirty Models

error bounds with optimal rates. To circumvent this problem, Yang & Ravikumar (2013) utilize the decomposability
of each component separately, but this requires restricted
strong convexity (RSC) to hold jointly for all component
parameters. In order to have the â€œjointâ€ RSC property from
â€œlocalâ€ RSC with respect to each individual component,
Yang & Ravikumar (2013) assume a structural incoherence
condition. Even if the loss function is strongly convex with
respect to each component, such incoherence across components is essential for the joint RSC due to the linearity
across components. To see this more clearly, suppose we
have the function z 2 for z âˆˆ R, which is obviously strongly
convex. If we assume, however, that z is the sum of two
components x, y âˆˆ R, then one can immediately see that
(x + y)2 is not strongly convex jointly in x and y because
x and y are completely coherent in this one dimensional
example.
The problem is that the structural incoherence condition for
the `1 +`1,a setting is way too restrictive because the sparse
and the group-sparse structures essentially share the same
model and its orthogonal spaces1 . In order to see this, we
consider the popular linear model setting (3) for example.
Let sâˆ— (and bâˆ— ) be the support set of true sparse (groupsparse) component and sc be its complement. Furthermore, [ n1 X > X](sc âˆ©bâˆ— ) denotes the projection of the sample covariance onto sc âˆ© bâˆ— -coordinate space (j-th coordinate becomes zero if j âˆˆ
/ sc âˆ© bâˆ— ). Projections on other
spaces are defined similarly. Then, the structural incoherence condition for joint RSC can be reduced as: for all
(s, b) âˆˆ {(sc , bâˆ— ), (sâˆ— , bc ), (sc , bc )},

Ïƒmax [ n1 X > X](sâˆ©b) â‰¤ CÎº1
(9)
where Ïƒmax (Â·) is the maximum singular value of a matrix, Îº1 is the curvature of (restricted) eigenvalue condition, and C is some fixed constant. Informally, this condition requires the maximum singular value of sample covariance (modulo the projection onto the true model and
its orthogonal space) to be smaller than its minimum singular value (Note that for linear models, the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance). This condition can be easily shown to fail in many cases. For instance consider the popular setting where the design matrix X is a set of samples from Gaussian ensemble with
covariance Î£, and the true parameter is the sum of group
sparse + a single nonzero component as depicted in Figure 1. Then, the incoherence condition in (9) implies
maxi,j |[ n1 X > X]ij | â‰¤ 1/128Ïƒmin (Î£), which can be easily
violated in many natural setting of Î£ because the minimum
eigenvalue of Î£ is smaller than the maximum element of Î£
1

Note that the sparse + group sparse setting is outstanding.
The structural incohence assumption makes sense in other dirty
models settings, e.g. sparse + low rank dirty models.

Figure 1. Example illustrating why the incoherence condition required by previous work fails to hold.

by the Rayleigh quotient.
This naturally leads to the following question:
Can we provide tight error bounds for the problem (2) not
requiring the joint RSC across individual structures and
hence bypassing the incoherence condition?

4. Our key strategy: Constructing surrogate
components that are always decomposable.
In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components
dependent of our estimation. Consider arbitrary target parameter Î¸ âˆ— such that Î¸ âˆ— = Î±âˆ— + Î² âˆ— . Note that we do not
impose additional constraints on defining the sparse component Î±âˆ— and the group sparse component Î² âˆ— , hence the
possible combination of (Î±âˆ— , Î² âˆ— ) is not unique. As we
will see later, we provide estimation error bounds that depend on the selection of (Î±âˆ— , Î² âˆ— )â€“more precisely on the
sparsity level of Î±âˆ— and the group sparsity level of Î² âˆ— . In
that sense our theorems provide sets of estimation bounds.
However, it is important to note that we still do not need to
worry about the identifiability between structures, because
we only care about the `2 and `âˆ error rates of the final
or â€œsummedâ€ estimator (we do not recover (nor care about)
the individual components).
e and
Suppose we compute Î¸e from the program (6) where Î±
Î²e are minimizing its dirty regularizer (7). Then, rather than
e âˆ’ Î±âˆ— and
directly deriving error bounds of Î¸e âˆ’ Î¸ âˆ— from Î±
âˆ—
e
Î² âˆ’ Î² , which are not decomposable, we introduce an additional set of vectors, Î±Ì„, Î²Ì„ and Î¸Ì„ from the following rules:
1. If Î±âˆ—j = Î²jâˆ— = 0, then Î±Ì„j = Î²Ì„j := 0.
2. If Î±âˆ—j 6= 0 and Î²jâˆ— = 0, then Î²Ì„j := Î²ej , and Î±Ì„j :=
Î¸jâˆ— âˆ’ Î²ej .
e j and Î²Ì„j := Î¸jâˆ— âˆ’ Î±
ej.
3. If Î²jâˆ— 6= 0, then Î±Ì„j := Î±
4. Î¸Ì„ is defined as the sum of Î±Ì„ and Î²Ì„.

Theta^*

a^*

b^*

a_hat

b_hat

Sparse + Group Sparse Dirty Models

Del^*

Gam^*

3

1

1

0

0

0

3

1

1

1

2

1

0

0

0

1

2

0

0

1

0

0

1

0

0

0

1

0

0

1

0

0

0

0

0

0

0

0

0

0

(a) Î¸

0

a_bar

âˆ—

b_bar
âˆ—

(b) Î±

1.9

0

0.2

0.8

1

0

1.1

0

0

0.1

0

1

0

0

0.9

0.1

0

0

(c) Î²

Del^bar

âˆ—

0.8

0.8

1.1

1.1

1.1

0.2

-0.2

0.2

0.2

0

0

0

0

0

0

0

Gam^bar

e
(e) Î²

e
(d) Î±

1.9

0

0.2

-2.2

-0.2

-0.2

1.9

0

0.2

1.1

1

0.8

0

0

0

-0.3

-0.2

0

0

1.1

0

0.1

-0.9

0.1

0

1.1

0

1

0.9

1

0

0

0

0.1

0.2

0.1

0.1

0

0

0.2

-0.2

0.2

0

0

0.8

0

0

0.2

0.1

0

0.2

0.2

-0.2

0

0

-0.1

0.2

0

0

0

0

1

0

0

0

0

0

-0.1

0.2

0

0

0

0.1

0

0

0

0

0

0

0

0

0

0

0

0.1

0

0

0

0

0

e âˆ’ Î±âˆ—
(f) Î±

e âˆ’ Î²âˆ—
(g) Î²

(h) Î±Ì„

(i) Î²Ì„

e âˆ’ Î±Ì„
(j) Î±

e âˆ’ Î²Ì„
(k) Î²

e via transformation T (Â·). Error
e and (e) Î²
Figure 2. Example of constructing surrogate target parameters given (b) Î±âˆ— , (c) Î² âˆ— , (d) Î±
vectors based on surrogates are decomposable (see text for details).

By construction, Î¸Ì„ is same as Î¸ âˆ— , but Î±Ì„ has different spare The same
sity pattern and values from Î±âˆ— , depending on Î±.
holds for Î²Ì„ as well. We denote the above transformation as
e
e Î²).
(Î±Ì„, Î²Ì„) := T (Î±âˆ— , Î² âˆ— ; Î±,
It turns out that the error vectors computed based on the
surrogate Î±Ì„ and Î²Ì„ are always decomposable as described
in the following proposition, and the consequence of this
decomposability plays a key role for showing i) `2 -error
bounds without incoherence condition and ii) support set
recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).
Proposition 1. Consider any local optimum Î¸e of convex or non-convex dirty models, and corresponding Î¸Ì„ :=
e Then, the error vectors for individual
e Î²).
T (Î±âˆ— , Î² âˆ— ; Î±,
e
e âˆ’ Î±Ì„ and Î“ :=
components, âˆ† := Î±
 Î² âˆ’ Î²Ì„, are decom

posable in the sense that [âˆ† + Î“]j = |âˆ†j | + |Î“j | for all
j, and the overall `2 error kÎ¸e âˆ’ Î¸ âˆ— k2 is lower bounded as
follows:
kÎ¸e âˆ’ Î¸ âˆ— k22 â‰¥ kâˆ†k22 + kÎ“k22 .

(10)

Moreover, let sâˆ— := supp(Î±âˆ— ) (the support set of Î±âˆ— ), sÌ„ :=
supp(Î±âˆ— ) âˆª supp(Î±Ì„) and U := supp(Î±âˆ— ) âˆª supp(Î² âˆ— ).
Similarly, we also define bâˆ— := supp(Î² âˆ— ) and bÌ„ :=
supp(Î²Ì„). Then, by construction of T , sâˆ— âŠ† sÌ„ âŠ† U and
bâˆ— âŠ† bÌ„ âŠ† U . However, it is always guaranteed that
âˆ†sâˆ— = âˆ†sÌ„ = âˆ†U

and

Î“bâˆ— = Î“bÌ„ = Î“U

Illustrative example. Figure 2 describes an example:
consider a 5 Ã— 3 matrix parameter with 5 known groups in
rows. Suppose (i) the target parameter is given by (a), (ii)
we define (b) and (c) as the sparse and group sparse components of Î¸ âˆ— , and (iii) the minimizer of program (6) are come Then, (f)
e and Î².
puted as in (d) and (e), respectively for Î±
and (g) show the error vectors for sparse and group-sparse
components, which are not decomposable ((10) does not
hold for (f) and (g)). On the other hand, for Î±Ì„ in (h) and Î²Ì„
in (i) derived from T (Â·), we can verify that surrogate error
vectors (shown in (j) and (k)) are decomposable; at every
e âˆ’ Î±Ì„ and Î²e âˆ’ Î²Ì„ are sign consistent (or at least
position, Î±
one of them is zero).

5. Statistical Guarantees of Models with
Convex Regularizers
Throughout our analysis, we assume that the loss function
L(Â·) is twice differentiable and and satisfies the restricted
strong convexity condition
(RSC) For any vector Î¸1 , Î¸2 âˆˆ Rp , the loss function L(Â·)
satisfies



âˆ‡L(Î¸1 + Î¸2 ) âˆ’ âˆ‡L(Î¸1 ), Î¸2

â‰¥

Îº1 kÎ¸2 k22 âˆ’ Ï„1 kÎ¸2 k2Î· ,

for all kÎ¸2 k2 â‰¤ 1 , (12)

Îº2 kÎ¸2 k2 âˆ’ Ï„2 kÎ¸2 kÎ· ,

for all kÎ¸2 k2 â‰¥ 1 . (13)

(11)

where âˆ†sâˆ— represents the projection of âˆ† onto the sâˆ— coordinate space; that is, [âˆ†sâˆ— ]j is âˆ†j if j âˆˆ sâˆ— , and 0
otherwise.

RSC of the loss is also used to guarantee `2 -consistency
(Negahban et al., 2012; Loh & Wainwright, 2015) or `âˆ consistency (Loh & Wainwright, 2014) of â€œcleanâ€ structurally constrained problems (i.e. problems with a single

Sparse + Group Sparse Dirty Models

structure). Note that there are slight variations in the definition of RSC conditions in the literature. Here we adopt
the form with tolerance terms in Loh & Wainwright (2014;
2015), to allow for a wide class of non quadratic and/or
non-convex loss functions. We will show that RSC with
tolerance in dirty norm holds with high probability under
the popular setting of Gaussian ensembles, as an example.
For the analysis, we consider a slight modification of the
program (6):
minimize L(Î¸) + kÎ¸kÎ»
kÎ¸kÎ· â‰¤r

(14)

where L is possibly non-convex, but satisfies (RSC). The
additional constraint kÎ¸kÎ· â‰¤ r also involves the dirty norm
(7) but with a different parameter vector Î·. This constraint
is a safety radius commonly used for analyzing non-convex
problems to ensure that the global minimum exists (see e.g.
Loh & Wainwright (2014; 2015)). In practice, we can disregard this additional constraint.
Theorem 1. Consider the dirty model for problem
(14) where L(Â·) is possibly non-convex but satisfies the restricted strong convexity (RSC). Suppose
that Î¸ âˆ— is feasible and the regularization parameters
are set so that Î»1 â‰¥ 4kâˆ‡L(Î¸ âˆ— )kâˆ and Î»2 â‰¥
âˆ—
4kâˆ‡L(Î¸
	 that r â‰¤
 Îº2 )kâˆ,aâˆ— . Îº2 Suppose Î»furthermore
min 4Ï„2 , 5 max{Î»1 /Î·1 ,Î»2 /Î·2 } , 8Ï„11Î·1 , 8Ï„Î»12Î·2 . Then, any local optimum Î¸e of (14) is guaranteed to be `2 consistent:


	
 âˆš
Î¸e âˆ’ Î¸ âˆ—  â‰¤ 3 max Î»1 s , Î»2 âˆšsG
2
Îº1

(15)

where s is the number of nonzero elements in Î±âˆ— and sG is
the number of nonzero groups in Î² âˆ— .
Remarks. The error bound in (15) scales with
(n, p, s, sG ) at the same rate as previous analysis (Yang &
Ravikumar, 2013) for the sparse plus group sparse setting,
which required a much stringent incoherence condition,
as we already discussed. It is also instructive to note
that Theorem 1 holds for any combination of (Î±âˆ— , Î² âˆ— )
such that Î±âˆ— + Î² âˆ— = Î¸ âˆ— , but different views of (Î±âˆ— , Î² âˆ— )
constructing Î¸ âˆ— give different bounds depending on
sparsity/group sparsity levels of (Î±âˆ— , Î² âˆ— ) (i.e. s and sG ).
In this sense, Theorem 1 provides a set of `2 estimation
upper bounds.
Linear model and modified graphical Lasso. In the following corollaries, we apply Theorem 1 to the linear model
(3) and the modified graphical Lasso problem (5) and derive their corresponding `2 estimation bounds.
Corollary 1. Consider the linear model (3). Assume that
(i) each row Xi of the observation matrix X is independently sampled from N (0, Î£), (ii) X is (group) column normalized by scaling as in (Negahban et al., 2012), and (iii)

w is independent sub-Gaussian with parameter Ïƒ. Now
suppose that in (14) we set a := 2 (where a is the parameter for the group norm both for kÎ¸kÎ· and kÎ¸kÎ» ), rpconstant
(r only depends on p
Î£ and Ïƒ),p
Î»1 = Î·1 := 8Ïƒ log p/n
and Î»2 = Î·2 := 8Ïƒ( m/n + log q/n) for q groups and
maximum group size m (maxg=1,...,q |Gg |). Suppose that
Î¸ âˆ— is feasible to program (14) with these settings. Then
with probability at least 1 âˆ’ c1 exp(âˆ’c2 nÎ»2s ) âˆ’ c3 /q 2 , any
local optimum Î¸e satisfies
r
r
r

s
log
p
s
m
sG log q
24Ïƒ
G
âˆ—
max
,
+
.
kÎ¸e âˆ’ Î¸ k2 â‰¤
Îº1
n
n
n
where Îº1 is some constant depending on Î£.
Corollary 2. Consider the modified graphical Lasso (5) to
estimate inverse covariance Î˜âˆ— . Suppose we set the pa

rameters of (14) as Î»1 = Î·1 := 4 maxi6=j b
Î£ij âˆ’ (Î˜âˆ— )âˆ’1
ij ,


âˆ— âˆ’1 
b
Î»2 = Î·2 := 4 maxgâˆˆG Î£g âˆ’ (Î˜ )g aâˆ— and r â‰¤
1
5(|||Î˜âˆ— |||2 +1)2 where ||| Â· |||2 is the spectral norm of the matrix and a â‰¥ 2. In addition, assume that Î˜âˆ— is feasible for
e satisfies
this problem. Then, any local optimum Î˜

	
âˆš
e âˆ’ Î˜âˆ— kF â‰¤ 3(|||Î˜âˆ— |||2 + 1)2 max Î»1 s , Î»2 âˆšsG .
kÎ˜
(16)
Remark. Since kÎ¸kÎ· scales with âˆš1n for the specified
values of Î·, the constraint kÎ¸kÎ· â‰¤ r gets milder as n increases. It is also important to note that this constraint is
no more stringent than those of non-convex analyses with
a single regularizer (Loh & Wainwright, 2015; 2014): their
constraints can be written as Î·1 kÎ¸k1 â‰¤ r (since Î·1 
p
log p/n for linear models for example.) in our notation,
which directly implies kÎ¸kÎ· â‰¤ r since kÎ¸kÎ· â‰¤ Î·1 kÎ¸k1 by
the definition of k Â· kÎ· .

6. Statistical Guarantees of Models with
Non-convex Penalties
A natural extension of (14) is to incorporate non-convex
regularizers that have some advantages such as unbiasedness. For that purpose, in this section we consider the following formulation
minimize L(Î¸) + R(Î¸; Î»)
kÎ¸kÎ· â‰¤r

(17)



where R(Î¸; Î») = inf Î±,Î² {ÏÎ»1 Î± + Ï†Î»2 ,a Î² : Î± + Î² =
Î¸}. While the `2 analysis in Theorem 1 can be extended
to non-convex regularizers following proof techniques recently developed in Loh & Wainwright (2015), using nonconvex unbiased regularizers has no benefit in terms of
asymptotic convergence rates of `2 estimation errors. Instead, we here investigate the `âˆ -norm bound and related
support set recovery guarantees where non-convex unbiased regularizers help. To derive `âˆ bounds, we use the

Sparse + Group Sparse Dirty Models

primal-dual witness method described in the supplementary materials.
Theorem 2. Consider the dirty program with nonconvex penalties in (17), under (RSC). Suppose 2r(Ï„2 +
2 max{ Î»Î·11 , Î»Î·22 }) â‰¤ 1. Also suppose that for some


Î´ âˆˆ max{ 4rÏ„Î»11Î·1 , 4rÏ„Î»12Î·2 }, 1 , the strict dual feasibility
of primal-dual witness holds. Then, any stationary point
Î¸e of (17) is supported by U (recall U := supp(Î±âˆ— ) âˆª
supp(Î² âˆ— )) if the
of samples is large enough to
âˆš number
âˆš
satisfy max{Î»1 s, Î»2 sG }2 < c for some constant c depending only on Îº1 , Ï„1 and Î´.

Corollary 4. Consider the multitask regression model.
Suppose that for each task, design matrix X (k) is a zeromean Gaussian ensemble and is column normalized, w(k)
is independent sub-Gaussian with parameter Ïƒ. Now suppose we set parameters of (18) as a :=
p âˆ, r constant (only
log(pm)/n, Î»2 :=
depends
on
Î£
and
Ïƒ.),
Î»
:=
c
Ïƒ
1
1
p
c2 Ïƒ (log p + m log 2)/n, and Î˜âˆ— is feasible to program
(14) with these settings. Then, for any local optimum
e with probability at least 1 âˆ’ c1 exp(âˆ’c2 log(pm)) âˆ’
Î˜,

c3 exp âˆ’ c4 (log p + m log 2) (which is approaching to 1)
for some positive constants c1 âˆ’ c4 ,

As in Theorem 1, the decomposability in (10) and (11) with
respect to the surrogates Î±Ì„ and Î²Ì„, plays a crucial role in
establishing the support set recovery guarantee of any local
optimum in Theorem 2.

e âŠ† supp(Î˜âˆ— ),
1. supp(Î˜)

Based on Theorem 2, we can derive the `âˆ bounds following the standard steps in (Loh & Wainwright, 2014):
Corollary 3. Suppose the assumptions in Theorem 2 hold.
Then,
2
âˆš
âˆš
1. If Îº12âˆ’Âµ â‰¥ Ï„1 max{Î·1 s, Î·2 sG } holds for large
enough sample size n, the program (17) has a unique
b specified by the construction of the
stationary point Î¸,
primal dual witness.

R
b := 1 âˆ‡2 L Î¸ âˆ— + t(Î¸b âˆ’ Î¸ âˆ— ) dt, it holds
2. Letting Q
0



b U U âˆ’1 âˆ‡L(Î¸ âˆ— )U  +
that kÎ¸b âˆ’ Î¸ âˆ— kâˆ â‰¤  Q
âˆ





b U U âˆ’1  where ||| Â· |||âˆ denotes a
min{Î»1 , Î»2 } Q
âˆ
matrix induced norm (maximum absolute row sum).
3. Moreover, if Ï is (Âµ, Î³)-amenable, and the miniâˆ—
mum absolute value Î¸min
:= minj |Î¸jâˆ— | is lower


âˆ—
b U U âˆ’1 âˆ‡L(Î¸ âˆ— )U  +
bounded by Î¸min
â‰¥  Q
âˆ

 
b U U âˆ’1 
min{Î»1 , Î»2 } Q
+
2
max{Î»
,
Î»
1
2 }Î³.
âˆ
Then, the error bound in the statement 2 is
reduced to tighter bound as kÎ¸b âˆ’ Î¸ âˆ— kâˆ â‰¤



b U U âˆ’1 âˆ‡L(Î¸ âˆ— )U  .
 Q
âˆ
Multi-task high-dimensional linear regression. We
consider the multi-task high-dimensional linear regression,
as a concrete example of using non-convex dirty regularizers. This is the counterpart of model (4) which uses convex
dirty regularizer. In the following corollary, we analyze the
sparsistency of dirty multi-task linear regression with nonconvex regularizers:
minimize
pÃ—m

Î˜âˆˆR

s.t.kÎ˜kÎ· â‰¤r

m
X

1 
y (k) âˆ’ X (k) Î˜( ,k) 2 + R(Î˜; Î»)
Â· 2
2n

k=1

(18)




where R(Î˜; Î») = inf Î±,Î² {ÏÎ»1 Î± + Ï†Î»2 ,a Î² : Î± + Î² =
Î˜}. Now, we derive a corollary for this particular nonconvex dirty model.

2. if additionally the regularizer ÏÎ» is (Âµ, Î³)-amenable
with Âµ < Î»min (Î£) (the minimum eigenvalue of Î£)
(k) 
and Cmin := mink=1,...,m Î»min Î£Uk Uk > 0, then
e = supp(Î˜âˆ— ) and the element-wise difference
supp(Î˜)
is bounded as follows:
s
e âˆ’ Î˜âˆ— |||max := max |[Î˜
e âˆ’ Î˜âˆ— ]i,j | â‰¤ Ïƒ 100 log(pm)
|||Î˜
i,j
nCmin
q
log(pm)
âˆ—
+ min{Î»1 , Î»2 }
provided that Î¸min
â‰¥ Ïƒ 100nC
min
(k)

maxk=1,...,m |||(Î£Uk Uk )âˆ’1 |||âˆ + 2 max{Î»1 , Î»2 }Î³.
Remark. In order to highlight the benefit of using
(Âµ, Î³)-amenable regularizers, we briefly compare the result of Corollary 4 with that of `1 + `1,a case in
(Jalali et al., 2010). Not only the result in (Jalali
et al., 2010) requires the incoherence on X (specifically,
Pm  (k)
(k) âˆ’1 
 < 1), but it also
maxjâˆˆU c k=1 Î£j,Uk Î£j,Uk
1
sÎ»1âˆš
e âˆ’ Î˜âˆ— |||max bound.
has an additional
term in |||Î˜
Cmin

n

Moreover, the required Î»1 and âˆš
Î»2 there can converge
log(pm)
to zero more slowly: Î»1  âˆš âˆš
and Î»2 
nâˆ’ s log(pm)
âˆš
m(m+log p)
âˆš
.
âˆš
nâˆ’

sm(m+log p)

7. Experiments
To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties,
we perform experiments on both simulated and real-world
data and compare convex and non-convex dirty models for
sparse + group-sparse structures.
Simulated data. We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings
of parameters (s, sG ) âˆˆ {(2p/10, p/20), (p/10, p/10)} with
respectively less / more support overlap across tasks (recall
s and sG are the number of nonzero elements in Î±âˆ— and
the number of nonzero groups in Î² âˆ— , respectively). The

Sparse + Group Sparse Dirty Models

0.8

GLasso
GSCAD
GMCP
Convex DM (Lasso/GLasso)
Nonâˆ’convex DM (SCAD/Gâˆ’SCAD)
Nonâˆ’convex DM (MCP/Gâˆ’MCP)

0.2

0.2

0.3

0.4

0.4

error
0.5

error
0.6

0.6

0.7

Lasso
SCAD
MCP
Convex DM (Lasso/GLasso)
Nonâˆ’convex DM (SCAD/Gâˆ’SCAD)
Nonâˆ’convex DM (MCP/Gâˆ’MCP)

0.2

0.4

0.6
n/p

0.8

1.0

0.2

0.4

0.6
n/p

0.8

1.0

Figure 3. `âˆ -error for comparison methods for varying sample size n. Left: less sharing across tasks. Right: more sharing across tasks

rows of the design matrices X are sampled i.i.d. from a
zero-mean Gaussian distribution with correlation of 0.2 between feature pairs. For each set of parameters (s, sG ),
we generate 100 instances of the problem where for each
instance the non-zero entries of the true model parameter matrix are i.i.d. zero-mean Gaussian to agree with s
and sG . Gaussian error with standard deviation of 4 is
added to each observation. For varying sample size n we
measure the `âˆ error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty
model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty. We
also evaluate the following baselines: Lasso, MCP, SCAD,
Group-Lasso, Group-MCP and Group-SCAD 2 . The regularization parameters of each method are tuned via 5-folds
cross-validation. The results are presented in Figure 3 (To
avoid cluttering the graphs, we do not display standard errors as these are much lower than the gaps between the
pertinent groups of methods, and we only display the best
group of baselines for each setting). As can be seen from
the figure, dirty models with non-convex penalties enjoy
superior performance over their counterparts with convex
penalties as a function of the sample size. In terms of
computational cost, (Group) coordinate descent steps for
(group) lasso, (group) MCP and (group) SCAD all have
simple closed-form expressions (Huang et al., 2012), similarly for proximal-based approaches. We noticed that for
a wide range of (Î»1 , Î»2 ), non-convex procedures took less
time and converged faster (See supplements). As future
work it would be interesting to study their theoretical numerical convergence rates.
Real data analysis. We consider the problem of predicting biological activities of molecules given features
extracted from their chemical structures.
We analyze three biological activity datasets from the â€œmolec2
Our theorem on `âˆ consistency requires the sample size to
be larger than the maximum of two terms, which precludes from
presenting graphs with curve alignment across p (by rescaling the
x-axis with a control parameter as in Jalali et al. (2010)).

ular activity challengeâ€ (http://www.kaggle.com/
c/MerckActivity). Specifically we consider multitask
regression with three tasks corresponding to predicting the
raw value (âˆ’ log(IC50)) of three different types of biological activities : â€˜binding to cannabinoid receptor 1â€™, â€˜inhibition of dipeptidyl peptidase 4â€™ and â€˜time dependent 3A4
inhibitionsâ€™. For each task we used n = 200 observations
with p = 3000 molecular features. We consider 20 random
data splits into training and validation sets, using 2/3 of the
data for tranining and 1/3 for validation, and report the average R2 over these random splits. As shown in table1,
dirty models outperformed â€œcleanâ€ models suggesting the
importance to strike a balanc e between task specificity and
sharing for this data. Non-convex dirty models achieved
the best R2 , which illustrate their capability as a valuable
tool for high-dimensional data analysis.
Table 1. Average R2 for comparison methods on molecular activity data
Method
R2
Lasso
0.36 Â± 0.03
0.37 Â± 0.03
SCAD
MCP
0.36 Â± 0.04
GLasso
0.35 Â± 0.03
GSCAD
0.37 Â± 0.03
GMCP
0.38 Â± 0.03
Convex DM (Lasso/GLasso)
0.43 Â± 0.04
Nonconvex DM (SCAD/GSCAD) 0.49 Â± 0.04
Nonconvex DM (MCP/GMCP)
0.49 Â± 0.03

8. Concluding Remarks
This paper finally resolved the outstanding case of sparse +
group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not
require implausible assumptions, thereby fully justifying
their practical success. In addition we proposed and studied dirty models with non-convex penalties and showed that
they enjoy superior theoretical guarantees that translate into
significant practical impact. An interesting direction for
future work is to investigate whether our proof technique
might be applicable to other dirty models and beyond.

Sparse + Group Sparse Dirty Models

Acknowledgments
E.Y. acknowledges the support of MSIP/NRF (National Research Foundation of Korea) via NRF2016R1A5A1012966 and MSIP/IITP (Institute for
Information & Communications Technology Promotion of Korea) via ICT R&D program 2016-0-00563,
2017-0-00537.

References
Agarwal, A., Negahban, S., and Wainwright, M. J. Noisy
matrix decomposition via convex relaxation: Optimal
rates in high dimensions. The Annals of Statistics, 40
(2):1171â€“1197, 2012.
Baraniuk, R. Compressive sensing. IEEE Signal Processing Magazine, 24(4):118â€“121, 2007.
CandeÌ€s, E. J. and Tao, T. The power of convex relaxation:
Near-optimal matrix completion. Information Theory,
IEEE Transactions on, 56(5):2053â€“2080, 2010.

Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A
dirty model for multi-task learning. In Neur. Info. Proc.
Sys. (NIPS), 23, 2010.
Loh, P. and Wainwright, M. J. Support recovery without
incoherence: A case for nonconvex regularization. Arxiv
preprint arXiv:1412.5632, 2014.
Loh, P. and Wainwright, M. J. Regularized m-estimators
with nonconvexity: Statistical and algorithmic theory for
local optima. Journal of Machine Learning Research
(JMLR), 16:559â€“616, 2015.
Negahban, S. and Wainwright, M. J. Estimation of (near)
low-rank matrices with noise and high-dimensional scaling. In Inter. Conf. on Machine learning (ICML), 2010.
Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A unified framework for high-dimensional analysis
of M-estimators with decomposable regularizers. Statistical Science, 27(4):538â€“557, 2012.

CandeÌ€s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the ACM, 58(3),
May 2011.

Raskutti, G., Wainwright, M. J., and Yu, B. Restricted
eigenvalue properties for correlated gaussian designs.
Journal of Machine Learning Research (JMLR), 99:
2241â€“2259, 2010.

Chandrasekaran, V., Sanghavi, S., Parrilo, P. A., and Willsky, A. S. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572â€“
596, 2011.

Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu,
B. Model selection in gaussian graphical models: Highdimensional consistency of `1 -regularized mle. In Neur.
Info. Proc. Sys. (NIPS), 21, 2008.

Fan, J. and Li, R. Variable selection via non-concave penalized likelihood and its oracle properties. Jour. Amer.
Stat. Ass., 96(456):1348â€“1360, December 2001.

Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L.
Sparse additive models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB),
(5):1009â€“1030, 2009.

Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse
covariance estimation with the graphical Lasso. Biostatistics, 2007.
Gong, P., Ye, J., and Zhang, C. Multi-stage multi-task feature learning. In Pereira, F., Burges, C. J. C., Bottou, L.,
and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1988â€“1996. 2012.
Hara, S. and Washio, T. Learning a common substructure
of multiple graphical gaussian models. Neural Networks,
38:23â€“38, 2013.
Hsu, D., Kakade, S. M., and Zhang, T. Robust matrix decomposition with sparse corruptions. Information Theory, IEEE Transactions on, 57(11):7221â€“7234, 2011.
Huang, J., Breheny, P., and Ma, S. A selective review of group selection in high-dimensional models. Statist. Sci., 27(4):481â€“499, 11 2012. doi: 10.
1214/12-STS392. URL http://dx.doi.org/10.
1214/12-STS392.

Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed
minimum-rank solutions of linear matrix equations via
nuclear norm minimization. Allerton Conference 07,
Allerton House, Illinois, 2007.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58(1):267â€“288, 1996.
Wainwright, M. J. Sharp thresholds for high-dimensional
and noisy sparsity recovery using `1 -constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183â€“2202, May 2009.
Yang, E. and Ravikumar, P. Dirty statistical models. In
Neur. Info. Proc. Sys. (NIPS), 26, 2013.
Yang, E., Ravikumar, P., Allen, G. I., and Liu, Z. Graphical models via univariate exponential family distributions. Journal of Machine Learning Research (JMLR),
16:3813â€“3847, 2015.

Sparse + Group Sparse Dirty Models

Yuan, M. and Lin, Y. Model selection and estimation in
regression with grouped variables. Journal of the Royal
Statistical Society B, 1(68):49, 2006.
Zhang, C. H. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(8):894â€“
942, 2010.

