Innovation Pursuit: A New Approach to the Subspace Clustering Problem

Mostafa Rahmani 1 George Atia 1

Abstract
This paper presents a new scalable approach,
termed Innovation Pursuit (iPursuit), to the problem of subspace clustering. iPursuit rests on a
new geometrical idea whereby each subspace is
identified based on its novelty with respect to the
other subspaces. The subspaces are identified
consecutively by solving a series of simple linear optimization problems, each searching for a
direction of innovation in the span of the data. A
detailed mathematical analysis is provided establishing sufficient conditions for the proposed approach to correctly cluster the data points. Moreover, the proposed direction search approach can
be integrated with spectral clustering to yield
a new variant of spectral-clustering-based algorithms. Remarkably, the proposed approach can
provably yield exact clustering even when the
subspaces have significant intersections. The numerical simulations demonstrate that iPursuit can
often outperform the state-of-the-art subspace
clustering algorithms – more so for subspaces
with significant intersections – along with substantial reductions in computational complexity.

1. Introduction
Principal Component Analysis (PCA) is a popular and efficient procedure to approximate the data with a single low
dimensional subspace (Lerman et al., 2015). Nonetheless,
in numerous contemporary applications the data points
may originate from multiple independent sources, in which
case a union of subspaces can better model the data (Vidal,
2011). The problem of subspace clustering is concerned
with learning these low-dimensional subspaces and clustering the data points to their respective subspaces, generally
without prior knowledge about the number of subspaces
and their dimensions, nor the membership of the data points
1
University of Central Florida, Orlando, Florida, USA. Correspondence to: Mostafa Rahmani <mostafa@knights.ucf.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

to these subspaces. Subspace clustering naturally arises
in many machine learning and data analysis problems, including computer vision (e.g. motion segmentation (Vidal
et al., 2008), face clustering (Ho et al., 2003)), image processing (Yang et al., 2008) and system identification (Vidal
et al., 2003). Numerous approaches for subspace clustering
have been studied in prior work, including statistical-based
approaches (Yang et al., 2006; Rao et al., 2010), spectral clustering (Soltanolkotabi et al., 2012; Von Luxburg,
2007; Dyer et al., 2013; Elhamifar & Vidal, 2013; Heckel
& Bölcskei, 2013; Liu et al., 2013; Chen & Lerman, 2009),
the algebraic-geometric approach (Vidal et al., 2005) and
iterative methods (Bradley & Mangasarian, 2000). We refer the reader to (Vidal, 2011) for a comprehensive survey
on the topic.
This paper aims to advance the state-of-the-art research on
subspace clustering on several fronts. First, the proposed
approach – termed iPursuit – rests on a novel geometrical
idea whereby one subspace is identified at a time based on
its novelty with respect to (w.r.t.) the other subspaces. Second, the proposed method is a provable and scalable subspace clustering algorithm – the computational complexity of iPursuit only scales linearly in the number of subspaces and quadratically in their dimensions (c.f. Section
2.5). In contrast to the spectral-clustering-based algorithms
such as (Dyer et al., 2013; Elhamifar & Vidal, 2013; Liu
et al., 2013), which need to solve an M22 -dimensional optimization problem to build the similarity matrix (where
M2 is the number of data points), the proposed method
requires solving few M2 -dimensional linear optimization
problems. This feature makes iPursuit remarkably faster
than the state-of-the-art algorithms. Third, innovation pursuit in the data span enables superior performance when the
subspaces have considerable intersections in comparison to
the state-of-the-art subspace clustering algorithms.
1.1. Notation and definitions
Given a matrix A, kAk denotes its spectral norm. For a
vector a, kak denotes its `2 -norm and kak1 its `1 -norm.
Given matrices {Ai }ni=1 with equal number of rows, we
n

use the union symbol ∪ to define the matrix ∪ Ai :=
i=1

[A1 A2 ... An ] as the concatenation of the matrices
{Ai }ni=1 . For a matrix D, we overload the set member-

Submission and Formatting Instructions for ICML 2017

that the given data matrix follows the following data model.
Data Model 1. The data matrix D ∈ RM1 ×M2 can
be represented as D = [D1 ... DN ]T, where T is an arbitrary permutation matrix. The columns of Di ∈ RM1 ×ni
lie in Si , where Si is an
PNri -dimensional linear subspace,
for 1 ≤ i ≤ N , and, i=1 ni = M2 . Define Vi as an
orthonormal basis for Si . In addition, define D as the
N

space spanned by the data, i.e., D = ⊕ Si . Moreover,
i=1

Figure 1. The subspace I (S2 ⊥ S1 ) is the innovation subspace
of S2 over S1 . The subspace I (S2 ⊥ S1 ) is orthogonal to S1 .

it is assumed that every subspace in the set of subspaces
{Si }N
i=1 has an innovation over the other subspaces,
to say that, for 1 ≤ i ≤ N , the subspace Si does not

ship operator by using the notation d ∈ D to signify that
d is a column of D. A collection
{Gi }ni=1 is
 of subspaces

n
Pn
said to be independent if dim ⊕ Gi = i=1 dim(Gi ),

completely lie in ⊕ Sk . In addition, the columns of D are

where ⊕ denotes the direct sum operator and dim(Gi ) is
the dimension of Gi . Given a vector a, a| is the vector
of absolute values of the elements of a. For a real number
a, sgn(a) denotes the sign of a. The complement of a set
L is denoted Lc . For any positive integer n, the index set
{1, . . . , n} is denoted [n].

2.1. iPursuit: Basic idea

N

k=1
k6=i

normalized, i.e., each column has an `2 -norm equal to one.

i=1

Consider two subspaces S1 and S2 , such that S2 6⊆ S1
and S1 6⊆ S2 . This means that each of the subspaces S1
and S2 carries some innovation w.r.t. the other. As such,
corresponding to each subspace we define an innovation
subspace capturing its novelty (innovation) w.r.t. the other
subspaces, defined formally as follows.
Definition 1. Assume that V1 and V2 are two orthonormal bases for S1 and S2 , respectively. We define the innovation subspace of S2 over S1 , denoted I (S2 ⊥ S1 ),
as the subspace spanned by I − V1 V1T V2 . In other
words, I (S2 ⊥ S1 ) is the complement of S1 in the subspace S1 ⊕ S2 .
Similarly, we can also define I (S1 ⊥ S2 ) as the innovation subspace of S1 over S2 . Fig. 1 illustrates a scenario
in which the data lies in a union of a two-dimensional and
a one-dimensional subspace. Note that the innovation subspace of S2 over S1 is orthogonal to S1 and is the complement of S1 in S1 ⊕ S2 .

2. Proposed Approach
In this section, the main geometrical idea underlying iPursuit is first presented. This idea is based on a non-convex
`0 -norm minimization problem searching for a direction
of innovation in the span of the data. Then, we provide a
convex relaxation to a linear optimization problem, whose
solution is shown to yield the correct subspaces under mild
sufficient conditions. Due to space limitations, the proofs
of all the theoretical results are deferred to an extended version of this paper (Rahmani & Atia, 2015). It is assumed

iPursuit is a multi-step algorithm that identifies one subspace at a time. In each step, the data is clustered into two
subspaces. One subspace is the identified subspace and the
other one is the direct sum of the other subspaces. The
data points of the identified subspace are removed and the
algorithm is applied to the remaining data to find the next
subspace. Accordingly, each step of the algorithm can be
interpreted as a subspace clustering problem with two subspaces. Therefore, for ease of exposition we first investigate the two-subspace scenario then extend the result to
multiple (more than two) subspaces. Thus, in this subsection, it is assumed that the data follows Data model 1 with
N = 2.
To gain some intuition, we first consider a simple example before stating our main result. Suppose that S1 and
S2 are not orthogonal and assume that n2 < n1 . The
non-orthogonality of S1 and S2 is not a requirement, but
is merely used herein to simplify the exposition of the basic idea underlying the proposed approach. Define c∗ as
the optimal point of the following optimization problem
min kĉT Dk0
ĉ

subject to ĉ ∈ D

and kĉk = 1, (1)

where k.k0 is the `0 -norm. The first constraint limits the
search space to the span of the data, and the equality constraint kĉk = 1 is used to avoid the trivial ĉ = 0 solution.
Assume that the columns of D1 and D2 are uniformly distributed in S1 and S2 , respectively. Accordingly, with high
probability (whp) the data is not aligned along any specific
direction in S1 and S2 .
The `0 -norm minimization problem (1) searches for a nonzero vector in D that is orthogonal to the maximum number
of data points. Since c∗ has to lie in D, we claim that the
optimal point of (1) lies in I (S2 ⊥ S1 ) whp given the assumption that the number of data points in S1 is greater

Submission and Formatting Instructions for ICML 2017

than the number of data points in S2 . To clarify, consider
the following cases:
∗

I. If c ∈ S1 , then it would not be orthogonal to the majority of the data points in S1 given that the columns of D1
are uniformly distributed in S1 . In addition, it cannot be
orthogonal to most of the data points in S2 since S1 and
S2 are not orthogonal. Since the optimal vector should be
orthogonal to the maximum number of data points, it is
highly likely that c∗ 6∈ S1 . Similarly, it is highly unlikely
that the optimal point lies in S2 . II. If c∗ ∈ I (S1 ⊥ S2 ),
then according to Definition 1, it is orthogonal to the data
points in D2 . However, since it was assumed that n2 < n1 ,
the cost function of (1) can be decreased if c∗ lies in
I (S2 ⊥ S1 ) (which is orthogonal to S1 ). III. If c∗ does
not lie in any of the subspaces S1 , S2 , I (S2 ⊥ S1 ) and
I (S2 ⊥ S1 ), then it is neither orthogonal to S1 nor to S2 .
Now, since the data points are distributed uniformly in the
subspaces, we see that c∗ would not be orthogonal to the
maximum number of data points.
Hence, it is highly likely that c∗ lies in I (S2 ⊥ S1 ). It
follows that the columns of D corresponding to the nonzero elements of (c∗ )T D lie in S2 . The following lemma
ensures that these columns span S2 .
Lemma 1. The columns of D corresponding to the nonzero elements of (c∗ )T D span S2 if the following conditions are satisfied: (i) c∗ ∈ I (S2 ⊥ S1 ) , (ii) D2 cannot
follow Data model 1 with N > 1, that is, the data points in
D2 do not lie in the union of lower dimensional subspaces
within S2 each with innovation w.r.t. the other subspaces.
It is important to note that the conditions of Lemma 1 are
by no means restrictive. Specifically, if the requirement (ii)
of Lemma 1 is not satisfied, then the problem can be viewed
as a subspace clustering problem with more than two subspaces. In Section 2.4, we will investigate the clustering
problem with more than two subspaces.
Remark 1. At a high level, the innovation search problem (1) finds the most sparse vector in the row space of
D. Interestingly, finding the most sparse vector in a linear subspace has bearing on, and has been effectively used
in, other machine learning problems, including dictionary
learning and spectral estimation (Qu et al., 2014).
2.2. Convex relaxation
The `1 -norm is known to provide an efficient convex relaxation of the `0 -norm. Thus, we relax the non-convex cost
function and rewrite (1) as
min kĉT Dk1
ĉ

subject to ĉ ∈ D

and kĉk = 1 . (2)

Since the feasible set of (2) is non-convex, we further substitute the equality constraint with a linear constraint to

consider the following convex program
kĉT Dk1
(IP) min
ĉ

s. t. ĉ ∈ D

and ĉT q = 1. (3)

(IP) is the core program of iPursuit to find a direction of
innovation. The vector q is a unit `2 -norm vector that
should not be orthogonal to D. In Section 2.6, we present a
methodology to obtain a good choice for the vector q from
the given data. However, our investigations have shown
that iPursuit performs well generally even when q is chosen as a random vector in D.
2.3. Segmentation of two subspaces: Performance
guarantees
Suppose that D follows Data model 1 with N = 2, i.e.,
the data lies in a union of two subspaces. In order to show
that the optimal point of (IP) yields correct clustering, it
suffices to show that the optimal point lies in I (S2 ⊥ S1 )
given that condition (ii) of Lemma 1 is satisfied for D2 (or
lies in I (S1 ⊥ S2 ) given that the condition is satisfied for
D1 ). The following theorem provides sufficient conditions
for the optimal point of (3) to lie in I (S2 ⊥ S1 ) provided
that
inf

c∈I(S2 ⊥S1 )
cT q=1

kcT Dk1 <

inf

c∈I(S1 ⊥S2 )
cT q=1

kcT Dk1 .

(4)

If the inequality in (4) is reversed, then similar sufficient
conditions can be established for the optimal point of (3)
to lie in I (S1 ⊥ S2 ). Hence, assumption (4) does not
lead to any loss of generality. The subspaces I (S2 ⊥ S1 )
and I (S1 ⊥ S2 ) are orthogonal to S1 and S2 , respectively.
Thus, (4) is equivalent to
inf

c∈I(S2 ⊥S1 )
cT q=1

kcT D2 k1 <

inf

c∈I(S1 ⊥S2 )
cT q=1

kcT D1 k1 .

(5)

Henceforth, “innovation subspace” refers to I (S2 ⊥ S1 )
whenever the two-subspace scenario is considered and (5)
is satisfied. Theorem 2 stated next provides sufficient
conditions for the optimal point of (IP) in (3) to lie in
I (S2 ⊥ S1 ). These conditions are characterized in terms
of the optimal solution to an oracle optimization problem
(OP), wherein the feasible set of (IP) is replaced by the innovation subspace. Define c2 as the optimal point of the
following optimization problem
min kĉT D2 k1
(OP)

ĉ

subject to ĉ ∈ I (S2 ⊥ S1 ) and

ĉT q = 1.

(6)

Theorem 2 establishes that c2 is the optimal point of (3).
Before we state the theorem, we define the index set L0 :=
{i ∈ [n2 ] : cT2 di = 0, di ∈ D2 }, with cardinality n0 =
|L0 | and a complement set Lc0 , comprising the indices of
the columns of D2 orthogonal to c2 .

Submission and Formatting Instructions for ICML 2017

Theorem 2. Suppose the data matrix D follows Data
model 1 with N = 2. Also, assume that condition (5) and
the requirement of Lemma 1 for D2 are satisfied (condition (ii) of Lemma 1). Let c2 be the
of the
P optimal point
T
oracle (OP) in (6) and define α =
di ∈D2 sgn(c2 di )di .
c

with the innovation subspace is an important performance
factor for iPursuit. The coherence property could have a
more serious effect on the performance of the algorithm for
non-independent subspaces, especially when the dimension
of their intersection is significant. For instance, consider
i∈L0
the scenario where the vector q is chosen randomly from
Also, let P2 denote an orthonormal basis for I (S2 ⊥ S1 )
D, and define y as the dimension of the intersection of S1
and assume that q is a unit `2 -norm vector in D that is not
and S2 . It follows that I (S2 ⊥ S1 ) has dimension r2 − y.
orthogonal to I (S2 ⊥ S1 ). If
E[kqT P2 k]
r2 −y
Thus, E[kq
T V k] =


r1 . Therefore, a randomly chosen
1
X
 T 
1
vector q is likely to have a small projection on the innoδ di  > kV1T V2 k kαk + n0 , and
inf
δ∈S1
2 kδk=1
vation subspace when y is large. As such, in dealing with
di ∈D1
!
subspaces with significant intersection, it may not be favor

X 

kqT P2 k
T
T


δ di > kV2 P2 k kαk + n0 , able to choose the vector q at random. In Section 2.6 and
inf
δ∈S1
2kqT V1 k kδk=1
section 2.7, we develop a simple technique to learn a good
di ∈D1
choice for q from the given data. It makes iPursuit remark(7)
ably powerful in dealing with subspaces with intersection
as shown in the numerical results section.
then c2 ∈ I (S2 ⊥ S1 ) is the optimal point of (IP) in (3),
Remark 2. We conjecture that if the ratios {ni /ri }N
and iPursuit clusters the data correctly.
i=1 are
sufficiently large, the optimal point of (2) always lies in an
In what follows, we provide a detailed discussion of the
innovation subspace; our investigations have shown that
significance of the sufficient conditions (7) of Theorem 2,
the optimal point of (IP) always lies in an innovation subwhich reveal some interesting facts about the properties of
space provided that q is sufficiently coherent with the iniPursuit.
novation subspace regardless how large the intersections
between the subspaces are. This is particularly compelling
1. Data distribution: The LHS of (7) is known as the perif the subspaces are too close, in which case spectralmeance statistic, an efficient measure of how well the data
clustering-based methods can seldom construct a correct
points are distributed in a subspace (Lerman et al., 2015).
similarity matrix (which leads to large clustering errors).
As such, the sufficient conditions (7) imply that the distriBy contrast, in such cases iPursuit can yield accurate clusbution and the number of the data points within the subtering provided that the constraint vector is sufficiently cospaces are important performance factors for iPursuit. For
herent with the innovation subspace. In Section 2.6, we
a set of data points Di in a subspace SP
i , the permeance
 T 
present a method to identify a coherent constraint vector.


statistic is defined as P(Di , Si ) = inf
di ∈Di u di .
u∈Si
kuk=1
Now, we demonstrate that the sufficient conditions (7) are
From this definition, we see that the permeance statistic is
not restrictive. The following lemma simplifies the condifairly small if a set of data points are aligned along a given
tions when the data points are randomly distributed in the
direction (i.e. not well distribued). In addition, having n0
subspaces. In this setting, we show that the conditions are
on the RHS reveals that the distribution of the data points
naturally satisfied.
within S2 also matters since c2 cannot be simultaneously
Lemma 3. Assume that the distribution of the columns of
orthogonal to a large number of columns of D2 if the data
D1 in S1 and D2 in S2 is uniformly random and consider
does not align along specific directions. Hence, according
the same setup of Theorem 2. If
to (7), iPursuit yields correct clustering if the data is well
r
r
√
distributed within the subspaces.
2 n1
n1
− 2 n1 − t1
π
r
r
1
1−1
2. The coherency of q with I (S2 ⊥ S1 ): For the optimal


√
point of (IP) to lie in I (S2 ⊥ S1 ), the vector q should not
T
> 2kV1 V2 k t2 n2 − n0 + n0 ,
be too coherent with S1 . This can be seen by observing that
!
r
(8)
r
if q has a small projection on I (S2 ⊥ S1 ) – in which case
√
kqT P2 k
2 n1
n1
it would be more coherent with S1 – the Euclidean norm
−
2
n
−
t
1
1
kqT V1 k
π r1
r1 − 1
of any feasible point of (3) lying in I (S2 ⊥ S1 ) will have


√
to be large to satisfy the equality constraint in (IP). Such
T
>
2kV
P
k
t
n
−
n
+
n
,
2
2
2
0
0
2
points are less likely to be optimal in the sense of attaining
the minimum of the objective function in (3). The aforementioned intuition is affirmed by the analysis. Indeed,
the factor kqT P2 k/kqT V1 k in the sufficient conditions of
Theorem 2 and Lemma 3 indicates that the coherency of q

then the optimal point of (3) lies in I (S2 ⊥ S1 ) with

r2 2
2
probability
 2  at least 1 − exp − 2 (t2 − log(t2 ) − 1) −
t
exp − 21 , for all t2 > 1 , t1 ≥ 0.

Submission and Formatting Instructions for ICML 2017

When the data points are not aligned along any specific
directions, c2 can only be simultaneously orthogonal to a
small number of columns of D2 . Thus, n0 will be much
smaller than n2 . The LHS of (8) has order n1 and the
√
RHS has order n2 + n0 (which is much smaller than n2 ).
Therefore, the sufficient conditions are naturally satisfied
when the data is well distributed within the subspaces.

Table 1. Run time of different algorithms (M1 = 50, N = 3,
{ri }3i=1 = 10, {ni }3i=1 = M2 /3)

M2
300
3000
15000
30000

SSC
1.1 s
209 s
>2h
>2h

LRR
0.9 s
26 s
2800
>2h

iPursuit
0.14 s
0.35 s
2.78 s
10.6 s

SSC-OMP
1.6 s
56 s
5340 s
>2h

2.4. Clustering multiple subspaces

sequently, we can also establish sufficient conditions for
exact subspace segmentation. Due to space limitations, we
defer the analysis to (Rahmani & Atia, 2015).

Remark 3. The proposed method brings about substantial speedups over existing algorithms due to the following:
i) unlike existing multi-step algorithms (such as RANSAC)
which have exponential complexity in the number and dimension of subspaces, the complexity of iPursuit is linear
in the number of subspaces and quadratic in their dimension. In addition, while iPursuit has linear complexity in
M2 , spectral-clustering-based algorithms have complexity
O(M22 N ) for their spectral clustering step plus the complexity of obtaining the similarity matrix; ii) more importantly, the solver of the proposed optimization problem has
O(rM2 ) complexity per iteration, while the other operations – whose complexity are O(r2 M2 ) and O(r3 ) – sit
outside of the iterative solver. This feature makes the proposed method notably faster than most of the existing algorithms which solve high-dimensional optimization problems. For instance, solving the optimization problem of the
SSC algorithm has roughly O(M23 + rM2 ) complexity per
iteration (Elhamifar & Vidal, 2013).
For instance, suppose M1 = 100, the data lies in a union of
three 10-dimensional subspaces and ni = M2 /3. Table 1
compares the running time of the algorithms. One can observe that iPursuit is remarkably faster. More running time
comparisons are available in (Rahmani & Atia, 2015).

2.5. Complexity analysis

2.6. How to choose the vector q?

Define U as an orthonormal basis for D.
Thus,
the optimization problem (3) is equivalent to
min kaT UT Dk1 subject to aT UT q = 1. Fur-

Our investigations have shown that iPursuit performs very
well when the subspaces are independent or have small intersections even if q is chosen randomly. However, in the
more challenging scenarios in which the dimensions of the
intersections between the subspaces are significant, randomly choosing the vector q could be unfavorable since
the dimension of the innovation subspace decreases as the
dimension of the intersection increases. This motivates
the methodology described next that aims to search for a
“good” vector q. Consider the optimization problem

Suppose that D follows Data Model 1 with N = m where
m > 2. Similar to (5), without loss of generality assume
that
inf
m
c∈I Sm ⊥ ⊕ Sk
k=1
k6=m
cT qm =1

T
 kc Dm k1 <

inf
m
c∈I Sj ⊥ ⊕ Sk
k=1
k6=j
cT qm =1

T
 kc Dj k1 (9)

for all 1 ≤ j ≤ m − 1 , where qm is a unit `2 -norm in
⊕m
k=1 Sk . Given (9), we expect the optimal point of (IP)
with q = qm to lie in the innovation subspace of Sm over

m
m
⊕ Sk , i.e., I Sm ⊥ ⊕ Sk , in which case Sm will be
k=1
k6=m

k=1
k6=m

identified. Accordingly, in this step the clustering probm−1

m−1

i=1

i=1

lem separates (Dm , Sm ) and ( ∪ Di , ⊕ Si ). Theorem
2 can be used to derive sufficient conditions for the optimal

m
point to lie in I Sm ⊥ ⊕ Sk by substituting (D2 , S2 )
k=1
k6=m

m−1

m−1

i=1

i=1

with (Dm , Sm ) and (D1 , S1 ) with ( ∪ Di , ⊕ Si ). Con-

a

ther, define f = UT q. This optimization problem can be
efficiently solved using the Alternating Direction Method
of Multipliers (ADMM) (Boyd et al., 2011). Due to space
constraints, we defer the details of the iterative solver
to (Rahmani & Atia, 2015).
The complexity of the
initialization step of the solver is O(r3 ) plus the complexity of obtaining U. Obtaining an appropriate U has
O(r2 M2 ) complexity by applying the clustering algorithm
to a random subset of the rows of D (with the rank of
sampled rows equal to r). In addition, the complexity of
each iteration of the solver is O(rM2 ). Thus, the overall
complexity is less than O((r3 + r2 M2 )N ) since the
number of data points remaining keeps decreasing over
the iterations. In most cases, r  M2 , hence the overall
complexity is roughly O(r2 M2 N ).

min kq̂T Dk2
q̂

subject to q̂ ∈ D

and

kq̂k = 1, (10)

which searches for a non-zero vector in D with small projections on the columns of D. It is straightforward to show
that the optimal point of (10) is the singular vector corresponding to the least non-zero singular value of D. When
the subspaces are close to each other, it is not hard to see

Submission and Formatting Instructions for ICML 2017

that the least singular vector is highly coherent with the innovation subspace, thus can be a good candidate for the
vector q. For subspaces with remarkable intersections, this
choice of q brings about substantial improvement in performance compared to using a randomly generated q (cf.
Section 3). Clearly, when the data is noisy, we utilize the
least dominant singular vecor. In addition, when the singular values of the noisy data decay rapidly, it may be hard
to accurately estimate the rank of D, which may lead to an
unfavorable use of a singular vector corresponding to noise
as the constraint vector. Alternatively, we can choose the
data point closest to the least dominant singular vector as
our vector q. This technique makes the proposed method
robust to the presence of noise (cf. Section 2.7).
2.7. Noisy data
In the presence of additive noise, we model the data as
De = D + E , where De is the given noisy data matrix,
D is the clean data which follows Data model 1 and E represents the noise component. The rank of D is equal to
r. Thus, the singular values of De can be divided into two
subsets: the dominant singular values (the first r singular
values) and the small singular values (or the singular values corresponding to the noise component). Estimating the
number of dominant singular values is a fairly well-studied
topic (Stoica & Selen, 2004).
Consider the optimization problem (IP) using De , i.e.,
min kĉT De k1
ĉ

s.t.

ĉ ∈ span(De ) and ĉT q = 1. (11)

Clearly, the optimal point of (11) is very close to the subspace spanned by the singular vectors corresponding to the
small singular values. Thus, if ce denotes the optimal solution of (11), then all the elements of cTe De will be fairly
small and the subspaces cannot be distinguished. However,
the span of the dominant singular vectors is approximately
equal to D. Accordingly, we propose the following approximation to (IP),
min kĉT De k1
ĉ

s.t.

ĉ ∈ span(Q) and ĉT q = 1 (12)

where Q is an orthonormal basis for the span of the dominant singular vectors. The first constraint of (12) forces
the optimal point to lie in span(Q), which serves as a
good approximation to span(D). For instance, consider
D = [D1 D2 ], where the columns of D1 ∈ R40×100
lie in a 5-dimensional subspace S1 , and the columns of
D2 ∈ R40×100 lie in another 5-dimensional subspace S2 .
Define ce and cr as the optimal points of (11) and (12), respectively. Fig. 2 shows |cTe De | and |cTr De | with the maximum element scaled to one. Clearly, cTr De can be used to
correctly cluster the data. In addition, when D is low rank,
the subspace constraint in (12) can filter out a remarkable
portion of the noise component.

Figure 2. The left plot shows the output of (11), while the right
plot shows the output of iPursuit when its search domain is restricted to the subspace spanned by the dominant singular vectors
as per (12).

When the data is noisy and the singular values of D decay rapidly, it may be hard to accurately estimate r. If the
dimension is incorrectly estimated, Q may contain some
singular vectors corresponding to the noise component,
wherefore the optimal point of (12) could end up lying
close to a noise singular vector. In the sequel, we present
two effective techniques to effectively avoid this undesirable scenario.
1. Using a data point as a constraint vector: A singular vector corresponding to the noise component is nearly
orthogonal to the entire data, i.e., has small projection on
all the data points. Thus, if the optimal vector is forced to
have strong projection on a data point, it will be unlikely for
the optimal direction to be close to a noise singular vector.
Thus, we modify (12) as follows
min kĉT De k1
ĉ

s.t.

ĉ ∈ span(Q) and ĉT dek = 1 , (13)

where dek is the k th column of De . The modified constraint in (13) ensures that the optimal point is not orthogonal to dek . If dek lies in the subspace Si , the optimal point
of (13) will lie in the innovation subspace corresponding
to Si whp. In order to determine a good data point for
the constraint vector, we leverage the principle presented
in section 2.6. Specifically, we use the data point that is
closest to the least dominant singular vector rather than the
least dominant singular vector itself.
2. Sparse representation of the optimal point: When D
is low rank, i.e., r  min(M1 , M2 ), any direction in the
span of the data – including the optimal direction sought by
iPursuit – can be represented as a sparse combination of the
data points. For such settings, we propose the alternative
optimization problem
min
a,z

kaT QT De k1 + γkzk1

subject to a = QT De z and

aT QT dek = 1 ,

(14)

where γ is a regularization parameter. Forcing a sparse
representation in (14) for the optimal direction averts a solution that lies in close proximity with the small singular
vectors, which are normally obtained through linear combinations of a large number of data points. This alternative

Submission and Formatting Instructions for ICML 2017

formulation is particularly useful when the dimension of
the data cannot be accurately estimated. When D is not
a low rank matrix, we can set γ equal to zero. The table
of Algorithm 1 details the proposed method for noisy data
along with the used notation and definitions.
2.7.1. E RROR PROPAGATION
If κ (or ci ) and the threshold co in Algorithm 1 are chosen appropriately, the algorithm exhibits strong robustness
in the presence of noise. Nonetheless, if the data is too
noisy, an error incurred in one step of the algorithm may
propagate and unfavorably affect the performance in subsequent steps. Two types of error could occur. The first
type is that some data points are erroneously included in
G1 or G2 . An example is when Sm is the subspace to be
identified in a given step of the algorithm (i.e., the optimal
point of (13) lies in the innovation subspace corresponding
to Sm ), but few data points from the other subspaces are erroneously included in G1 . The second type of error is that
some of the data points remain unidentified. For instance,
Sm is to be identified in a given iteration, yet not all the data
points belonging to Sm are identified. In an extended version of this work (Rahmani & Atia, 2015), we discuss the
two main sources of error and present some techniques to
effectively neutralize their impact on subsequent iterations.
In addition, a brief discussion about handling the presence
of outliers is provided.
2.8. Subspace clustering using direction search and
spectral clustering
In (Rahmani & Atia, 2017), we showed that the direction search optimization problem (13) can be utilized to
find a neighborhood set for the k th data point. We leveraged this feature to propose a new spectral-clusteringbased subspace segmentation algorithm, dubbed Direction
search based Subspace Clustering (DSC) (Rahmani & Atia,
2017). We showed that DSC often outperforms the existing spectral-clustering-based methods particularly for hard
scenarios involving high levels of noise and close subspaces, and notably improves the state-of-the-art result for
the challenging problem of face segmentation using subspace clustering.

3. Numerical Simulations
3.1. The coherency parameter
In this experiment, we examine the impact of the coherency
of q with the innovation subspace. Here, it is assumed that
the data follows Data Model 1 with N = 2 and M1 = 50.
The dimension of the subspaces is equal to 15 and the dimension of their intersection varies between 0 to 14. Each
subspace contains 100 data points distributed uniformly at

random within the subspace. Let cr = kqT P2 k/kqT V1 k.
Thus, cr captures the coherency of q with the innovation
subspace. Define V̂1 and V̂2 as orthonormal bases for the
identified subspaces. A trial is considered successful if
k(I − V1 V1T )V̂1 kF + k(I − V2 V2T )V̂2 kF ≤ 10−3 . (15)
The left plot of Fig. 3 shows the phase transition in the
plane of cr and y, where y is the dimension of the intersection of the two subspaces. In this figure, white designates exact identification of the subspaces with probability
almost equal to one. As shown, the probability of correct
clustering increases if cr is increased. Remarkably, the left
plot of Fig. 3 shows that when cr is sufficiently large, the
algorithm yields exact segmentation even when y = 14.

Figure 3. Left: Phase transition plot in the plane of the coherency
parameter and the dimension of intersection. Right: The clustering error of iPursuit, SSC and LRR versus the dimension of the
intersection.

3.2. Clustering data in union of multiple subspaces
In this simulation, we consider the subspace clustering
problem with 15 30-dimensional subspaces {Si }15
i=1 and
M1 = 500. Each subspace contains 90 data points and the
distribution of the data within the subspaces is uniformly
random. We compare the performance of the proposed
approach to the state-of-the-art sparse subspace clustering
(SSC) (Elhamifar & Vidal, 2013) algorithm and low rank
representation (LRR) based clustering (Liu et al., 2013).
The number of replicates used in spectral clustering for
SSC and LRR is equal to 20. Define the clustering error
as the ratio of misclassified points to the total number of
data points. The right plot of Fig. 3 shows the clustering
error versus the dimension of the intersection. The dimension of intersection varies between 1 and 29. Each point in
the plot is obtained by averaging over 40 independent runs.
iPursuit is shown to yield the best performance.
3.3. Noisy data
In this section, we study the performance of the proposed
approach, SSC, LRR, SCC (Chen & Lerman, 2009), TSC
(Heckel & Bölcskei, 2013) and SSC-OMP (Dyer et al.,

Submission and Formatting Instructions for ICML 2017
Table 2. CE (%) of algorithms on Hopkins155 dataset (Mean - Median).

N
N =2
N =3

SSC
1.52 - 0
4.40 - 1.56 s

LRR
2.13 - 0
4.03 - 1.43

iPursuit
3.33 - 0.27
6.91 - 2.44

SSC-OMP
16.92 - 12.77 s
27.96 - 30.98

TSC
18.44 - 16.92
28.58 - 29.67

K-flats
13.62 - 10.65
14.07 - 14.18

SCC
2.06 - 0
6.37 - 0.21

Thus, the problem here is to cluster data lying in two or
three subspaces. Table 2 shows the clustering error (in percentage) for iPursuit, SSC, LRR, TSC, SSC-OMP and Kflats. We use the results reported in (Elhamifar & Vidal,
2013; Heckel & Bölcskei, 2013; Vidal, 2011; Park et al.,
2014). For SSC-OMP and TSC, the number of parameters
for motion segmentation are equal to 8 and 10. One can
observe that iPursuit yields competitive results comparable
to SSC, SCC, and LRR and outperforms TSC, SSC-OMP
and K-flats.
Algorithm 1 Innovation pursuit (iPursuit) for noisy data
Initialization Set κ, n̂ and N̂ as integers greater than 1, and set
ci and co as positive real numbers less than 1.

Figure 4. Performance of the algorithms versus the dimension of
intersection for different noise levels.

2013) with different noise levels, and varying dimensions
of the intersection between the subspaces, which gives rise
to both low rank and high rank data matrices. It is assumed
that D follows Data model 1 with M1 = 100, M2 = 500,
N = 6 and {ri }6i=1 = 15. The dimension of the intersection between the subspaces varies from 0 to 14. Thus, the
rank of D ranges from 20 to 90. The Noisy data is modeled as De = D + E, with the elements of E sampled independently from a zero mean Gaussian distribution. Fig.
4 shows the performance of the different algorithms verkEkF
sus the dimension of the intersection for τ = kDk
equal
F
to 1/20, 1/10, 1/5 and 1/2. One can observe that even
with τ = 1/5, iPursuit significantly outperforms the other
algorithms. In addition, when the data is very noisy, i.e.,
τ = 1/2, it yields better performance when the dimension of the intersection is large. SSC, LRR, and SSC-OMP
yield a better performance for lower dimension of intersection. This is explained by the fact that the rank of the data
is high when the dimension of the intersection is low, and
the subspace projection operation QT De may not always
filter out the additive noise effectively.
3.4. Real data
We apply iPursuit to the problem of motion segmentation
using the Hopkins155 (Tron & Vidal, 2007) dataset, which
contains video sequences of 2 or 3 motions. In motion
segmentation, each motion corresponds to one subspace.

While The number of identified subspaces is less than N̂ or the
number of the columns of De is less than n̂.
1. Obtaining the basis for the remaining Data: Construct Q as
the orthonormal matrix formed by the dominant singular vectors
of De .
2. Choosing the vector q: Set q = the column of De closest to
the last column of Q.
∗
∗
3. Solve (14) and define c∗ =
 Qa  , where a is the optimal point
DTe c∗ 

 .
of (14) and define h1 =
∗
max(DT
e c )
4. Finding a basis for the identified subspace: Construct G1 as
the matrix consisting of the columns of De corresponding to the
elements of h1 that are greater than ci . Alternatively, construct
G1 using the columns of De corresponding to the κ largest elements of h1 . Define F1 as an orthonormal basis for the dominant
left singular vectors of G1 .
5. Finding a basis for the rest of the data:
Define the vector h2 whose entries are equal to the `2 -norm of the
columns of (I − F1 FT1 )De . Scale h2 as h2 := h2 / max(h2 ).
Construct G2 as the columns of De corresponding to the elements
of h2 greater than co . Define F2 as an orthonormal basis for the
dominant left singular vectors of of G2 .
6. Find the data point belonging to the identified subspace:
Assign dei to the identified subspace if kFT1 dei k ≥ kFT2 dei k.
7. Remove the data points belonging to the identified subspace: Update De by removing the columns corresponding to
the identified subspace.
End While

Acknowledgment: This work was supported by NSF
CAREER Award CCF-1552497 and NSF Grant CCF1320547.

Submission and Formatting Instructions for ICML 2017

References
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine Learning,
3(1):1–122, 2011.
Bradley, Paul S and Mangasarian, Olvi L. k-plane clustering. Journal of Global Optimization, 16(1):23–32, 2000.
Chen, Guangliang and Lerman, Gilad. Spectral curvature
clustering (scc). International Journal of Computer Vision, 81(3):317–330, 2009.
Dyer, Eva L, Sankaranarayanan, Aswin C, and Baraniuk,
Richard G. Greedy feature selection for subspace clustering. The Journal of Machine Learning Research, 14
(1):2487–2517, 2013.
Elhamifar, Ehsan and Vidal, Rene. Sparse subspace clustering: Algorithm, theory, and applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35
(11):2765–2781, 2013.
Heckel, Reinhard and Bölcskei, Helmut. Robust subspace clustering via thresholding.
arXiv preprint
arXiv:1307.4891, 2013.
Ho, Jason, Yang, Ming-Hsuan, Lim, Jongwoo, Lee, KuangChih, and Kriegman, David. Clustering appearances of
objects under varying illumination conditions. In Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR), volume 1, 2003.

Rahmani, Mostafa and Atia, George. Innovation pursuit:
A new approach to subspace clustering. arXiv preprint
arXiv:1512.00907, 2015.
Rahmani, Mostafa and Atia, George. A direction search
and spectral clustering based approach to subspace clustering. arXiv preprint, 2017.
Rao, Shankar, Tron, Roberto, Vidal, Rene, and Ma, Yi.
Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(10):1832–
1845, 2010.
Soltanolkotabi, Mahdi, Candes, Emmanuel J, et al. A geometric analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
Stoica, Petre and Selen, Yngve. Model-order selection: a
review of information criterion rules. Signal Processing
Magazine, IEEE, 21(4):36–47, 2004.
Tron, Roberto and Vidal, René. A benchmark for the comparison of 3-d motion segmentation algorithms. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1–8, 2007.
Vidal, René, Soatto, Stefano, Ma, Yi, and Sastry, Shankar.
An algebraic geometric approach to the identification of
a class of linear hybrid systems. In Proceedings of the
42nd IEEE Conference on Decision and Control (CDC),
volume 1, pp. 167–172, 2003.
Vidal, Rene, Ma, Yi, and Sastry, Shankar. Generalized
principal component analysis (GPCA). Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27
(12):1945–1959, 2005.

Lerman, Gilad, McCoy, Michael B, Tropp, Joel A, and
Zhang, Teng. Robust computation of linear models by
convex relaxation. Foundations of Computational Mathematics, 15(2):363–410, 2015.

Vidal, René, Tron, Roberto, and Hartley, Richard. Multiframe motion segmentation with missing data using
powerfactorization and GPCA. International Journal of
Computer Vision, 79(1):85–105, 2008.

Liu, Guangcan, Lin, Zhouchen, Yan, Shuicheng, Sun, Ju,
Yu, Yong, and Ma, Yi. Robust recovery of subspace
structures by low-rank representation. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 35(1):
171–184, 2013.

Vidal, Rene. Subspace clustering. IEEE Signal Processing
Magazine, 2(28):52–68, 2011.
Von Luxburg, Ulrike. A tutorial on spectral clustering.
Statistics and computing, 17(4):395–416, 2007.

Park, Dohyung, Caramanis, Constantine, and Sanghavi,
Sujay. Greedy subspace clustering. In Advances in
Neural Information Processing Systems, pp. 2753–2761,
2014.

Yang, Allen Y, Rao, Shankar R, and Ma, Yi. Robust
statistical estimation and segmentation of multiple subspaces. In Computer Vision and Pattern Recognition
Workshop, 2006. CVPRW’06. Conference on, pp. 99–99.
IEEE, 2006.

Qu, Qing, Sun, Ju, and Wright, John. Finding a sparse vector in a subspace: Linear sparsity using alternating directions. In Advances in Neural Information Processing
Systems, pp. 3401–3409, 2014.

Yang, Allen Y, Wright, John, Ma, Yi, and Sastry,
S Shankar. Unsupervised segmentation of natural images via lossy data compression. Computer Vision and
Image Understanding, 110(2):212–225, 2008.

