A Semismooth Newton Method for Fast, Generic Convex Programming

Alnur Ali * 1 Eric Wong * 1 J. Zico Kolter 2
i.e., the space of m × m positive semidefinite matrices Sm
+,
yields a semidefinite program; and taking K as the secondorder (or Lorentz) cone {(x, y) ∈ Rm−1 × R : kxk2 ≤ y}
yields a second-order cone program (a quadratic program
is a special case).

Abstract
We introduce Newton-ADMM, a method for fast
conic optimization. The basic idea is to view
the residuals of consecutive iterates generated by
the alternating direction method of multipliers
(ADMM) as a set of fixed point equations, and
then use a nonsmooth Newton method to find a
solution; we apply the basic idea to the Splitting Cone Solver (SCS), a state-of-the-art method
for solving generic conic optimization problems.
We demonstrate theoretically, by extending the
theory of semismooth operators, that NewtonADMM converges rapidly (i.e., quadratically) to
a solution; empirically, Newton-ADMM is significantly faster than SCS on a number of problems. The method also has essentially no tuning parameters, generates certificates of primal
or dual infeasibility, when appropriate, and can
be specialized to solve specific convex problems.

1. Introduction and related work
Conic optimization problems (or cone programs) are convex optimization problems of the form
minimize
cT x
n
x∈R

subject to b − Ax ∈ K,

(1)

where c ∈ Rn , A ∈ Rm×n , b ∈ Rm , K are problem data,
specified by the user, and K is a proper cone (Nesterov &
Nemirovskii, 1994; Ben-Tal & Nemirovski, 2001; Boyd &
Vandenberghe, 2004); we give a formal treatment of proper
cones in Section 2, but a simple example of a proper cone,
for now, is the nonnegative orthant, i.e., the set of all points
in Rm with nonnegative components. These problems are
quite general, encapsulating a number of standard problem
classes: e.g., taking K as the nonnegative orthant yields a
linear program; taking K as the positive semidefinite cone,
*
Equal contribution 1 Machine Learning Department, Carnegie
Mellon University 2 Computer Science Department, Carnegie
Mellon University. Correspondence to: Alnur Ali <alnurali@cmu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Due, in part, to their generality, cone programs have been
the focus of much recent work, and additionally form the
basis of many convex optimization modeling frameworks,
e.g., sdpsol (Wu & Boyd, 2000), YALMIP (Lofberg, 2005),
and the CVX family of frameworks (Grant, 2004; Diamond
& Boyd, 2016; Udell et al., 2014). These frameworks generally make it easy to quickly solve small and mediumsized convex optimization problems to high accuracy; they
work by allowing the user to specify a generic convex optimization problem in a way that resembles its mathematical
representation, then convert the problem into a form similar to (1), and finally solve the problem. Primal-dual interior point methods, e.g., SeDuMi (Sturm, 2002), SDPT3
(Toh et al., 2012), and CVXOPT (Andersen et al., 2011),
are common for solving these cone programs. These methods are useful, as they generally converge to high accuracy
in just tens of iterations, but they solve a Newton system
on each iteration, and so have difficulty scaling to highdimensional (i.e., large-n) problems.
In recent work, O’Donoghue et al. (2016) use the alternating direction method of multipliers (ADMM) (Boyd
et al., 2011) to solve generic cone programs; operator splitting methods (e.g., ADMM, Peaceman-Rachford splitting
(Peaceman & Rachford, 1955), Douglas-Rachford splitting (Douglas & Rachford, 1956), and dual decomposition)
generally converge to modest accuracy in just a few iterations, so the approach (called the splitting conic solver, or
SCS) is scalable, and also has a number of other benefits,
e.g., provding certificates of primal or dual infeasibility.
In this paper, we introduce a new method (called “NewtonADMM”) for solving large-scale, generic cone programs
rapidly to high accuracy. The basic idea is to view the usual
ADMM recurrence relation as a fixed point iteration, and
then use a truncated, nonsmooth Newton method to find a
fixed point; to justify the approach, we extend the theory
of semismooth operators, coming out of the applied mathematics literature over the last two decades (Mifflin, 1977;
Qi & Sun, 1993; Martı́nez & Qi, 1995; Facchinei et al.,

A Semismooth Newton Method for Fast, Generic Convex Programming

1996), although it has received little attention from the machine learning community (Ferris & Munson, 2004). We
apply the approach to the fixed point iteration associated
with SCS, to obtain a general purpose conic optimizer. We
show, under regularity conditions, that Newton-ADMM is
quadratically convergent; empirically, Newton-ADMM is
significantly faster than SCS, on a number of problems.
Also, Newton-ADMM has essentially no tuning parameters, and generates certificates of infeasibility, helpful in
diagnosing problem misspecification.
The rest of the paper is organized as follows. In Section
2, we give the background on cone programs, SCS, and
semismooth operators, required to derive our method for
solving generic cone programs, Newton-ADMM. . In Section 3, we present Newton-ADMM, and establish some of
its basic properties. In Section 4, we give various convergence guarantees. In Section 5, we empirically evaluate
Newton-ADMM, and describe an extension as a specialized solver. We conclude with a discussion in Section 6.

2. Background
We first give some background on cones. Using this background, we go on to describe SCS, the cone program solver
of O’Donoghue et al. (2016), in more detail. Finally, we
give an overview of semismoothness (Mifflin, 1977), a generalization of smoothness, central to our Newton method.
2.1. Cone programming
We say that a set C is a cone if, for all x ∈ C, and θ ≥ 0,
we get that θx ∈ C. The dual cone C ∗ , associated with the
cone C, is defined as the set {y : y T x ≥ 0, ∀x ∈ C}.
Additionally, a cone C is a convex cone if, for all x, y ∈ C,
and θ1 , θ2 ≥ 0, we get that θ1 x + θ2 y ∈ C. A cone C
is a proper cone if it is (i) convex; (ii) closed; (iii) solid,
i.e., its interior is nonempty; and (iv) pointed, i.e., if both
x, −x ∈ C, then we get that x = 0.
The nonnegative orthant, second-order cone, and positive
semidefinite cone are all proper cones (Boyd & Vandenberghe, 2004, Section 2.4.1); these cones, along with the
exponential cone (defined below), can be used to represent
most convex optimization problems encountered in practice. The exponential cone (see, e.g., Serrano (2015)), Kexp ,
is a three-dimensional proper cone, defined as the closure
of the epigraph of the perspective of exp(x), with x ∈ R:
Kexp = {(x, y, z) : x ∈ R, y > 0, z ≥ y exp(x/y)}
∪ {(x, 0, z) : x ≤ 0, z ≥ 0} .
Cone programs resembling (1) were first described by Nesterov & Nemirovskii (1994, page 67), although special
cases were, of course, considered earlier. Standard refer-

ences include Ben-Tal & Nemirovski (2001) and Boyd &
Vandenberghe (2004, Section 4.6.1).
2.2. SCS
Roughly speaking, SCS is an application of ADMM to
a particular feasibility problem arising from the KarushKuhn-Tucker (KKT) optimality conditions associated with
a cone program. To see this, consider a reformulation of
the cone program (1), with slack variable s ∈ Rm :
minimize
cT x subject to Ax + s = b, s ∈ K.
n
x∈R , s

(2)

The KKT conditions can be seen, after introducing dual
variables r ∈ Rn , y ∈ K∗ , for the implicit constraint x ∈
Rn and the explicit constraints, respectively, to be
AT y + c = r
Ax + s = b, s ∈ K
n

r ∈ {0} , y ∈ K
T

∗

(stationarity)
(primal feasibility)
(dual feasibility)

T

−c x − b y = 0 (complementary slackness),
where K∗ is the dual cone of K; thus, we can obtain a solution to (2), by solving the KKT system



 



0
AT
c
r
x
 −A
0 
+  b  =  s  , (3)
y
T
T
0
0
−c
−b
x ∈ Rn , y ∈ K∗ , r ∈ {0}n , s ∈ K.
Self-dual homogeneous embedding. When the cone
program (2) is primal/dual infeasible, there is no solution
to the KKT system (3); so, consider embedding the system
(3) in a larger system, with new variables τ, κ, and solving


 

0
AT c
x
r
 −A
0
b  y  =  s ,
(4)
τ
κ
−cT −bT 0
x ∈ Rn , y ∈ K∗ , τ ∈ R+ , r ∈ {0}n , s ∈ K, κ ∈ R+ ,
which is always solvable. The embedding (4), due to Ye
et al. (1994), has a number of other nice properties. Observe that when τ ? = 1, κ? = 0 are solutions to the embedding (4), we recover the KKT system (3); it turns out
that the solutions τ ? , κ? characterize the primal or dual
(in)feasibility of the cone program (2). In particular, if
τ ? > 0, κ? = 0, then the cone program (2) is feasible,
with a primal-dual solution (1/τ ? )(x? , y ? , r? , s? ); on the
other hand, if τ ? = 0, κ? ≥ 0, then (2) is primal or dual infeasible (or both), depending on the exact values of τ ? , κ?
(O’Donoghue et al., 2016, Section 2.3). The embedding (4)
can also be seen as first-order homogeneous, in the sense
that (x? , y ? , τ ? , r? , s? , κ? ) being a solution to (4) implies
that k(x? , y ? , τ ? , r? , s? , κ? ), for k ≥ 0, is also a solution.
Finally, viewing the embedding (4) as a feasibility problem,
the dual of the feasibility problem turns out to be the original feasibility problem, i.e., the embedding is self-dual.

A Semismooth Newton Method for Fast, Generic Convex Programming

ADMM-based algorithm. As mentioned, the embedding (4) can be viewed as the feasibility problem
find

subject to Qu = v, (u, v) ∈ C × C ∗ ,

u, v

where we write C = Rn ×K∗ ×R+ , C ∗ = {0}n ×K ×R+ ,


0
Q =  −A
−cT

AT
0
−bT


c
b ,
0




x
u =  y ,
τ




r
v =  s .
κ
(5)

Introducing new variables ũ, ṽ ∈ Rk , where k = n+m+1,
and rewriting so that we may apply ADMM, we get:
IC×C ∗ (u, v) + IQu? =v? (ũ, ṽ)

 

u
ũ
subject to
=
,
v
ṽ
u, v, ũ, ṽ

where Z =
tion of Z.

i

λi qi qiT is the eigenvalue decomposi-

• Exponential cone, Kexp . If z ∈ Kexp , then PKexp (z) =
∗
z. If −z ∈ Kexp
, then PKexp (z) = 0. If z1 , z2 < 0,
i.e., the first two components of z are negative, then
PKexp = (z1 , max{z2 , 0}, max{z3 , 0}). Otherwise,
the projection is given by
argmin
z̃∈R3 :z̃2 >0

(1/2)kz̃ − zk22

(12)

z̃2 exp(z̃1 /z̃2 ) = z̃3 ,

which can be computed using a Newton method
(Parikh & Boyd, 2014, Section 6.3.4).

where IC×C ∗ and IQu? =v? are the indicator functions of the
product space C × C ∗ , and the affine space of solutions to
Qu = v, respectively; after simplifying (see O’Donoghue
et al. (2016, Section 3)), the ADMM recurrences are just
ũ ← (I + Q)−1 (u + v).

(6)

u ← PC (ũ − v)

(7)

v ← v − ũ + u,

(8)

where PC denotes the projection onto C. For the update (6),
Q is a skew-symmetric matrix, hence I + Q is nonsingular,
so the update can be done efficiently via the Schur complement, matrix inversion lemma, and LDLT factorization.
Projections onto dual cones. For the update (7), the projection onto C boils down to separate projections onto the
“free” cone Rn , the dual cone of K, and the nonnegative orthant R+ . These projections, for many K, are well-known:
• Free cone. Here, PRn (z) = z, for z ∈ Rn .
• Nonnegative orthant, Kno . The projection onto Kno is
simply given by applying the positive part operator:
(9)

• Second-order cone, Ksoc . Write z = (z1 , z2 ) ∈
Rm , z1 ∈ Rm−1 , z2 ∈ R. Then the projection is


0,
PKsoc (z) = z,

 1 (1 +
2

i

P

subject to

minimize

PKno (z) = max{z, 0}.

• Positive semidefinite cone, Kpsd . The projection is
X
PKpsd (Z) =
max{λi , 0}qi qiT ,
(11)

kz1 k2 ≤ −z2
kz1 k2 ≤ z2
z2
kz1 k2 )(z1 , kz1 k2 ), otherwise.
(10)

The nonnegative orthant, second-order cone, and positive
semidefinite cone are all self-dual, so projecting onto these
cones is equivalent to projecting onto their dual cones; to
project onto the dual of the exponential cone, we use the
Moreau decomposition to get
∗ (z) = z + PK
PKexp
(−z).
exp

(13)

2.3. Semismooth operators
Here, we give an overview of semismoothness; good references include Ulbrich (2011) and Izmailov & Solodov
(2014). We consider maps F : Rk → Rk that are locally Lipschitz, i.e., for all z1 ∈ Rk , and z2 ∈ N (z1 , δ),
where N (z1 , δ) is a ball centered at z1 with radius δ > 0,
there exists some Lz1 > 0, such that kF (z1 ) − F (z2 )k2 ≤
Lz1 kz1 − z2 k2 . By a result known as Rademacher’s theorem (Evans & Gariepy, 2015, Section 3.1.2, Theorem 2),
we get that F is differentiable almost everywhere; we let
D denote the points at which F is differentiable, so that
Rk \ D is a set of measure zero.
The generalized Jacobian. Clarke (1990) suggested the
generalized Jacobian as a way to define the derivative of
a locally Lipschitz map F : Rk → Rk , at all points. The
generalized Jacobian is related to the subgradient, as well
as the directional derivative, as we discuss later on; the
generalized Jacobian, though, turns out to be quite useful
for defining effective nonsmooth Newton methods. The
generalized Jacobian J (z) at a point z ∈ Rk of a map
F : Rk → Rk , is defined as (co denotes convex hull)
n
o
J (z) = co lim J(zi ) : (zi ) ∈ D, (zi ) → z , (14)
i→∞

where J(zi ) ∈ Rk×k is the usual Jacobian of F at zi . Two
useful properties of the generalized Jacobian (Clarke, 1990,

A Semismooth Newton Method for Fast, Generic Convex Programming

Proposition 1.2): (i) J (z), at any z, is always nonempty;
and (ii) if each component Fi is convex, then the ith row of
any element of J (z) is just a subgradient of Fi at z.

Now, we would like to apply a Newton method to F ,
but projections onto proper cones are not differentiable,
in general. However, for many cones of interest, they are
(strongly) semismooth; the following lemma summarizes.

(Strong) semismoothness and consequences. We say
that a map F : Rk → Rk is semismooth if it is locally
Lipschitz, and if, for all z, δ ∈ Rk , the limit

Lemma 3.1. Projections onto the nonnegative orthant,
second-order cone, and positive semidefinite cone are all
strongly semismooth; see, e.g., Kong et al. (2009, Section
1), Kanzow & Fukushima (2006, Lemma 2.3), and Sun &
Sun (2002, Corollary 4.15), respectively.

lim

Jδ

δ→0, J∈J (z+δ)

(15)

exists (see, e.g., Mifflin (1977, Definition 1) and Qi &
Sun (1993, Section 2)). The above definition is somewhat
opaque, so various works have provided an alternative characterization of semismoothness: F is semismooth if and
only if it is (i) locally Lipschitz; (ii) directionally differentiable, in every direction; and (iii) we get
kF (z + δ) − F (z) − Jδk2
= 0,
lim
kδk2
δ→0, J∈J (z+δ)
i.e., kF (z + δ) − F (z) − Jδk2 = o(kδk2 ), δ → 0 (see,
e.g., Qi & Sun (1993, Theorem 2.3), Hintermüller (2010,
Theorem 2.9), Qi & Sun (1999, page 2), and Martı́nez &
Qi (1995, Proposition 2)). Examples of semismooth functions include log(1 + |x|), all convex functions, and all
smooth functions
p (Mifflin, 1977; Śmietański, 2007); on the
other hand, |x| is not semismooth. A linear combination of semismooth functions is semismooth (Izmailov &
Solodov, 2014, Proposition 1.75). Finally, we say that a
map is strongly semismooth if, under the same conditions
as above, we can replace (15) with
kF (z + δ) − F (z) − Jδk2
< ∞,
kδk22
δ→0, J∈J (z+δ)
lim sup

i.e., kF (z + δ) − F (z) − Jδk2 = O(kδk22 ), δ → 0 (see
Facchinei et al. (1996, Proposition 2.3) and Facchinei &
Kanzow (1997, Definition 1)).

3. Newton-ADMM and its basic properties
Next, we describe Newton-ADMM, our nonsmooth Newton method for generic convex programming; again, the basic idea is to view the ADMM recurrences (6) – (8), used by
SCS, as a fixed point iteration, and then use a nonsmooth
Newton method to find a fixed point. Accordingly, we let


ũ − (I + Q)−1 (u + v)
,
u − PC (ũ − v)
F (z) = 
ũ − u
which are just the residuals of the consecutive ADMM iterates given by (6) – (8), and z = (ũ, u, v) ∈ R3k ; multiplying by diag(I + Q, I, I) to change coordinates gives


(I + Q)ũ − (u + v)
.
u − PC (ũ − v)
F (z) = 
(16)
ũ − u

Additionally, we give the following new result, for the exponential cone, which may be of independent interest.
Lemma 3.2. The projection onto the exponential cone is
semismooth.
We defer all proofs to the supplement.
Putting the pieces together, the following lemma establishes that F , defined in (16), is (strongly) semismooth.
Lemma 3.3. When K, from the cone program (1), is
the nonnegative orthant, second-order cone, or positive
semidefinite cone, then the map F , defined in (16), is
strongly semismooth; when K is the exponential cone, then
the map F is semismooth.
The preceding results lay the groundwork for us to use a
semismooth Newton method (Qi & Sun, 1993), applied to
F , where we replace the usual Jacobian with any element
of the generalized Jacobian (14); however, as many have
observed (Khan & Barton, 2017), it is not always straightforward to compute an element of the generalized Jacobian.
Fortunately, for us, we can just compute a subgradient of
each row of F , as the following lemma establishes.
Lemma 3.4. The ith row of each element of the generalized
Jacobian J (z) at z of the map F is just a subgradient of
Fi , i = 1, . . . , 3k, at z.
Using the lemma, an element J ∈ R3k×3k of the generalized Jacobian of the map F ∈ R3k is then just


I + Q −I −I
,
Ju
J =
(17)
I
−I 0
where



0
JPK∗ 0 
0
`
(18)
is a (k×3k)-dimensional matrix forming the second row of
J; ` equals 1 if ũτ − vκ ≥ 0 and 0 otherwise; and JPK∗ ∈
Rm×m is the Jacobian of the projection onto the dual cone
K∗ . Here and below, we use subscripts to select components, e.g., ũτ selects the τ -component of ũ from (5), and
we write J to mean J(z), where z = (ũ, u, v) ∈ R3k .
−I
Ju =  0
0

0
−JPK∗
0

0 I
0 0
−` 0

0
I
0

0
0
1

I
0
0

0

A Semismooth Newton Method for Fast, Generic Convex Programming

Algorithm 1 Newton-ADMM for convex optimization

3.1. Final algorithm
Later, we discuss computing JPK∗ , the Jacobian of the projection onto the dual cone K∗ , for various cones K; these
pieces let us compute an element J, given in (17) – (18),
of the generalized Jacobian of the map F , defined in (16),
which we use instead of the usual Jacobian, in a semismooth Newton method; below, we describe a way to scale
the method to larger problems (i.e., values of n).
Truncated, semismooth Newton method. The conjugate gradient method is, seemingly, an appropriate choice
here, as it only approximately solves the Newton system
J∆ = −F,

(19)

with variable ∆ ∈ R3k ; unfortunately, in our case, J is nonsymmetric, so we appeal instead to the generalized minimum residual method (GMRES) (Saad & Schultz, 1986).
We run GMRES until
ˆ 2 ≤ εkF k2 ,
kF + J ∆k

(20)

ˆ is the approximate solution from a particular itwhere ∆
eration of GMRES, and ε is a user-defined tolerance; i.e.,
we run GMRES until the approximation error is acceptable.
After GMRES computes an approximate Newton step, we
use backtracking line search to compute a step size.
Now recall, from Section 2, that ∆? = 0 is always a trivial solution to the Newton system (19), due to homogeneity; so, we initialize the ũτ , uτ , vκ -components of z to 1,
which avoids converging to the trivial solution. Finally, we
mention that when K, in the cone program (1), is the direct
product of several proper cones, then Ju , in (18), simply
consists of multiple such matrices, just stacked vertically.
We describe the entire method in Algorithm 1. The method
has essentially no tuning parameters, since, for all the experiments, we just fix the maximum number of Newton
iterations T = 100; the backtracking line search parameters α = 0.001, β = 0.5; and the GMRES tolerances
ε(i) = 1/(i + 1), for each Newton iteration i. The cost of
each Newton iteration is the number of backtracking line
search iterations times the sum of two costs: the cost of
projecting onto a dual cone and the cost of GMRES, i.e.,
O(max{n2 , m2 }), assuming GMRES returns early. Similarly, the cost of each ADMM iteration of SCS is the cost
of projecting onto a dual cone plus O(max{n2 , m2 }).
3.2. Jacobians of projections onto dual cones
Here, we derive the Jacobians of projections onto the dual
cones of the nonnegative orthant, second-order cone, positive semidefinite cone, and the exponential cone; here, we
write JPK∗ to mean JPK∗ (z), where z = ũy − vs ∈ Rm .

Input: problem data c ∈ Rn , A ∈ Rm×n , b ∈ Rm ;
cones K; maximum number of Newton iterations T ;
backtracking line search parameters α ∈ (0, 1/2), β ∈
(0, 1); GMRES approximation tolerances (ε(i) )Ti=1
Output: a solution to (2)
(1)
(1)
initialize ũ(1) = u(1) = v (1) = 0 and ũτ = uτ =
(1)
vκ = 1 // avoids trivial solution
initialize z (1) = (ũ(1) , u(1) , v (1) )
for i = 1, . . . , T do
compute J(z (i) ), F (z (i) ) // see (16), (17), Sec. 3.2
compute the Newton step ∆(i) , i.e., by approximately
solving J(z (i) )∆(i) = −F (z (i) ) using GMRES with
approximation tolerance ε(i) // see (20)
initialize t(i) = 1 // initialize step size t(i)
while kF (z (i) + t(i) ∆(i) )k22 ≥ (1 − αt(i) )kF (z (i) )k22
do
t(i) = βt(i) // for backtracking line search
end while
update z (i+1) = z (i) + t(i) ∆(i)
end for
return the ux - divided by the uτ -components of z (T )
Nonnegative orthant. Since the nonnegative orthant is
self-dual, we can simply find a subgradient of each component in (9), to get that JPK∗ is diagonal with, say, (JPK∗ )ii
set to 1 if (ũy −vs )i ≥ 0 and 0 otherwise, for i = 1, . . . , m.
Second-order cone. Write z = (z1 , z2 ), z1 ∈
Rm−1 , z2 ∈ R. The second-order cone is self-dual, as
well, so we can find subgradients of (10), to get that


0, kz1 k2 ≤ −z2
(21)
JPK∗ = I, kz1 k2 ≤ z2


D, otherwise,
where D is a low-rank matrix (details in the supplement).
Positive semidefinite cone. The projection map onto the
(self-dual) positive semidefinite cone is matrix-valued, so
computing the Jacobian is more involved. We leverage the
fact that most implementations of GMRES need only the
product JPK∗ (vec Z), provided by the below lemma using
matrix differentials (Magnus & Neudecker, 1995); here,
vec is the vectorization of a real, symmetric matrix Z.
Lemma 3.5. Let Z = QΛQT be the eigenvalue decomposition of Z, and let Z̃ be a real, symmetric matrix. Then
JPKpsd (vec Z)(vec Z̃) = vec (dQ) max(Λ, 0)QT

+Q(d max(Λ, 0))QT + Q max(Λ, 0)(dQ)T ,
where, here, the max is interpreted diagonally;
dQi = (Λii I − Z)+ Z̃Qi ; [d max(Λ, 0)]ii = I+ (Λii )QTi Z̃Qi ;

A Semismooth Newton Method for Fast, Generic Convex Programming

Z + denotes the pseudo-inverse of Z; and I+ (·) is the indicator function of the nonnegative orthant.
Exponential cone. Recall, from (12), that the projection
onto the exponential cone is not analytic, so computing the
Jacobian is much more involved, as well. The following
lemma provides a Newton method for computing the Jacobian, using the KKT conditions for (12) and differentials.
Lemma 3.6. Let z ∈ R3 . Then JPK∗exp (z) = I −
JPKexp (−z), where


z ∈ Kexp
I,
∗
JPKexp (z) = −I,
z ∈ Kexp


diag(1, I+ (z2 ), I+ (z3 )), z1 , z2 < 0;
otherwise, JPKexp (z) is a particular 3x3 matrix given in the
supplement, due to space constraints.

4. Convergence guarantees
Here, we give some convergence results for NewtonADMM, the method presented in Algorithm 1.
First, we show that, under standard regularity assumptions,
the iterates (z (i) )∞
i=1 generated by Algorithm 1 are globally convergent, i.e., given some initial point, the iterates
converge to a solution of F (z) = 0, where i is a Newton iteration counter. We break the statement (and proof) of the
result up into two cases. Theorem 4.1 establishes the result, when the sequence of step sizes (t(i) )∞
i=1 converges to
some number bounded away from zero and one. Theorem
4.2 establishes the result when the step sizes converge zero.
Below, we state our regularity conditions, which are similar
to those given in Han et al. (1992); Martı́nez & Qi (1995);
Facchinei et al. (1996); we elaborate in the supplement.
A1. For Theorem 4.1, we assume lim supi→∞ t(i) < 1.

A5. For Theorem 4.2, we assume, for all z ∈ R3k and
∆ ∈ R3k , and for some C2 > 0, that
C2 k∆k2 ≤ kF̂ (z, ∆)k2 .
A6. For Theorem 4.3, we assume, for all z ∈ R3k , J(z) ∈
J (z), (i) that kJ(z)k2 ≤ C3 , for some constant C3 >
0; and (ii) that every element of J (z) is invertible.
The two global convergence results are given below; the
proofs are based on arguments in Martı́nez & Qi (1995,
Theorem 5a), but we use fewer user-defined parameters,
and a different line search method.
Theorem
4.1
(Global
convergence,
with
lim supi→∞ t(i) = t, for some 0 < t < 1). Assume
condition (A1) stated above. Then limi→∞ F (z (i) ) = 0.
Theorem
4.2
(Global
convergence,
with
lim supi→∞ t(i) = 0). Assume conditions (A2), (A3), (A4),
and (A5) stated above. Suppose the sequence (z (i) )∞
i=1
converges to some z ∈ R3k . Then F (z) = 0.
Next, we show, in Theorem 4.3, that when F is strongly
semismooth, i.e., K is the nonnegative orthant, secondorder cone, or positive semidefinite cone, the iterates
(z (i) )∞
i=1 generated by Algorithm 1 are locally quadratically convergent; the proof is similar to that of Facchinei
et al. (1996, Theorem 3.2b), for semismooth maps.
Theorem 4.3 (Local quadratic convergence). Assume condition (A6) stated above. Then the sequence of iterates (z (i) )∞
i=1 → z generated by Algorithm 1 converges
quadratically, with F (z) = 0, for large enough i.
When K is the exponential cone, i.e., F is semismooth, the
iterates generated by Algorithm 1 are locally superlinearly
convergent (Facchinei et al., 1996, Theorem 3.2b).

A2. For Theorem 4.2, we assume lim supi→∞ t(i) = 0.

5. Numerical examples

A3. For Theorem 4.2, we assume (i) that the GMRES approximation tolerances ε(i) are uniformly bounded by
ε as in ε(i) ≤ ε < 1 − α1/2 , (ii) that (ε(i) )∞
i=1 → 0,
and (iii) that ε(i) = O(kF (z (i) )k2 ).

Next, we present an empirical evaluation of NewtonADMM, on several problems; in these, we directly compare to SCS, which Newton-ADMM builds on, as it is the
most relevant benchmark for us (O’Donoghue et al. (2016)
observe that, with an optimized implementation, SCS outperforms SeDuMi, as well as SDPT3). We evaluate, for
both methods, the time taken to reach the solution as well
as the optimal objective value; we obtained these by running an interior point method (Andersen et al., 2011) to
high accuracy. Table 1 describes the problem sizes, for both
the cone form of (1), as well as the familiar form that the
problem is usually written in. Later, we also describe extending Newton-ADMM to accelerate any ADMM-based
algorithm, applied to any convex problem; here, we compare to state-of-the-art baselines for specific problems.

A4. For Theorem 4.2, we assume, for every convergent se(i) ∞
quence (z (i) )∞
i=1 → z, (γ )i=1 satisfying assump(j) ∞
tion (A2) above, and (∆ )j=1 → ∆, that
kF (z (i) + γ (i) ∆(j) )k22 − kF (z (i) )k22
i,j→∞
γ (i)
lim

≤ lim α1/2 F (z (i) )T F̂ (z (i) , ∆(j) ),
i,j→∞

where, for notational convenience, we write
F̂ (z (i) , ∆(j) ) = J(z (i) )∆(j) .

A Semismooth Newton Method for Fast, Generic Convex Programming
Table 1: Problem sizes, for the cone form (n, m) of (1), and the
familiar form (p, N ) that the problem is usually written in.

P ROBLEM
L INEAR PROG .
P ORTFOLIO OPT.
L OGISTIC REG .
ROBUST PCA

We consider `1 -penalized logistic regression, i.e.,
PN
minimize
i=1 log(1 + exp(yi Xi· θ)) + λkθk1 ,
p

n

m

p

N

C ONES

600
2,501
3,200
4,376

1,200
2,504
7,200
8,103

600
2,500
100
25

300
–
1,000
25

Kno
Ksoc , Kno
Kexp , Kno
Kpsd , Kno

θ∈R

5.1. Random linear programs (LPs)
We compare Newton-ADMM and SCS on a linear program
minimize
cT x
p
x∈R

subject to Gx = h, x ≥ 0,

where c ∈ Rp , G ∈ RN ×p , h ∈ RN are problem
data, and the inequality is interpreted elementwise. To ensure primal feasibility, we generated a solution x? by sampling its entries from a normal distribution, then projecting onto the nonnegative orthant; we generated G (with
p = 600, N = 300, so G is wide) by sampling entries from
a normal distribution, then taking h = Gx? . To ensure dual
feasibility, we generated dual solutions ν ? , λ? , associated
with the equality and inequality constraints, by sampling
their entries from a normal and Uniform(0, 1) distribution,
respectively; to ensure complementary slackness, we set
c = −GT ν ? + λ? . Finally, to put the linear program into
the cone form of (1), and hence (2), we just take




h
G
A =  −G  , b =  −h  , K = Kno .
0
I
The first column of Figure 1 presents the time taken, by
both Newton-ADMM and SCS, to reach the optimal objective value, as well as to reach the solution; we see that
Newton-ADMM outperforms SCS in both metrics.
5.2. Minimum variance portfolio optimization

θ∈R

subject to 1T θ = 1,

(23)

where, here, y ∈ RN here is a response vector; X ∈ RN ×p
is a data matrix, with Xi· denoting the ith row of X; and
λ ≥ 0 is a tuning parameter. We generated p = 100 sparse
underlying coefficients θ? , by sampling entries from a normal distribution, then setting ≈ 90% of the entries to zero;
we generated X (with N = 1, 000) by sampling its entries
from a normal distribution, then set y = Xθ? + δ, where
δ is (additive) Gaussian noise. For simplicity, we set the
tuning parameter λ = 1. Putting the above problem into
the cone form of (1) yields, for K, the direct product of the
exponential cone and the nonnegative orthant (details in the
supplement); the problem size in cone form ends up being
large (see Table 1). In the third column of Figure 1, we see
that Newton-ADMM outperforms SCS.
5.4. Robust principal components analysis (PCA)
Finally, we consider robust PCA,
minimize kLk∗ subject to kSk1 ≤ λ, L + S = X, (24)

L,S∈RN ×p

where k · k∗ and k · k1 are the nuclear and elementwise `1 norms, respectively, and X ∈ RN ×p , λ ≥ 0 (Candès et al.,
2011, Equation 1.1). We generated a low-rank matrix L? ,
with rank ≈ 12 N ; a sparse matrix S ? , by sampling entries
from Uniform(0, 1), then setting ≈ 90% of the entries to
zero; and finally set X = L? + S ? . We set λ = 1. The
goal is to decompose the obsevations X into low-rank L
and sparse S components. Putting the above problem into
the cone form of (1) yields, for K, the direct product of the
positive semidefinite cone and nonnegative orthant (details
in the supplement). We see that Newton-ADMM and SCS
are comparable, in the fourth column of Figure 1.
5.5. Extension as a specialized solver

We consider a minimum variance portfolio optimization
problem (see, e.g., Khare et al. (2015); Ali et al. (2016)),
minimize
θT Σθ
p

5.3. `1 -penalized logistic regression

(22)

where, here, the problem data Σ ∈ Sp++ is the covariance
matrix associated with the prices of p = 2, 500 assets; we
generated Σ by sampling a positive definite matrix. The
goal of the problem is to allocate wealth across p assets
such that the overall risk is minimized; shorting is allowed.
Putting the above problem into the cone form of (1) yields,
for K, the direct product of the second-order cone and the
nonnegative orthant (details in the supplement). The second column of Figure 1 shows the results; we again see
that Newton-ADMM outperforms SCS.

Finally, we observe that the basic idea of treating the residuals of consecutive ADMM iterates as a fixed point iteration, and then finding a fixed point using a Newton method,
is completely general, i.e., the same idea can be used to accelerate (virtually) any ADMM-based algorithm, for a convex problem. To illustrate, consider the lasso problem,
minimize
(1/2)ky − Xθk22 + λkθk1 ,
p
θ∈R
N

(25)

where y ∈ R , X ∈ RN ×p , λ ≥ 0; the ADMM recurrences (Parikh & Boyd, 2014, Section 6.4) are
θ ← (X T X + ρI)−1 (X T y + ρ(κ − µ))

(26)

κ ← Sλ/ρ (θ + µ)

(27)

µ ← µ + θ − κ,

(28)

0

10

20
30
Seconds

SCS
Newton-ADMM

0

10

20
30
Seconds

40

10 2
10 1
10 0
10 -1
10 -2
10 -3
10 -4
10 -5
10 -6
10 -7
10 -8
10 -9

0

5

10

15

20
25
Seconds

30

35

40

SCS
Newton-ADMM

0

20

40
60
Seconds

80

100

10 14
10 12
10 10
10 8
10 6
10 4
10 2
10 0
10 -2
10 -4
10 -6
10 -8
10 -10
10 -12

10 14
10 12
10 10
10 8
10 6
10 4
10 2
10 0
10 -2
10 -4
10 -6
10 -8
10 -10
10 -12

SCS
Newton-ADMM

Suboptimality

Suboptimality

10 -5

10 -7

40

Estimation error

Estimation error

10 -4

10 -6

2

10
10 1
10 0
10 -1
10 -2
10 -3
10 -4
10 -5
10 -6
10 -7
10 -8
10 -9
10 -10
10 -11
10 -12

SCS
Newton-ADMM

10 -3

0

200

400
600
Seconds

800

1000

SCS
Newton-ADMM

0

200

400
600
Seconds

800

Estimation error

10 -2

SCS
Newton-ADMM

Estimation error

10 2
10 1
10 0
10 -1
10 -2
10 -3
10 -4
10 -5
10 -6
10 -7
10 -8
10 -9
10 -10
10 -11
10 -12

Suboptimality

Suboptimality

A Semismooth Newton Method for Fast, Generic Convex Programming

1000

10 14
10 12
10 10
10 8
10 6
10 4
10 2
10 0
10 -2
10 -4
10 -6
10 -8

10 14
10 12
10 10
10 8
10 6
10 4
10 2
10 0
10 -2
10 -4
10 -6
10 -8

Newton-ADMM
SCS

0

5

10
Seconds

15

20

Newton-ADMM
SCS

0

5

10
Seconds

15

20

Figure 1: Comparison of Newton-ADMM and SCS (O’Donoghue et al., 2016), on several convex problems. Columns, from left to right:
linear programming, portfolio optimization, `1 -penalized logistic regression, robust PCA. Top row: wallclock time vs. log-distance to
the optimal objective value, obtained by running an interior point method. Bottom row: wallclock time vs. log-distance, in a Euclidean
norm sense, to the solution. Each plot is one representative run out of 20 (the variance was negligible). Best viewed in color.

where z = (θ, κ, µ) ∈ R3p , and we also changed coordinates, similar to before. An element J ∈ R3p×3p of the
generalized Jacobian of F is then

 T
X X + ρI −ρI ρI
−D
I
D ,
J =
−I
I
0
where D ∈ Rp×p is diagonal with Dii set to 1 if |θi +µi | >
λ/ρ and 0 otherwise, for i = 1, . . . , m.
In the left panel of Figure 2, we compare a specialized
Newton-ADMM applied directly to the lasso problem (25),
with the ADMM algorithm for (26) – (28), a proximal
gradient method (Beck & Teboulle, 2009), and a heavilyoptimized implementation of coordinate descent (Friedman
et al., 2007); we set p = 400, N = 200, λ = 10, ρ = 1.
Here, the specialized Newton-ADMM is quite competitive with these strong baselines; the specialized NewtonADMM outperforms Newton-ADMM applied to the cone
program (2), so we omit the latter from the comparison.
Stella et al. (2016) recently described a related approach.
In the right panel of Figure 2, we present a similar comparison, for sparse inverse covariance estimation, with the
QUIC method of Hsieh et al. (2014); Newton-ADMM
clearly performs best (p = N = 1, 000, λ = ρ = 1,
details in the supplement).

10 3
10 2
10

0

10 -1
10 -2
10 -3

Proximal gradient method
Coordinate descent
ADMM
Newton-ADMM

10 -4
10 -5
10 -6
10 -7
0.0

0.5

1.0

1.5
2.0
Seconds

2.5

3.0

Suboptimality

10 1
Suboptimality

where ρ > 0, κ, µ ∈ Rp are the tuning parameter and
auxiliary variables, introduced by ADMM, respectively,
and Sλ/ρ (·) is the soft-thresholding operator. The map
F : R3p → R3p , from (16), with components set to the
residuals of the ADMM iterates given in (26) – (28), is then


(X T X + ρI)θ − (X T y + ρ(κ − µ))
,
κ − Sλ/ρ (θ + µ)
F (z) = 
κ−θ

10 5
10 4
10 3
10 2
10 1
10 0
10 -1
10 -2
10 -3
10 -4
10 -5
10 -6
10 -7
10 -8
10 -9

QUIC
ADMM
Newton-ADMM

0

100

200

300 400 500
Seconds

600

700

800

Figure 2: Left: wallclock time vs. log-distance to the optimal
objective value, on the lasso problem, for the specialized NewtonADMM method, standard ADMM, a proximal gradient method,
and a heavily-optimized coordinate descent implementation (as
a reference benchmark). Right: for a sparse inverse covariance
estimation problem, with specialized Newton-ADMM, standard
ADMM, and QUIC (Hsieh et al., 2014). Best viewed in color.

6. Discussion
We introduced Newton-ADMM, a new method for generic
convex programming. The basic idea is use a nonsmooth
Newton method to find a fixed point of the residuals of the
consecutive ADMM iterates generated by SCS, a state-ofthe-art solver for cone programs; we showed that the basic idea is fairly general, and can be applied to accelerate
(virtually) any ADMM-based algorithm. We presented theoretical and empirical support that Newton-ADMM converges rapidly (i.e., quadratically) to a solution, outperforming SCS across several problems.
Acknowledgements. AA was supported by the DoE
Computational Science Graduate Fellowship DE-FG0297ER25308. EW was supported by DARPA, under award
number FA8750-17-2-0027. We thank Po-Wei Wang and
the referees for a careful proof-reading.

A Semismooth Newton Method for Fast, Generic Convex Programming

References
Ali, Alnur, Khare, Kshitij, Oh, Sang-Yun, and Rajaratnam,
Bala. Generalized pseudolikelihood methods for inverse
covariance estimation. Technical report, 2016. Available
at http://arxiv.org/pdf/1606.00033.pdf.
Andersen, Martin, Dahl, Joachim, Liu, Zhang, and Vandenberghe, Lieven. Interior point methods for large-scale
cone programming. Optimization for machine learning,
pp. 55–83, 2011.
Beck, Amir and Teboulle, Marc. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Ben-Tal, Aharon and Nemirovski, Arkadi. Lectures on
Modern Convex Optimization: Analysis, Algorithms,
and Engineering Applications. SIAM, 2001.
Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization. Cambridge University Press, 2004.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.
Candès, Emmanuel, Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal
of the ACM, 58(3):11, 2011.
Clarke, Frank. Optimization and Nonsmooth Analysis.
SIAM, 1990.
Diamond, Steven and Boyd, Stephen. CVXPY: A Pythonembedded modeling language for convex optimization.
Journal of Machine Learning Research, 17(83):1–5,
2016.
Douglas, Jim and Rachford, Henry. On the numerical solution of heat conduction problems in two and three space
variables. Transactions of the American Mathematical
Society, 82(2):421–439, 1956.
Evans, Lawrence and Gariepy, Ronald. Measure Theory
and Fine Properties of Functions. CRC Press, 2015.
Facchinei, Francisco and Kanzow, Christian. A nonsmooth inexact Newton method for the solution of largescale nonlinear complementarity problems. Mathematical Programming, 76(3):493–512, 1997.
Facchinei, Francisco, Fischer, Andreas, and Kanzow,
Christian. Inexact Newton methods for semismooth
equations with applications to variational inequality
problems, 1996.

Ferris, Michael and Munson, Todd. Semismooth support
vector machines. Mathematical Programming, 101(1):
185–204, 2004.
Friedman, Jerome, Hastie, Trevor, Höfling, Holger, and
Tibshirani, Robert. Pathwise coordinate optimization.
The Annals of Applied Statistics, 1(2):302–332, 2007.
Grant, Michael. Disciplined Convex Programming. PhD
thesis, Stanford University, 2004.
Han, Shih-Ping, Pang, Jong-Shi, and Rangaraj, Narayan.
Globally convergent Newton methods for nonsmooth
equations. Mathematics of Operations Research, 17(3):
586–607, 1992.
Hintermüller, Michael. Semismooth Newton methods
and applications. Technical report, 2010. Available at
http://www.math.uni-hamburg.de/home/
hinze/Psfiles/Hintermueller_OWNotes.
pdf.
Hsieh, Cho-Jui, Sustik, Mátyás, Dhillon, Inderjit, and
Ravikumar, Pradeep. QUIC: Quadratic approximation
for sparse inverse covariance estimation. Journal of Machine Learning Research, 15(1):2911–2947, 2014.
Izmailov, Alexey and Solodov, Mikhail. Newton-Type
Methods for Optimization and Variational Problems.
Springer, 2014.
Kanzow, Christian and Fukushima, Masao. Semismooth
methods for linear and nonlinear second-order cone programs. Technical report, 2006.
Khan, Kamil A and Barton, Paul. Generalized derivatives
for hybrid systems. IEEE Transactions on Automatic
Control, 2017.
Khare, Kshitij, Oh, Sang-Yun, and Rajaratnam, Bala. A
convex pseudolikelihood framework for high dimensional partial correlation estimation with convergence
guarantees. Journal of the Royal Statistical Society: Series B, 77(4):803–825, 2015.
Kong, Lingchen, Tunçel, Levent, and Xiu, Naihua. Clarke
generalized Jacobian of the projection onto symmetric
cones. Set-Valued and Variational Analysis, 17(2):135–
151, 2009.
Lofberg, Johan. YALMIP: A toolbox for modeling and
optimization in MATLAB. In 2004 IEEE International
Symposium on Computer Aided Control Systems Design,
pp. 284–289. IEEE, 2005.
Magnus, Jan and Neudecker, Heinz. Matrix Differential
Calculus with Applications in Statistics and Econometrics. John Wiley & Sons, 1995.

A Semismooth Newton Method for Fast, Generic Convex Programming

Martı́nez, José and Qi, Liqun. Inexact Newton methods for
solving nonsmooth equations. Journal of Computational
and Applied Mathematics, 60(1):127–145, 1995.

Sun, Defeng and Sun, Jie. Semismooth matrix-valued functions. Mathematics of Operations Research, 27(1):150–
169, 2002.

Mifflin, Robert. Semismooth and semiconvex functions in
constrained optimization. SIAM Journal on Control and
Optimization, 15(6):959–972, 1977.

Toh, Kim-Chuan, Todd, Michael, and Tütüncü, Reha. On
the implementation and usage of SDPT3 — a MATLAB software package for semidefinite/quadratic/linear
programming, version 4.0. In Handbook on Semidefinite, Conic, and Polynomial Optimization, pp. 715–754.
Springer, 2012.

Nesterov, Yurii and Nemirovskii, Arkadii. Interior Point
Polynomial Algorithms in Convex Programming. SIAM,
1994.
O’Donoghue, Brendan, Chu, Eric, Parikh, Neal, and Boyd,
Stephen. Conic optimization via operator splitting and
homogeneous self-dual embedding. Journal of Optimization Theory and Applications, pp. 1–27, 2016.

Udell, Madeleine, Mohan, Karanveer, Zeng, David, Hong,
Jenny, Diamond, Steven, and Boyd, Stephen. Convex
optimization in Julia. In Proceedings of the 1st First
Workshop for High Performance Technical Computing
in Dynamic Languages, pp. 18–28. IEEE, 2014.

Parikh, Neal and Boyd, Stephen. Proximal algorithms.
Foundations and Trends in Optimization, 1(3):127–239,
2014.

Ulbrich, Michael. Semismooth Newton Methods for Variational Inequalities and Constrained Optimization Problems in Function Spaces. SIAM, 2011.

Peaceman, Donald and Rachford, Henry. The numerical
solution of parabolic and elliptic differential equations.
Journal of the Society for Industrial and Applied Mathematics, 3(1):28–41, 1955.

Wu, Shao-Po and Boyd, Stephen. sdpsol: A parser/solver
for semidefinite programs with matrix structure. Advances in Linear Matrix Inequality Methods in Control,
pp. 79–91, 2000.

Qi, Liqun and Sun, Defeng. A survey of some nonsmooth
equations and smoothing Newton methods, 1999.

Ye, Yinyu,
Todd, Michael, and Mizuno, Shinji. An
√
O( nL)-iteration homogeneous and self-dual linear
programming algorithm. Mathematics of Operations Research, 19(1):53–67, 1994.

Qi, Liqun and Sun, Jie. A nonsmooth version of Newton’s
method. Mathematical Programming, 58(1-3):353–367,
1993.
Saad, Youcef and Schultz, Martin. GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 7(3):856–869, 1986.
Serrano, Santiago. Algorithms for Unsymmetric Cone Optimization and an Implementation for Problems with the
Exponential Cone. PhD thesis, Stanford University,
2015.
Śmietański, Marek. A generalized Jacobian based Newton method for semismooth block triangular system of
equations. Journal of Computational and Applied Mathematics, 205(1):305–313, 2007.
Stella, Lorenzo, Themelis, Andreas, and Patrinos, Panagiotis. Forward-backward quasi-Newton methods for
nonsmooth optimization problems. Technical report,
2016. Available at https://arxiv.org/pdf/
1604.08096.pdf.
Sturm, Jos. Implementation of interior point methods for
mixed semidefinite and second order cone optimization
problems. Optimization Methods and Software, 17(6):
1105–1154, 2002.

