No Spurious Local Minima in Nonconvex Low Rank Problems:
A Unified Geometric Analysis

Rong Ge 1 Chi Jin 2 Yi Zheng 1

Abstract
In this paper we develop a new framework that
captures the common landscape underlying the
common non-convex low-rank matrix problems
including matrix sensing, matrix completion and
robust PCA. In particular, we show for all above
problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no highorder saddle points exists. These results explain
why simple algorithms such as stochastic gradient descent have global converge, and efficiently
optimize these non-convex objective functions in
practice. Our framework connects and simplifies
the existing analyses on optimization landscapes
for matrix sensing and symmetric matrix completion. The framework naturally leads to new
results for asymmetric matrix completion and robust PCA.

2016; Park et al., 2016) and matrix completion (Ge et al.,
2016) have well-behaved optimization landscape: all local
optima are also globally optimal. Combined with recent
results (e.g. Ge et al. (2015); Carmon et al. (2016); Agarwal et al. (2016); Jin et al. (2017)) that are guaranteed to
find a local minimum for many non-convex functions, such
problems can be efficiently solved by basic optimization
algorithms such as stochastic gradient descent.
In this paper we focus on optimization problems that look
for low rank matrices using partial or corrupted observations. Such problems are studied extensively (Fazel,
2002; Rennie & Srebro, 2005; Candès & Recht, 2009) and
has many applications in recommendation systems (Koren,
2009), see survey by Davenport & Romberg (2016). These
optimization problems can be formalized as follows:
min

M∈Rd1 ×d2

f (M),

(1)

s.t. rank(M) = r.

1. Introduction
Non-convex optimization is one of the most powerful tools
in machine learning. Many popular approaches, from traditional ones such as matrix factorization (Hotelling, 1933)
to modern deep learning (Bengio, 2009) rely on optimizing non-convex functions. In practice, these functions are
optimized using simple algorithms such as alternating minimization or gradient descent. Why such simple algorithms
work is still a mystery for many important problems.
One way to understand the success of non-convex optimization is to study the optimization landscape: for the objective function, where are the possible locations of global
optima, local optima and saddle points. Recently, a line
of works showed that several natural problems including
tensor decomposition (Ge et al., 2015), dictionary learning (Sun et al., 2015a), matrix sensing (Bhojanapalli et al.,
Authors listed alphabetically. 1 Duke University, Durham NC
UC Berkeley, Berkeley CA. Correspondence to: Rong Ge
<rongge@cs.duke.edu>, Chi Jin <chijin@cs.berkeley.edu>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Here M is an d1 × d2 matrix and f is a convex function
of M. The non-convexity of this problem stems from the
low rank constraint. Several interesting problems, such
as matrix sensing (Recht et al., 2010), matrix completion
(Candès & Recht, 2009) and robust PCA (Candès et al.,
2011) can all be framed as optimization problems of this
form(see Section 3).
In practice, Burer & Monteiro (2003) heuristic is often
used – replace M with an explicit low rank representation
M = UV> , where U ∈ Rd1 ×r and V ∈ Rd2 ×r . The new
optimization problem becomes
min

U∈Rd1 ×r ,V∈Rd2 ×r

f (UV> ) + Q(U, V).

(2)

Here Q(U, V) is a (optional) regularizer. Despite the objective being non-convex, for all the problems mentioned
above, simple iterative updates from random or even arbitrary initial point find the optimal solution in practice. It is
then natural to ask: Can we characterize the similarities
between the optimization landscape of these problems?
We show this is indeed possible:
Theorem 1 (informal). The objective function of matrix
sensing, matrix completion and robust PCA have similar

No Spurious Local Minima in Nonconvex Low Rank Problems

optimization landscape. In particular, for all these problems, 1) all local minima are also globally optimal; 2) any
saddle point has at least one strictly negative eigenvalue in
its Hessian.
More precise theorem statements appear in Section 3. Note
that there were several cases (matrix sensing (Bhojanapalli
et al., 2016; Park et al., 2016), symmetric matrix completion (Ge et al., 2016)) where similar results on the optimization landscape were known. However the techniques
in previous works are tailored to the specific problems and
hard to generalize. Our framework captures and simplifies all these previous results, and also gives new results on
asymmetric matrix completion and robust PCA.
The key observation in our analysis is that for matrix
sensing, matrix completion, and robust PCA (when fixing
sparse estimate), function f (in Equation (1)) is a quadratic
function over the matrix M. Hence the Hessian H of f with
respect to M is a constant. More importantly, the Hessian
H in all above problems has similar properties (that it approximately preserves norm, similar to the RIP properties
used in matrix sensing (Recht et al., 2010)), which allows
their optimization landscapes to be characterized in a unified way. Specifically, our framework gives principled way
of defining a direction of improvement for all points that are
not globally optimal.
Another crucial property of our framework is the interaction between the regularizer and the Hessian H. Intuitively,
the regularizer makes sure the solution is in a nice region B
(e.g. set of incoherent matrices for matrix completion), and
only within B the Hessian has the norm preserving property. On the other hand, regularizer should not be too large
to severely distort the landscape. This interaction is crucial
for matrix completion, and is also very useful in handling
noise and perturbations. In Section 4, we discuss ideas required to apply this framework to matrix sensing, matrix
completion and robust PCA.
Using this framework, we also give a way to reduce
asymmetric matrix problems to symmetric PSD problems
(where the desired matrix is of the form UU> ). See Section 5 for more details.
In addition to the results of no spurious local minima,
our framework also implies that any saddle point has at
least one strictly negative eigenvalue in its Hessian. Formally, we proved all above problems satisfy a robust version of this claim — strict saddle property (see Definition 2), which is one of crucial sufficient conditions to admit efficient optimization algorithms, and thus following
corollary:
Corollary 2 (informal). For matrix sensing, matrix completion and robust PCA, simple local search algorithms can
find the desired low rank matrix UV> = M? from an ar-

bitrary starting point in polynomial time with high probability.
Several algorithms, including many variants of gradient descent Ge et al. (2015); Carmon et al. (2016); Agarwal et al.
(2016); Jin et al. (2017) are known to converge to a local
optimum for strict-saddle functions, and hence can be applied to the problems discussed in this paper. There are
some technicalities in the exact guarantees, which we defer
to supplementary material.
For simplicity, we present most results in the noiseless setting, but our results can also be generalized to handle noise.
See supplementary material for details.
1.1. Related Works
The landscape of low rank matrix problems have recently
received a lot of attention. Ge et al. (2016) showed symmetric matrix completion has no spurious local minimum.
At the same time, Bhojanapalli et al. (2016) proved similar result for symmetric matrix sensing. Park et al. (2016)
extended the matrix sensing result to asymmetric case. All
of these works guarantee global convergence to the correct
solution.
There has been a lot of work on the local convergence
analysis for various algorithms and problems. For matrix sensing or matrix completion, the works (Keshavan
et al., 2010a;b; Hardt & Wootters, 2014; Hardt, 2014; Jain
et al., 2013; Chen & Wainwright, 2015; Sun & Luo, 2015;
Zhao et al., 2015; Zheng & Lafferty, 2016; Tu et al., 2015)
showed that given a good enough initialization, many simple local search algorithms, including gradient descent and
alternating least squares, succeed. Particularly, several
works (e.g. Sun & Luo (2015); Zheng & Lafferty (2016))
accomplished this by showing a geometric property which
is very similar to strong convexity holds in the neighborhood of optimal solution. For robust PCA, there are also
many analysis for local convergence (Lin et al., 2010; Netrapalli et al., 2014; Yi et al., 2016; Zhang et al., 2017).
Several works also try to unify the analysis for similar problems. Bhojanapalli et al. (2015) gave a framework for local
analysis for these low rank problems. Belkin et al. (2014)
showed a framework of learning basis functions, which
generalizes tensor decompositions. Their techniques imply
the optimization landscape for all such problems are very
similar. For problems looking for a symmetric PSD matrix,
Li & Tang (2016) showed for objective similar to (2) (but in
the symmetric setting), restricted smoothness/strong convexity on the function f suffices for local analysis. However, their framework does not address the interaction between regularizer and the function f , hence cannot be directly applied to problems such as matrix completion or
robust PCA.

No Spurious Local Minima in Nonconvex Low Rank Problems

Organization We will first introduce notations and basic optimality conditions in Section 2. Then Section 3 introduces the problems and our results. For simplicity, we
present our framework for the symmetric case in Section 4,
and briefly discuss how to reduce asymmetric problem to
symmetric problem in Section 5. For clean presentation,
many proofs are deferred to supplementary material.

Definition 2. We say function f (·) is (θ, γ, ζ)-strict saddle. That is, for any x, at least one of followings holds:
1. k∇f (x)k ≥ θ.
2. λmin (∇2 f (x)) ≤ −γ.

2. Preliminaries
In this section we introduce notations and basic optimality
conditions.
2.1. Notations
We use bold letters for matrices and vectors. For a vector v
we use kvk to denote its `2 norm. For a matrix M we use
kMk to denote its spectral norm, and kMkF to denote its
Frobenius norm. For vectors we use hu, vi toP
denote innerproduct, and for matrices we use hM, Ni = i,j Mij Nij
to denote the trace of MN> . We will always use M? to
denote the optimal low rank solution. Further, we use σ1?
to denote its largest singular value, σr? to denote its r-th
singular value and κ? = σ1? /σr? be the condition number.
We use ∇f to denote the gradient and ∇2 f to denote its
Hessian. Since function f can often be applied to both M
(as in (1)) and U, V (as in (2)), we use ∇f (M) to denote
gradient with respect to M and ∇f (U, V) to denote gradient with respect to U, V. Similar notation is used for Hessian. The Hessian ∇2 f (M) is a crucial object in our framework. It can be interpreted as a linear operator on matrices.
This linear
 operator
 can be viewed as a d1 d2 × d1 d2 matrix
d+1
(or d+1
×
matrix in the symmetric case) that ap2
2
plies to the vectorized version of matrices. We use the notation M : H : N to denote the quadratic form hM, H(N)i.
Similarly, the Hessian of objective (2) is a linear operator
on a pair of matrices U, V, which we usually denote as
∇2 f (U, V).
2.2. Optimality Conditions
Local Optimality Suppose we are optimizing a function
f (x) with no constraints on x. In order for a point x to be
a local minimum, it must satisfy the first and second order
necessary conditions. That is, we must have ∇f (x) = 0
and ∇2 f (x)  0.
Definition 1 (Optimality Condition). Suppose x is a local
minimum of f (x), then we have
∇f (x) = 0,

saddle property, which is a quantitative version of the optimality conditions, and can lead to efficient algorithms to
find local minima.

∇2 f (x)  0.

3. x is ζ-close to X ? – the set of local minima.
Intuitively, this definition says for any point x, it either violates one of the optimality conditions significantly (first
two cases), or is close to a local minima. Note that ζ and θ
are often closely related. For a function with strict-saddle
property, it is possible to efficiently find a point near a local
minimum.
Local vs. Global However, of course finding a local minimum is not sufficient in many case. In this paper we are
also going to prove that all local minima are also globally
optimal, and they correspond to the desired solutions.

3. Low Rank Problems and Our Results
In this section we introduce matrix sensing, matrix completion and robust PCA. For each problem we give the results
obtained by our framework. The proof ideas are illustrated
later in Sections 4 and 5.
3.1. Matrix Sensing
Matrix sensing (Recht et al., 2010) is a generalization of
compressed sensing (Candes et al., 2006). In the matrix
sensing problem, there is an unknown low rank matrix
M? ∈ Rd1 ×d2 . We make linear observations on this matrix: let A1 , A2 , ..., Am ∈ Rd1 ×d2 be m sensing matrices, the algorithm is given {Ai }’s and the corresponding
bi = hAi , M? i. The goal is now to find the unknown matrix M? . In order to find M? , we need to solve the following nonconvex optimization problem
m

min
M∈Rd1 ×d2 ,rank(M)=r

1 X
(hM, Ai i − bi )2 .
2m i=1

We can transform this constraint problem to an unconstraint problem by expressing M as M = UV> where
U ∈ Rd1 ×r and V ∈ Rd2 ×r . We also need an additional
regularizer (common for all asymmetric problems):
m

Intuitively, if one of these conditions is violated, then it
is possible to find a direction that decreases the function
value. (Ge et al., 2015) characterized the following strict-

f (M) =

min
U,V

1 X
1
(hUV> , Ai i − bi )2 + kU> U − V> Vk2F .
2m i=1
8
(3)

No Spurious Local Minima in Nonconvex Low Rank Problems

The regularizer has been widely used in previous works
(Zheng & Lafferty, 2016; Park et al., 2016). In Section 5 we
show how this regularizer can be viewed as a way to deal
with the additional invariants in asymmetric case, and reduce the asymmetric case to the symmetric case. A crucial
concept in standard sensing literature is Restrict Isometry
Property (RIP), which is defined as follows:
Definition 3. A group of sensing matrices {A1 , .., Am }
satisfies the (r, δ)-RIP condition, if for every matrix M of
rank at most r,

A well-known problem in matrix completion is that when
the true matrix M? is very sparse, then we are very likely
to observe only 0 entries, and has no chance to learn the
other entries of M? . To avoid this case, previous works
have assumed following incoherence condition:
Definition 4. A rank r matrix M ∈ Rd1 ×d2 is µincoherent, if for the rank-r SVD XDY> of M, we have
for all i ∈ [d1 ], j ∈ [d2 ]
p
p
µr/d2 , ke>
µr/d1 .
ke>
j Yk ≤
i Xk ≤

m

(1 − δ)kMk2F ≤

1 X
hAi , Mi2 ≤ (1 + δ)kMk2F .
m i=1

Pm
1
2
Intuitively, RIP says operator m
i=1 hAi , ·i approximately perserve norms for all low rank matrices. When
the sensing matrices are chosen to be i.i.d. matrices with
independent Gaussian entries, if m ≥ c(d1 + d2 )r for large
1
)enough constant c, the sensing matrices satisfy the (2r, 20
RIP condition (Candes & Plan, 2011). Using our framework we can show:
1
)Theorem 3. When measurements {Ai } satisfy (2r, 20
RIP, for matrix sensing objective (3) we have 1) all local minima satisfy UV> = M? 2) the function is
(, Ω(σr? ), O( σ? ))-strict saddle.

We assume the unknown optimal low rank matrix M? is
µ-incoherent.
In the non-convex program, we try to make sure the
decomposition UV> is also incoherent by adding a
Pd1
4
(ke>
regularizer Q(U, V) = λ1 i=1
i Uk − α1 )+ +
Pd2
4
λ2 j=1 (ke>
j Vk − α2 )+ . Here λ1 , λ2 , α1 , α2 are parameters that we choose later, (x)+ = max{x, 0}. Using this
regularizer, we can now transform the objective function to
the unconstraint form
1
kUV> − M? k2Ω
U,V
2p
1
+ kU> U − V> Vk2F + Q(U, V).
8

min

(4)

r

This in particular says 1) no spurious local minima existsl;
2) whenever at some point (U, V) so that the gradient is
small and the Hessian does not have significant negative
eigenvalue, then the distance to global optimal (see Definition 6 and Definition 7) is guaranteed to be small. Such a
point can be found efficiently (see supplementary material).
3.2. Matrix Completion

Theorem 4. Let d = max{d1 , d2 }, when sample rate p ≥
4 6
µrσ ?
µrσ ?
r (κ? )6 log d
), choose α12 = Θ( d1 1 ), α22 = Θ( d2 1 )
Ω( µ min{d
1 ,d2 }
d1
d2
and λ1 = Θ( µrκ
? ), λ2 = Θ( µrκ? ). With probability at
least 1 − 1/poly(d), for Objective Function (4) we have 1)
all local minima satisfy UV> = M? 2) The objective is
(, Ω(σr? ), O( σ? ))-strict saddle for polynomially small .
r

Matrix completion is a popular technique in recommendation systems and collaborative filtering (Koren, 2009; Rennie & Srebro, 2005). In this problem, again we have an
unknown low rank matrix M? . We observe each entry
of the matrix M? independently with probability p. Let
Ω ⊂ [d1 ] × [d2 ] be a set of observed entries. For any matrix
M, we use MΩ to denote the matrix whose entries outside
of Ω are set to 0. That is, [MΩ ]i,j = Mi,j if (i, j) ∈ Ω,
and [MΩ ]i,j = 0 otherwise. We further use kMkΩ to denote kMΩ kF . Matrix completion can be viewed as a special case of matrix sensing, where the sensing matrices only
have one nonzero entry. However such matrices do not satisfy the RIP condition.
In order to solve matrix completion, we try to optimize the
following:
min
M∈Rd1 ×d2 ,rank(M)=r

Using the framework, we can show following:

1
kM − M? k2Ω .
2p

3.3. Robust PCA
Robust PCA (Candès et al., 2011) is a generalization to
the standard Principled Component Analysis. In Robust
PCA, we are given an observation matrix Mo , which is an
true underlying matrix M? corrupted by a sparse noise S?
(Mo = M? + S? ). In some sense the goal is to decompose the matrix M into these two components. There are
many models on how many entries can be perturbed, and
how they are distributed. In this paper we work in the setting where M? is µ-incoherent, and the rows/columns of
S? can have at most α-fraction non-zero entries.
In order to express robust PCA as an optimization problem,
we need constraints on both M and S:
1
kM + S − Mo k2F .
2
s.t. rank(M) ≤ r, S is sparse.

min

(5)

No Spurious Local Minima in Nonconvex Low Rank Problems

There can be several ways to specify the sparsity of S. In
this paper we restrict attention to the set Sα which is the set
of matrices that have at most α-fraction non-zero entries in
each column/row,
and entries have absolute value at most
µrσ ?
2 √d d1 .
1 2

Assuming the true sparse matrix S? is in Sα . Note that the
infinite norm requirement on S? is without loss of generality, because by incoherence M? cannot have entries with
µrσ ?
absolute value more than √d d1 . Any entry larger than that
1 2
is obviously in the support of S? and can be truncated.
In objective function, we allow S to be γ times denser (in
Sγα ) where γ is a parameter we choose later. Now the
constraint optimization problem can be tranformed to the
unconstraint problem
1
(6)
min f (U, V) + kU> U − V> Vk2F ,
U,V
8
1
f (U, V) := min kUV> + S − Mo k2F .
S∈Sγα 2
Of course, we can also think of this as a joint minimization
problem of U, V, S. However we choose to present it this
way in order to allow extension of the strict-saddle condition. Since f (U, V) is not twice-differetiable w.r.t U, V,
it does not admit Hessian matrix, so we use the following
generalized version of strict-saddle
Definition 5. We say function f (·) is (θ, γ, ζ)-pseudo
strict saddle if for any x, at least one of followings holds:
1. k∇f (x)k ≥ θ.
2. ∃gx (·) so that ∀y, gx (y) ≥ f (y); gx (x) = f (x);
λmin (∇2 gx (x)) ≤ −γ.
3. x is ζ-close to X ? – the set of local minima.
Note that in this definition, the upperbound in 2 can be
viewed as similar to the idea of subgradient. For functions with non-differentiable points, subgradient is defined
so that it still offers a lowerbound for the function. In our
case this is very similar – although Hessian is not defined,
we can use a smooth function that upperbounds the current
function (upper-bound is required for minimization). In the
case of robust PCA the upperbound is obtained by a fixed
S. Using this formalization we can prove
Theorem 5. There is an absolute constant c > 0, if γ >
c, and γα · µr · (κ? )5 ≤ 1c holds, for objective function
Eq.(6) we have 1) all local minima satisfies
UV> = M? ;
√
 κ?
?
2) objective function is (, Ω(σr ), O( σ? ))-pseudo strict
r
saddle for polynomially small .

4. Framework for Symmetric Positive Definite
Problems
In this section we describe our framework in the simpler
setting where the desired matrix is positive semidefinite. In
particular, suppose the true matrix M? we are looking for
can be written as M? = U? (U? )> where U? ∈ Rd×r . For
objective functions that is quadratic over M, we denote its
Hessian as H and we can write the objective as
min
rank(M)=r

M∈Rd×d
sym ,

1
(M − M? ) : H : (M − M? ),
2
(7)

We call this objective function f (M). Via Burer-Monteiro
factorization, the corresponding unconstraint optimization
problem, with regularization Q can be written as
min

U∈Rn×r

1
(UU> −M? ) : H : (UU> −M? )+Q(U). (8)
2

In this section, we also denote f (U) as objective function
with respect to parameter U, abuse the notation of f (M)
previously defined over M.
Direction of Improvement The optimality condition
(Definition 1) implies if the gradient is non-zero, or if we
can find a negative direction of the Hessian (that is a direction v, so that v> ∇2 f (x)v < 0), then the point is not
a local minimum. A common technique in characterizing
the optimization landscape is therefore trying to explicitly
find this negative direction. We call this the direction of
improvement. Different works (Bhojanapalli et al., 2016;
Ge et al., 2016) have chosen very different directions of
improvement.
In our framework, we show it suffices to choose a single
direction ∆ as the direction of improvement. Intuitively,
this direction should bring us close to the true solution U?
from the current point U. Due to rotational symmetry (U
and UR behave the same for the objective if R is a rotation
matrix), we need to carefully define the difference between
U and U? .
Definition 6. Given matrices U, U? ∈ Rd×r , define their
difference ∆ = U − U? R, where R ∈ Rr×r is chosen as
R = argminZ> Z=ZZ> =I kU − U? Zk2F .
Note that this definition tries to “align” U and U? before
taking their difference, and therefore is invariant under rotations. In particular, this definition has the nice property
that as long as M = UU> is close to M? = U? (U? )> ,
we have ∆ is small (we defer the proof to Appendix):
Lemma 6. Given matrices U, U? ∈ Rd×r , let M = UU>
and M? = U? (U? )> , and let ∆ be defined as in Definition 6, then we have k∆∆> k2F ≤ 2kM − M? k2F , and
σr? k∆k2F ≤ 2(√12−1) kM − M? k2F .

No Spurious Local Minima in Nonconvex Low Rank Problems

Now we can state the main Lemma:
Lemma 7 (Main). For the objective (8), let ∆ be defined as
in Definition 6 and M = UU> . Then, for any U ∈ Rd×r ,
we have

Recall that matrices {Ai : i = 1, 2, ..., m} are known sensing matrices, and bi = hAi , M? i is the result of i-th observation. The intended solution is the unknown low rank
matrix M? = U? (U? )> . For any low rank matrix M, the
Hessian operator satisfies

∆ : ∇2 f (U) : ∆ = ∆∆> : H : ∆∆>
− 3(M − M? ) : H : (M − M? )

M:H:M=

+ 4h∇f (U), ∆i + [∆ : ∇2 Q(U) : ∆ − 4h∇Q(U), ∆i]
(9)
To see why this lemma is useful, let us look at the simplest
case where Q(U) = 0 and H is identity. In this case, if
gradient is zero, by Eq. (9)
∆ : ∇2 f (U) : ∆ = k∆∆k2F − 3kM − M? k2F
By Lemma 6 this is no more than −kM − M? k2F . Therefore, all stationary point with M 6= M∗ must be saddle
points, and we immediately conclude all local minimum
satisfies UU> = M? !
Interaction with Regularizer For problems such as matrix completion, the Hessian H does not preserve the norm
for all low rank matrices. In these cases we need to use
additional regularizer. In particular, conceptually we need
the following steps:
1. Show that the regularizer Q ensures for any U such
that ∇f (U) = 0, U ∈ B for some set B.
2. Show that whenever U ∈ B, the Hessian operator H
behaves similarly as identity: for some c > 0 we have:
∆∆> : H : ∆∆> − 3(M − M? ) : H : (M − M? ) <
−ck∆k2F .
3. Show that the regularizer does not contribute a large
positive term to ∆ : ∇2 f (U) : ∆. This means
we show an upperbound for 4h∇f (U), ∆i + [∆ :
∇2 Q(U) : ∆ − 4h∇Q(U), ∆i].
Interestingly, these steps are not just useful for handling
regularizers. Any deviation to the original model (such as
noise, or if the optimal matrix is not exactly low rank) can
be viewed as an additional “regularizer” function Q(U)
and argued in the same framework. See supplementary material for more details.

Therefore if the sensing matrices satisfy the RIP property
(Definition 3), the Hessian operator is close to identity for
all low rank matrices! In the symmetric case there is no
regularizer, so the landscape for symmetric matrix sensing
follows immediately from our main Lemma 7.
1
)Theorem 8. When measurement {Ai } satisfies (2r, 10
RIP, for matrix sensing objective (10) we have 1) all local minima U satisfy UU> = M? ; 2) the function is
(, Ω(σr? ), O( σ? ))-strict saddle.
r

Proof. For point U with small gradient satisfying
k∇f (U)kF ≤ , by (2r, δ2r )-RIP property:
∆ : ∇2 f (U) : ∆ ≤(1 + δ2r )k∆∆> k2F
− 3(1 − δ2r )kM − M? k2F + 4k∆kF
≤ − (1 − 5δ2r )kM − M? k2F + 4k∆kF
≤ − 0.4σr? k∆k2F + 4k∆kF
The second last inequality is due to Lemma 6 that
k∆∆> k2F ≤ 2kM − M? k2F , and last inequality is due to
1
and second part of Lemma 6. This means if U
δ2r = 10
is not close to U? , that is, if k∆kF ≥ 20
σr? , we have ∆ :
∇2 f (U) : ∆ ≤ −0.2σr? k∆k2F . This proves (, 0.2σr? , 20
σr? )strict saddle property. Take  = 0, we know all stationary
points with k∆kF 6= 0 are saddle points. This means all
local minima are global minima (satisfying UU> = M? ),
which finishes the proof.
4.2. Matrix Completion
For matrix completion, we need to ensure the incoherence
condition (Definition 4). In order to do that, we add a regularizer Q(U) that penalize the objective function when
some row of U is too large. We choose the same reguPd
larizer as (Ge et al., 2016): Q(U) = λ i=1 (kUi k − α)4+ .
The objective is then

U∈Rd×r

Matrix sensing is the ideal setting for this framework. For
symmetric matrix sensing, the objective function is

hAi , Mi2 .

i=1

min

4.1. Matrix Sensing

m
X

1
kM? − UU> k2Ω + Q(U).
2p

(11)

Using our framework, we first need to show that the regularizer ensures all rows of U are small (step 1).

m

min

U∈Rd×r

1 X
(hAi , UU> i − bi )2 .
2m i=1

(10)

Lemma 9. There exists an absolute constant?c, when samµrσ
2
ple rate p ≥ Ω( µr
= Θ( d 1 ) and λ =
d log d), α

No Spurious Local Minima in Nonconvex Low Rank Problems
d
Θ( µrκ
? ), we have for any points U with k∇f (U)kF ≤ 
for polynomially small , with probability at least 1 −
1/poly(d):


(µr)1.5 κ? σ1?
2
max ke>
Uk
≤
O
i
i
d

This is a slightly stronger version of Lemma 4.7 in (Ge
et al., 2016). Next we show under this regularizer, we can
still select the direction ∆, and the first part of Equation (9)
is significantly negative when ∆ is large (step 2):
3 4

? 4

Lemma 10. When sample rate p ≥ Ω( µ r (κd) log d ), by
µrσ ?
d
choosing α2 = Θ( d 1 ) and λ = Θ( µrκ
? ) with probability at least 1 − 1/poly(d), for all U with k∇f (U)kF ≤ 
for polynomially small  we have
>

>

?

?

∆∆ : H : ∆∆ −3(M−M ) : H : (M−M ) ≤

−0.3σr? k∆k2F

This lemma follows from several standard concentration inequalities, and is made possible because of the incoherence
bound we proved in the previous lemma.
Finally we show the additional regularizer related term in
Equation (9) is bounded (step 3).
Lemma 11. By choosing α2 = Θ(
O(σr? ), we have:

µrσ1?
d )

and λα2 ≤

1
[∆ : ∇2 Q(U) : ∆ − 4h∇Q(U), ∆i] ≤ 0.1σr? k∆k2F
4
Combining these three lemmas, it is easy to see
3 4

? 4

Theorem 12. When sample rate p ≥ Ω( µ r (κd) log d ), by
µrσ ?
d
choosing α2 = Θ( d 1 ) and λ = Θ( µrκ
? ). Then with
probability at least 1 − 1/poly(d), for matrix completion
objective (11) we have 1) all local minima satisfy UU> =
M? 2) the function is (, Ω(σr? ), O( σ? ))-strict saddle for
r
polynomially small .
Notice that our proof is different from (Ge et al., 2016), as
we focus on the direction ∆ for both first and second order
conditions while they need to select different directions for
the Hessian. The framework allowed us to get a simpler
proof, generalize to asymmetric case and also improved the
dependencies on rank.
4.3. Robust PCA
In the robust PCA problem, for any given matrix M the
objective function try to find the optimal sparse perturbation S. In the symmetric PSD case, recall we observe
Mo = M? +S? , we define the set Sα to be the set of matrices whose rows/columns have at most α-fraction
nonzero
µrσ ?
entries, and entries are bounded by 2 d 1 . Note the projection onto set Sα be computed in polynomial time (using
a max flow algorithm).

We assume S? ∈ Sα , the objective can be written as
minf (U), wheref (U) := min
U

S∈Sγα

1
kUU> + S − Mo k2F .
2
(12)

Here γ is a slack parameter that we choose later.
Note that now the objective function f (U) is not quadratic,
so we cannot use the framework directly. However, if we
fix S, then fS (U) := 12 kUU> + S − Mo k2F is a quadratic
function with Hessian equal to identity. We can still apply
our framework to this function. In this case, since the Hessian is identity for all matrices, we can skip the first step.
The problem becomes a matrix factorization problem:
min

U∈Rd×r

1
kA − UU> k2F .
2

(13)

The difference here is that the matrix A (which is M? +
S? − S) is not equal to M? and is in general not low rank.
We can use the framework to analyze this problem (and
treat the residue A − M? as the “regularizer” Q(U)).
Lemma 13. Let A ∈ Rd×d be a symmetric PSD matrix,
and matrix factorization objective to be:
f (U) = kUU> − Ak2F
where σr (A) ≥ 15σr+1 (A). then 1) all local minima satisfies UU> = Pr (A) (best rank-r approximation), 2) objective is (, Ω(σr? ), O( σ? ))-strict saddle.
r

To deal with the case S not fixed (but as minimizer
of Eq.(12)), we let U† (U† )> be the best rank rapproximation of M? + S? − S. The next lemma shows
when U is close to U† up to some rotation, U will actually
be already close to U? up to some rotation.
Lemma 14. There is an absolute constant c, assume γ > c,
and γα · µr · (κ? )5 ≤ 1c . Let U† (U† )> be the best rank rapproximation of M? +S? −S, where S is the minimizer as
in Eq.(12). Assume minR> R=RR> =I kU − U† RkF√≤ .
Let ∆ be defined as in Definition 6, then k∆kF ≤ O( κ? )
for polynomially small .
The proof of Lemma 14 is inspired by Yi et al. (2016)
and uses the property of the optimally chosen sparse set
S. Combining these two lemmas we get our main result:
Theorem 15. There is an absolute constant c, if γ > c, and
γα · µr · (κ? )5 ≤ 1c holds, for objective function Eq.(12) we
have 1) all local minima satisfies
UU> = M? ; 2) objec√
 κ?
?
tive function is (, Ω(σr ), O( σ? ))-pseudo strict saddle
r
for polynomially small .

5. Handling Asymmetric Matrices
In this section we show how to reduce problems on asymmetric matrices to problems on symmetric PSD matrices.

No Spurious Local Minima in Nonconvex Low Rank Problems

Let M? = U? V?> , and M = UV> , and objective function:
f (U, V) = 2(M − M? ) : H0 : (M − M? )
1
+ kU> U − V> Vk2F + Q0 (U, V)
2

∆ : ∇2 f (W) : ∆ ≤ ∆∆> : H : ∆∆>
(14)

Note this is a scaled version of objectives introduced in
Sec.3 (multiplied by 4), and scaling will not change the
property of local minima, global minima and saddle points.
We view the problem as if it is trying to find a (d1 + d2 ) × r
matrix, whose first d1 rows are equal to U, and last d2 rows
are equal to V.
?

Lemma 16. For the objective (16), let ∆, N, N? be defined
as in Definition 7. Then, for any W ∈ R(d1 +d2 )×r , we have
− 3(N − N? ) : H : (N − N? ) + 4h∇f (W), ∆i
+ [∆ : ∇2 Q(W) : ∆ − 4h∇Q(W), ∆i]

(17)

where H = 4H1 + G. Further, if H0 satisfies M : H0 :
M ∈ (1 ± δ)kMk2F for some matrix M = UV> , let W
and N be defined as in (15), then N : H : N ∈ (1 ±
2δ)kNk2F .
Intuitively, this lemma shows the same direction of improvement works as before, and the regularizer is exactly
what it requires to maintain the norm-preserving property
of the Hessian.

Definition 7. Suppose M is the optimal solution, and
1
its SVD is X? D? Y?> . Let U? = X? (D? ) 2 , V? =
1
Y? (D? ) 2 , M = UV> is the current point, we reduce the
problem into a symmetric case using following notations.
The proofs are deferred to supplementary material.
 
 ?
U
U
W=
, W? =
, N = WW> , N? = W? W?> 6. Conclusions
V
V?
(15)
In this paper we give a framework that explains the recent
Further, ∆ is defined to be the difference between W and
success in understanding optimization landscape for low
W? up to rotation as in Definition 6.
rank matrix problems. Our framework connects and simplifies the existing proofs, and generalizes to new settings
We will also transform the Hessian operators to operate on
such as asymmetric matrix completion and robust PCA.
(d1 + d2 ) × r matrices. In particular, define Hessian H1 , G
The key observation is when the Hessian operator preserves
such that for all W we have:
the norm of certain matrices, one can use the same directions of improvement to prove similar optimization landN : H1 : N = M : H0 : M
scape. We show the regularizer 41 kU> U − V> Vk2F is
N : G : N = kU> U − V> Vk2F
exactly what it requires to maintain this norm preserving
property in the asymmetric case.Our analysis also allows
Now, let Q(W) = Q(U, V), and we can rewrite the obthe interaction between regularizer and Hessian to handle
jective function f (W) as
difficult settings such as.
1
For low rank matrix problems, there are generalizations
[(N − N? ) : 4H1 : (N − N? ) + N : G : N] + Q(W)
2
such as weighted matrix factorization(Li et al., 2016) and
(16)
1-bit matrix sensing(Davenport et al., 2014) where the HesWe know H0 perserves the norm of low rank matrices M.
sian operator may behave differently as the settings we can
To reduce asymmetric problems to symmetric problem, inanalyze. How to characterize the optimization landscape in
tuitively, we also hope H0 to approximately preserve the
these settings is still an open problem.
norm of N. However this is impossible as by definition,
In order to get general ways of understanding optimization
H0 only acts on M, which is the off-diagonal blocks of N.
landscapes for more generally, there are still many open
We can expect N : H0 : N to be close to the norm of
>
>
problems. In particular, how can we decide whether two
UV , but for all matrices U, V with the same UV , the
problems are similar enough to share the same optimization
matrix N can have very different norms. The easiest examlandscape? A minimum requirement is that the non-convex
ple is to consider U = diag(1/, ) and V = diag(, 1/):
>
problem should have the same symmetry structure – the set
while UV = I no matter what  is, the norm of N is
2
of equivalent global optimum should be the same. In this
of order 1/ and can change drastically. The regularizer
work, we show if the problems come from convex objective
is exactly there to handle this case: the Hessian G of the
functions with similar Hessian properties, then they have
regularizer will be related to the norm of the diagonal comthe same optimization landscape. We hope this serves as a
ponents, therefore allowing the full Hessian H = 4H1 + G
first step towards general tools for understanding optimizato still be approximately identity.
tion landscape for groups of problems.
Now we can formalize the reduction as the following main
Lemma:

No Spurious Local Minima in Nonconvex Low Rank Problems

References

Davenport, Mark A, Plan, Yaniv, van den Berg, Ewout, and
Wootters, Mary. 1-bit matrix completion. Information
and Inference, 3(3):189–223, 2014.

Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding approximate local minima for nonconvex optimization in linear time.
arXiv preprint arXiv:1611.01146, 2016.

Fazel, Maryam. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stanford University, 2002.

Belkin, Mikhail, Rademacher, Luis, and Voss, James. Basis learning as an algorithmic primitive. arXiv preprint
arXiv:1411.1420, 2014.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Escaping from saddle points—online stochastic gradient
for tensor decomposition. arXiv:1503.02101, 2015.

Bengio, Yoshua. Learning deep architectures for AI. FounR in Machine Learning, 2(1):1–127,
dations and trends
2009.
Bhojanapalli, Srinadh, Kyrillidis, Anastasios, and Sanghavi, Sujay. Dropping convexity for faster semi-definite
optimization. arXiv:1509.03917, 2015.
Bhojanapalli, Srinadh, Neyshabur, Behnam, and Srebro,
Nathan. Global optimality of local search for low
rank matrix recovery. arXiv preprint arXiv:1605.07221,
2016.
Burer, Samuel and Monteiro, Renato DC. A nonlinear programming algorithm for solving semidefinite programs
via low-rank factorization. Mathematical Programming,
95(2):329–357, 2003.

Ge, Rong, Lee, Jason D, and Ma, Tengyu. Matrix completion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pp. 2973–2981,
2016.
Hardt, Moritz. Understanding alternating minimization for
matrix completion. In FOCS 2014. IEEE, 2014.
Hardt, Moritz and Wootters, Mary. Fast matrix completion
without the condition number. In COLT 2014, pp. 638–
678, 2014.
Hotelling, Harold. Analysis of a complex of statistical variables into principal components. Journal of educational
psychology, 24(6):417, 1933.

Candes, Emmanuel J and Plan, Yaniv. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions
on Information Theory, 57(4):2342–2359, 2011.

Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Sujay. Low-rank matrix completion using alternating minimization. In Proceedings of the forty-fifth annual ACM
symposium on Theory of computing, pp. 665–674. ACM,
2013.

Candès, Emmanuel J and Recht, Benjamin. Exact matrix completion via convex optimization. Foundations
of Computational mathematics, 9(6):717–772, 2009.

Jin, Chi, Ge, Rong, Netrapalli, Praneeth, Kakade, Sham M,
and Jordan, Michael I. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.

Candes, Emmanuel J, Romberg, Justin K, and Tao, Terence. Stable signal recovery from incomplete and inaccurate measurements. Communications on pure and
applied mathematics, 59(8):1207–1223, 2006.

Keshavan, Raghunandan H, Montanari, Andrea, and Oh,
Sewoong. Matrix completion from a few entries. Information Theory, IEEE Transactions on, 56(6):2980–
2998, 2010a.

Candès, Emmanuel J, Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM (JACM), 58(3):11, 2011.
Carmon, Yair, Duchi, John C, Hinder, Oliver, and Sidford,
Aaron. Accelerated methods for non-convex optimization. arXiv preprint arXiv:1611.00756, 2016.

Keshavan, Raghunandan H, Montanari, Andrea, and Oh,
Sewoong. Matrix completion from noisy entries. The
Journal of Machine Learning Research, 11:2057–2078,
2010b.
Koren, Yehuda. The bellkor solution to the netflix grand
prize. Netflix prize documentation, 81, 2009.

Chen, Yudong and Wainwright, Martin J. Fast lowrank estimation by projected gradient descent: General
statistical and algorithmic guarantees. arXiv preprint
arXiv:1509.03025, 2015.

Li, Qiuwei and Tang, Gongguo. The nonconvex geometry
of low-rank matrix optimizations with general objective
functions. arXiv preprint arXiv:1611.03060, 2016.

Davenport, Mark A and Romberg, Justin. An overview of
low-rank matrix recovery from incomplete observations.
IEEE Journal of Selected Topics in Signal Processing,
10(4):608–622, 2016.

Li, Yuanzhi, Liang, Yingyu, and Risteski, Andrej. Recovery guarantee of weighted low-rank approximation via alternating minimization.
arXiv preprint
arXiv:1602.02262, 2016.

No Spurious Local Minima in Nonconvex Low Rank Problems

Lin, Zhouchen, Chen, Minming, and Ma, Yi. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. arXiv preprint
arXiv:1009.5055, 2010.

Zhao, Tuo, Wang, Zhaoran, and Liu, Han. A nonconvex
optimization framework for low rank matrix estimation.
In Advances in Neural Information Processing Systems,
pp. 559–567, 2015.

Nesterov, Yurii and Polyak, Boris T. Cubic regularization
of Newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006.

Zheng, Qinqing and Lafferty, John. Convergence analysis
for rectangular matrix completion using burer-monteiro
factorization and gradient descent.
arXiv preprint
arXiv:1605.07051, 2016.

Netrapalli, Praneeth, Niranjan, UN, Sanghavi, Sujay,
Anandkumar, Animashree, and Jain, Prateek. Nonconvex robust pca. In Advances in Neural Information
Processing Systems, pp. 1107–1115, 2014.
Park, Dohyung, Kyrillidis, Anastasios, Caramanis, Constantine, and Sanghavi, Sujay. Non-square matrix
sensing without spurious local minima via the burermonteiro approach. arXiv preprint arXiv:1609.03240,
2016.
Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review,
52(3):471–501, 2010.
Rennie, Jasson DM and Srebro, Nathan. Fast maximum
margin matrix factorization for collaborative prediction.
In Proceedings of the 22nd international conference on
Machine learning, pp. 713–719. ACM, 2005.
Sun, Ju, Qu, Qing, and Wright, John. Complete dictionary
recovery over the sphere I: Overview and the geometric
picture. arXiv:1511.03607, 2015a.
Sun, Ju, Qu, Qing, and Wright, John. When are nonconvex
problems not scary? arXiv preprint arXiv:1510.06096,
2015b.
Sun, Ruoyu and Luo, Zhi-Quan. Guaranteed matrix completion via nonconvex factorization. In Foundations
of Computer Science (FOCS), 2015 IEEE 56th Annual
Symposium on, pp. 270–289. IEEE, 2015.
Tu, Stephen, Boczar, Ross, Soltanolkotabi, Mahdi, and
Recht, Benjamin. Low-rank solutions of linear matrix equations via procrustes flow.
arXiv preprint
arXiv:1507.03566, 2015.
Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Caramanis, Constantine. Fast algorithms for robust pca via gradient descent. In Advances in neural information processing systems, pp. 4152–4160, 2016.
Zhang, Xiao, Wang, Lingxiao, and Gu, Quanquan. A nonconvex free lunch for low-rank plus sparse matrix recovery. arXiv preprint arXiv:1702.06525, 2017.

