Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Guangyong Chen 1 Shengyu Zhang 1 Di Lin 2 Hui Huang 2 Pheng Ann Heng 1 3

Abstract
While crowdsourcing has been a cost and time
efficient method to label massive samples, one
critical issue is quality control, for which the key
challenge is to infer the ground truth from noisy
or even adversarial data by various users. A large
class of crowdsourcing problems, such as those
involving age, grade, level, or stage, have an ordinal structure in their labels. Based on a technique of sampling estimated label from the posterior distribution, we define a novel separating
width among the labeled observations to characterize the quality of sampled labels, and develop
an efficient algorithm to optimize it through solving multiple linear decision boundaries and adjusting prior distributions. Our algorithm is empirically evaluated on several real world datasets,
and demonstrates its supremacy over state-ofthe-art methods.

1. Introduction
Crowdsourcing has drawn increasing popularity in the field
of machine learning by annotating millions of items in a
short time with relatively low cost (Howe, 2006; Welinder & Perona; Deng et al., 2013; Jiang et al., 2015). This
provides a great opportunity to build up large-scale training sets for complex models, such as deep neural networks
(Krizhevsky et al., 2012), and to reach consensus among
non-experts, such as peer grading in today’s popular massive open online course (MOOC) systems. However, the
quality of the collected results is often unreliable and diverse, and there are spammers who give random labels to
make easy money, or even adversaries who deliberately
give wrong answers. To address this issue, most crowd1
The Chinese University of Hong Kong, Hong Kong, China.
Shenzhen University, China. 3 Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of
Sciences, Shenzhen, China. Correspondence to: Shengyu Zhang
<syzhang@cse.cuhk.edu.hk>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

sourcing systems resort to distributing each item to a number of redundant workers. This raises a challenging question of how to aggregate such noisy and redundant labels.
An intuitive and baseline approach for crowdsourcing is
to identify each item following the majority voting (MV)
result of workers. Unfortunately, this approach is errorprone since it treats each worker equally, and the accuracy severely deteriorates with the fraction of less qualified
workers, spammers or adversaries. Weighted majority voting (WMV) (Karger et al., 2011) method tries to address
this issue by associating each worker with a weight to characterize his expertise. Specially, max-margin majority voting (M3 V) method (Tian & Zhu, 2015) optimizes the associated variables in WMV by maximizing the minimal difference between the aggregated score of the potential true
label and the aggregated scores of others.
In a different approach, Dawid-Skene (DS) model (Dawid
& Skene, 1979) represents each worker’s expertise by a
confusion matrix and uses a latent variable model to generate collected labels, which implicitly assumes a worker
to perform equally well across all items in a common
class. This model can be iteratively inferred by the famous
Expectation-Maximization (EM) method (Dempster et al.,
1977), and works well in practice. In particular, (Zhang
et al., 2016) employs the spectral method (Anandkumar
et al., 2012) to initialize the DS model, and obtains an optimal convergence rate up to a logarithmic factor by EM
method. Recently, (Zhou et al., 2012; Tian & Zhu, 2015)
proposed to improve the aggregating performance by integrating the merits of MV method with DS model. The performance of DS model and its variants often relies on the
specially conceived priors with some manually configured
hyperparameters.
All the above mentioned approaches are for aggregating
general multiclass labels. In many practical applications,
however, the labels have a natural ordinal structure. For
instance, in MOOCs, students are often required to grade
their own assignments on an ordinal scale of 5 (excellent),
4 (good), 3 (fair), 2 (pass) and 1 (failure). In medical imaging, doctors are often required to mark images on an ordinal
scale of stage 1, stage 2, stage 3, and stage 4. Ordinal label faces an issue of diverse standards. For example, given
four assignments whose true grades are 5, 4, 3 and 2, a

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

strict marker may rate them as 4, 3, 2 and 1. If we ignore
the ordinal structure, we may identify this marker as an adversary because all his answer labels are considered wrong.
But actually this marker grades all assignments in a correct
order, which should be incorporated to improve the crowdsourcing performance.

i
j labels the item as k, and Rjk
= 0 otherwise. Note that
i
R is a highly sparse matrix since each item is usually assigned to a small number of workers. The objective of a
crowdsourcing problem is to identify the true label zi of
item i based on the sparse matrices {R1 , . . . , RN }.

This inspires us to transform the K-class ordinal labeling
to K − 1 binary classifications. That is, instead of directly
using a label answer k ∈ [K] = {1, 2, . . . , K}, we use it
to answer K − 1 questions “Is the label greater than i” for
all i ∈ [K − 1]. In this way, the harsh marker’s answer
4 for the first assignment would give three correct answers
(on thresholds i = 1, 2, 3) and only one wrong answer (on
threshold i = 4), making the marker’s answers highly useful in the aggregation.

2.1. Majority Voting Method and its Variants

For each binary problem, we can employ a Gibbs sampler
to generate label estimations from the generative model of
crowdsourcing. However, these label estimations could be
error-prone especially for the difficult items, whose labels
may be sampled according to the uniform distribution, and
it is well known that the performance of a generative model
heavily relies on the specially conceived priors. To address these issues in binary crowdsourcing tasks, we define a separating width to characterize the quality of label
estimations, and solve it by optimizing a linear decision
boundary. The similar idea has been previously explored in
(Cortes & Vapnik, 1995) and found a lot of success for supervising learning problems. By optimizing the separating
width among two classes, we can improve the sampling
accuracy and update the prior distributions automatically
during the learning process. To characterize the quality of
aggregating ordinal labels from K classes, we introduce
K − 1 decision boundaries to help optimize the separating
width. As demonstrated empirically, our method achieves
the best performance on the real-world datasets compared
to other state-of-the-art methods.
The rest of this paper is organized as follows. Sec. 2 introduces some preliminary works for crowdsourcing tasks.
Sec. 3 presents the generative model employed in this paper. Sec. 4.1 derives the objective function for binary aggregating problem, which is extended for the ordinal case
in Sec. 4.2. The derivations of inference method are discussed in Sec. 5. Sec. 6 evaluates the performance of our
method on some real-world datasets, and Sec. 7 concludes
this paper.

Majority Voting (MV) has been widely used to find the
most likely label for item i by solving the following problem,
zi = argk max 1TM Ri ek ,
(1)
where 1M is a all-one column vector of dimension M and
ek is the k-th standard basis. Weighted majority voting
(WMV) (Karger et al., 2011) generalizes MV by assigning weight vector η ∈ RM ×1 to the workers and solving
the following problem
zi = argk max η T Ri ek .

(2)

Specially, max-margin majority voting (M3 V) (Tian &
Zhu, 2015) defines the crowdsourcing margin as the minimal difference between the aggregated score of the potential true label and aggregated scores of other labels, and
solves η by maximizing the sum of the crowdsourcing margins of all items.
2.2. Dawid-Skene Model and its Variants
Dawid-Skene (DS) model has been another popular way to
aggregate collected labels by capturing the uncertainties of
labeling behaviors in a generative model. Compared with
WMV and M3 V, both of which characterize the expertise
of worker j by a scaler variable, DS model characterizes
the expertise of worker j with an individual confusion matrix Aj ∈ RK×K , where the (k, d)-th entry denotes the
probability that worker j labels a class k sample as class d.
Denote A = {Aj }M
j=1 . DS model aims to maximize the
likelihood of observed samples R = {Ri }N
i=1 as follows,
max L =
A

N
X

Z
ln

p(Ri |zi , A)p(zi )dzi ,

(3)

i=1

QM QK
i
where p(Ri |zi , A) = j=1 d=1 (Ajzi d )Rjd and zi is a
1
, ∀i ∈ [N ]. This likelilatent variable with p(zi ) = K
hood function can be optimized iteratively by EM method
(Dempster et al., 1977) as,

2. Problem Setting and Preliminary Work
In this section, we formalize the problem and survey some
preliminary methods. Suppose that there are M workers
and N items taken from a total of K classes. For item i,
i
define an M × K matrix Ri by putting Rjk
= 1 if worker

E-step: q(zi = k) ∝ exp

M X
K
X

i
Rjd
ln Ajkd ,

j=1 d=1

M-step:

Ajkd

∝

N
X
i=1

q(zi =

i
k)Rjd
.

(4)

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Thus, the collected labelsPare aggregated
following the
M PK
i
rule zi = argk max exp j=1 d=1 Rjd
ln Ajkd , where
the unknown parameters A can be updated in M-step
through maximum likelihood estimation (MLE) principle.
Recently, spectral methods have been applied to obtain a
better initialization of the DS model (Zhang et al., 2016),
which achieves an optimal convergence rate up to a logarithmic factor. By assuming some special structures of the
confusion matrices A, (Raykar et al., 2010) studies homogeneous DS model, and (Moreno et al., 2015) studies the
existence of clusters of workers.

𝑦1

𝑦2

α=0.1

3. Generative Model of Crowdsourcing
In this section, we present a fully Bayesian model to generate observed matrices R. First we note that some items
may be intrinsically hard to label even for experts (which
is not uncommon in, for example, medical imaging). To
model such difficulty, we introduce a K-dimensional vector ω i to denote the prior distribution of true label of the
item i even for experts. (For items clearly from category
k, the vector ω i would be just the standard basis ek .) Denote ω = {ω i }N
i=1 . We can obtain a joint distribution as
follows.
Y
i
I(zi =k)
p(R, z|A, ω) =
(Ajkd )Rjd I(zi =k) ωki
, (5)
i,j,d,k

where A contains the confusion matrices of all workers like
DS model, z is the label vector to be solved and I(·) is an
indicator function. Since usually most workers just annotate a few items, we may not have sufficient samples to
infer z, A and ω. To overcome this, we formulate a fully

α=1

𝑦2

𝑦1

α=2

𝑦2

Figure 1. Illustrating how the Dirichlet density changes with respect to the scalar value α.

2.3. Recent Achievements
Some recent improvements have been achieved by combining MV related methods and DS related methods. (Zhou
et al., 2012) assumes labels are generated according to a
distribution over workers, items and labels, which can be
inferred by minimizing its entropy with constraints developed from MV method and DS model. (Tian & Zhu, 2015)
incorporates M3 V method with DS model in a regularized
Bayesian framework (Zhu et al., 2014), and approximates
the posterior distribution over the true labels with a Gibbs
sampler. Nowadays, binary and general multi-class crowdsourcing problems have been widely studied in the literature, but the ordinal sibling has not received nearly as much
attention yet. The work (Zhou et al., 2014) tries to use
the ordinal structure and makes an assumption that workers have difficulty distinguishing between two adjacent ordinal classes whereas it is much easier to distinguish between two far-away classes. In this paper, we will develop a
novel objective function to aggregate the ordinal labels, and
achieve the best performance on the real-world datasets.

𝑦1

𝑀

𝛼

𝛽

𝜔

𝐴

𝑅

𝑧
𝑁

Figure 2. Graphical model of our generative model.

Bayesian framework over A and ω with prior from Dirichlet distributions (Minka, 2000), a family that has found numerous successful applications (such as topic models) to
generate prior distributions. We assume that both workers’
expertise A and items’ difficulty ω are random variables
from the family of Dirichlet distributions
D(x|α) =

K
Γ(Kα) Y α−1
x
,
ΓK (α) t=1 t

(6)

where Γ(·) is the gamma function. As illustrated in Figure 1, the concentration parameter α controls the sparsity
preference of random vector. A precisely described item i
should have ω i be associated with a small concentration parameter, resulting in a sparse prior vector, while a vaguely
described item should be associated with a large concentration parameter. To model the expertise Aj of the worker
j, we also prefer that it has a small concentration parameter. We formulate the prior distributions over A and ω as
follows.
Y
Y
p(A|α) =
D(Ajk: |αj ), p(ω|β) =
D(ω i |βi ), (7)
j,k

i

N
where α = {αj }M
j=1 and β = {βi }i=1 . Combining Eq. (5)
and (7) gives the following joint distribution,

p(R, A, z, ω|α, β) = p(R, z|A, ω)p(ω|β)p(A|α).
The graphical model can be found in Fig. 2.
Given the matrices R, we can get the posterior distribution
over A, z and ω, which can be formulated as
p(R, A, z, ω|α, β)
.
p(R, A, z, ω|α, β)dAzω
(8)
We can obtain
a
classifier
as
arg
max
p(z|R,
α,
β)
=
z
R
argz max p(A, z, ω|R, α, β)dAω to label the item,
p(A, z, ω|R, α, β) = R

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

parametrized by the hyperparameters α and β. Conventionally, researchers mainly focus on how to approximate
the posterior distribution with better accuracy and runningtime performance with the fixed prior distributions, or updating the prior distributions by introducing new priors
over α and β (Kim & Ghahramani, 2012; Moreno et al.,
2015). However, the performance of the above generative
model heavily relies on the specially conceived priors to
incorporate domain knowledge, which transmits affects on
the posterior estimations through Bayes’ rules. Given a
family of prior choices, we prefer the classifier with the
more powerful discriminative capability to achieve better
generalization performance.

4. Maximizing the Separating Distance
4.1. Binary Crowdsourcing Problem
Before we present the objective function to aggregate ordinal labels, we firstly consider a simple case, the binary
crowdsourcing problem with K = 2. As shown in Eq. (8),
by varying the hyperparameters α and β, we can obtain
a series of posterior approximations to identify unlabeled
items via Bayes rule. Moreover, by fixing the hyperparameters α and β, we can get multiple estimations of the true
label of one item, which are randomly sampled from the
posterior distribution over its true label. Thus, we are motivated to find the most favored set of label estimations over
all items.
For a better generalization performance, we try to maximize the separation width between two classes. As shown
in Fig. 3, the label set 2 is preferred to the set 1, because
the set 2 has a larger separation width between two classes.
To evaluate the separating width of samples with the label
set z = {zi }N
i=1 , with zi ∈ {−1, 1}, ∀i ∈ [N ], we introduce a linear decision boundary f (Ri ) = aT Ri b with
a ∈ RM ×1 and b ∈ RK×1 . Our decision boundary is formulated refer to the formulas in Eq. (1) and (2), where a
denote the worker expertise and b transforms worker’s label into a scale variable. Thus, we define an optimization
problem as,
min L(a, b) =
a,b

s.t.

kak22 kbk22 ,

zi aT Ri b ≥ 1,

(9)

∀i ∈ [N ],

where kxk22 = xT x and the minimal value L(a∗ , b∗ ) characterizes the separating width of the label set z. This optimization problem can be understood from the objective
function used in support vector machine (SVM) (Cortes &
Vapnik, 1995), where the objective function is to maximize
the margin width (kbaT kF )−1 = (kak22 kbk22 )−1 and the
constrains state that all samples lie on the correct side of
the margin. (The constraint in the above optimization problem can be viewed as the inner product of Ri and a rank-

1 matrix baT . One may wonder why confining to rank-1
measurements. Note that MV (Eq.(1)) and WMV (Eq.(2))
are also of the rank-1 form, and our experiments also show
that using higher rank measurements actually makes the
generalization performance worse; see experiments in Appendix.) Since z is a random variable generated from the
posterior distribution (8), we need to reformulate the objective function (9) as follows,
L(a, b, α, β) = kak22 kbk22 ,

min

a,b,α,β

(10)

s.t. Ep(zi |Ri ,α,β) zi aT Ri b ≥ 1,

∀i ∈ [N ],

p(A, z, ω|R, α, β) ∝ p(R, A, z, ω|α, β).
Practically, the labeled samples are often linearly inseparable by a single hyperplane; see Set 3 in Fig. 3. To cope
with this issue, we relax the hard constrains by introducing
non-negative slack variables ξi , one for each sample, and
obtain a “soft” model as follows.
min

a,b,α,β,ξ

L(a, b, α, β) = kak22 kbk22 +

N
λ2 X
ξi , (11)
λ1 i=1

s.t. Ep(zi |Ri ,α,β) zi aT Ri b ≥ 1 − ξi ,

∀i ∈ [N ],

p(A, z, ω|R, α, β) ∝ p(R, A, z, ω|α, β),
where λλ12 is used as a positive regularization parameter for
later convenience, and 1 − ξi is the soft-margin for item
i. If Ri lies on the correct side of the margin, ξi = 0.
For sample on the wrong side, ξi is proportional to the
distance to the margin. Thus, the value of ξi reflects the
difficulty of identifying item i, or the error allowed to misclassify the item i. The calculation of p(A, z, ω|R, α, β)
is intractable because it involves that of the marginal distribution p(R|α, β). To address this issue, we introduce a
redundant distribution q(A, z, ω) and rewrite the optimization problem as follows.
min

a,b,α,β,q,ξ

L(a, b, α, β, ξ) = kak22 kbk22 +

N
λ2 X
ξi ,
λ1 i=1

(12)
T

i

s.t. Eq(zi ) zi a R b ≥ 1 − ξi ,

∀i ∈ [N ],

KL(qkp) = 0.
where q and p are shorthand for q(A, z, ω) and
p(A, z, ω|R, α, β), respectively, to simplify the presentations in the rest of the paper. Let ζi = 1 − zi aT Ri b, we can
turn the optimization problem into the following one with
two regularization terms: mina,b,α,β,q,ξ L(a, b, α, β, q),
where L is defined by
L(a, b, α, β, q) = KL(qkp) + λ1 kak22 kbk22
+ 2λ2

N
X

X

i=1 zi ∈{−1,1}

q(zi )(ζi )+ , (13)

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

𝑥2

𝑥2

𝒞1

𝑙1

𝒞1

𝑥2

𝒞1

𝑙1

𝜁1
𝒞2
𝑥1

𝒞2
𝒞2

𝑥1

Set 2

𝑙1

𝒞1

𝜁2

𝒞2

Set 1

𝑥2

𝑙2
𝒞3

𝑥1

𝑥1

Set 3

Set 4

Figure 3. Four label sets. Set 1 contains one possible label set. Set 2 contains another label set, which is preferred to the set 1. Set
3 contains a label set, whose separating width can be estimated with the slack variables. Set 4 contains a set of ordinal labels, whose
separating width can be estimated by transforming the ordinal problem into multiple binary ones.

where (ζi )+ = max{0, ζi } is the hinge loss function
widely used in training classifiers . The factor 2λ2 is introduced here to simplify the derivations of inference methods
later. By optimizing the unknown parameters in the objective function in Eq.(13), we can obtain the estimated labels
with the largest separating width.
4.2. Ordinal Crowdsourcing Problem
As introduced in Section 1, transforming a K-class ordinal labeling problem (“what is the label of this item?”) to
(K − 1) binary classification problems (“Is the label of this
item greater than k?” for k ∈ [K − 1]) allows us to exploit
more useful information from workers. The transform is
illustrated in Set 4 of Fig. 3, where we have items coming
from K ordered classes, C1 , . . . , CK . We look for K −1 decision boundaries, with boundary t discriminating classes
C1 ∪ · · · ∪ Ct and classes Ct+1 ∪ · · · ∪ CK . For the t-th
binary question, we introduce a linear decision boundary
as ft (Ri ) = aTt Ri bt . It is easily verified that all boundaries intersecting at the zero point. With a = {at }K−1
t=1 and
b = {bt }K−1
t=1 , the loss function in Eq. (13) becomes
L(a, b, α, β, q) =KL(qkp) + λ1

K−1
X

kat k22 kbt k22

t=1

+ 2λ2

N X
K
X
i=1 zi =1

q(zi )

K−1
X

(ζit )+ , (14)

t=1

where ζit = 1 − sgnt (zi )aTt Ri bt with sgnt (zi ) = −1 if
zi ≤ t and sgnt (zi ) = 1 if zi > t. It is obvious that our ordinal model will degenerate into binary one when K = 2.
When K ≥ 3, the ordinal label should be estimated by considering the predicted results from K − 1 binary problems.

5. Inference Details
In this section, we present the implementation details to
infer the true labels and all other unknown parameters involved in ordinal crowdsourcing problems. Our inference
method consists of two parts. In the first part, we employ

a Gibbs sampler to approximately sample from the posterior distribution p = p(A, z, ω|R, α, β). In the second
part, we update the hyperparameters (α, β) and linear decision boundaries based on the gradient method to achieve
the largest separating width.
To approximate the intractable posterior distribution p,
there are two standard approaches, which are Variational
Bayesian (VB) and Gibbs sampling. Compared with the
Gibbs sampling method, VB is usually difficult in its functional optimization, especially hard in our case due to the
hinge loss function. Moreover, VB often suffers from inaccuracy because of the potentially impractical assumption
of independence of variables. Gibbs sampling is applicable
here, because it provides numerical approximations to the
integration problems in large dimensional spaces by generating an instance from the conditional distribution of each
variable in turn. It can be shown that the sequence of samples constitutes a Markov chain, which finally converges to
the targeted posterior distribution as the stationary distribution.
Since the sampling process of the confusion matrices A
and the items’ difficulties ω can be developed in the standard way, we leave their derivations in Appendix and
mainly discuss the sampling process of true labels z here.
The difficulty of sampling z is mainly due to the hinge
loss function (ζit )+ . We employ data augmented technique (Polson & Scott, 2011) to approximate the hinge loss
function. According to the equality (Andrews & Mallows,
1974),
Z
exp(−2λ2 (ζit )+ ) = φ(zi , γit |Ri )dγit ,
(15)
1

−1
with φ(zi , γit |Ri ) = (2πγit )− 2 exp( 2γ
(γit + λ2 ζit )2 )
it
and γit as a non-negative augmented variable, we can reformulate the objective function in Eq.(14) as follows.
X
L(a, b, α, β, q) ≤ KL(qkp) + λ1
aTt at bTt bt
(16)
t

+

XZ
i,zi ,t

q(zi )q(γit ) ln

q(γit )
dγit ,
φ(zi , γit |Ri )

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

where the inequality comes from Jensen’s inequality with
a new distribution q(γit ) to help to approximate the hinge
loss function exp(−2λ2 (ζit )+ ). Note that the right hand
side of the inequality is tractable, minimizing which would
give an upper bound of the original optimization problem.
Before we sample the true labels of items, we need to first
generate augmented variables γ = {γit }N,K−1
i=1,t=1 . When
fixing other random variables, we can generate the (i, t)-th
augmented parameter according to the following generalized inverse Gaussian (GIG) distribution,
γit ∼

1
λ2 ζ 2
1 − 12
γit exp[− (γit + 2 it )],
Z
2
γit

(17)

where Z is the normalization term. It has been shown that
1
γit can be drawn efficiently with O(1) time complexity
(Michael et al., 1976).
Here, we can Q
sample
the true labels of all items. Let
N QK−1
φ(z, γ|R) = i=1 t=1 φ(zi , γit |Ri ). Rewrite the objective function shown in Eq. (16) with respect to q(zi ) as
follows,
L(q(zi ))
≤ KL(qkp) +

XZ

q(zi )q(γit ) ln

i,zi ,t

(18)

Thus, with all other parameters fixed, we can sample zi ∈
[K] according to the following distribution,
K−1
Y

φ(zi , γit |Rti ).

(19)

t=1

Let us examine the two terms on the right hand side. The
first term comes from the generative model of crowdsourcing, while the second term maximizes the separating width
of the estimated ordinal labels. For the binary crowdsourcing problem, we have only one decision boundary to measure the separating width, while for the ordinal crowdsourcing problem with K classes, we get K − 1 intersected decision boundaries to measure the separating width. After
obtaining a set of random samples to approximate the joint
posterior distribution q over all model parameters and augmented variables, the objective function shown in Eq. (14)
becomes a parametric function with respect to a, b, α and
β. Thus, we can intuitively update these parameters based
on the gradient method. The derivations of the updating
formulas over a, b, α and β can be found in Appendix.
R
Let hf (x)i =
q(x)f (x)dx denote the expectation of
f (x) with respect to the distribution of q(x). Our method is
outlined in Algorithm 1, in which each while iteration consists of two for loops, and the source code with demo can
be found on the website1 . In the first for loop, we employ
1

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

1
1 −2
Z γit
i

λ2 ζ 2

exp[− 12 (γit + γ2itit )]
QK−1
zi ∼ p(R , A, zi , ωi |α, β) t=1 φ(zi , γit |Rti )
end for
for t = 1 : K − 1 do
PN λ2
T
Σat = 2λ1 kbt k22 I + i=1 hγit2 i Ri bt bTt Ri
PN
λ22
i
at = Σ−1
at (
i=1 (λ2 + hγit i )hsgnt (zi )iR bt )
2
PN λ
T
Σbt = 2λ1 kat k22 I + i=1 hγit2 i aTt Ri Ri at
PN
λ22
T i
bt = Σ−1
i=1 (λ2 + hγit i )hsgnt (zi )iat R )
bt (
end for
∂L(α )
αj ← αj − η ∂αjj , ∀j ∈ [M ]
γit ∼

17:
β i ← βi − η
18: end while

∂L(βi )
∂βi , ∀i

∈ [N ]

q(γit )
dγit
φ(zi , γit |Ri )

= KL(q · q(γ)kp · φ(z, γ)).

q(zi ) ∝ p(Ri , A, zi , ωi |α, β)

Algorithm 1 Our Ordinal Crowdsourcing Method
1: Input: R = {Ri }N
i=1 , λ1 , λ2 and the learning rates η.
2: Initializing z = {zi }N
i=1 by MV, a, b, α and β
3: while not convergence do
4:
for i = 1 : N do
PN
i
5:
Ajkd ∼ D(Ajkd |αj + i=1 Rjd
I(zi = k))
i
i
6:
ωk ∼ D(ωk |βi + I(zi = k))

http://appsrv.cse.cuhk.edu.hk/˜gychen/

a Gibbs sampler to generate the random variables to approximate the posterior distribution. In the second part, we
solve the separating width by optimizing K − 1 decision
boundaries, and update the prior distributions by gradient
method. Compared with the traditional generative model
of crowdsourcing, including DS model and its variants, our
method introduces an augment variable γit to approximate
the hinge loss function, which is further involved in the
generation of true labels. This algorithm is iteratively implemented to reach a local optimum.

6. Experiments and Discussions
To fully evaluate the ideas employed in this paper, we
present empirical studies of our aggregating method in
comparison with competitive ones not only on ordinal
crowdsourcing tasks, but also binary crowdsourcing tasks.
For our method, we configure λ1 = λ2 = 1, α = 1M , β =
1N , η = 1 × 10−5 and initialize zi by the majority voting
result. In each run of our method, we generate 80 samples
to approximate the posterior distribution and discard the
first 10 samples as burn-in steps. The reported error rate of
our method is averaged over 10 runs, and all experiments
are conducted in a PC with Intel Core i7 1.8GHz CPU and
8.00GB RAM.
6.1. Binary Crowdsourcing Tasks
We first evaluate our method on three binary benchmark
datasets shown in Table 1, include labeling bird species

Learning to Aggregate Ordinal Labels by Maximizing Separating Width
Table 1. The summary of real-world datasets used in the comparison experiments.

Name
Bird
RTE
TREC
Web
Age

Classes
2
2
2
5
7

Items
108
800
19,033
2,665
1,002

Workers
39
164
762
177
165

Labels/item
39.0
10.0
4.64
5.84
10

𝑏1
𝑏2
𝑏3
𝑏4

Figure 4. The average value of 80 samples of {bt }4t=1 in a random
run.

(Welinder et al., 2010) (Bird dataset), recognizing textual
entailment (Snow et al., 2008) (RTE dataset) and accessing the relevance of topic-document pairs with a binary
judgment in TREC 2011 crowdsourcing track (Gabriella &
Matthew, 2011) (TREC dataset). The competitive methods include the pure majority voting estimator (refereed to
as MV), the EM method for DS model initialized by majority voting (refereed to as MV-DS), the EM method for
DS model initialized by spectral method (refereed to as
Opt-DS) (Zhang et al., 2016), the Gibbs sampler for the
Bayesian extension of M3 V (Tian & Zhu, 2015) (referred
to as G-CrowdSVM), the SVD-based algorithm proposed
in (Ghosh et al., 2011) (referred to as Gh-SVD), and the
Eigenvalues of Ratio algorithm proposed in (Dalvi et al.,
2013) (referred to as Eig-Ratio). The performance of all
1
methods are evaluated by error as l0 = 1 − |z|
kz − ẑk0 ,
where z contains true labels of items (available in all these
datasets) and ẑ contains estimations given by our algorithm
(not using any information of z). Noted that the reported
error rates of G-CrowdSVM are the average over 10 random runs as our method.
As shown in Table 2, our method achieves the best performance among all methods on three benchmark datasets.
Without regards to the prior knowledges over workers’ expertise and items’ difficulties, we can degenerate our model
into MV-DS model by setting λ2 = λ1 = 0. Comparing with the performance of MV-DS model, especially
Opt-DS method, we present a more complicated generative model, leading to better predictive results. Compared
with G-CrowdSVM method, our method updates prior distributions and improves the sampling accuracy by optimizing the separating width during the learning process, which
leads to the improvements of predictive performance on all
datasets.

6.2. Ordinal Crowdsourcing Tasks
In this part, we report empirical results of our method on
ordinal benchmark datasets in comparison with competitive ones. We consider MV, MV-DS, and G-CrowdSVM as
baselines, and compare our method with Entropy(O) (Zhou
et al., 2014), which consider the ordinal structures in labels. As shown in Table 1, we have two ordinal datasets.
One is to judge the relevance of query-URL pairs with
a 5-level rating score (Web dataset), and the other is to
identify the age of each subject with a 7-level rating score
(Age dataset). To evaluate the performance of aggregating ordinal labels, we define three following error measure1
1
kz − ẑk0 , l1 = |z|
kz − ẑk1 and
ments as: l0 = 1 − |z|
1
l2 = |z| kz − ẑk2 . Compared with the error rate l0 , the
measures l1 and l2 take precision into consideration, and
may be preferred for aggregating ordinal labels when one
cares about the severity of error.
Table 3 summarizes the performance of all methods on
the ordinal datasets, and shows that our method consistently outperforms the others in predicting the ordinal labels of items. Similar with our method, G-CrowdSVM
attempts to maximize the margin between the aggregated
score of potential true label and the aggregated score of
others, and achieves the better performance in comparison
with the state-of-the-art method to aggregate ordinal labels,
Entropy(O). Compared with G-CrowdSVM, we treat the
problem of aggregating collected labels as the classification
problem, and introduce K − 1 decision boundaries to consider the ordinal relationship among categories. As shown
in Table 3, on the Web dataset, our method significantly reduces the average l0 error rate from 7.99% to 3.22%. In addition, the average l1 error of our method is 3.69%, which
is only slightly larger than the l0 error rate of 3.22%. It
means that, even for the incorrect labels ẑi outputted by
our algorithm, our ẑi is not far away from its true answer
zi , resulting in a relatively small damage.
The Web dataset has been widely used in the evaluation
of ordinal crowdsourcing problem. On this dataset, our
method introduces 4 decision boundaries to measure the
separating width of generated true labels. To help to understand the linear decision boundaries learned by our method,
we illustrate the average value over 80 samples of {bt }4t=1
as Fig. 4. It can be seen that the absolute value of all entries in bt is approximated to 1. To be more concrete, let us
consider the first decision boundary with b1 , which calculate Ri b1 for the item i. Thus, Ri b1 successfully reduces
the ordinal problem into a binary one, the j-th entry in Ri bi
would be −1 if worker j ranks item i as 1 and 1 if worker
j rank item i as 2, 3, 4 and 5. Note that at characterizes the
expertise of all workers for the t-th binary problem. Fig. 5
contains three confusion matrices, including the averaged
confusion matrix of all workers, the confusion matrices of

Learning to Aggregate Ordinal Labels by Maximizing Separating Width
Table 2. l0 error rate (%) in predicting the latent labels on three binary benchmark datasets.

Binary Dataset
Bird
RTE
TREC

Ours
9.25±0.17
7.00±0.29
29.30±0.11

G-CrowdSVM
10.37±0.41
7.72±0.22
31.32±0.34

Opt-DS
10.09
7.12
29.80

MV-DS
11.11
7.12
30.02

MV
24.07
10.31
34.86

Gh-SVD
27.78
49.13
42.99

Eig-Ratio
27.78
9.00
43.96

Table 3. Errors in predicting the latent labels on two ordinal benchmark datasets.

Ordinal Dataset
l0
l1
l2
l0
l1
l2

Web

Age

𝒞1

𝒞2

𝒞3

𝒞4

Ours
0.0322±0.0013
0.0369±0.0032
0.2153±0.0019
0.3210±0.0025
0.3493±0.0036
0.6192±0.0047
𝒞5

𝒞1

G-CrowdSVM
0.0799±0.0026
0.0940±0.0057
0.3629±0.0044
0.3298±0.0036
0.3737±0.0033
0.6592±0.0028

𝒞2

𝒞3

𝒞4

Entropy(O)
0.1040
0.1173
0.3816
0.3732
0.4541
0.7936
𝒞5

𝒞1

MV-DS
0.1574
0.2149
0.5358
0.3962
0.5000
0.8518
𝒞2

MV
0.2693
0.4251
0.9247
0.3488
0.4083
0.7297
𝒞3

𝒞4

𝒞5

𝒞1
𝒞2
𝒞3
𝒞4
𝒞5

(a)

(b)

(c)

Figure 5. (a) the averaged confusion matrix over all worker, (b) the confusion matrix of an expert, (c) the confusion matrix of a spammer.
0.18

MV-DS
Entropy(O)
G-CrowdSVM
Ours

0.16

Error rate

0.14
0.12
0.1
0.08
0.06
0.04
0.02
0

100

200

300

400

500

600

Time (s)
Figure 6. Error rates per iteration of various estimators on the Web
dataset.

an expert and a spammer. It can be found that the spammer ranks all items randomly to make easy money, while
the expert has a confusion matrix similar to the identical
matrix. Our method can accurately estimate the confusion
matrices of all workers even given the averaged confusion
matrix acts like a spammer. Fig. 6 summarizes the training
time and error rates after each iteration for all estimators on
the Web dataset. It can be found that the proposed method
coverages to a lower error rate and all other three methods
have error convergence curves all above ours.

7. Conclusions
In this paper, we develop a novel method to aggregate
ordinal labels by optimizing the separating width among
classes. To measure the separating width among ordinal
labels, we first investigate a binary case, and then extend
our achievements to the ordinal one. With K − 1 decision
boundaries, we define an optimization problem for measuring the separating width among ordinal classes. The newly
introduced boundaries not only help to optimize the hyperparameters, but also calibrate the estimated labels sampled
from the generative model. A Gibbs sampler is adopted to
approximate the posterior distribution, while the gradient
method is used to calculate the separating width and optimize the hyperparameters.
As demonstrated on the ordinal datasets, which is the main
focus of this paper, our method consistently achieves the
best performance compared with competitive ones, and
the improvements on Web dataset are significant. As
demonstrated by the experimental results on the binary
datasets, our algorithm works slightly better than any previous method. Thus, our algorithm provides a uniform
method in both binary and ordinal cases, and can be practically useful for real-world applications.

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Acknowledgements
We would like to thank anonymous reviewers for their
valuable comments to improve the presentation of this paper. This work was supported by the China 973 Program
(Project No. 2015CB351706) and a grant from the National Natural Science Foundation of China (Project No.
61233012). Shengyu Zhang was supported by Research
Grants Council of the Hong Kong S.A.R. (Project no.
CUHK14239416).

References
Anandkumar, A., Liu, Y. K., Hsu, D. J., Foster, D. P., and
Kakade, S. M. A spectral algorithm for latent dirichlet
allocation. In Advances in Neural Information Processing Systems, pp. 917–925, 2012.
Andrews, D. F. and Mallows, C. L. Scale mixtures of normal distributions. Journal of the Royal Statistical Society. Series B (Methodological), pp. 99–102, 1974.
Cortes, C. and Vapnik, V. Support-vector networks. Machine learning, 20(3):273–297, 1995.
Dalvi, N., Dasgupta, A., and Kumar, R. .and Rastogi, V.
Aggregating crowdsourced binary ratings. In Proceedings of the 22nd international conference on World Wide
Web, pp. 285–294. ACM, 2013.
Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em algorithm.
Applied statistics, pp. 20–28, 1979.
Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the royal statistical society. Series B (methodological), pp. 1–38, 1977.
Deng, J., Krause, J., and Li, F. F. Fine-grained crowdsourcing for fine-grained recognition. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 580–587, 2013.
Gabriella, K. and Matthew, L. Overview of the trec 2011
crowdsourcing track. In Proceedings of TREC 2011,
2011.
Ghosh, A., Kale, S., and McAfee, P. Who moderates
the moderators?: crowdsourcing abuse detection in usergenerated content. In Proceedings of the 12th ACM conference on Electronic commerce, pp. 167–176. ACM,
2011.
Howe, J. The rise of crowdsourcing. Wired magazine, 14
(6):1–4, 2006.

Jiang, M., Huang, S., Duan, J., and Zhao, Q. Salicon:
Saliency in context. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
1072–1080. IEEE, 2015.
Karger, D. R., Oh, S., and Shah, D. Iterative learning for
reliable crowdsourcing systems. In Advances in Neural
Information Processing Systems, pp. 1953–1961, 2011.
Kim, H. C. and Ghahramani, Z. Bayesian classifier combination. In AISTATS, pp. 619–627, 2012.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classification with deep convolutional neural networks.
In Advances in Neural Information Processing Systems,
pp. 1097–1105, 2012.
Michael, J. R., Schucany, W. R., and Haas, R. W. Generating random variates using transformations with multiple
roots. The American Statistician, 30(2):88–90, 1976.
Minka, T. Estimating a dirichlet distribution, 2000.
Moreno, P. G., Artés-Rodrı́guez, A., Teh, Y. W., and
Perez-Cruz, F. Bayesian nonparametric crowdsourcing.
Journal of Machine Learning Research, 16:1607–1627,
2015.
Polson, N. G. and Scott, S. L. Data augmentation for
support vector machines. Bayesian Analysis, 6(1):1–23,
2011.
Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H., Florin,
C., Bogoni, L., and Moy, L. Learning from crowds. Journal of Machine Learning Research, 11(Apr):1297–1322,
2010.
Snow, R., O’Connor, B., Jurafsky, D., and Ng, A. Y. Cheap
and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the
conference on empirical methods in natural language
processing, pp. 254–263. Association for Computational
Linguistics, 2008.
Tian, T. and Zhu, J. Max-margin majority voting for learning from crowds. In Advances in Neural Information
Processing Systems, pp. 1621–1629, 2015.
Welinder, P. and Perona, P. Online crowdsourcing: Rating
annotators and obtaining cost-effective labels. In 2010
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition-Workshops.
Welinder, P., Branson, S., Perona, P., and Belongie, S. J.
The multidimensional wisdom of crowds. In Advances in
Neural Information Processing Systems, pp. 2424–2432,
2010.

Learning to Aggregate Ordinal Labels by Maximizing Separating Width

Zhang, Y., Chen, X., Zhou, D., and Jordan, M. I. Spectral methods meet em: a provably optimal algorithm for
crowdsourcing. Journal of Machine Learning Research,
17(102):1–44, 2016.
Zhou, D., Basu, S., Mao, Y., and Platt, J. C. Learning
from the wisdom of crowds by minimax entropy. In Advances in Neural Information Processing Systems, pp.
2195–2203, 2012.
Zhou, D., Liu, Q., Platt, J. C., and Meek, C. Aggregating ordinal labels from crowds by minimax conditional
entropy. In ICML, pp. 262–270, 2014.
Zhu, J., Chen, N., and Xing, E. P. Bayesian inference with
posterior regularization and applications to infinite latent
svms. Journal of Machine Learning Research, 15(1):
1799–1847, 2014.

