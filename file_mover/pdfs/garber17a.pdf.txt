Communication-efficient Algorithms for
Distributed Stochastic Principal Component Analysis
Dan Garber 1 Ohad Shamir 2 Nathan Srebro 3

Abstract
We study the fundamental problem of Principal
Component Analysis in a statistical distributed
setting in which each machine out of m stores
a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms
for estimating the leading principal component
of the population covariance matrix that are both
communication-efficient and achieve estimation
error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply
averaging the local ERM solutions cannot guarantee error that is consistent with the centralized
ERM. We show that this unfortunate phenomena
can be remedied by performing a simple correction step which correlates between the individual
solutions, and provides an estimator that is consistent with the centralized ERM for sufficientlylarge n. We also introduce an iterative distributed
algorithm that is applicable in any regime of n,
which is based on distributed matrix-vector products. The algorithm gives significant acceleration
in terms of communication rounds over previous
distributed algorithms, in a wide regime of parameters.

1. Introduction
Principal Component Analysis (PCA) (Pearson, 1901;
Hotelling, 1933; Jolliffe, 2002) is one of the most celebrated and popular techniques in data analysis and ma1
Technion - Israel Institute of Technology, Haifa, Israel 2 Weizmann Institute of Science, Rehovot, Israel
3
Toyota Technological Institute, Illinois, USA. Correspondence to:
Dan Garber <dangar@technion.ac.il>, Ohad
Shamir <ohad.Shamir@weizmann.ac.il>, Nathan Srebro
<nati@ttic.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

chine learning. For data that consists of N vectors in
RdP
, x1 , ..., xN , with normalized covariance matrix X̂ =
N
1
>
i=1 xi xi , The PCA method finds the k-dimensional
N
subspace (which corresponds to the span of the top k principal components) such that the projection of the data onto
the subspace has largest variance, i.e., it is the solution to
the optimization problem:
max

W2Rd⇥k ,WT W=I

kX̂Wk2F .

(1)

PCA is often considered in a statistical setting in which the
assumption is that the input vectors are not arbitrary but
sampled i.i.d. from some fixed but unknown distribution
with certain general characteristics D. Then, it is often of
interest to use the observed sample to estimate the top k
principal components of the population covariance matrix,
rather then that of the sample, which leads to the modified
optimization problem:
⇥
⇤
max
kEx⇠D xx> Wk2F .
(2)
W2Rd⇥k ,WT W=I

Of course the empirical estimation problem (1) and the
population estimation problem (2) are well connected, and
it is well-known that under mild assumptions on the distribution D and given a sufficiently large sample, we can
guarantee small estimation error in (2) by solving optimization problem (1).
In this work we consider the problem of estimating the first
principal component (i.e., k = 1) in a statistical and distributed setting. We assume the availability of m machines,
each of which stores a sample of n vectors sampled i.i.d
from a fixed distribution D over Rd , and we are interested
in algorithms that can be applied efficiently to solve Problem (2) for k = 1, with estimation error that approaches
that of a centralized algorithm, which has access to all mn
samples and does not pay for communication between machines. Indeed, when considering the efficiency of algorithms, we will mainly focus on the amount of communication between machines they require, since this is often
the most expensive resource in distributed computing. We
note that the i.i.d. assumption is standard in many applications of PCA, and can be leveraged to get more efficient
algorithms than when the data partition is arbitrary. Also,
we will make a standard assumption that the population covariance matrix has a non-zero additive gap between the

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

first and second eigenvalues, which makes the problem of
estimating the leading principal component meaningful.
A main challenge that often arises in many computational
settings of principal components is that it leads to inherently non-convex optimization problems. While many
times these problems turn out to admit efficient algorithms,
the rich toolbox of optimization and statistical estimation
procedures developed for convex problems often cannot be
directly applied to problems such as (1) and (2). Instead,
one often needs to consider a specialized and more involved
analysis, to get analogous convergence results for the PCA
problem. This for instance was the case in a recent wave
of results that applied concepts such as stochastic gradient updates (Balsubramani et al., 2013; Shamir, 2016a; Jain
et al., 2016b; Allen Zhu & Li, 2016b) and variance reduction (Shamir, 2015; 2016c; Garber & Hazan, 2015; Garber
et al., 2016; Allen Zhu & Li, 2016a) to the PCA problem.
This is also the case in our distributed setting. For instance,
(Zhang et al., 2013) proposed communication-efficient algorithms for a distributed statistical estimation settings,
similar to ours, but under convexity assumptions. The authors show that under their assumptions, in a wide regime
of parameters (namely when the per-machine sample size
n is large enough), then a simple averaging of the empirical
risk minimizers (ERM), computed locally on each machine,
leads to estimation error of the population parameters of
the order the centralized ERM solution. While averaging
makes perfect sense in a convex setting, it is clear that it
can completely fail in a non-convex setting. Indeed, we
show that already for the PCA problem with k = 1, simply
averaging the local ERM solutions (and normalizing to obtain a unit vector as required), cannot improve significantly
over the estimation error of any single machine. We then
show that a simple fix to the above scheme, namely correlating the directions of individual ERM solutions, remedies this phenomena and results in estimation error similar
to that of the centralized ERM solution. Much like the results of (Zhang et al., 2013), this result only holds in the
regime when the per-machine sample size n is sufficiently
large. As discussed, due to the inherent non-convexity of
the PCA objective, this approach requires a novel analysis
tailored to the PCA problem. In this context, we view this
work as an initiation of a research effort to understand how
to efficiently aggregate statistical estimators in a distributed
non-convex setting.
A second line of results for distributed estimation under
convexity assumptions consider iterative algorithms that
perform multiple communication rounds and are based on
distributed gradient computations (some examples include
(Shamir et al., 2014; Zhang & Lin, 2015; Lee et al., 2015;
Shamir, 2016b; Jaggi et al., 2014; Reddi et al., 2016)). The
benefit of these methods is that (a) they provide meaningful
estimation error guarantees in a much wider regime of pa-

rameters than the “one-shot” aggregation methods (namely
in terms of the number of samples per machine), and (b),
due to their iterative nature, they allow to approximate the
centralized ERM solution arbitrary well. Unfortunately,
these methods, all of which rely heavily on convexity assumption, cannot be directly applied to the PCA problem.
Towards designing efficient distributed iterative methods
for our PCA setting, we consider the application of the
recently proposed method of Shift-and-Invert power iterations (S&I) for PCA (Garber & Hazan, 2015; Garber et al.,
2016). The S&I method reduces the problem of computing
the leading eigenvector of a real positive semidefinite matrix to that of approximately solving a small number (i.e.
poly-logarithmic in the problem parameters) of systems of
linear equations. These in turn, could be efficiently solved
by arbitrary distributed convex solvers. We show that coupling the S&I method with the stochastic pre-conditioning
technique for linear systems proposed in (Zhang & Lin,
2015) and well known fast gradient methods such as the
conjugate gradient method, gives state-of-the-art guarantees in terms of communication costs, and provides a significant improvement over distributed variants of classical
fast eigenvector algorithms such as power iterations and the
faster Lanczos algorithm. Much like its convex counterparts, which only rely on distributed gradient computations
and simple vector aggregations, our iterative method only
relies on distributed matrix-vector products, i.e., it requires
each machine to only send products of its local empirical
covariance matrix with some input vector.
Beyond the results described so far, (Liang et al., 2014;
Boutsidis et al., 2016) studied distributed algorithms for
PCA in a deterministic setting in which the partition of
the data across machines is arbitrary and communication
is measured in terms of number of transmitted bits. The
approximation guarantees provided in these works are in
terms of the projection of the data onto the leading principal components (instead of alignment between the estimate
and the optimal solution, studied in this paper). Applying
these results to our setting will give a number of communication rounds that scales like poly(✏ 1 1 ), where ✏ is the
desired error and is the population eigengap. In our setting, ✏ will scale with the inverse of the size of the sample,
i.e., ✏ ⇡ (mn) 1 , which for these algorithms will result in
amount of communication that is polynomial in the size of
the data. In contrast, we will be interested in algorithms
whose communication costs does not scale with n at all. In
this context we note that, by focusing on algorithms that
either perform simple aggregation of local ERM solutions,
or perform only distributed matrix-vector products with the
empirical covariance matrix, we can circumvent the need to
measure communication explicitly in terms of the number
of bits transmitted, which often burdens the analysis of natural algorithms, such as those proposed here.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

2. Preliminaries
2.1. Notation and problem setting
We write vectors in Rd in boldface lower-case letters (e.g.,
v), matrices in boldface upper-case letters (e.g., X), and
scalars are written as lightface letters (e.g., c). We let k ·
k denote the standard Euclidean norm for vectors and the
spectral norm for matrices.
We consider the following statistical distributed setting.
Let D be a distribution over vectors in Rd with squared
`2 norm at most b, for some b > 0. We consider a setting
in which m machines, numbered 1...m, are each given a
dataset of n samples drawn i.i.d. from D. We let v1 denote
a leading eigenvector of the population covariance matrix
X = Ex⇠D [xx> ]. Our goal is to efficiently (mainly in
terms of communication) find an estimate w for v1 , i.e., a
unit vector that maximizes the product (v1> w)2 with high
probability. Towards this end, we assume that the population covariance matrix X has a non-zero eigengap , i.e.,
:= 1 (X)
2 (X) > 0, where i (·) denotes the ith
largest eigenvalue of a symmetric real matrix. Note that
> 0 is necessary for v1 to be uniquely defined (up to
sign).
In addition, we let X̂i denote the empirical covariance matrix of the sample stored on machine i for every i 2 [m],
Pn
(i) (i)>
(i)
(i)
i.e., X̂i = n1 j=1 xj xj , where x1 ...xn are the
samples stored on machine i. We let X̂ denote the empirical covariance matrix
Pmof the union of points across all
1
machines i.e., X̂ = m
i=1 X̂i .
Our model of communication assumes that the m machines
work in rounds during which a central machine (w.l.o.g.
machine 1) can send a single vector in Rd to all other machines, or every machine can send either the leading eigenvector of its local empirical covariance matrix, or the product of a single input vector with its local covariance, to machine 1. We will measure communication complexity in
terms of number of such rounds required to achieve a certain estimation error.
2.1.1. T HE CENTRALIZED SOLUTION
Our primary benchmark for measuring performance will be
the centralized empirical risk minimizer which is the leading eigenvector of the aggregated empirical covariance matrix X̂.
The following standard result bounds the error of the centralized ERM.
Lemma 1 (Risk of centralized ERM). Fix p 2 (0, 1). Suppose that > 0 and let v̂1 denote the leading eigenvector
of X̂, i.e., v̂1 2 arg maxv:kvk=1 v> X̂v. Then it holds w.p.
at least 1 p that

32b2 ln(d/p)
.
(3)
mn 2
Lemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the
Davis-Kahan sin(✓) theorem (whose proof is given in the
appendix for completeness):
Theorem 1 (Matrix Hoeffding, see (Tropp, 2012)). Let D
be a distribution over vectors with squared `2 norm
Pn at most
b, and let X = Ex⇠D [xx> ]. Let X̂ = n1 i=1 xi x>
i ,
where x1 , ..., xn are
⇣ sampled i.i.d.
⌘ from D. Then,
⇣ 2it holds
⌘
✏ n
that 8✏ > 0 : Pr kX̂ Xk ✏  d · exp
16b2 .
1

(v1> v̂1 )2  ✏ERM (p) :=

Theorem 2 (Davis-Kahan sin(✓) theorem). Let X, Y be
symmetric real d⇥d matrices with leading eigenvectors vX
and vY respetively. Also, suppose that (X) := 1 (X)
2
Yk2
>
vX
vY  2 kX(X)
2 .
2 (X) > 0. Then it holds that 1
2.2. Informal statement of main results and previous
algorithms

We now informally describe our main results, followed by a
detailed description of previous approaches that are directly
applicable to our setting. The algorithmic results (both new
and old) are summarized in Table 1.
2.2.1. M AIN RESULTS
Failure of simple averaging of local ERM solutions We
show that a natural approach of simply averaging the individual leading eigenvectors of the empirical covariance
matrices X̂i (and normalizing the obtain a unit vector)
cannot significantly improve (beyond logarithmic factors)
over the performance of any of the individual eigenvectors.
(i)
More concretely, if we let v̂1 denote the leading eigenvector of X̂i for any i 2 [m], and we denote their average
Pm (i)
1
by v̄1 = m
i=1 v̂1 , then there exists a distribution D
over vectors with magnitude O(1) and covariance eigengap = 1, such that
"
✓ > ◆2 #
✓ ◆
v̄1 v1
1
8m, n : ED 1
=⌦
,
kv̄1 k
n
See Theorem 3 in Section 3 for the complete and formal
argument.
A successful single communication round algorithm via
correlation of individual ERM solutions We show that
if prior to averaging the local ERM solutions, as suggested
above, we correlate their directions by aligning them according to any single machine (say machine number 1),
Pm
(i)> (1)
(i)
1
i.e., we let v̄1 = m
v̂1 )v̂1 , then this
i=1 sign(v̂1
guarantees that for any p 2 (0, 1), w.p. at least 1 p,
⇣ ⌘
⇣ ⌘1
0
2 dm
dm
✓ > ◆2
2
4
b
ln
b
ln
p
p
v̄1 v1
A . (4)
1
= O@
+
2 mn
4 n2
kv̄1 k

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis
(w> v1 )2 w.p. 3/4
2
ln d
✏ERM = ⇥( b 2 mn
)
✏ERM · (1 + o(1))
✏ERM · (1 + o(1))
O(✏ERM⇣)
⌘

Method
Centralized ERM
Distributed Power Method
Distributed Lanczos
“Hot-potato” SGD

1

Average of ERMs with sign-fixing (Theorem 4)

O(✏ERM ) + O

Distributed Shift&Invert + precond. linear systems (Theorem 6)

b4 ln2 d
4 n2

✏ERM · (1 + o(1))

# communcation rounds
Õ( 1 / )
p
Õ(
1/ )
m
1
Õ(min{(b/ )1/2 n

1/4

, m1/4 })

Table 1. Comparison of estimation error and number of communication rounds. For simplicity we fix the failure probability to p = 1/4
and assume mn is in the regime in which Lemma 1 is meaningful, i.e, mn = ⌦(b2 2 ln d). The Õ(·) suppresses logarithmic factors
in b, d, 1/p, 1/✏ERM . For the result of Theorem 4 we assume the regime m = O(d). The sub-constant o(1) factors could be made, in
principle, arbitrary small in all relevant results by trading approximation with communication.

See Theorem 4 in Section 3 for the complete and formal
result.
In particular, in the likely scenario when m = O(d/p)
2
we have that w.p. at least 1 p, 1
v̄1> v1 /kv̄1 k =
✏ERM (p)) · O 1 + m2 · ✏ERM (p) , where ✏ERM (p)) is defined in Eq. (3). Another related interpretation of the results is that the bound in Eq. (4) is comparable with ✏ERM
2 2
(up to poly-log factors) when n = ⌦
b m ln(dm/p) .
We also show a matching lower bound that the bound in
Eq. (4) is tight (up to poly-log factors) for this aggregation
method.
A multi communication round algorithm We present a
distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation (Garber &
Hazan, 2015; Garber et al., 2016) which is applied to explicitly solving the centralized ERM problem. We show
that for any p 2 (0, 1), when mn = ⌦(b2 ln(d/p)/ 2 ) (i.e.,
when Lemma 3 is meaningful), the algorithm produces a
solution w such that w.p. at least 1 p,
1

(v1> w)2  ✏ERM (p)) · (1 + o(1)) ,

(5)

where ✏ERM (p)) is
p defined in Eq. (3). The algorithm performs overall Õ( b 1/2 n 1/4 ) distributed matrix-vector
products with the centralized empirical covariance matrix
X̂ 1 . The Õ(·) notation hides poly-logarithmic factors in
1/p, 1/ , d, 1/✏ERM (p). See Theorem 6 in Section 4 for the
complete and formal result.
We note that in particular, under our assumption that mn =
˜ 2 / 2 ), it holds that the number of distributed matrix⌦(b
vector products is upper bounded by Õ(m1/4 ). Moreover,
in the regime n = ⌦(b2 2 ), we can see that the number
of distributed matrix-vector products depends only polylogarithmically on the problem parameters.
In general, the sub-constant o(1) factor in (5) could be
made arbitrarily small by trading the approximation error
1

i.e., on each round, each machine i sends the product of an
input vector in Rd with its local covariance matrix X̂i .

with the number of distributed matrix-vector products.
2.2.2. P REVIOUS ALGORITHMS
Distributed versions of classical iterative algorithms:
Classical fast iterative algorithms for computing the leading eigenvector of a positive semidefinite matrix, such as
the well-known Power Method and the Lanczos Algorithm, require iterative multiplications of the input matrix (X̂ in our case) with the current estimate. It is thus
straightforward to implement these algorithms in our distributed setting, by multiplying the same vector with the
covariance matrices at each machine, and averaging the
result. Thus, by well-known convergence guarantees of
these two methods, we will have that for a fixed ✏ > 0,
these methods produce a unit vector w such that, for any
p 2 (0, 1), 1 (w> v̂1 )2  ✏ w.p. at least 1 p, after p
O( ˆ 1 ˆ 1 ln(d/p✏)) rounds for the Power Method and
O( ˆ 1 ˆ 1 ln(d/p✏)) for the Lanczos Algorithm, where
ˆ 1 , ˆ denote the leading eigenvalue and eigengap of X̂,
respectively. Moreover, in the regime of mn in which
Lemma 1 is meaningful, we can replace ˆ 1 , ˆ with 1 ,
in the above bounds, and the result will still hold with high
probability.
Simple calculations show that in the regime of mn in which
Lemma 1 is meaningful, it holds that our Shift-and-Invertbased algorithm outperforms distributed Lanczos (in terms
˜ 2 / 2 ).
of worst-case guarantees) whenever n = ⌦(b
1
“Hot potato” SGD: Another straightforward approach is
to apply a sequential algorithm for direct risk minimization that can process the data-points one by one, such as
stochastic gradient descent (SGD), by passing its state from
one machine to the next, after completing a full pass over
the machine’s data. Clearly, this process of making a full
pass over the data of a certain machine before sending the
final estimate to the next one, requires overall m communication rounds in order to make a full pass over all mn
points. SGD for PCA was studied in several results in
recent years (Balsubramani et al., 2013; Shamir, 2016a;c;

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

Jain et al., 2016a; Allen Zhu & Li, 2016b). For instance
applying the result of (Jain et al., 2016a) in this way will
result in a final estimate w satisfying
✓ 2
◆
b ln d
>
2
1 (w v1 ) = O
w.p. at least 3/4. (6)
2 mn
We note that in the regime in which the bound in (6) is
meaningful it holds that the number of communication
rounds of our Shift-and-Invert-based algorithm is upperbounded by Õ(m1/4 ) which for sufficiently large m dominates the communication complexity of SGD.

3. Single Communication Round Algorithms
via ERM on Each Machine
In this section we consider distributed algorithms that require only a single round of communication. Naturally for
this regime, all algorithms will be based on aggregating the
ERM solutions of the individual machines, i.e., each machine i only sends the leading eigenvector of its empirical covariance matrix X̂i to a centralized machine (without
loss of generality, machine 1) which it turn combines them
to a single unit vector in some manner.
3.1. Simple averaging of eigenvectors fail
Perhaps the simplest method to aggregate the individual
eigenvectors of each machine is to average them, and then
normalize to obtain a unit vector. For instance, in the
distributed statistical setting considered in (Zhang et al.,
2013), in which the objective is strongly convex, it was
shown that simply averaging the individual ERM solutions
leads, in a meaningful regime of parameters, to estimation
error of the order of the centralized ERM solution. However, here we show that for PCA, in which the objective is
certainly not convex, this approach fails practically in any
regime, in the sense that the error of the returned aggregated solution can be no better than that returned by any
single machine.
Theorem 3. There exists a distribution over vectors in R2
with `2 norm bounded by a universal constant for which the
eigengap in the covariance matrix is 1 (i.e., = 1), such
(i)
that if each machine i returns an estimate v̂1 which is
an unbiased leading eigenvector of X̂i (i.e., both outcomes
(i)
(i)
v̂1 , +v̂1 are equally likely), then the aggregated vector
P
(i)
m
1
v̄1 = m
i=1 v̂1 satisfies
"
#
⌧
2
v̄1
8m, n : E 1
, v1
= ⌦(1/n).
kv̄1 k
The proof is given in the appendix.

3.2. Averaging with Sign Fixing
As evident from the statement of Theorem 3, an important
assumption is that each machine produces an unbiased estimate, in the sense that the sign of the outcome is uniform
and independent of the other machines. This hints that correlating the signs of the different estimates can circumvent
the lower bound result in Theorem 3. It turns out that this
is indeed the case, as captured by the following theorem:
Theorem 4. Let w̃i be the leading eigenvector of X̂i for
any i 2 [m], and consider
the unit vector
Pm
>
i=1 sign(w̃i w̃1 )w̃i
w = Pm
.
(7)
k i=1 sign(w̃i> w̃1 )w̃i k
Then, for any p 2 (0,0
1), it holds
⇣ w.p.
⌘ at least 1 ⇣ p that
⌘1
dm
2
b log p
b4 log2 dm
p
A.
1 (v1> w)2 = O @
+
2 mn
4 n2

For ease of presentation, throughout the rest of this section
we denote the correlated vector ŵi = sign(w̃i> w̃1 )w̃i for
any i 2 [m].

The main step towards proving Theorem 4 is to consider
each ŵi as an approximately unbiased perturbation of the
true leading eigenvector v1 and to upper bound the magnitude of this perturbation. This is carried out in the following much more general and self-contained lemma, which
might be of independent interest. The proof is given in the
appendix.
Lemma 2. Let A be a positive semidefinite matrix with
some fixed leading eigenvector v1 , a leading eigenvalue 1
and an eigengap := 1 (A)
2 (A) > 0. Let Â be some
positive semidefinite matrix such that kÂ Ak  /4.
Then there is a unique leading eigenvector v̂1 of Â such
that hv̂1 , vi 0, and
ckÂ Ak2
v̂1 v1 ( 1 I A)† (Â A)v1 
,
2
where † denotes the pseudo-inverse, and c is a positive numerical constant.
Lemma 2 is central to the proof of the following Lemma,
of which the proof of Theorem 4 is an easy consequence.
We defer the proof of both the Lemma and that of Theorem
4 to the appendix.
Lemma 3. The following two conditions hold with probability at least 1 p d exp( 2 n/cb2 ), for some numerical
constants c, c0 > 0:
• The leading eigenvalue of every X̂i is simple, i.e.,
1 (X̂i )
2 (X̂i ) > 0.
• Fixing v1 , there exist unique leading eigeni
vectors v̂1i , . . . , v̂m
of X̂1 , . . . , X̂m , such that
Pm i
1
i
maxi kv̂1
v1 k  14 , and m
i=1 v̂1
q
⇣ 2
⌘
2
v1  c0 b log(2dm/p)
+ b log(2dm/p)
.
2n
2 mn

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

3.3. Lower Bound for Sign Fixing
We now show that the result of Theorem 4 is tight up to
poly-logarithmic factors and cannot be improved in general:
Theorem 5. For any 2 (0, 1) and d > 1, there exist a
distribution over vectors in Rd (of norm at most a universal
constant) with eigengap in the covariance matrix, such
that for any number of machines m and for per-machine
sample size any n sufficiently larger than 1/ 2 , the aggrePm (i)
1
gated vector v̄1 = m
i=1 v̂1 (even after sign fixing with
the population
eigenvector
"
#v1 ) satisfies
⌧
✓
◆
2
v̄1
1
1
E 1
, e1
= ⌦
+ 4 2
2 mn
kv̄1 k
n
The proof is given in the appendix.

4. A Multi-round Algorithm based on
Shift-and-Invert Iterations
In this section we move on to consider distributed algorithms that perform multiple communication rounds. The
main motivation, beyond improving some poly-logarithmic
factors in the estimation error, is to obtain a result that does
not require the per-machine sample size n to grow with the
number of machines m, as in the result of Theorem 4.
Towards this end we consider the use of the Shift-andInvert meta-algorithm, originally described in (Garber &
Hazan, 2015; Garber et al., 2016), to explicitly solve the
centralized ERM objective, i.e., find a unit vector that is an
approximate solution to maxv:kvk=1 v> X̂v.
Throughout this section we let ˆ 1 , ˆ denote the leading
eigenvalue and eigengap of X̂, respectively. Also, we assume without loss of generality that b = 1 (i.e., all data
points lie in the unit Euclidean ball).
Since our approach is to approximate the population risk
by approximating the empirical risk, we state the following simple lemma for completeness (a proof is given in the
appendix).
Lemma 4 (Risk of approximated-ERM for PCA). Let w
be a unit vector such that (w> v̂1 )2 1 ✏, for some fixed
✏ > 0, where v̂1 is the leading eigenvector p
of X̂. Then it
holds that 1 (w> v1 )2  1 (w> v̂1 )2 + 2✏.
4.1. The Shift-and-Invert meta-algorithm
The Shift-and-Invert algorithm (Garber & Hazan, 2015;
Garber et al., 2016) efficiently reduces the problem of
computing the leading eigenvector of a positive semidefinite matrix X̂ to that of approximately-solving a polylogarithmic number of linear systems, i.e., finding approximate minimizers of convex quadratic optimization problems of the form

min {F

z2Rd

,w (z)

:=

1 >
z ( I
2

X̂)z

z> w},

(8)

where > 1 (X̂) is a shifting parameter. The algorithm is
essentially based on applying power iterations to a shifted
and inverted matrix ( I X̂) 1 , where the shifting parameter is carefully chosen. The algorithm that implements
this reduction, originally described in (Garber & Hazan,
2015), is given below (see Algorithm 1).
Algorithm 1 S HIFT- AND -I NVERT P OWER M ETHOD
1: Input: estimate ˜ for the gap ˆ, accuracy ✏ 2 (0, 1),
failure probability p
⇣ ⌘
2: Set: m1
d8 ln 144d/p2 e, m2
d 32 ln 18d
p2 ✏ e
n ⇣ ⌘m1 +1 ⇣ ⌘m2 +1 o
1
˜/8
3: Set: ✏˜
min 16
, 4✏ ˜/8
4: Set: (0)
1 + ˜ , ŵ0
random unit vector, s
0
5: repeat
6:
s
s + 1 , Ms
( (s 1) I X̂)
7:
for t = 1...m1 do
8:
Find approx. minimizer - ŵt of F (s 1) ,ŵt 1 (z)
such that kŵt Ms 1 ŵt 1 k  ✏˜
9:
end for
10:
ws
ŵm1 /kŵm1 k
11:
Find approx. minimizer - vs of F (s 1) ,ws (z) such
that kvs Ms 1 ws k  ✏˜
1
1
s
12:
s
(s)
(s 1)
2 · ws> vs ✏˜ ,
2
13: until s  ˜
14: (f )
( (f ) I X̂)
(s) , Mf
15: for t = 1...m2 do
16:
Find approx. minimizer - ŵt of F (f ) ,ŵt 1 (z) such
that kŵt Mf 1 ŵt 1 k  ✏˜
17: end for
18: Return: wf
ŵm2 /kŵm2 k
Lemma 5 (Efficient reduction of top eigenvector to convex optimization; originally Theorem 4.2 in (Garber &
Hazan, 2015)). Suppose that ˆ := 1 (X̂)
2 (X̂) > 0
and suppose that the estimate ˜ in Algorithm 1 satisfies
˜ 2 [ ˆ/2, 3 ˆ/4]. Then, with probability at least 1 p,
Algorithm 1 finds a unit vector wf such that (wf> v̂1 )2
1 ✏, and the total number of optimization problems of
the form (8) solved ⇣
during the run of the algorithm,
⇣ ⌘⌘ is upd
1
ˆ
per bounded by O ln(d/p) ln(
) + ln p✏
. Moreover, throughout the run of the algorithm it holds that
ˆ 1 = ⌦( ˆ).
1+ˆ
(s)
Remark: the purpose of the repeat-until loop in Algorithm 1 is to efficiently find a shifting parameter (f ) such
that c1 ˆ  (f ) ˆ 1  c2 ˆ for some universal constants
c2 > c1 > 0. When n satisfies n = ⌦( 2 ln(d/p)),
we can directly find (w.h.p) such a shifting parameter, by

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

simply estimating ˆ 1 , ˆ from the data of a single machine.
Also, we can take ŵ0 to be the leading eigenvector of any
single machine, since this will already have a constant correlation with v̂1 . Thus, for such n, the total number of
optimization problems can be reduced to O(ln(p 1 ✏ 1 )).
Algorithm 1 is a meta-algorithm in the sense that the choice
of solver for the optimization problems min F ,w is unspecified, and any solver will do. A simple calculation
shows that a naive application of either the conjugate gradient method or Nesterov’s accelerated gradient method to
solve these optimization problems in a distributed manner,
i.e., the computation of the gradient vector
q is distributed
ˆ 1 / ˆ commuacross machines, will require overall Õ
nication rounds, which does not give any improvement over
the distributed Lanczos approach, described in Subsection
2.2.2. However, this can be substantially improved by taking advantage of the fact that the data on all machines is
sampled i.i.d. from the same distribution. In particular,
we present below an approach based on applying a preconditioner to the optimization Problem (8), in the spirit of
the one described in (Zhang & Lin, 2015).
4.2. Faster Distributed Approximation of Linear
Systems via Local Preconditioning
Let M = I X̂, for some shift parameter > ˆ 1 , and define the pre-conditioning matrix C = ( +µ)I X̂1 , where
µ is required so C is invertible. Consider now solving the
following modified quadratic problem:
1
F̃ ,w (y) := y> C 1/2 MC 1/2 y y> C 1/2 w. (9)
2
⇤
Note that if y is the optimal solution to Problem (9), i.e.,
y⇤ = C1/2 M
then z := C
⇤

1

C1/2 C

1/2

w = C1/2 M

1

w,

y is the optimal solution to Problem (8).

1/2 ⇤

The idea behind choosing C this way is very intuitive. Ideally we could have chosen C = M, making the condition number of F̃ ,w equal to (F̃ ,w ) = 1, which is the
best we can hope for. The problem of course is that this
requires us to explicitly compute M 1/2 , which is more
challenging then just computing the leading eigenvector of
X̂. The next best thing is thus to choose C based only
on the data available on any single machine, which allows
computing C 1/2 without additional communication overhead, and leads to the choice described above. The following lemma, rephrased from (Zhang & Lin, 2015), quantifies
exactly how such a choice of C helps in improving the condition number of the new optimization problem, Problem
(9). The proof is given in the appendix.
Lemma 6. Suppose
⇣ that µ ⌘ kX̂ X̂1 k. Then, F̃ ,w (y)
is 1-smooth and
lar, (F̃

,w )

ˆ1

(

ˆ 1 )+2µ

 1 + 2µ/(

-strongly convex. In particu-

ˆ 1 ). Moreover, fixing ỹ 2 R ,
d

if we let z̃ := C 1/2 ỹ, then it holds that kz̃ M 1 wk 
ˆ 1 ) 1/2 kỹ C1/2 M 1 wk. In particular, for any
(
p
p 2 (0, 1), if we set µ = 4 ln(d/p)/n, then the above
holds with probability at least 1 p, where this probability
depends only on the randomness in X̂1 .
4.2.1. S OLVING THE PRE - CONDITIONED LINEAR
SYSTEMS

We now discuss the application of gradient-based algorithms for finding an approximate minimizer of the preconditioned problem, Problem (9), in our distributed setting. Towards this end we require a distributed implementation for the first-order oracle of F̃ ,w (y) (i.e., computation of the value and gradient vector at a queried point).
A straight-forward implementation of the first-order oracle
in our distributed setting is given in Algorithm 2.
Algorithm 2 Distributed First-Order Oracle for F̃ ,w (y)
1: Input: shift parameter > 0, regularization parameter
µ > 0, vector w 2 Rd , query vector y 2 Rd
2: send ỹ := C 1/2 y to machines {2, . . . , m} for C :=
( + µ)I X̂1 {executed on machine 1}
3: for i = 1...m do
4:
send r̃i := X̂i ỹ to machine 1 {executed on each
machine i}
5: end for
Pm
1
6: aggregate r̃ := m
i=1 r̃i {executed on machine 1}
7: compute F̃ ,w (y) = 12 ( y> C 1 y y> C 1/2 r̃)
y> C 1/2 w {executed on machine 1}
8: compute rF̃ ,w (y) = C 1 y C 1/2 r̃ C 1/2 w
{executed on machine 1}
9: return: (F̃ ,w (y), rF̃ ,w (y))
We have the following lemma, the proof of which is deferred to the appendix.
Lemma 7. Fix some > 1 (X̂) and w 2 Rd , and let
1
µ > 0 be as in Lemma 6. Fix ✏ > 0. Consider the
following two-step algorithm:
1. Apply either the conjugate gradient method or Nesterov’s accelerated method with the distributed firstorder oracle described in Algorithm 2 to find ỹ 2 Rd
such that F̃ ,w (ỹ) miny2Rd F̃ ,w (y)  ✏0
2. Return z̃ = C
Then, for ✏ =
0

✏
2
1

1/2

⇣

ỹ.

1+

2µ
ˆ1

⌘

1

(

ˆ 1 ) it holds that

kz̃ ( I X̂1 ) wk  ✏, and the total number distributed matrix-vector products with the empirical covariance matrix X̂ required to compute z̃ is upper-bounded by
O

✓q

1 + 2µ(

ˆ1)

1

ln

⇣⇣

1+

2µ ⌘
kwk/[(
ˆ1

ˆ 1 )✏]

⌘◆

.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

We now state our main result for this section, which is a
simple consequence of the previous lemmas. The full proof
is given in the appendix.
Theorem 6. Fix ✏ 2 (0, 1) and p 2
p(0, 1). Suppose that
mn = ⌦( 2 ln(d/p)). Set µ = 4 ln(3d/p)/n. Applying the Shift-and-Invert algorithm, Algorithm 1, with the
parameters ✏, p/3, and applying the algorithm in Lemma
7 with the parameter µ, to approximately solve the linear
systems, yields with probability at least 1 p a unit vector
wf such that (wf> v̂1 )2 1 ✏, after executing at most
0s p

O@

" ✓
◆
ln(d/p)
d
p
ln
ln
p✏2
n

+ ln

2

✓

d
p✏2

◆

ln

✓ ◆ ◆
1

p

!
ln(d/p)
p
2 n
s

=

Õ

1
p

n

!

distributed matrix-vector products with the empirical covariance matrix X̂.

5. Experiments
To validate some of our theoretical findings we conducted experiments with single-round algorithms on synthetic data. We generated synthetic datasets using two distributions. For both distributions we used the covariance
matrix X = U⌃U> with U being a random d ⇥ d orthonormal matrix and ⌃ is diagonal satisfying: ⌃(1, 1) =
1, ⌃(2, 2) = 0.8, 8j 3 : ⌃(j, j) = 0.9·⌃(j 1, j 1),
i.e., = 0.2. One dataset was generated according to the
normal distributions N (0, X), and for p
the second datasets
we generated samples by taking x = 3/2X1/2 y where
y ⇠ U [ 1, 1]. In both cases we set d = 300.
Beyond the single-round algorithms that are based on
aggregating the individual ERM solutions described so
far, we propose an additional natural aggregation approach, based on aggregating the individual projection ma(i)
trices. More concretely, letting {v̂1 }m
i=1 denote the leading eigenvectors of the individual machines, let P̄1 :=
Pm (i) (i)>
1
. We then take the final estimate w to
i=1 v̂1 v̂1
m
be the leading eigenvector of the aggregated matrix P̄1 .
Note that as with the sign-fixing based aggregation, this
approach also resolves the sign-ambiguity in the estimates
produced by the different machines, which circumvents the
lower bound result of Theorem 3.

For both datasets we fixed the number of machines to
m = 25. We tested the estimation error (i.e., the value
1 (w> v1 )2 where v1 is the leading eigenvector of X and
w is the estimator) of five benchmarks vs. the per-machine
sample size n: the centralized solution v̂1 , the average
of the individual (unbiased) ERM solutions (normalized to
unit norm),the average of ERM solutions with sign-fixing,
and the leading eigenvector of the averaged projection ma-

trix. We also plotted the average loss of the individual ERM
solutions. Results are averaged over 400 independent runs.
The results for the normal distribution appear in Figure 1.
The results for the uniform-based distribution are very similar and are deferred to the appendix. We can see that, as
our lower bound in Theorem 3 suggests, simply averaging
and normalizing the individual ERM solutions has significantly worse performance than the centralized ERM solution. Perhaps surprisingly, the performance of this estimator is even worse than the average error of an estimate
computed using only a single machine. We see that both
aggregation methods that are based on correlating the individual ERM solutions, namely the sign-fixing-based estimator, and the proposed averaging-of-projections heuristic,
are asymptotically consistent with the centralized ERM.
In particular, the averaging-of-projections scheme, at least
empirically, significantly outperforms the sign-fixing approach, which justifies further theoretical investigation of
this heuristic. For the sign fixing approach, we can see that
as suggested by our bounds, the estimator is not consistent
with the centralized ERM solution for small values of n.
0.9

centralized ERM
avg. of ERMs
sign-fix avg. of ERMs.
projection avg.
avg. machine loss

0.8

0.7

0.6

avg. error

4.3. Putting it all together

0.5

0.4

0.3

0.2

0.1

0
0

100

200

300

400

500

600

n

Figure 1. Estimation error vs. the per-machine sample size n for
a normal distribution.

6. Discussion
We presented communication-efficient algorithms for distributed statistical estimation of principal components. Focusing on our results for methods based on a single communication round, we initiated a study of how to correctly aggregate distributed ERM solutions in a non-convex setting.
An important take-home message of our work is that in a
non-convex setting, simply averaging the local solutions is
not a good idea. On the positive side, we show that a very
simple correction (i.e., sign-fixing) is possible by leveraging the specific structure of the problem at hand. It is thus
interesting to develop a richer theory of how to perform
such aggregations in more involved non-convex problems.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

References
Eigenvalues and eigenvectors of 2x2 matrices.
http://www.math.harvard.edu/archive/
21b_fall_04/exhibits/2dmatrices/.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Praneeth, and Sidford, Aaron. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm. arXiv preprint
arXiv:1602.06929, 2016b.

Allen Zhu, Zeyuan and Li, Yuanzhi. Even faster SVD decomposition yet without agonizing pain. In Advances
in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pp. 974–
982, 2016a.

Jolliffe, IT. Principal component analysis. 2002. Springverlag, New York, 2002.

Allen Zhu, Zeyuan and Li, Yuanzhi. Fast global convergence of online PCA. CoRR, abs/1607.07837, 2016b.

Liang, Yingyu, Balcan, Maria-Florina F, Kanchanapally,
Vandana, and Woodruff, David. Improved distributed
principal component analysis. In NIPS, 2014.

Balsubramani, Akshay, Dasgupta, Sanjoy, and Freund,
Yoav. The fast convergence of incremental PCA. In
Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Processing Systems 2013, pp. 3174–3182, 2013.
Boutsidis, Christos, Woodruff, David P, and Zhong, Peilin.
Optimal principal component analysis in distributed and
streaming models. In Proceedings of the 48th Annual
ACM SIGACT Symposium on Theory of Computing, pp.
236–249. ACM, 2016.
Garber, Dan and Hazan, Elad. Fast and simple pca via
convex optimization. arXiv preprint arXiv:1509.05647,
2015.
Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Faster eigenvector computation via shift-andinvert preconditioning. CoRR, abs/1605.08754, 2016.
Golub, Gene H and Pereyra, Victor. The differentiation
of pseudo-inverses and nonlinear least squares problems
whose variables separate. SIAM Journal on numerical
analysis, 10(2):413–432, 1973.
Hotelling, H. Analysis of a complex of statistical variables
into principal components. J. Educ. Psych., 24, 1933.
Jaggi, Martin, Smith, Virginia, Takác, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jordan, Michael I. Communication-efficient distributed
dual coordinate ascent. In Advances in Neural Information Processing Systems, pp. 3068–3076, 2014.
Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Praneeth, and Sidford, Aaron. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm. arXiv preprint
arXiv:1602.06929, 2016a.

Lee, Jason D., Ma, Tengyu, and Lin, Qihang. Distributed
stochastic variance reduced gradient methods. CoRR,
abs/1507.07595, 2015.

Magnus, Jan R. On differentiating eigenvalues and eigenvectors. Econometric Theory, 1(02):179–191, 1985.
Pearson, K. On lines and planes of closest fit to systems of
points in space. Philosophical Magazine, 2(6):559–572,
1901.
Reddi, Sashank J., Konecný, Jakub, Richtárik, Peter,
Póczos, Barnabás, and Smola, Alexander J. AIDE:
fast and communication efficient distributed optimization. CoRR, abs/1608.06879, 2016.
Shamir, Ohad. A stochastic PCA and SVD algorithm with
an exponential convergence rate. In Proceedings of the
32nd International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, pp. 144–152,
2015.
Shamir, Ohad. Convergence of stochastic gradient descent
for PCA:. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016, pp. 257–265, 2016a.
Shamir, Ohad. Without-replacement sampling for stochastic gradient methods. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 46–54, 2016b.
Shamir, Ohad. Fast stochastic algorithms for svd and pca:
Convergence properties and convexity. In Proceedings of
The 33rd International Conference on Machine Learning, pp. 248–256, 2016c.
Shamir, Ohad, Srebro, Nathan, and Zhang, Tong.
Communication-efficient distributed optimization using
an approximate newton-type method. In Proceedings of
the 31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pp. 1000–
1008, 2014.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

Tropp, Joel A. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389–434, 2012.
Yu, Yi, Wang, Tengyao, and Samworth, Richard J. A useful variant of the davis–kahan theorem for statisticians.
Biometrika, 102(2):315–323, 2015.
Zhang, Yuchen and Lin, Xiao. Disco: Distributed optimization for self-concordant empirical loss. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 362–370, 2015.
Zhang, Yuchen, Duchi, John C, and Wainwright, Martin J.
Communication-efficient algorithms for statistical optimization. Journal of Machine Learning Research, 14:
3321–3363, 2013.

