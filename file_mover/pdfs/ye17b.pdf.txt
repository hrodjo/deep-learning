A Simulated Annealing Based Inexact Oracle
for Wasserstein Loss Minimization
Jianbo Ye 1 James Z. Wang 1 Jia Li 2

Abstract
Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging
research topic for gaining insights from a large
set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein
distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated
annealing for solving WLMs. Particularly, we
have developed a Gibbs sampler to approximate
effectively and efficiently the partial gradients of
a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability
and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which
the oracle for computing the value and gradient
of a loss function is embedded. We applied the
method to optimal transport with Coulomb cost
and the Wasserstein non-negative matrix factorization problem, and made comparisons with the
existing method of entropy regularization.

1. Introduction
An oracle is a computational module in an optimization
procedure that is applied iteratively to obtain certain characteristics of the function being optimized. Typically, it
calculates the value and gradient of loss function l(x, y). In
the vast majority of machine learning models, where those
loss functions are decomposable along each dimension
(e.g., Lp norm, KL divergence, or hinge loss), rx l(·, y) or
1
College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA. 2 Department of
Statistics, The Pennsylvania State University, University Park,
PA.. Correspondence to: Jianbo Ye <jxy198@ist.psu.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ry l(x, ·) is computed in O(m) time, m being the complexity of outcome variables x or y. This part of calculation is
often negligible compared with the calculation of full gradient with respect to the model parameters. But this is no
longer the case in learning problems based on Wasserstein
distance due to the intrinsic complexity of the distance.
We will call such problems Wasserstein loss minimization
(WLM). Examples of WLMs include Wasserstein barycenters (Li & Wang, 2008; Agueh & Carlier, 2011; Cuturi &
Doucet, 2014; Benamou et al., 2015; Ye & Li, 2014; Ye
et al., 2017b), principal geodesics (Seguy & Cuturi, 2015),
nonnegative matrix factorization (Rolet et al., 2016; Sandler & Lindenbaum, 2009), barycentric coordinate (Bonneel et al., 2016), and multi-label classification (Frogner
et al., 2015).
Wasserstein distance is defined as the cost of matching two
probability measures, originated from the literature of optimal transport (OT) (Monge, 1781). It takes into account
the cross-term similarity between different support points
of the distributions, a level of complexity beyond the usual
vector data treatment, i.e., to convert the distribution into a
vector of frequencies. It has been promoted for comparing
sets of vectors (e.g. bag-of-words models) by researchers
in computer vision, multimedia and more recently natural
language processing (Kusner et al., 2015; Ye et al., 2017a).
However, its potential as a powerful loss function for machine learning has been underexplored. The major obstacle
is a lack of standardized and robust numerical methods to
solve WLMs. Even to empirically better understand the
advantages of the distance is of interest.
As a long-standing consensus, solving WLMs is challenging (Cuturi & Doucet, 2014). Unlike the usual optimization in machine learning where the loss and the (partial)
gradient can be calculated in linear time, these quantities
are non-smooth and hard to obtain in WLMs, requiring solution of a costly network transportation problem (a.k.a.
OT). The time complexity, O(m3 log m), is prohibitively
high (Orlin, 1993). In contrast to the Lp or KL counterparts, this step of calculation elevates from a negligible
fraction of the overall learning problem to a dominant portion, preventing the scaling of WLMs to large data. Recently, iterative approximation techniques have been developed to compute the loss and the (partial) gradient at com-

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

plexity O(m2 /") (Cuturi, 2013; Wang & Banerjee, 2014).
However, nontrivial algorithmic efforts are needed to incorporate these methods into WLMs because WLMs often
require multi-level loops (Cuturi & Doucet, 2014; Frogner
et al., 2015). Specifically, one must re-calculate through
many iterations the loss and its partial gradient in order to
update other model dependent parameters.
We are thus motivated to seek for a fast inexact oracle
that (i) runs at lower time complexity per iteration, and
(ii) accommodates warm starts and meaningful early stops.
These two properties are equally important for efficiently
obtaining adequate approximation to the solutions of a sequence of slowly changing OTs. The second property ensures that the subsequent OTs can effectively leverage the
solutions of the earlier OTs so that the total computational
time is low. Approximation techniques with low complexity per iteration already exist for solving a single OT, but
they do not possess the second property. In this paper, we
introduce a method that uses a time-inhomogeneous Gibbs
sampler as an inexact oracle for Wasserstein losses. The
Markov chain Monte Carlo (MCMC) based method naturally satisfies the second property, as reflected by the intuition of physicists that MCMC samples can efficiently
“remix from a previous equilibrium.”
We propose a new optimization approach based on Simulated Annealing (SA) (Kirkpatrick et al., 1983; Corana
et al., 1987) for WLMs where the outcome variables are
treated as probability measures. SA is especially suitable
for the dual OT problem, where the usual Metropolis sampler can be simplified to a Gibbs sampler. To our knowledge, existing optimization techniques used on WLMs are
different from MCMC. In practice, MCMC is known to
easily accommodate warm start, which is particularly useful in the context of WLMs. We name this approach GibbsOT for short. The algorithm of Gibbs-OT is as simple
and efficient as the Sinkhorn’s algorithm — a widely accepted method to approximately solve OT (Cuturi, 2013).
We show that Gibbs-OT enjoys improved numerical stability and several algorithmic characteristics valuable for
general WLMs. By experiments, we demonstrate the effectiveness of Gibbs-OT for solving optimal transport with
Coulomb cost (Benamou et al., 2016) and the Wasserstein
non-negative matrix factorization (NMF) problem (Sandler
& Lindenbaum, 2009; Rolet et al., 2016).

lem with a strongly convex term such that the regularized
objective becomes a smooth function of all its coordinating parameters. Neither the Sinkhorn’s algorithm nor Bregman ADMM can be readily integrated into a general WLM.
Based on the entropic regularization of primal OT, Cuturi &
Peyré (2016) recently showed that the Legendre transform
of the entropy regularized Wasserstein loss and its gradient can be computed in closed form, which appear in the
first-order condition of some complex WLM problems. Using this technique, the regularized primal problem can be
converted to an equivalent Fenchel-type dual problem that
has a faster numerical solver in the Euclidean space (Rolet et al., 2016). But this methodology can only be applied
to a certain class of WLM problems of which the Fencheltype dual has closed forms of objective and full gradient.
In contrast, the proposed SA-based approach directly deals
with the dual OT problem without assuming any particular
mathematical structure of the WLM problem, and hence is
more flexible to apply.
More recent approaches base on solving the dual OT problems have been proposed to calculate and optimize the
Wasserstein distance between a single pair of distributions
with very large support sets — often as large as the size
of an entire machine learning dataset (Montavon et al.,
2016; Genevay et al., 2016; Arjovsky et al., 2017). For
these methods, scalability is achieved in terms of the support size. Our proposed method has a different focus on
calculating and optimizing Wasserstein distances between
many pairs all together in WLMs, with each distribution
having a moderate support size (e.g., dozens or hundreds).
We aim at scalability for the scenarios when a large set of
distributions have to be handled simultaneously, that is, the
optimization cannot be decoupled on the distributions. In
addition, existing methods have no on-the-fly mechanism
to control the approximation quality at a limited number of
iterations.

3. Preliminaries of Optimal Transport
In this section, we present notations, mathematical backgrounds, and set up the problem of interest.
Definition 3.1 (Optimal Transportation, OT). Let p 2
m1 , q 2
m2 , where
m is the set of m-dimensional
def.
m
simplex: m = {q 2 R+ : hq, i = 1}. The set of trans-

def.

2. Related Work
Recently, several methods have been proposed to overcome
the aforementioned difficulties in solving WLMs. Representatives include entropic regularization (Cuturi, 2013;
Cuturi & Doucet, 2014; Benamou et al., 2015) and Bregman ADMM (Wang & Banerjee, 2014; Ye et al., 2017b).
The main idea is to augment the original optimization prob-

portation plans between p and q is defined as ⇧(p, q) =
{Z 2 Rm1 ⇥m2 : Z · m2 = p; Z T · m1 = q; }. Let
1 ⇥m2
M 2 Rm
be the matrix of costs. The optimal trans+
port cost between p and q with respect to M is
def.

W (p, q) =

min hZ, M i .

Z2⇧(p,q)

In particular, ⇧(·, ·) is often called the coupling set.

(1)

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Now we relate primal version of (discrete) OT to a variant of its dual version. One may refer to Villani (2003) for
the general background of the Kantorovich-Rubenstein duality. In particular, our formulation introduces an auxiliary
parameter CM for the sake of mathematical soundness in
defining Boltzmann distributions.
Definition 3.2 (Dual Formulation of OT). Let CM > 0, denote vector [g1 , . . . , gm1 ]T by g, and vector [h1 , . . . , hm2 ]T
by h. We define the dual domain of OT by
def.

⌦(M ) =

n

CM < gi

f = [g; h] 2 Rm1 +m2

o
hj  Mi,j , 1  i  m1 , 1  j  m2 .

(2)

Informally, for a sufficiently large CM (subject to p, q, M ),
the LP problem Eq. (1) can be reformulated as 1
W (p, q) = max hp, gi
f 2⌦(M )

hq, hi .

(3)

Let the optimum set be ⌦⇤ (M ). Then any optimal point
f ⇤ = (g⇤ , h⇤ ) 2 ⌦⇤ (M ) constructs a (projected) subgradient such that g⇤ 2 @W/@p and h⇤ 2 @W/@q . The main
computational difficulty of WLMs comes from the fact that
(projected) subgradient f ⇤ is not efficiently solvable.
Note that ⌦(M ) is an unbound set in Rm1 +m2 . In order
to constrain the feasible region to be bounded, we alternatively define
⌦0 (M ) = {f = [g; h] 2 ⌦(M ) | g1 = 0}.

(4)

One can show that the maximization in ⌦(M ) as Eq. (3)
is equivalent to the maximization in ⌦0 (M ) because
hp, m1 i = hq, m2 i.

4. Simulated Annealing for Optimal
Transport via Gibbs Sampling
Following the basic strategy outlined in the seminal paper of simulated annealing (Kirkpatrick et al., 1983), we
present the definition of Boltzmann distribution supported
on ⌦0 (M ) below which, as we will elaborate, links the
dual formulation of OT to a Gibbs sampling scheme (Algorithm 1 below).
Definition 4.1 (Boltzmann Distribution of OT). Given a
temperature parameter T > 0, the Boltzmann distribution
1
However, for any proper M and strictly positive p, q, there
exists CM such that the optimal value of primal problem is equal
to the optimal value of the dual problem. This modification is
solely for an ad-hoc treatment of a single OT problem. In general
cases of (p, q, M ), when CM is pre-fixed, the solution of Eq. (3)
may be suboptimal.

of OT is a probability measure on ⌦0 (M ) ✓ Rm1 +m2 1
such that

1
p(f ; p, q) / exp
(hp, gi hq, hi) .
(5)
T
It is a well-defined probability measure for an arbitrary finite CM > 0.
The basic concept behind SA states that the samples from
the Boltzmann distribution will eventually concentrate at
the optimum set of its deriving problem (e.g. W (p, q)) as
T ! 0. However, since the Boltzmann distribution is often difficult to sample, a practical convergence rate remains
mostly unsettled for specific MCMC methods.
Because ⌦(M ) defined by Eq. (2) (also ⌦0 ) has a conditional independence structure among variables, a Gibbs
sampler can be naturally applied to the Boltzmann distribution defined by Eq. (5). We summarize this result below.
Proposition 4.1. Given any f = (g; h) 2 ⌦0 (M ) and any
CM > 0, we have for any i and j,
gi  Ui (h)
hj

Lj (g)

def.

=

def.

=

min (Mi,j + hj ) ,

(6)

max (gi

(7)

1jm2
1im1

Mi,j ) .

and
b i (h)
gi > L

bj (g)
hj < U

def.

=

def.

=

max ( CM + hj ) ,

(8)

max (CM + gi ) .

(9)

1jm2
1im1

Here Ui = Ui (h) and Lj = Lj (g) are auxiliary variables.
Suppose f follows the Boltzmann distribution by Eq. (5),
gi ’s are conditionally independent given h, and likewise
hj ’s are also conditionally independent given g. Furthermore, it is immediate from Eq. (5) that each of their conditional probabilities within its feasible region (subject to
CM ) satisfies
⇣g p ⌘
i i
b i (h) < gi  Ui (h),
p(gi |h) / exp
, L
(10)
T
✓
◆
hj q j
bj (g), (11)
p(hj |g) / exp
, Lj (g)  hj < U
T
where 2  i  m1 and 1  j  m2 .
bj (g) ! +1 and L
b i (h) !
Remark 1. As CM ! +1, U
1. For 2  i  m1 and 1  j  m2 , one can approximate the conditional probability p(gi |h) and p(hj |g)
by exponential distributions.

By Proposition. 4.1, our proposed time-inhomogeneous
Gibbs sampler is given in Algorithm 1. Specifically in Algorithm 1, the variable g1 is fixed to zero by the definition

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Algorithm 1 Gibbs Sampling for Optimal Transport
Given f
2 ⌦0 (M ), p 2 m1 and q 2 m2 , and
T (1) , . . . , T (2N ) > 0, for t = 1, . . . , N , we define the following Markov chain
(0)

1. Randomly sample
✓1 , . . . , ✓ m2

i.i.d.

⇠ Exponential(1).

For j = 1, 2, . . . , m2 , let
⇣
( (t)
(t
Lj := max1im1 gi
(t)

hj :=

(t)

Lj + ✓j · T (2t

1)
1)

Mi,j

/qj

approximate the Wasserstein gradient. In practice, we find
this bound helps one quickly select the beginning temperature of Gibbs-OT algorithm.
Definition 4.2 (Notations for Auxiliary Statistics). Besides
the Gibbs coordinates g and h, the Gibbs-OT sampler naturally introduces two auxiliary variables, U and L. Let
h
iT
h
iT
(t)
(t)
(t)
(t)
L(t) = L1 , . . . , Lm2 and U(t) = U1 , . . . , Um1 .
(t)

⌘

(12)

[. . . , z2t

2. Randomly sample
✓2 , . . . , ✓ m1

(t)

Ui

(t)

gi

:=

⇠ Exponential(1).

:=

⇣
⌘
(t)
min1jm2 Mi,j + hj
(t)

Ui

✓i · T (2t) /pi

def.

1



, z2t , z2t+1 , . . . , ] =
 (t)
 (t)  (t+1)
L
L
L
...,
,
,
,...
U(t 1)
U(t)
U(t)

(14)

for t = 1, . . . , N is also a Markov chain. They can be redefined equivalently by specifying the transition probabilities p(zn+1 |zn ) for n = 1, . . . , 2N , a.k.a., the conditional
p.d.f. p(U(t) |L(t) ) for t = 1, . . . , N and p(L(t+1) |U(t) )
for t = 1, . . . , N 1.

i.i.d.

For i = (1), 2, . . . , m1 , let
(

(t)

Likewise, denote the collection of gi and hj by vectors
g(t) and h(t) respectively. The following sequence of auxiliary statistics

(13)

of ⌦0 (M ). But we have found in experiments that by cal(t)
(t)
culating U1 and sampling g1 in Algorithm 1 according
to Eq. (13), one can still generate MCMC samples from
⌦(M ) such that the energy quantity hp, gi hq, hi converges to the same distribution as that of MCMC samples
from ⌦0 (M ). Therefore, we will not assume g1 = 0 from
now on and develop analysis solely for the unconstrained
version of Gibbs-OT.
Figure 1 illustrates the behavior of the proposed Gibbs sampler with a cooling schedule at different temperatures. As
T decreases along iterations, the 95% percentile band for
sample f becomes thinner and thinner.
Remark 2. Algorithm 1 does not specify the actual cooling schedule, nor does the analysis of the proposed Gibbs
sampler in Theorem A.2. We have been agnostic here for a
reason. In the SA literature, cooling schedules with guaranteed optimality are often too slow to be useful in practice.
To our knowledge, the guaranteed rate of SA approach is
worse than the combinatorial solver for OT. As a result, a
well-accepted practice of SA for many complicated optimization problems is to empirically adjust cooling schedules, a strategy we take for our experiments.
Remark 3. Although the exact cooling schedule is not specified, we still provide a quantitative upper bound of the chosen temperature T at different iterations in Appendix A
Eq. (24). One can calculate such bound at the cost of
m log m at certain iterations to check whether the current
temperature is too high for the used Gibbs-OT to accurately

One may notice that the alternative representation converts
the Gibbs sampler to one whose structure is similar to a
hidden Markov model, where the g, h chain is conditional
independent given the U, L chain and has (factored) exponential emission distributions. We will use this equivalent
representation in Appendix A and develop analysis based
on the U, L chain accordingly.
Remark 4. We now consider the function
def.

V (x, y) = hp, xi

hq, yi ,

0

and define a few additional notations. Let V (Ut , Lt ) be
0
denoted by V (zt+t ), where t0 = t or t 1. If g, h are
independently resampled according to Eq. (12) and (13),
we will have the inequalities that
E [V (g, h)|zn ]  V (zn ) .

Both V (z) and V (g, h) converges to the exact loss
W (p, q) at the equilibrium of Boltzmann distribution
p(f ; p, q) as T ! 0. 2

5. Gibbs-OT: An Inexact Oracle for WLMs
In this section, we introduce a non-standard SA approach
for the general WLM problems. The main idea is to replace
the standard Boltzmann energy with an asymptotic consistent upper bound, outlined in our previous section. Let
R(✓) :=

|D|
X

W (pi (✓), qi (✓))

i=1

2

The conditional quantity V (zn ) V (g, h)|zn is the sum
of two Gamma random variables: Gamma(m1 , 1/T (2t) ) +
0
Gamma(m2 , 1/T (2t +1) ) where t0 = t or t0 = t 1.

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

2.5

2
U
L
(1)
f
f(2)

2
1.5

2
U
L
(1)
f
f(2)

1.5
1

1

1

0.5

0.5

U
L
(1)
f
f(2)

1.5

0.5

0

0

-0.5

-0.5

0
-0.5
-1
-1.5

-1

-1

-1.5

-1.5

-2

-2

-2

-2.5

-2.5

-2.5

0

0.2

0.4

0.6

0.8

1

0

0.2

(a) 20 iterations

0.4

0.6

0.8

1

0

0.2

(b) 40 iterations

0.4

0.6

0.8

1

(c) 60 iterations

Figure 1. The Gibbs sampling of the proposed SA method. From left to right is an illustrative example of a simple 1D optimal transportation problem with Coulomb cost and plots of variables for solving this problem at different number of iterations 2 {20, 40, 60}
using the inhomogeneous Gibbs sampler. Particularly, the 95% percentile of the exponential distributions are marked by the gray area.

be our prototyped objective function, where D represents a
dataset, pi , qi are prototyped probability densities for representing the i-th instance. We now discuss how to solve
min✓2⇥ R(✓).
To minimize the Wasserstein losses W (p, q) approximately in such WLMs, we propose to instead optimize
its asymptotic consistent upper bound E[V (z)] at equilibrium of Boltzmann distribution p(f ; p, q) using its stochastic gradients: U 2 @V (z)/@p and L 2 @V (z)/@q .
Therefore, one can calculate the gradient approximately:
r✓ R ⇡

|D|
X

[J✓ (pi (✓))Ui

J✓ (qi (✓))Li ]

i=1

where J✓ (·) is the Jacobian, Ui , Li are computed from
Algorithm 1 for the problem W (pi , qi ) respectively. Together with the iterative updates of model parameters ✓,
one gradually anneals the temperature T . The equilibrium
of p(f ; p, q) becomes more and more concentrated. We assume the inexact oracle at a relatively higher temperature
is adequate for early updates of the model parameters, but
sooner or later it becomes necessary to set T smaller to better approximate the exact loss.
It is well known that the variance of stochastic gradient
usually affects the rate of convergence. The reason to replace V (g, h) with V (z) as the inexact oracle (for some
T > 0) is motivated by the same intuition. The variances
(t)
(t)
of MCMC samples gi , hj of Algorithm 1 can be very
large if pi /T and qj /T are small, making the embedded
first-order method inaccurate unavoidably. But we find the
(t)
(t)
variances of max/min statistics Ui , Lj are much smaller.
Fig. 1 shows an example. The bias introduced in the replacement is also well controlled by decreasing the temperature parameter T . For the sake of efficiency, we use
a very simple convergence diagnostics in the practice of
Gibbs-OT. We check the values of V (z(2t) ) such that the

Markov chain is roughly considered mixed if every ⌧ iteration the quantity V (z(2t) ) (almost) stops increasing (⌧=5
by default), say, for some t,
V (z(2t) )

V (z(2(t

⌧ ))

) < 0.01⌧ T · V (z(2t) ),

we terminate the Gibbs iterations.

6. Applications of Gibbs-OT
6.1. Toy OT Examples
1D Case with Euclidean Cost. We first illustrate the differences between the approximate primal solutions computed by different methods by replicating a toy example
in (Benamou et al., 2015). The toy example calculates the
OT between two 1D two-mode distributions. We visualize
their solved coupling as a 2D image in Fig. 2 at the budgets
in terms of different number of iterations. Given their different convergence behaviors, when one wants to compromise with using pre-converged primal solutions in WLMs,
he or she has to account for the different results computed
by different numerical methods, even though they all aim
at the Wasserstein loss.
As a note, Sinkhorn, B-ADMM and Gibbs-OT share the
same computational complexity per iteration. The difference in their actual CPU time comes from the different
arithmetic operations used. B-ADMM may be the slowest
because it requires log() and exp() operations. When
memory efficiency is of concern, both the implementations of Sinkhorn and Gibbs-OT can be modified to take
only O(m1 + m2 ) additional memory besides the space for
caching the cost matrix M .
Two Electrons with Coulomb Cost in DFT. In quantum
mechanics, Coulomb cost (or electron-electron Coulomb
repulsion) is an important energy functional in Density
Functional Theory (DFT). Numerical methods that solve

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

l=1

0.5/N ). For Gibbs-OT, we use a geometric temperature
scheme such that T = 2.0(1/l4 )n/l /N at the n-th iteration,
where l is the max iteration number. For the unbounded
Coulomb cost, Bregman ADMM (Wang & Banerjee, 2014)
does not converge to a solution close to the true optimum.

l=10

6.2. Wasserstein NMF

l=50

We now illustrate how the proposed Gibbs-OT can be used
as a ready-to-plugin inexact oracle for a typical WLM —
Wasserstein NMF (Sandler & Lindenbaum, 2009; Rolet
et al., 2016). The data parallelization of this framework is
natural because the Gibbs-OT samplers subject to different
instances are independent.

l=200
l=1000
l=5000
IBP, rho=0.1/N

IBP, rho=0.5/N

IBP, rho=2.0/N

B-ADMM

SimulAnn

Figure 2. A simple example for OT between two 1D distribution: The solutions by Iterative Bregman Projection, B-ADMM,
and Gibbs-OT are shown in pink, while the exact solution
by linear programming is shown in green. Images in the
rows from top to bottom present results at different iterations
{1, 10, 50, 200, 1000, 5000}; The left three columns are by IBP
with " = {0.1/N, 0.5/N, 2/N }, where [0, 1] is discretized with
N = 128 uniformly spaced points. The fourth column is by BADMM (with default parameter ⌧0 = 2.0). The last column is
the proposed Gibbs-OT, with a geometric cooling schedule. With
a properly selected cooling schedule, one can achieve fast convergence of OT solution without comprising much solution quality.

the multi-marginal OT problem with unbounded costs remains an open challenge in DFT (Benamou et al., 2016).
We consider two uniform densities on 1D domain [0, 1]
with Coulomb cost c(x, y) = 1/|x y| which has analytic
solutions. Coulumb cost is different from the usual metric
cost in the OT literature, which is unbounded and singular at x = y. As observed in (Benamou et al., 2016), the
entropic regularized primal solution becomes more concentrated at boundaries, which is not physically plausible. This
effect is not observed in the Gibbs-OT solution as shown in
Appendix Fig. 3. As shown by Fig 1, the variables U, V
in computation are always in bounded range (with an overwhelming probability), thus the algorithm does not endure
any numerical difficulties.
For entropic regularization (Benamou et al., 2015; 2016),
we empirically select the minimal " which does not cause
numerical overflow before 5000 iterations (in which " =

Problem Formulation. Given a set of discrete probability measures { i }ni=1 (data) over Rd , we want to estimate a model ⇥ = { k }K
i,
k=1 , such that for each
there exists a membership vector (i) 2
K:
i ⇡
PK
(i)
k , where each
k is again a discrete probk=1 k
ability measure to be estimated.
Wasserstein
⇣ Therefore,
⌘
Pn
PK
(i)
NMF reads min⇥,⌅ i=1 W
, where
i,
k
k=1 k
⌅ = ( (1) , . . . , (n) ) is the collection of membership vectors, and W is the Wasserstein distance. One can write the
problem by plugging Eq. (3) in the dual formulation:
i
Xn h
b (i) , gi i hw(i) , hi i (15)
min maxn
hw
i=1

⇥,⌅ F ={fi }i=1

s.t.

k

=

Xm

b (i) =

i=1
XK

⇣

(k)

vi

k=1

xi

(i)
k

fi 2 ⌦ M ( b (i) ,

k

i)

(16)

,

⌘

,

(17)

,

(18)

b (i) 2 m is the weight vector of discrete probwhere w
ability measure b (i) and w(i) 2 mi is the weight vector of (i) . M (·, ·) denotes the transportation cost matrix
between the supports of two measures. The global optimization solves all three sets of variables (⇥, ⌅, F ). In the
m
sequel, we assume support points of { k }m
k=1 — {xi }i=1
are shared and pre-fixed.
Algorithm. At every epoch, one updates variables either
sequentially (indexed by i) or all together. It is done by
first executing the Gibbs-OT oracle subject to the i-th instance and then updating v(k) and the membership vector (i) accordingly at a chosen step size > 0. At the
end of each
parameter T is adjusted
q the temperature
⇣ epoch,
⌘
Pn
1
1
T := T 1
i=1 mi . For
m+m̄ , where m̄ = n
each instance i, the algorithm proceeds with the following
steps iteratively:
1. Initiate from the last computed U/V sample subject
to instance i, execute the Gibbs-OT Gibbs sampler at

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure 3. The recovered primal solutions for two uniform 1D distribution with Coulumb cost. The approximate solutions are shown in
pink, while the exact solution by linear programming is shown in green. Top row: entropic regularization with " = 0.5/N . Bottom row:
Gibbs-OT. Images in the rows from left to right present results at different max iterations {1, 10, 50, 200, 1000, 2000, 5000}.

constant temperature T until a mixing criterion is met,
and get Ui .
2. For k = 1, . . . , K, update v(k) 2 m based on gradi(i)
ent k Ui using the iterates of online mirror descent
(MD) subject to the step-size
(Beck & Teboulle,
2003).
3. Also update the membership vector (i) 2 K based
on gradient (hv(1) , Ui i, . . . , hv(K) , Ui i)T using the
iterates of accelerated mirror descent (AMD) with
restarts subject to the same step-size
(Krichene
et al., 2015).
We note that the practical speed-ups we achieved via the
above procedure is the warm-start feature in Step 1. If one
uses a black-box OT solver, this dimension of speed-ups is
not viable.
Results. We investigate the empirical convergence of the
proposed Wasserstein NMF method by two datasets: one
is a subset of MNIST handwritten digit images which contains 200 digits of “5”, and the other is the ORL 400-face
dataset. Our results are based on a C/C++ implementation
with vectorization. In particular, we set K = 40, = 2.0
for both datasets. The learned components are visualized together with alternative approaches (smoothed WNMF (Rolet et al., 2016) and regular NMF) in Appendix
Figs. 4 and 5. From these figures, we observe that our
learned components using Gibbs-OT are shaper than the
smoothed W-NMF. This can be explained by the fact that
Gibbs-OT can potentially push for higher quality of approximation by gradually annealing the temperature. We
also observe that the learned components might possess
some salt-and-pepper noise. This is because the Wasserstein distance by definition is not very sensitive to the subpixel displacements. On a single-core of a 3.3 GHz Intel

Core i5 CPU, the average time spent for each epoch for
these two datasets are 0.84 seconds and 16.8 seconds, respectively. It is about two magnitude faster than fully solving all OTs via a commercial LP solver 3 .

7. Discussions
The solution of primal OT (Monge-Kantorovich problem)
have many direct interpretations, where the solved transport is a coupling between two measures. Hence, it could
be well motivated to consider regularizing the solution on
the primal domain in those problems (Cuturi, 2013). Meanwhile, the solution of dual OT can be meaningful in its
own right. For instance, in finance, the dual solution is
directly interpreted as the vanilla prices implementing robust static super-hedging strategies. The entropy regularized OT, under the Fenchel-type dual, provides a smoothed
unconstrained dual problem as shown in (Cuturi & Peyré,
2016). In this paper, we develop Gibbs-OT, whose solutions respect the dual feasibility of OT and are subject to a
different regularization effect as explained by (Abernethy
& Hazan, 2015). It is a numerical stable and computational
suitable oracle to handle WLM.
Acknowledgement. This material is based upon work supported by the National Science Foundation under Grant
Nos. ECCS-1462230 and DMS-1521092. The authors
would also like to thank anonymous reviewers for their
valuable comments.
3

We use the specialized network flow solver in Mosek
(https://www.mosek.com) for the computation, which is
found faster than general simplex or IPM solver at moderate problem scale.

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

Figure 4. NMF components learned by different methods (K =
40) on the 200 digit “5” images. Top: regular NMF; Middle:
W-NMF with entropic regularization (" = 1/100, ⇢1 = ⇢2 =
1/200); Bottom: W-NMF using Gibbs-OT. It is observed that the
components of W-NMF with entropic regularization are smoother
than those optimized with Gibbs-OT.

Abernethy, Jacob and Hazan, Elad. Faster convex optimization: Simulated annealing with an efficient universal barrier. arXiv preprint arXiv:1507.02528, 2015.

Figure 5. NMF components learned by different methods (K =
40) on the ORL face images. Top: regular NMF; Middle: WNMF with entropic regularization (" = 1/100, ⇢1 = ⇢2 =
1/200); Bottom: W-NMF using Gibbs-OT, in which the salt and
pepper noises are observed due to the fact that Wasserstein distance is insensitive to the subpixel mass displacement (Cuturi &
Peyré, 2016).

Agueh, Martial and Carlier, Guillaume. Barycenters in the
Wasserstein space. SIAM J. Math. Analysis, 43(2):904–
924, 2011.

Wasserstein GAN. arXiv preprint arXiv:1701.07875,
2017.

Arjovsky, Martin, Chintala, Soumith, and Bottou, Léon.

Beck, Amir and Teboulle, Marc. Mirror descent and non-

References

A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization

linear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175,
2003.
Benamou, Jean-David, Carlier, Guillaume, Cuturi, Marco,
Nenna, Luca, and Peyré, Gabriel. Iterative Bregman projections for regularized transportation problems. SIAM J.
on Scientific Computing, 37(2):A1111–A1138, 2015.
Benamou, Jean-David, Carlier, Guillaume, and Nenna,
Luca. A numerical method to solve multi-marginal optimal transport problems with Coulomb cost. In Splitting
Methods in Communication, Imaging, Science, and Engineering, pp. 577–601. Springer, 2016.
Bonneel, Nicolas, Peyré, Gabriel, and Cuturi, Marco.
Wasserstein barycentric coordinates: Histogram regression using optimal transport. ACM Trans. on Graphics,
35(4), 2016.
Corana, Angelo, Marchesi, Michele, Martini, Claudio, and
Ridella, Sandro. Minimizing multimodal functions of
continuous variables with the simulated annealing algorithm corrigenda for this article is available here. ACM
Trans. on Mathematical Software, 13(3):262–280, 1987.
Cuturi, Marco. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, pp. 2292–2300, 2013.
Cuturi, Marco and Doucet, Arnaud. Fast computation of
Wasserstein barycenters. In Proc. Int. Conf. Machine
Learning, pp. 685–693, 2014.
Cuturi, Marco and Peyré, Gabriel. A smoothed dual approach for variational Wasserstein problems. SIAM J. on
Imaging Sciences, 9(1):320–343, 2016.

Kusner, Matt, Sun, Yu, Kolkin, Nicholas, and Weinberger,
Kilian. From word embeddings to document distances.
In Proc. of the Int. Conf. on Machine Learning, pp. 957–
966, 2015.
Li, Jia and Wang, James Z. Real-time computerized annotation of pictures. IEEE Trans. Pattern Analysis and
Machine Intelligence, 30(6):985–1002, 2008.
Monge, Gaspard. Mémoire sur la théorie des déblais et des
remblais. De l’Imprimerie Royale, 1781.
Montavon, Grégoire, Müller, Klaus-Robert, and Cuturi,
Marco. Wasserstein training of restricted boltzmann machines. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 3711–3719. 2016.
Orlin, James B. A faster strongly polynomial minimum
cost flow algorithm. Operations Research, 41(2):338–
350, 1993.
Rolet, Antoine, Cuturi, Marco, and Peyré, Gabriel. Fast
dictionary learning with a smoothed Wasserstein loss. In
AISTAT, 2016.
Sandler, Roman and Lindenbaum, Michael. Nonnegative
matrix factorization with earth mover’s distance metric.
In Proc. of the Conf. on Computer Vision and Pattern
Recognition, pp. 1873–1880. IEEE, 2009.
Seguy, Vivien and Cuturi, Marco. Principal geodesic analysis for probability measures under the optimal transport
metric. In Advances in Neural Information Processing
Systems, pp. 3294–3302, 2015.
Villani, Cédric. Topics in Optimal Transportation. Number 58. American Mathematical Soc., 2003.

Frogner, Charlie, Zhang, Chiyuan, Mobahi, Hossein,
Araya, Mauricio, and Poggio, Tomaso A. Learning with
a Wasserstein loss. In Advances in Neural Information
Processing Systems, pp. 2044–2052, 2015.

Wang, Huahua and Banerjee, Arindam. Bregman alternating direction method of multipliers. In Advances in
Neural Information Processing Systems, pp. 2816–2824,
2014.

Genevay, Aude, Cuturi, Marco, Peyré, Gabriel, and Bach,
Francis. Stochastic optimization for large-scale optimal
transport. In Lee, D. D., Sugiyama, M., Luxburg, U. V.,
Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 3440–3448. 2016.

Ye, Jianbo and Li, Jia. Scaling up discrete distribution clustering using admm. In Proc. of the Int. Conf. on Image
Processing, pp. 5267–5271. IEEE, 2014.

Kirkpatrick, Scott, Gelatt, C. Daniel, Jr, and Vecchi,
Mario P. Optimization by simmulated annealing. Science, 220(4598):671–680, 1983.

Ye, Jianbo, Li, Yanran, Wu, Zhaohui, Wang, James Z, Li,
Wenjie, and Li, Jia. Determining gains acquired from
word embedding quantitatively using discrete distribution clustering. In Proc. of the Annual Meeting of the
Association for Computational Linguistics, 2017a.

Krichene, Walid, Bayen, Alexandre, and Bartlett, Peter L.
Accelerated mirror descent in continuous and discrete
time. In Advances in Neural Information Processing Systems, pp. 2827–2835, 2015.

Ye, Jianbo, Wu, Panruo, Wang, James Z, and Li, Jia.
Fast discrete distribution clustering using Wasserstein
barycenter with sparse support. IEEE Trans. on Signal
Processing, 65(9):2317–2332, 2017b.

