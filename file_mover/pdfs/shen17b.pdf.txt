GSOS: Gauss-Seidel Operator Splitting Algorithm for
Multi-Term Nonsmooth Convex Composite Optimization

Li Shen 1 Wei Liu 1 Ganzhao Yuan 2 Shiqian Ma 3

Abstract

differentiable convex function with its gradient satisfying
the inequality that

In this paper, we propose a fast Gauss-Seidel
Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in
machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the
operator splitting technique to reduce the computational complexity. In addition, we develop
a new technique to establish the global convergence of the GSOS algorithm. To be specific, we
first reformulate the iterations of GSOS as a twostep iterations algorithm by employing the tool of
operator optimization theory. Subsequently, we
establish the convergence of GSOS based on the
two-step iterations algorithm reformulation. At
last, we apply the proposed GSOS algorithm to
solve overlapping group Lasso and graph-guided
fused Lasso problems. Numerical experiments
show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of
both efficiency and effectiveness.





1
‚àáf (x) ‚àí ‚àáf (y)2 ‚â§ ‚àáf (x) ‚àí ‚àáf (y), x ‚àí y . (2)
L

1. Introduction
In this paper, we focus on the multi-term nonsmooth convex composite optimization
min f (x) +

x‚ààX

n
X

gi (x),

(1)

i=1

where X is a linear space, gi : X ‚Üí (‚àí‚àû, +‚àû] is
a proper, lower semicontinuous convex function for all i = 1, ¬∑ ¬∑ ¬∑ , n, and f : X ‚Üí (‚àí‚àû, +‚àû) is a continuous
1
Tencent AI Lab, China 2 Sun Yat-sen University, China
The Chinese University of Hong Kong, China. Correspondence to: Li Shen <mathshenli@gmail.com>, Wei Liu <wliu@ee.columbia.edu>.
3

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications
in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009;
Mairal et al., 2010), graph-guided fused Lasso (Chen et al.,
2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; DupeÃÅ
et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007). By
introducing
the multi-term nonsmooth regularization term
Pn
g
(x)
such as structured sparsity (Huang et al., 2011;
i
i=1
Bach et al., 2012; Bach, 2010) and nonnegativity (Chen &
Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models. However,
Pn due to the multi-term nonsmooth
regularization term i=1 gi (x), the optimization problem
(1) is too complicated to be solved even for small n. For
n ‚â§ 2, some existing popular first-order optimization
methods are accelerated proximal gradient method (Beck
& Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three
operator splitting method (Davis & Yin, 2015), and some
primal-dual operator splitting methods such as majorized
alternating direction method of multiplier (ADMM) (Cui
et al., 2016; Lin et al., 2011), fast proximity method (Li &
Zhang, 2016), and so on.
On the other hand, when n ‚â• 3, there also exist some algorithms for solving problem (1). A directly method for
(1) is smoothing accelerated proximal gradient (S-APG)
proposed by Nesterov (Nesterov, 2005a;b). Then, Yu (Yu,
2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average
approximation technique and Nesterov‚Äôs acceleration technique, which has been enhanced very recently by Shen
et al. (Shen et al., 2017). Their proposed method called
APA-APG adopts an adaptive stepsize strategy. However,

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

the above mentioned methods S-APG, PA-APG and its enhanced version APA-APG all need a strict restriction on the
nonsmooth functions {gi (x)} that each gi (x) must be Lipschitz continuous. In addition, some primal-dual parallel
splitting methods (Briceno-Arias et al., 2011; Combettes
& Pesquet, 2007; 2008; Condat, 2013; VuÃÉ, 2013) generalized from traditional operator splitting, such as forward
backward splitting method (Chen & Rockafellar, 1997) and
Douglas Rachford splitting method (Eckstein & Bertsekas,
1992), can also solve the multi-term nonsmooth convex
composite optimization problem (1). Different from prior work, Raguet et al. (Raguet et al., 2013) proposed an
efficient primal operator splitting method called generalized forward backward splitting method using the classic
forward backward splitting technique, which has shown
the superiority over numerous existing primal-dual splitting
methods (Monteiro & Svaiter, 2013; Combettes & Pesquet,
2012; Chambolle & Pock, 2011) in dealing with variational image restoration problems. All the above mentioned
methods for problem (1) with n ‚â• 3 share a common
feature
that they all split the nonsmooth composite term
Pn
i=1 gi (x) in the Jacobi iteration manner, i.e., parallelly.
This is one of the main differences between existing splitting methods and our proposed method in this paper.
Pn
To split the nonsmooth composite term i=1 gi (x) more
efficiently, we propose a novel operator splitting algorithm
to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal
mapping of the current function gi (x) uses the proximal
mappings of gj (x) for all j < i which have already
been computed ahead. In addition, to further improve the
algorithm‚Äôs efficiency, we leverage the over-relaxation
acceleration technique. What‚Äôs more, we provide a new
strategy that the over-relaxation stepsize can be determined
adaptively, ensuring a larger value to accelerate the algorithm. The most important is that the convergence of our
proposed GSOS algorithm is established by a newly developed analysis technique. In detail, given an invertible linear
operator R, we first argue that the optimal solution set
Pn
‚àí1
[‚àáf + i=1 ‚àÇgi ] (0) of problem (1) can be recovered

‚àí1
by the zero point set (R‚àó )‚àí1 SR, ‚àÇg+A‚ó¶‚àáf ‚ó¶A, NV
(0).
This is fulfilled through adopting the tool of operator
optimization theory, in which the composite operator
SR, ‚àÇg+A‚ó¶‚àáf ‚ó¶A, NV is generalized from the definition of
the composite monotone operator SŒª,A,B in (Eckstein
& Bertsekas, 1992). Next, by unitizing the definition of
the -enlargement of maximal monotone (Burachik et al.,
1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000),
we establish a key property for SR, ‚àÇg+A‚ó¶‚àáf
 ‚ó¶A, NV ,
that
is,
gph SR, (‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A)[] , NV
‚äÜ

gph R‚àó [(R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ][] .
Based on
this observation, we equivalently reformulate the GSOS
algorithm as a two-step iterations algorithm. Then, the

global convergence of the proposed GSOS algorithm is
easily established based on this reformulation.
The closest algorithm to our proposed GSOS algorithm
is the generalized forward backward splitting method proposed by Raguet et al. (Raguet et al., 2013). By carefully
selecting the scaling matrix H in the forthcoming GSOS
algorithm, it is easy to check that GSOS covers the generalized forward backward splitting method as a special case.
Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method (Luo & Tseng, 1991;
Yuan et al., 2016). Choosing the scaling matrix H suitably,
the proposed GSOS algorithm can inherit the advantage of
the matrix splitting technique which has shown the efficiency in (Yuan et al., 2016) for coping with a special class of
coordinate separable composite optimization problems.
The rest of this paper is organized as follows. In Section 2,
we first give the definitions of some useful notations which
can make the paper much more readable. We also establish
some lemmas and propositions based on monotone operator theory (Bauschke & Combettes, 2011), which are the
key to the convergence of the GSOS algorithm. In Section
3, we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity. In Section
4, we conduct numerical experiments on overlapping group
Lasso and graph-guided fused Lasso problems to evaluate
the efficacy of the GSOS algorithm. Finally, we draw conclusions in Section 5.

2. Preliminaries and Notations
Qn
Let Y = i=1 Xi be the product space of Xi with Xi = X
for all i ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , n}. Let V be a linear space and V ‚ä•
be its complementary space with the following definitions
n
X
	

	

yi = 0 .
V = y ‚àà Y | y1 = ¬∑ ¬∑ ¬∑ = yn , V ‚ä• = y ‚àà Y |
i

Let IX : X ‚Üí X be the identity map and EY :
X ‚Üí Y be a block
‚àó linear operator defined as EY =
IX ¬∑ ¬∑ ¬∑ IX
. Let A : Y ‚Üí X be a linear operPn
ator defined as Ay = n1 EY‚àó y = n1 i=1 yi . Hence, its
adjoint operator A‚àó : X ‚Üí Y is defined as A‚àó x = n1 EY x.
Let H, R : Y ‚Üí Y be block lower triangular linear invertible operators satisfying (R‚àó )‚àí1 = H and H + H‚àó  0.
Moreover, H is defined as
H1,1
..
Ô£¨
Ô£¨
.
Ô£≠
Hn‚àí1,1
Hn,1
Ô£´

0
..
.
¬∑¬∑¬∑
¬∑¬∑¬∑

¬∑¬∑¬∑
..
.
Hn‚àí1,n‚àí1
Hn‚àí1,n

0
..
.
0
Hn,n

Ô£∂
Ô£∑
Ô£∑,
Ô£∏

(3)

where Hi,j : X ‚Üí X is a linear operator for all (i, j) ‚àà
{1, ¬∑ ¬∑ ¬∑ , n}. It is worthwhile to emphasize that Hi,i is also
possible to be a lower triangular linear operator satisfying

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization
‚àó
 0. Next, we abuse the notation k ¬∑ kH which
Hi,i + Hi,i
is induced by the inner product h¬∑, H¬∑i satisfying
p
p
k ¬∑ kH : = h¬∑, H¬∑i = h¬∑, H‚àó ¬∑i
r
H + H‚àó
(4)
= h¬∑,
¬∑i = k ¬∑ k H+H‚àó .
2
2

In addition, we define the generalized proximal mapping of
a proper, lower semicontinuous convex function gi (x) with
respect to the invertible linear operator Hi,i .
Definition 1 For a given x, the proximal mapping denoted
by ProxH‚àí1 gi (x) of a proper, lower semicontinuous coni,i
vex function gi with respect to an invertible linear operator
‚àó
 0 is defined to be the zero
Hi,i satisfying Hi,i + Hi,i
point of the following inclusion equation
0 ‚àà ‚àÇgi (¬∑) + Hi,i (¬∑ ‚àí x).

(5)

Moreover, if Hi,i is symmetric, it can be reformulated as
the following convex minimization
1
ProxHi,i gi (x) := arg min gi (y) + ky ‚àí xk2Hi,i .
y‚ààX
2
Next, we recall the definition of -enlargement of monotone
operators (Burachik et al., 1998; 1997; Burachik & Svaiter,
1999; Svaiter, 2000), which is an effective tool for establishing the convergence of the proposed GSOS algorithm.
Definition 2 Given a maximal monotone operator T :
X ‚áí X, the (‚â• 0)-enlargement of T is defined as the
set T [] (x) :=
	 v ‚àà Y | hw ‚àí v, z ‚àí xi ‚â• ‚àí for all z ‚àà
X, w ‚àà T (z) .
Recall that f (x) is a gradient Lipschitz convex function satb  LI such
isfying inequality (2). There exits 0  Œ£  Œ£
that the following two inequalities hold for any x, x0 ‚àà X
1
f (x) ‚â§ f (x0 ) + h‚àáf (x0 ), x ‚àí x0 i + kx ‚àí x0 k2Œ£
b,
2
1
f (x) ‚â• f (x0 ) + h‚àáf (x0 ), x ‚àí x0 i + kx ‚àí x0 k2Œ£ .
2

(6)
(7)

b
Actually, when f (x) is a quadratic function, it holds Œ£ = Œ£
directly in inequalities (6) and (7). The following lemma
establishes the property of the enlargement of the composite operator A‚àó ‚ó¶ ‚àáf ‚ó¶ A with f satisfying inequalities (6)(7) or (2), which is an essential ingredient for reformulating
the GSOS algorithm as a two-step iterations algorithm.
Proposition 1 Assume that f is a gradient Lipschitz continuous convex function satisfying inequality (2). For any
x1 , x2 ‚àà Y, it holds that
(A‚àó ‚ó¶ ‚àáf ‚ó¶ A)(x2 ) ‚àà (A‚àó ‚ó¶ ‚àáf ‚ó¶ A)[] (x1 )

(8)

with  = L4 kAx1 ‚àíAx2 k2 . In addition, if f further satisfies
inequalities (6)-(7), it holds that
(A‚àó ‚ó¶ ‚àáf ‚ó¶ A)(x2 ) ‚àà (A‚àó ‚ó¶ ‚àáf ‚ó¶ A)[] (x1 )

(9)

with  = 14 kAx1 ‚àí Ax2 k22Œ£‚àíŒ£
.
b
Remark 1 Two comments are made for Proposition 1:
(1) This proposition gives two types of estimations for  in
[]
A‚àó ‚ó¶ ‚àáf ‚ó¶ A
in (8) and (9). When f is a quadratic
function, it is easy to check that
L
1
kAx1 ‚àí Ax2 k22Œ£‚àíŒ£
‚â§ kAx1 ‚àí Ax2 k2
b
4
4
b = Œ£  LI. When f is a general gradient
due to Œ£
Lipschitz continuous function, we do not know which
estimation for  is tighter in (8) and (9).
(2) The second part of this proposition can be regarded as
an intensified version of Lemma 2.2 in (Svaiter, 2014)
for a specified composite operator A‚àó ‚ó¶ ‚àáf ‚ó¶ A. The
first part of the proposition coincides with the results
by applying Lemma 2.2 in (Svaiter, 2014) for A‚àó ‚ó¶
‚àáf ‚ó¶ A.
Next, we generalize the notation SŒª,T1 ,T2 in (Eckstein &
Bertsekas, 1992) for a given Œª > 0 and two maximal monotone operators T1 , T2 as SR,T1 ,T2 for a given invertible linear operator R defined as
gph SR, T1 , T2
n
:= (x1 + Ry2 , x2 ‚àí x1 ) | y1 ‚àà T1 (x1 ),

(10)

o
y2 ‚àà T2 (x2 ), x1 + R‚àó y1 = x2 ‚àí R‚àó y2 .
By (Eckstein & Bertsekas, 1992), we know that SŒª,T1 ,T2 is
maximal monotone if T1 and T2 are both maximal monotone. However, its generalized operator SR,T1 ,T2 is not
monotone unless the invertible linear operator R reduces
to be a constant. Very interesting, it can be shown that its
composition with (R‚àó )‚àí1 , i.e., (R‚àó )‚àí1 SR,T1 ,T2 , is maximal monotone for any invertible linear operator R.
Lemma 1 For any given invertible linear operator R, operator (R‚àó )‚àí1 SR,T1 ,T2 is maximal monotone if T1 and T2
are both maximal monotone operators.
Setting T1 = ‚àÇg + A‚àó ‚ó¶ ‚àáf ‚ó¶ A, T2 = NV , we obtain
SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV , which is defined as

gph SR,‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV
(11)
n
:= (x1 +Ry2 , x2 ‚àíx1 ) | y1 ‚àà (‚àÇg + A‚àó ‚ó¶ ‚àáf ‚ó¶ A)(x1 ),
o
y2 ‚àà NV (x2 ), x1 + R‚àó y1 = x2 ‚àí R‚àó y2 .

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

By Lemma 1, we know that (R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV
is maximal monotone due to the maximal monotonicity
of ‚àÇg + A‚àó ‚ó¶ ‚àáf ‚ó¶ A and NV . Hence, given a constant  ‚â• 0, the enlargement [(R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ][]
is well defined. In addition, based on the definition of
SR,T1 ,T2 again, we set T1 = ‚àÇg + (A‚àó ‚ó¶ ‚àáf ‚ó¶ A)[] , or
T1 = (‚àÇg + A‚àó ‚ó¶ ‚àáf ‚ó¶ A)[] and T2 = NV in (10).
Then we have the definition of SR,‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A)[] ,NV or
SR,(‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A)[] ,NV for any given invertible linear operator R and constant  ‚â• 0 as follows

gph SR,‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A)[] ,NV
(12)
n
:= (x1 +Ry2 , x2 ‚àíx1 )|y1 ‚àà (‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A)[] )(x1 ),
o
y2 ‚àà NV (x2 ), x1 + R‚àó y1 = x2 ‚àí R‚àó y2 ,

gph SR,(‚àÇg+A‚àó‚ó¶‚àáf‚ó¶A)[] ,NV
(13)
n
:= (x1 +Ry2 , x2 ‚àíx1 )|y1 ‚àà (‚àÇg + A‚àó ‚ó¶‚àáf ‚ó¶A)[] )(x1 ),
o
y2 ‚àà NV (x2 ), x1 + R‚àó y1 = x2 ‚àí R‚àó y2 .
In the proposition below, we will establish the relationships among the above mentioned three operators SR,‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A)[] ,NV , SR,(‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A)[] ,NV and
[(R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ][] .
Proposition 2 Given a constant  ‚â• 0 and an invertible
linear operator R, it holds that

gph SR, ‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A)[] , NV

‚äÜ gph SR, (‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A)[] , NV

‚äÜ gph R‚àó [(R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ][] .
In the following, we establish the relationship between
Pn
‚àí1
the optimal solution set [‚àáf + i=1 ‚àÇgi ] (0) of prob ‚àó ‚àí1
‚àí1
lem (1) and (R ) SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV
(0), which
means that we can recover the solution of problem (1)

‚àí1
through (R‚àó )‚àí1 SR, ‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV
(0).
Lemma 2 Let linear operators H and R satisfy (R‚àó )‚àí1 = H and H satisfy (3).
Denote

‚àí1
(0). It holds
‚Ñ¶ = (R‚àó )‚àí1 SR, (‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A), NV
that
"
#‚àí1
n
X
‚àí1 T ‚àó 
‚àáf +
‚àÇgi
(0) = EYT H‚àó EY
EY H ‚Ñ¶ .
i=1

3. GSOS Algorithm
In this section, we first propose the Gauss-Seidel operator
splitting algorithm for solving the multi-term nonsmooth
convex composite problem (1). Then, based on the preliminaries in Section 2, we establish the convergence and
iteration complexity of the GSOS algorithm.

Algorithm 1 GSOS Algorithm
Parameters: Choose œÉ ‚àà (0, 1), a linear operator H
satisfying(3) and a starting point z 0 ‚àà Z. Set Œ∏fix1 ‚àà
‚àí 1, Œ∏1 and Œ∏fix2 ‚àà ‚àí 1, Œ∏2 , where Œ∏1 and Œ∏2 are
defined via equations (14a) and (14b), respectively.
for k = 0, 1, 2, ¬∑ ¬∑ ¬∑ , K do
‚àí1 T
xk := EY EYT HEY
EY Hz k ;
for i = 1, 2 ¬∑ ¬∑ ¬∑ , n do
‚àí1 Pi
[ j=1 Hi,j (2xkj ‚àí zjk ) ‚àí
yik := ProxH‚àí1 gi Hi,i
i,i

Pi‚àí1
Pn
1
1
k
k
]
;
)
‚àí
H
y
‚àáf
(
x
i,j
j
i
j=1
i=1
n
n
end for
set Œ∏kadap1 as (14c) and Œ∏kadap2 as (14d);
set Œ∏k ‚àà [Œ∏fix1 , Œ∏kadap1 ] ‚à™ [Œ∏fix2 , Œ∏kadap2 ];
z k+1 := z k + (1 + Œ∏k )(y k ‚àí xk );
end for
‚àí1 T ‚àó K
return œâ K := EYT H‚àó EY
EY H z .

In Algorithm 1, parameters Œ∏1 , Œ∏1 , Œ∏kadap1 , Œ∏kadap1 are defined as
n
o
Ô£±
Ô£¥
Œ∏1 = max Œ∏|(Œ∏ ‚àí œÉ)(H + H‚àó ) + LA‚àó A  0 ; (14a)
Ô£¥
Ô£¥
Ô£¥
n
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Œ∏
=
max
Œ∏ | (Œ∏ ‚àí œÉ)(H + H‚àó )
(14b)
2
Ô£¥
Ô£¥
Ô£¥
Ô£¥
o
Ô£¥
Ô£¥
b ‚àí Œ£)A  0 ;
Ô£≤
+A‚àó (2Œ£
Ô£¥
Ô£¥
LkA(xk ‚àí y k )k2
Ô£¥
Ô£¥
Œ∏kadap1 = œÉ ‚àí
;
Ô£¥
Ô£¥
kxk ‚àí y k k2H+H‚àó
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
kA(xk ‚àí y k )k22Œ£‚àíŒ£
Ô£¥
b
Ô£¥
Ô£≥ Œ∏kadap2 = œÉ ‚àí
.
kxk ‚àí y k k2H+H‚àó

(14c)
(14d)

Remark 2 We make some comments on GSOS below.
(1) For the updating step of xk , we obtain xk =
‚àí1 PK PK
PK
k
EY
i,j=1 Hij
j=1
i=j Hij zj by using the
notations H and EY . Similarly, we have œâ k =
‚àí1 PK Pj
PK
H‚àó z k . Hence, we need
i=i
i,j=1 Hij
j=1
Pn ji j
to compute the inverse of i,j=1 Hi,j . However, if
Hi,j is a lower triangular matrix operator, xk and œâ k
can be obtained easily.
(2) By the definitions of ProxH‚àí1 gi and y k , we need to
i,i
solve the following inclusion equation
Gki ‚àà Hi,i yik + ‚àÇgi (yik ),

‚àí1 Pi
k
k
where Gki = Hi,i
j=1 Hi,j (2xj ‚àí zj ) ‚àí

P
P
n
i‚àí1
1
1
k
k
i=1 xi ) ‚àí
j=1 Hi,j yj . Usually, it is
n ‚àáf ( n
easy to choose a suitable Hi,i such that the solution
of the above inclusion equation has a closed form.

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

(3) Œ∏k is the over-relaxation stepsize for accelerating the
GSOS algorithm. If the computations of Œ∏kadap1
and Œ∏kadap2 are time consuming, we can set Œ∏k =
max{Œ∏fix1 , Œ∏fix2 }.

Theorem 1 indicates that kxk ‚àí y k k approaching to zero
implies the convergence of the GSOS algorithm. In the
theorem below, we measure the convergence rates of two
sequences kxk ‚àí y k k and kœâ k ‚àí œâ k+1 k.

(4) When H is a diagonal matrix, i.e., Hi,j = 0 and Hi,i =
ai I with some nonnegative constant ai , and the over
relaxation stepsize Œ∏k is fixed to a smaller region, the
GSOS algorithm reduces to the generalized forward
backward splitting method in (Raguet et al., 2013).

Theorem 2 Let z k be the sequence generated by the GSOS
algorithm. Then, there exists i ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , k} such that

In the following, we reformulate the GSOS algorithm as
a two-step iterations algorithm by utilizing monotone optimization theory established in Section 2, which is the key
to the convergence of the GSOS algorithm.
Proposition 3 Let g : YP‚Üí (‚àí‚àû, +‚àû] be the funcn
tion defined as g(x) =
i=1 gi (xi ). Assume that the
k k
k
sequences (x , y ) and z are generated by Algorithm
1 with œÉ ‚àà (0, 1). Let v k = (R‚àó )‚àí1 (xk ‚àí y k ) and
z k = y k + R(R‚àó )‚àí1 (z k ‚àí xk ). Then, for all k ‚àà N,
there exists k ‚â• 0 such that the iterations in Algorithm
1 can be reformulated as the following two-step iterations
algorithm:
Ô£± k
‚àó ‚àí1
[k ] k
(15a)
Ô£¥
Ô£≤ v ‚àà [(R ) SR, ‚àÇg+(A‚àó ‚ó¶‚àáf ‚ó¶A), NV ] (z ),
Œ∏k kR‚àó v k k2R‚àí1

Ô£¥
Ô£≥

+

‚àó k

k

kR v + z ‚àí z k k2R‚àí1
+2k ‚â§ œÉkz k ‚àí z k k2R‚àí1 ,

(15b)

and z k+1 = z k ‚àí (1 + Œ∏k )R‚àó v k .
Remark 3 Based on Proposition 3, the GSOS algorithm
can be regarded as an inexact over-relaxed metric proximal
point algorithm for the composite inclusion
0 ‚àà (R‚àó )‚àí1 SR,‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV (z).
By Proposition 3 and Lemma 2, we can establish the convergence of the GSOS algorithm based on the relationship
n
P
between the two zero point sets [‚àáf+ ‚àÇgi ]‚àí1 (0) and ‚Ñ¶.
i=1

Theorem 1 Let {(xk , y k , z k )} be the sequence generated
by Algorithm 1. We have:
(i) for any z ‚àó ‚àà [(R‚àó )‚àí1 SR,‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ]‚àí1 (0), it
holds that
kz

k+1

‚àí

z ‚àó k2R‚àí1

k

‚â§ kz ‚àí

z ‚àó k2R‚àí1

(16)
k

‚àí (1 ‚àí œÉ)(1 + Œ∏k )kx ‚àí

y k k2R‚àí1 ;

(ii) z k converges to a point belonging to zero
point
set
[(R‚àó )‚àí1 SR,‚àÇg+A‚àó ‚ó¶‚àáf ‚ó¶A, NV ]‚àí1 (0)
k
and œâ converges to a point belonging to
Pn
‚àí1
[‚àáf + i=1 ‚àÇgi ] (0), i.e., the optimal solution set
of problem (1).

kxi ‚àí y i k2 ‚â§ O

2
1
1 
, œâ i+1 ‚àí œâ i  ‚â§ O
.
k
k

Due to the space limit, all proofs of the propositions, lemmas and theorems are placed into the supplementary material.

4. Experiments
In this section, we apply the proposed algorithm to the
overlapping group Lasso (Zhao et al., 2009; Jacob et al.,
2009; Mairal et al., 2010) and graph-guided fused Lasso
problems (Chen et al., 2012; Kim & Xing, 2009), which
can be formulated as
K
X
1
gi (x).
min kSx ‚àí bk2 +
2
i=1

(17)

For overlapping group Lasso problem (21), gi (x) =
ŒΩŒ±i kxGi k and K denotes the number of groups. For graphguided Lasso problem (25), gi (x) = ŒΩŒ±ij kxi ‚àíxj k and K
denotes the number of edges in the graph edge set E.
We describe the detailed techniques in the experimental implementation for (17). Given a > 21 and a positive definite
operator D satisfying D  S T S, we set
 1
K 2 D, i ‚â• j ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , K};
Hi,j =
(18)
a
K 2 D, i = j ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , K}.
Hence, it easy to check that H + H‚àó = A‚àó DA +
2a‚àí1
K 2 Diag EY D  0. Due to the smooth term in overlapping group Lasso (21) is quadratic, the two estimations Œ∏2
and Œ∏kadap2 in (14b) and (14d) are preferred to be used. By
PK
D and
specific H, we obtain i,j=1 Hi,j = K(K‚àí1)+2Œ±K
2K 2
PK PK
PK
D
k
k
H
z
=
(a+K
‚àíj)z
,
which
fur2
i,j j
j
i=j
j=1
j=1
‚àí1 PK PK
PKK
k
k
ther imply x =
i=j Hi,j zj =
i,j=1 Hi,j
j=1
2

PK

k
j=1 (a+K‚àíj)zj

. Moreover, by the positive definiteness
Pn Pj
‚àó k
and D, it holds that
=
j=1
i=i Hj,i zj

K(K‚àí1)+2aK

of Hi,i
PK
D

2
j=1 (a + j
KP
k
2 K
j=1 (a+j‚àí1)zj
K(K‚àí1)+2aK .

‚àí 1)zjk .

Hence, we attain œâ k

=

In addition, by the definition of H, we
reformulate the estimation (14b) for Œ∏k as the following
form:
n


Œ∏ = max Œ∏ | EY (œÉ ‚àí Œ∏)D ‚àí S T S EY‚àó
o

+ (2a ‚àí 1) œÉ ‚àí Œ∏ Diag(EY D)  0 .

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Due to a ‚â• 12 and the positive definiteness of D, a sufficient
condition satisfying the constraint
in the above set is

	
(œÉ ‚àí Œ∏)D ‚àí S T S  0, Œ∏ ‚â§ œÉ . Hence, we have an alternative estimation for Œ∏ as

	
Œ∏ = max Œ∏ | (œÉ ‚àí Œ∏)D ‚àí S T S  0, Œ∏ ‚â§ œÉ .
(19)
Similarly, the adaptive stepsize estimation (14d) is reformulated as
K
2
P k

1 
k 
(x
‚àí
y
)
i 
2K 2 
i=1
ST S
. (20)
Œ∏kadap = œÉ ‚àí K K
P P k
(x ‚àí yik )T Hij (xk ‚àí yjk )
j=1 i=j

Therefore, the GSOS algorithm can be specified as the following form for solving problem (17).
Algorithm 2 GSOS Algorithm for Solving Problem (17)
Parameters: Choose œÉ ‚àà (0, 1), positive definite operators D and Hi,j satisfying (18), and a starting point
z 0 ‚àà Z. Set Œ∏ as (19) and Œ∏fix ‚àà ‚àí 1, Œ∏1 .
for k = 0, P
1, 2, ¬∑ ¬∑ ¬∑ , do
2

K

(Œ±+K‚àíj)z k

j
j=1
xk := K(K‚àí1)+2Œ±K
;
for i = 1, 2 ¬∑ ¬∑ ¬∑ , K do
‚àí1 Pi
[ j=1 Hi,j (2xk ‚àí zjk ) ‚àí
yik := ProxH‚àí1 gi Hi,i
i,i

Pi‚àí1
1 T
k
k
j=1 Hi,j yj ] ;
K S (Sx ‚àí b) ‚àí
end for
set Œ∏k ‚àà [Œ∏kfix , Œ∏kadap ], where Œ∏kadap is defined via (20);
for j = 1, 2 ¬∑ ¬∑ ¬∑ , K do
zjk+1 := zjk + (1 + Œ∏k )(yjk ‚àí xk );
end for
end for
P
N
2 K
j=1 (Œ±+j‚àí1)zj
return œâ N = K(K‚àí1)+2Œ±K
.

In this paper, we compare the proposed GSOS algorithm
with four state-of-the-art algorithms below.
‚Ä¢ GFB (Raguet et al., 2013): Generalized Forward
Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which
has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes
& Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.
‚Ä¢ PDM (Condat, 2013): A first-order Primal-Dual splitting Method (PDM) (Condat, 2013) for solving jointly
the primal and dual formulations of large-scale convex
minimization problems involving Lipschitz, proximal
and linear composite terms.

‚Ä¢ PA-APG (Yu, 2013): Proximal Average approximated
Accelerated Proximal Gradient (PA-APG) algorithm
(Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al.,
2008) to separate the multi-term nonsmooth function
in (1). It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov,
2005b;a).
‚Ä¢ APA-APG (Shen et al., 2017): An enhanced version of PA-APG, which incorporates the Adaptive
Proximal Average approximation technique with the
Accelerated Proximal Gradient (APA-APG) method
to improve the efficiency of the optimization procedure.
It is worthwhile to emphasize that PA-APG and APA-APG
algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization
is Lipschitz continuous. Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided
fused Lasso are all exactly Lipschitz continuous, the two
efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with
the GSOS algorithm to illustrate the efficacy of GSOS. In
the implementation, the approximation parameter for PAAPG is set as 1.0e ‚àí 5.
4.1. Overlapping Group Lasso
In this subsection, we apply the proposed GSOS algorithm
to the overlapping group Lasso problem, which takes the
following formal definition:
K

X
1
Œ±i kxGi k,
min kSx ‚àí bk2 + ŒΩ
2
i=1

(21)

where S ‚àà Rn√ód is the sampling matrix, b is the noisy
observation vector, G = {G1 , ¬∑ ¬∑ ¬∑ , GK } denotes the set of
SK
overlapping groupsT(Gi ‚äÇ {1, ¬∑ ¬∑ ¬∑ , d} satisfying i=1 Gi =
{1, ¬∑ ¬∑ ¬∑ , d} and Gi Gj 6= ‚àÖ for some i, j), xGi ‚àà Rd is a
duplication of x with x{1,¬∑¬∑¬∑ ,d}\Gi = 0, Œ±i is the weight
for the i-th group, and ŒΩ is the regularization parameter
controlling group sparsity.
During the implementation of Algorithm 2, we need to calculate the generalized proximal mapping of kxGi k in the
updating step of yik . By the positive definiteness of Hi,i ,
the calculation of yik in Algorithm 2 is equivalent to solving the following problem:
1
yik := arg min kx ‚àí bk k2Hi,i + ŒΩŒ±i kxGi k,
x 2

P
i
‚àí1
1 T
k
k
k
where bk = Hi,i
j=1 Hi,j (2x ‚àí zj ) ‚àí K S (Sx ‚àí

Pi‚àí1
k
b)‚àí j=1 Hi,j yj . In the proposition below, given c, diag-

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization
(n,K) = (1000,20)

5

(n,K) = (2000,40)

5

10

10

3

10

(n,K) = (4000,60)

6

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

4

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

4

10

3

GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

4

10

10

2

2

10

2

10

10

1

1

10

10

0

0

10

0

10

10

‚àí2

10
‚àí1

‚àí1

10

10

‚àí2

10

‚àí2

0

200

400

600

800

1000

10

‚àí4

0

200

400

600

800

1000

10

0

200

400

600

800

1000

Figure 1. Objective value vs. iteration on overlapping group Lasso.
(n,K) = (4000,80)

6

4

2

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

4

10

2

10

0

10

0

0

10

‚àí2

10

‚àí2

10

‚àí2

10

‚àí4

10

‚àí4

0

200

400

600

800

1000

10

GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

4

10

2

10

10

(n,K) = (5000,100)

6

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

10

10

(n,K) = (5000,80)

6

10

‚àí4

0

200

400

600

800

1000

10

0

200

400

600

800

1000

Figure 2. Objective value vs. iteration on overlapping group Lasso.

onal positive definite operatpr Hi,i and group G, we solve
1
x‚àó := arg min kx ‚àí ck2Hi,i + ŒΩkxG k.
(22)
x 2
When Hi,i is identity matrix I, (22) has the closed-form
solution
 ‚àó
xG , i ‚àà G,
x‚àó =
ci , else,

(1 ‚àí ŒΩ/kcG k)cG , kcG k ‚â• t;
where x‚àóG =
0,
else.
Proposition 4 Let (Hi,i )G be the subdiagonal matrix of
Hi,i with the index set G, and t‚àó be the optimal solution
of the one-dimensional optimization problem


‚àí1 
1
 
2
cG , (Hi,i )‚àí1
+
2tI
c
+
tŒΩ
.
(23)
min
G
G
t‚â•0
2
Hence, the optimal solution of (22) has the following form


‚àí1
cG ‚àí I + 2t‚àó (Hi,i )G
cG , i ‚àà G;
x‚àó =
(24)
ci ,
else.
Like (Chen et al., 2012; Yu, 2013), the entries of sampling
matrix S ‚àà Rn√ód are sampled from an i.i.d. normal distribution, and x ‚àà Rd with xj = (‚àí1)j exp‚àí(j‚àí1)/100 and
d = 90K + 10. Let Œæ be the noise sampled from the standard normal distribution, and the noisy observation satisfies
b = Sx + Œæ. In addition, we set ŒΩ = 1 and Œ±i = K12 for
each group Gi and the groups {Gi } are overlapped by 10
elements, that is


G1 = {1, ¬∑ ¬∑ ¬∑ , 100}
G2 = {91, ¬∑ ¬∑ ¬∑ , 190}
.
¬∑¬∑¬∑
GK = {d ‚àí 99, ¬∑ ¬∑ ¬∑ , d}

The sampling size and the number of groups (n, K) are
chosen from the following set


(1000, 20), (2000, 40), (4000, 60),
(n, K) ‚àà
.
(4000, 80), (5000, 80), (5000, 100)
To further reduce the computations, in Algorithm 2 we set
Hi,i = kS T SkI and the over-relaxation stepsize Œ∏k as Œ∏ in
(19). Hence, the compared five solvers GSOS, GFB, PDM,
PA-APG and APA-APG have the same computational cost
in each iteration. To be fair, all the compared algorithms
start with the same initial point. The following six pictures in Figures 1 and 2 display the comparisons of the five
solvers for a variety of (n, K). It is apparent that our proposed GSOS algorithm shows great superiorities over the
other four solvers. The primal-dual solver PDM is slightly
faster than the primal solver GFB. PA-APG is the slowest
algorithm, because the prespecified proximal average approximation precision is 1.0e ‚àí 5 which leads to a very
small stepsize. Also, APA-APG is much faster than the
other four solvers at the first 50 iterations. However, it is
slowed down since the stepsize used in AP-APG becomes
smaller and smaller as the iterations go on.
4.2. Graph-Guided Fused Lasso
In this subsection, we perform experiments on graphguided fused Lasso which is formulated as
X
1
min kSx ‚àí bk2 + ŒΩ
Œ±ij |xi ‚àí xj |,
(25)
2
(i,j)‚ààE

where Œ±ij ‚â• 0 is the weight for the fused term kxi ‚àí xj k
for all (i, j) ‚àà E (E is the given graph edge set), and ŒΩ is

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization
(n,d) = (2000,500)

9

(n,d) = (2000,1000)

10

10

10

7

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

9

10

8

10

10

7

GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

9

10

8

10

7

10

6

(n,d) = (5000,1000)

10

10

GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

8

10

10

6

6

10

10

5

10

5

5

10

10

4

10

4

4

10
3

10

3

3

10

2

10

10
10

2

0

200

400

600

800

10

1000

2

0

200

400

600

800

1000

10

0

200

400

600

800

1000

Figure 3. Objective value vs. iteration on graph-guided fused Lasso.
(n,d) = (5000,2000)

12

(n,d) = (10000,2000)

10

10

10

10

(n,d) = (10000,4000)

12

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

10
GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

9

10

8

GSOS
PDM
GFB
APA‚àíAPG
PA‚àíAPG

10

10

10
8

8

10

10

7

10

6

10

6

10

6

10

5

10
4

4

10

10
4

10
2

10

3

0

200

400

600

800

10

1000

2

0

200

400

600

800

1000

10

0

200

400

600

800

1000

Figure 4. Objective value vs. iteration on graph-guided fused Lasso.

the regularization parameter.
In the implementation of Algorithm 2 for tackling graphguided fused Lasso (25), we need to solve the following
optimization in the updating step of y k :
1
x‚àó := arg min kx ‚àí bk2Hi,i + ŒΩ|xi ‚àí xj |,
(26)
x 2
where Hi,i is a diagonal positive definite matrix, and b and
ŒΩ are given constants. Let hii and hjj be the i-th and j-th
diagonal elements of Hi,i , respectively.
Proposition 5 The optimal solution of (26) takes the following closed-form:
Ô£±
‚àó
Ô£≤ bl ‚àí h‚àí1
ll Œª , l = i,
‚àó
‚àí1 ‚àó
(27)
x =
b + hll Œª , l = j,
Ô£≥ l
bl ,
l 6= i, j,
where Œª‚àó is defined as
Ô£± b ‚àíb
Ô£≤ ‚àí1i j‚àí1 ,
hii +hjj
Œª‚àó =
Ô£≥ sign (bi ‚àí bj ) ŒΩ,



 bi ‚àíbj 
 h‚àí1 +h‚àí1  ‚â§ ŒΩ;
 ii jj 
 bi ‚àíbj 
 h‚àí1 +h‚àí1  > ŒΩ.
ii

jj

In the implementation, we use the similar parameter settings of S, ŒΩ as above. The dimension parameter pair (n, d)
is chosen from the following set


(2000, 500), (2000, 1000), (5000, 1000),
(n, d) ‚àà
,
(5000, 2000), (10000, 2000), (10000, 4000)
and the parameter Œ±i = 100/|E|2 . Similarly, all the compared algorithms start with the same initial point. The following six pictures in Figures 3 and 4 display the comparisons of the five solvers for six kinds of choices of (n, d). It

is obvious that the other four solvers GFB, PDM, AP-APG
and APA-APG are not as efficient as the proposed GSOS
algorithm, which demonstrates that the Gauss-Seidel technique is very useful for addressing nonsmooth optimization. It is worthwhile to point out that the primal solver
GFB is faster than the primal-dual solver PDM on graphguided fused Lasso. One possible reason is that the number
of nonsmooth terms is too large, which will lead to a large
quantity of dual variables introduced in PDM and hence
slow down the updating of primal variables.

5. Conclusions
In this paper, we proposed a novel first-order algorithm
called GSOS for addressing multi-term nonsmooth convex composite optimization. This algorithm inherits the
advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated. We found that the GSOS algorithm includes the generalized forward backward splitting method (Raguet et al.,
2013) as a special case. In addition, we developed a new
technique to establish the global convergence and iteration
complexity of the GSOS algorithm. Last, we applied the
proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared
it against several state-of-the-art algorithms. The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness.

Acknowledgements
Yuan is supported by NSF-China (61402182).

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

References
Bach, F. R. Structured sparsity-inducing norms through
submodular functions. Advances in Neural Information
Processing Systems, pp. 118‚Äì126, 2010.

Chen, X., Lin, Q., Kim, S., Carbonell, J. G., and Xing, E. P.
Smoothing proximal gradient method for general structured sparse regression. The Annals of Applied Statistics,
6(2):719‚Äì752, 2012.

Bach, F. R., Jenatton, R., Mairal, J., and Obozinski, G.
Structured sparsity through convex optimization. Statistical Science, 27(4):pgs. 450‚Äì468, 2012.

Combettes, P. L. and Pesquet, J. C. A douglas‚Äìrachford
splitting approach to nonsmooth convex variational signal recovery. IEEE Journal of Selected Topics in Signal
Processing, 1(4):564‚Äì574, 2007.

Bauschke, H. H. and Combettes, P. L. Convex Analysis and
Monotone Operator Theory in Hilbert Space. Springer
New York, 2011.

Combettes, P. L and Pesquet, J. C. A proximal decomposition method for solving convex variational inverse
problems. Inverse problems, 24(6):065014, 2008.

Bauschke, H. H., Goebel, R., Lucet, Y., and Wang, X. The
proximal average: basic theory. SIAM Journal on Optimization, 19(2):766‚Äì785, 2008.

Combettes, P. L. and Pesquet, J. C. Proximal splitting
methods in signal processing. In Fixed-point algorithms for inverse problems in science and engineering, pp.
185‚Äì212. Springer, 2011.

Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM journal on imaging sciences, 2(1):183‚Äì202, 2009.
Briceno-Arias, L. M., Combettes, P. L., Pesquet, J. C., and
Pustelnik, N. Proximal algorithms for multicomponent image recovery problems. Journal of Mathematical
Imaging and Vision, 41(1-2):3‚Äì22, 2011.
Burachik, R. S. and Svaiter, B. F. Œµ-enlargements of maximal monotone operators in banach spaces. Set-Valued
Analysis, 7(2):117‚Äì132, 1999.
Burachik, R. S., Iusem, A. N., and Svaiter, B. F. Enlargement of monotone operators with applications to variational inequalities. Set-Valued and Variational Analysis,
5(2):159‚Äì180, 1997.
Burachik, R. S., SagastizaÃÅbal, C. A., and Svaiter, B. F. Œµenlargements of maximal monotone operators: Theory
and applications. In Reformulation: nonsmooth, piecewise smooth, semismooth and smoothing methods, pp.
25‚Äì43. Springer, 1998.
Chambolle, A. and Pock, T. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):
120‚Äì145, 2011.
Chen, D. and Plemmons, R. J. Nonnegativity constraints in
numerical analysis. 2015.
Chen, G. H. G. and Rockafellar, R. T. Convergence rates
in forward‚Äìbackward splitting. SIAM Journal on Optimization, 7(2):421‚Äì444, 1997.
Chen, X., Lin, Q., Kim, S., Carbonell, J. G., and Xing,
E. P. An efficient proximal gradient method for general
structured sparse learning. stat, 1050:26, 2011.

Combettes, P. L. and Pesquet, J. C. Primal-dual splitting
algorithm for solving inclusions with mixtures of composite, lipschitzian, and parallel-sum type monotone operators. Set-Valued and variational analysis, 20(2):307‚Äì
330, 2012.
Condat, L. A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear
composite terms. Journal of Optimization Theory and
Applications, 158(2):460‚Äì479, 2013.
Cui, Y., Li, X., Sun, D. F., and Toh, K. C. On the convergence properties of a majorized alternating direction
method of multipliers for linearly constrained convex optimization problems with coupled objective functions.
Journal of Optimization Theory & Applications, 169(3):
1013‚Äì1041, 2016.
Davis, D. and Yin, W. A three-operator splitting scheme
and its optimization applications. Mathematics, 19(3):
407‚Äì12, 2015.
DupeÃÅ, F. X., Fadili, J. M., and Starck, J. L. A proximal
iteration for deconvolving poisson noisy images using sparse representations. IEEE Transactions on Image Processing, 18(2):310‚Äì321, 2009.
Eckstein, J. and Bertsekas, D. P. On the douglas≈Çrachford
splitting method and the proximal point algorithm for
maximal monotone operators. Mathematical Programming, 55(1):293‚Äì318, 1992.
Huang, J., Zhang, T., and Metaxas, D. Learning with structured sparsity. Journal of Machine Learning Research,
12(Nov):3371‚Äì3412, 2011.
Jacob, L., Obozinski, G., and Vert, J. P. Group lasso with
overlap and graph lasso. In International Conference
on Machine Learning, ICML 2009, Montreal, Quebec,
Canada, June, pp. 433‚Äì440, 2009.

GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization

Kim, S. and Xing, E. P. Statistical estimation of correlated genome associations to a quantitative trait network.
PLoS Genet, 5(8):e1000587, 2009.

Svaiter, B. F. A family of enlargements of maximal
monotone operators. Set-Valued Analysis, 8(4):311‚Äì328,
2000.

Li, Q. and Zhang, N. Fast proximity-gradient algorithms
for structured convex optimization problems . Applied
& Computational Harmonic Analysis, 41(2):491‚Äì517,
2016.

Svaiter, B. F. A class of fejeÃÅr convergent algorithms, approximate resolvents and the hybrid proximalextragradient method. Journal of Optimization Theory
and Applications, 162(1):133‚Äì153, 2014.

Lin, Z., Liu, R., and Su, Z. Linearized alternating direction method with adaptive penalty for low-rank representation. Advances in Neural Information Processing
Systems, pp. 612‚Äì620, 2011.

Teo, C. H., Smola, A., Vishwanathan, S., and Le, Q. V.
A scalable modular convex solver for regularized risk
minimization. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 727‚Äì736. ACM, 2007.

Luo, Z. Q. and Tseng, P. On the convergence of a matrix splitting algorithm for the symmetric monotone linear complementarity problem. SIAM Journal on Control
and Optimization, 29(5):1037‚Äì1060, 1991.
Mairal, J., Jenatton, R., Bach, F. R., and Obozinski, G. R.
Network flow algorithms for structured sparsity. In Advances in Neural Information Processing Systems, pp.
1558‚Äì1566, 2010.
Monteiro, R. D. C. and Svaiter, B. F. Iteration-complexity
of block-decomposition algorithms and the alternating
direction method of multipliers. SIAM Journal on Optimization, 23(1):475‚Äì507, 2013.
Nesterov, Y. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization, 16
(1):235‚Äì249, 2005a.
Nesterov, Y. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127‚Äì152,
2005b.
Nesterov, Y. Gradient methods for minimizing composite
functions. Mathematical Programming, 140(2007076):
125‚Äì161, 2007.
Pustelnik, N., Chaux, C., and Pesquet, J. C. Parallel proximal algorithm for image restoration using hybrid regularization. IEEE Transactions on Image Processing, 20
(9):2450‚Äì2462, 2011.
Raguet, H., Fadili, J., and PeyreÃÅ, G. A generalized forwardbackward splitting. SIAM Journal on Imaging Sciences,
6(3):1199‚Äì1226, 2013.
Richard, E., Savalle, P. A., and Vayatis, N. Estimation
of simultaneously sparse and low rank matrices. arXiv
preprint arXiv:1206.6474, 2012.
Shen, L., Liu, W., Huang, J., Jiang, Y. G., and Ma, S. Adaptive proximal average approximation for composite convex minimization. In AAAI, 2017.

Teo, C. H., Vishwanthan, S., Smola, A. J., and Le, Q. V.
Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11(Jan):311‚Äì365,
2010.
VuÃÉ, B. C. A splitting algorithm for dual monotone inclusions involving cocoercive operators. Advances in Computational Mathematics, 38(3):667‚Äì681, 2013.
Xu, Y. and Yin, W. A block coordinate descent method for
regularized multiconvex optimization with applications
to nonnegative tensor factorization and completion. Siam
Journal on Imaging Sciences, 6(3):1758‚Äì1789, 2013.
Yu, Y. Better approximation and faster algorithm using the
proximal average. Advances in Neural Information Processing Systems, pp. 458‚Äì466, 2013.
Yuan, G., Zheng, W. S., and Ghanem, B. A matrix splitting method for composite function minimization. arXiv
preprint arXiv:1612.02317, 2016.
Zhao, P., Rocha, G., and Yu, B. The composite absolute
penalties family for grouped and hierarchical variable selection. The Annals of Statistics, pp. 3468‚Äì3497, 2009.
Zhong, W. and Kwok, J. T. Y. Accelerated stochastic gradient method for composite regularization. In AISTATS,
pp. 1086‚Äì1094, 2014.
Zhou, K., Zha, H., and Song, L. Learning social infectivity in sparse low-rank networks using multi-dimensional
hawkes processes. In AISTATS, volume 31, pp. 641‚Äì649,
2013.

