Differentially Private Ordinary Least Squares

Or Sheffet 1

Abstract
Linear regression is one of the most prevalent
techniques in machine learning; however, it is
also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used
in statistics to establish a correlation between
an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model
that randomly generates the data, and derives tvalues ‚Äî representing the likelihood of each real
value to be the true correlation. Using t-values,
OLS can release a confidence interval, which is
an interval on the reals that is likely to contain
the true correlation; and when this interval does
not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation
is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss
Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with
l2 -regularization) we derive, under certain conditions, confidence intervals using the projected
data; lastly, we derive, under different conditions,
confidence intervals for the ‚ÄúAnalyze Gauss‚Äù algorithm (Dwork et al., 2014).

1. Introduction
Since the early days of differential privacy, its main goal
was to design privacy preserving versions of existing techniques for data analysis. It is therefore no surprise that several of the first differentially private algorithms were machine learning algorithms, with a special emphasis on the
ubiquitous problem of linear regression (Kasiviswanathan
1
Computing Science Dept., University of Alberta, Edmonton
AB, Canada. This work was done when the author was at Harvard University, supported by NSF grant CNS-123723. Correspondence to: Or Sheffet <osheffet@ualberta.ca>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

et al., 2008; Chaudhuri et al., 2011; Kifer et al., 2012; Bassily et al., 2014). However, all existing body of work on
differentially private linear regression measures utility by
bounding the distance between the linear regressor found
by the standard non-private algorithm and the regressor
found by the privacy-preserving algorithm. This is motivated from a machine-learning perspective, since bounds
on the difference in the estimators translate to error bounds
on prediction (or on the loss function). Such bounds are
(highly) interesting and non-trivial, yet they are of little use
in situations where one uses linear regression to establish
correlations rather than predict labels.
In the statistics literature, Ordinary Least Squares (OLS)
is a technique that uses linear regression in order to infer
the correlation between a variable and an outcome, especially in the presence of other factors. And so, in this paper, we draw a distinction between ‚Äúlinear regression,‚Äù by
which we refer to the machine learning technique of finding
a specific estimator for a specific loss function; and ‚ÄúOrdinary Least Squares,‚Äù by which we refer to the statistical inference done assuming a specific model for generating the
data and that uses linear regression. Many argue that OLS
is the most prevalent technique in social sciences (Agresti
& Finlay, 2009). Such works make no claim as to the labels of a new unlabeled batch of samples. Rather they aim
to establish the existence of a strong correlation between
the label and some feature. Needless to say, in such works,
the privacy of individuals‚Äô data is a concern.
In order to determine that a certain variable xj is positively
(resp. negatively) correlated with an outcome y, OLS assumes a model where the outcome y is a noisy version of
a linear mapping of all variables: y = Œ≤ ¬∑ x + e (with e
denoting random Gaussian noise) for some predetermined
xi , yi ) OLS
and unknown Œ≤ . Then, given many samples (x
establishes two things: (i) when fitting a linear function
to best predict y from x over the sample (via computing

P
P
T ‚àí1
Œ≤=
Œ≤ÃÇ
( i yix i )) the coefficient Œ≤ÃÇj is positive
i x ix i
(resp. negative); and (ii) inferring, based on Œ≤ÀÜj , that the
true Œ≤j is likely to reside in R>0 (resp. R<0 ). In fact, the
crux in OLS is by describing Œ≤j using a probability distribution over the reals, indicating where Œ≤j is likely to fall,
derived by computing t-values. These values take into account both the variance in the data as well as the variance of
the noise e.1 Based on this probability distribution one can
1

For example, imagine we run linear regression on a certain

Differentially Private Ordinary Least Squares

define the Œ±-confidence interval ‚Äî an interval I centered
at Œ≤ÃÇj whose likelihood to contain Œ≤j is 1 ‚àí Œ±. Of particular
importance is the notion of rejecting the null-hypothesis,
where the interval I does not contain the origin, and so
one is able to say with high confidence that Œ≤j is positive
(resp. negative). Further details regarding OLS appear in
Section 2.
In this work we give the first analysis of statistical inference for OLS using differentially private estimators. We
emphasize that the novelty of our work does not lie in the
differentially-private algorithms, which are, as we discuss
next, based on the Johnson-Lindenstrauss Transform (JLT)
and on additive Gaussian noise and are already known to
be differentially private (Blocki et al., 2012; Dwork et al.,
2014). Instead, the novelty of our work lies in the analyses of the algorithms and in proving that the output of the
algorithms is useful for statistical inference.
The Algorithms. Our first algorithm (Algorithm 1) is an
adaptation of Gaussian JLT. Proving that this adaptation
remains (, Œ¥)-differentially private is straightforward (the
proof appears in Appendix A.1). As described, the algorithm takes as input a parameter r (in addition to the
other parameters of the problem) that indicates the number
of rows in the JL-matrix. Later, we analyze what should
one set as the value of r. Our second algorithm is taken

Algorithm 1 Outputting a private Johnson-Lindenstrauss
projection of a matrix.
Input: A matrix A ‚àà Rn√ód and a bound B > 0 on the
l2 -norm of any row in A.
Privacy parameters: , Œ¥ > 0.
Parameter r indicating the number of rows in the resulting matrix.
p

2
Set w s.t. w2 = 8B
2r ln(8/Œ¥) + 2 ln(8/Œ¥) .
Sample Z ‚àº Lap(4B 2 /) and let œÉmin (A) denote the
smallest singular value of A.
2
if œÉmin (A)2 > w2 + Z + 4B ln(1/Œ¥)
then

Sample a (r√ón)-matrix R whose entries are i.i.d samples from a normal Gaussian.
return RA and ‚Äúmatrix unaltered‚Äù.
else
Let A0 denote the result of appending A with the d√ódmatrix wId√ód .
Sample a (r √ó (n + d))-matrix R whose entries are
i.i.d samples from a normal Gaussian.
returnRA0 and ‚Äúmatrix altered‚Äù.
end if
Œ≤ with coordinates Œ≤ÃÇ1 = Œ≤ÃÇ2 =
(X, y ) which results in a vector Œ≤ÃÇ
0.1. Yet while the column X1 contains many 1s and (‚àí1)s, the
column X2 is mostly populated with zeros. In such a setting,
OLS gives that it is likely to have Œ≤1 ‚âà 0.1, whereas no such
guarantees can be given for Œ≤2 .

verbatim from the work of Dwork et al (2014). We deAlgorithm 2 ‚ÄúAnalyze Gauss‚Äù Algorithm of Dwork et
al (2014).
Input: A matrix A ‚àà Rn√ód and a bound B > 0 on the
l2 -norm of any row in A.
Privacy parameters: , Œ¥ > 0.
N ‚Üê symmetric (d √ó d)-matrix
with upper

 triangle entries sampled i.i.d from N 0, 2B

4

ln(2/Œ¥)
2

.

T

return A A + N .
liberately focus on algorithms that approximate the 2nd moment matrix of the data and then run hypothesis-testing
by post-processing the output, for two reasons. First, they
enable sharing of data2 and running unboundedly many
hypothesis-tests. Since, we do not deal with OLS based
on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss
function ‚Äî but these algorithms do not minimize a private
loss-function but rather prove that outputting the minimizer
of the perturbed loss-function is private. This means that
differentially-private OLS based on these ERM algorithms
requires us to devise new versions of these algorithms,
making this a second step in this line of work... (After first
understanding what we can do using existing algorithms.)
We leave this approach ‚Äî as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei,
2009) (output merely reject / don‚Äôt-reject decision without justification), or releasing only relevant tests judging
by their p-values (Dwork et al., 2015) ‚Äî for future work.
Our Contribution and Organization. We analyze the
performances of our algorithms on a matrix A of the form
A = [X; y ], where each coordinate yi is generated according to the homoscedastic model with Gaussian noise, which
is a classical model in statistics. We assume the existence
of a vector Œ≤ s.t. for every i we have yi = Œ≤ Tx i + ei and
ei is sampled i.i.d from N (0, œÉ 2 ).3
We study the result of running Algorithm 1 on such data
in the two cases: where A wasn‚Äôt altered by the algorithm
and when A was appended by the algorithm. In the former
case, Algorithm 1 boils down to projecting the data under
a Gaussian JLT. Sarlos (2006) has already shown that the
JLT is useful for linear regression, yet his work bounds
the l2 -norm of the difference between the estimated re2
Researcher A collects the data and uses the approximation of
the 2nd -moment matrix to test some OLS hypothesis; but once
the approximation is published researcher B can use it to test for
a completely different hypothesis.
3
This model may seem objectionable. Assumptions like the
noise independence, 0-meaned or sampled from a Gaussian distribution have all been called into question in the past. Yet due to
the prevalence of this model we see fit to initiate the line of work
on differentially private Least Squares with this Ordinary model.

Differentially Private Ordinary Least Squares

gression before and after the projection. Following Sarlos‚Äô
work, other works in statistics have analyzed compressed
linear regression (Zhou et al., 2007; Pilanci & Wainwright,
2014a;b). However, none of these works give confidence
intervals based on the projected data, presumably for three
reasons. Firstly, these works are motivated by computational speedups, and so they use fast JLT as opposed to our
analysis which leverages on the fact that our JL-matrix is
composed of i.i.d Gaussians. Secondly, the focus of these
works is not on OLS but rather on newer versions of linear
regression, such as Lasso or when Œ≤ lies in some convex
set. Lastly, it is evident that the smallest confidence interval is derived from the data itself. Since these works do
not consider privacy applications, (actually, (Zhou et al.,
2007; Pilanci & Wainwright, 2014a) do consider privacy
applications of the JLT, but quite different than differential
privacy) they assume the analyst has access to the data itself, and so there was no need to give confidence intervals
for the projected data. Our analysis is therefore the first, to
the best of our knowledge, to derive t-values ‚Äî and therefore achieve all of the rich expressivity one infers from tvalues, such as confidence bounds and null-hypotheses rejection ‚Äî for OLS estimations without having access to X
itself. We also show that, under certain conditions, the sample complexity for correctly rejecting the null-hypothesis
increases from a certain
bound N0 (without privacy) to a
‚àö
bound of N0 + OÃÉ( N0 ¬∑ Œ∫( n1 AT A)/) with privacy (where
Œ∫(M ) denotes the condition number of the matrix M .) This
appears in Section 3.
In Section 4 we analyze the case Algorithm 1 does append
the data and the JLT is applied to A0 . In this case, solving
the linear regression problem on the projected A0 approximates the solution for Ridge Regression (Tikhonov, 1963;
Hoerl & Kennard,
we aim

P 1970).T In 2Ridge2 Regression
2
z
to solve minz
(y
‚àí
z
x
)
+
w
kz
k
,
which
means
i
i
i
we penalize vectors whose l2 -norm is large. In general, it
is not known how to derive t-values from Ridge regression,
and the literature on deriving confidence intervals solely
from Ridge regression is virtually non-existent. Indeed,
prior to our work there was no need for such calculations,
as access to the data was (in general) freely given, and so
deriving confidence intervals could be done by appealing
back to OLS. We too are unable to derive approximated
t-values in the general case, but under additional assumptions about the data ‚Äî which admittedly depend in part on
Œ≤ k and so cannot be verified solely from the data ‚Äî we
kŒ≤
show that solving the linear regression problem on RA0 allows us to give confidence intervals for Œ≤j , thus correctly
determining the correlation‚Äôs sign.
In Section 5 we discuss the ‚ÄúAnalyze Gauss‚Äù algorithm (Dwork et al., 2014) that outputs a noisy version of
a covariance of a given matrix using additive noise rather
than multiplicative noise. Empirical work (Xi et al., 2011)
shows that Analyze Gauss‚Äôs output might be non-PSD if
the input has small singular values, and this results in truly

bad regressors. Nonetheless, under additional conditions
(that imply that the output is PSD), we derive confidence
bounds for Dwork et al‚Äôs ‚ÄúAnalyze Gauss‚Äù algorithm. Finally, in Section 6 we experiment with the heuristic of
computing the t-values directly from the outputs of Algorithms 1 and 2. We show that Algorithm 1 is more ‚Äúconservative‚Äù than Algorithm 2 in the sense that it tends to
not reject the null-hypothesis until the number of examples is large enough to give a very strong indication of rejection. In contrast, Algorithm 2 may wrongly rejects the
null-hypothesis even when it is true.
Discussion. Some works have already looked at the intersection of differentially privacy and statistics (Dwork &
Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi
et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence). But only a handful
of works studied the significance and power of hypotheses
testing under differential privacy, without arguing that the
noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang
et al., 2015; Rogers et al., 2016). These works are experimentally promising, yet they (i) focus on different statistical tests (mostly Goodness-of-Fit and Independence testing), (ii) are only able to prove results for the case of simple
hypothesis-testing (a single hypothesis) with an efficient
data-generation procedure through repeated simulations ‚Äî
a cumbersome and time consuming approach. In contrast,
we deal with a composite hypothesis (we simultaneously
reject all Œ≤ s with sign(Œ≤j ) 6= sign(Œ≤ÃÇj )) by altering the
confidence interval (or the critical region).
One potential reason for avoiding confidence-interval analysis for differentially private hypotheses testing is that it
does involve re-visiting existing results. Typically, in statistical inference the sole source of randomness lies in the
underlying model of data generation, whereas the estimators themselves are a deterministic function of the dataset.
In contrast, differentially private estimators are inherently
random in their computation. Statistical inference that considers both the randomness in the data and the randomness
in the computation is highly uncommon, and this work, to
the best of our knowledge, is the first to deal with randomness in OLS hypothesis testing. We therefore strive in our
analysis to separate the two sources of randomness ‚Äî as
in classic hypothesis testing, we use Œ± to denote the bound
on any bad event that depends solely on the homoscedastic model, and use ŒΩ to bound any bad event that depends
on the randomized algorithm.4 (Thus, any result which is
originally of the form ‚ÄúŒ±-reject the null-hypothesis‚Äù is now
converted into a result ‚Äú(Œ±+ŒΩ)-reject the null hypothesis‚Äù.)
4
Or any randomness in generating the feature matrix X which
standard OLS theory assumes to be fixed, see Theorems 2.2
and 3.3.

Differentially Private Ordinary Least Squares

2. Preliminaries and OLS Background

kŒ∂k2 ‚àº œá2k , and taking the quantity Z/

p
kŒ∂k2 /k. It is a

k‚Üí‚àû

Notation. Throughout this paper, we use lower-case letters to denote scalars (e.g., yi or ei ); bold characters to
denote vectors; and UPPER-case letters to denote matrices. The l-dimensional all zero vector is denoted 0l , and
the l √ó m-matrix of all zeros is denoted 0l√óm . We use e
Œ≤ in our model; and
to denote the specific vector y ‚àí XŒ≤
though the reader may find it a bit confusing but hopefully
clear from the context ‚Äî we also use e j and e k to denote
elements of the natural basis (unit length vector in the direction of coordinate j or k). We use , Œ¥ to denote the privacy parameters of Algorithms 1 and 2, and use Œ± and ŒΩ to
denote confidence parameters (referring to bad events that
hold w.p. ‚â§ Œ± and ‚â§ ŒΩ resp.) based on the homoscedastic
model or the randomized algorithm resp. We also stick to
the notation from Algorithm 1 and
use w to denote the posip

8B 2
2
tive scalar for which w = 
2r ln(8/Œ¥) + ln(8/Œ¥)
throughout this paper. We use standard notation for SVD
composition of a matrix (M = U Œ£V T ), its singular values
and its Moore-Penrose inverse (M + ).
The Gaussian distribution.
A univariate Gaussian
N (¬µ, œÉ 2 ) denotes the Gaussian distribution whose mean
is ¬µ and variance œÉ 2 . Standard concentration
bounds on
p
Gaussians give that Pr[x > ¬µ + 2œÉ ln(2/ŒΩ)] < ŒΩ
¬µ, Œ£)
for any ŒΩ ‚àà (0, 1e ). A multivariate Gaussian N (¬µ
for some positive semi-definite Œ£ denotes the multivariate Gaussian distribution where the mean of the j-th coordinate is the ¬µj and the covariance between coordinates
j and k is Œ£j,k . The PDF of such Gaussian is defined
only on the subspace colspan(Œ£). A matrix Gaussian distribution, denoted N (Ma√ób , Ia√óa , V ) has mean M , independence among its rows and variance V for each of
its columns. We also require the following property of
Gaussian random variables: Let X and Y be two random
Gaussians s.t. X ‚àº N (0, œÉ 2 ) and Y ‚àº N (0, Œª2 ) where
2
1 ‚â§ œÉŒª2 ‚â§ c2 for some c, then for any S ‚äÇ R we have
1
c Prx‚ÜêY [x ‚àà S] ‚â§ Prx‚ÜêX [x ‚àà S] ‚â§ cPrx‚ÜêY [x ‚àà S/c]
(see Proposition A.2).
Additional Distributions. We denote by Lap(œÉ) the
Laplace distribution whose mean is 0 and variance is 2œÉ 2 .
The œá2k -distribution, where k is referred to as the degrees of freedom of the distribution, is the distribution
over the l2 -norm squared of the sum of k independent
normal Gaussians. That is, given i.i.d X1 , . . . , Xk ‚àº
def
N (0, 1) it holds that Œ∂ = (X1 , X2 , . . . , Xk ) ‚àº
2
N (00k , Ik√ók ), and kŒ∂Œ∂ k ‚àº œá2k . Existing tail bounds on
the hœá2k distribution (Laurent & Massart,
2000) give that
i
p
‚àö
2
2
Pr kŒ∂Œ∂ k ‚àà ( k ¬± 2 ln(2/ŒΩ)) ‚â• 1 ‚àí ŒΩ. The Tk distribution, where k is referred to as the degrees of freedom of the distribution, denotes the distribution over the
reals created by independently sampling Z ‚àº N (0, 1) and

known fact that Tk ‚Üí N (0, 1), thus it is a common practice to apply Gaussian tail bounds to the Tk -distribution
when k is sufficiently large.
Differential Privacy. In this work, we deal with input in
the form of a n √ó d-matrix with each row bounded by a
l2 -norm of B. Two inputs A and A0 are called neighbors if
they differ on a single row.
Definition 2.1 ((Dwork et al., 2006a)). An algorithm ALG
which maps (n √ó d)-matrices into some range R is (, Œ¥)differential privacy it holds that Pr[ALG(A) ‚àà S] ‚â§
e Pr[ALG(A0 ) ‚àà S] + Œ¥ for all neighboring inputs A and
A0 and all subsets S ‚äÇ R.
Background on OLS. For the unfamiliar reader, we give
here a very brief overview of the main points in OLS. Further details, explanations and proofs appear in Section A.3.
xi , yi )}ni=1 where ‚àÄi, x i ‚àà
We are given n observations {(x
p
R and yi ‚àà R. We assume the existence of Œ≤ ‚àà Rp s.t.
the label yi was derived by yi = Œ≤ Tx i + ei where ei ‚àº
N (0, œÉ 2 ) independently (also known as the homoscedastic
Gaussian model). We use the matrix notation where X denotes the (n √ó p)- feature matrix and y denotes the labels.
We assume X has full rank.
The parameters of the model are therefore Œ≤ and œÉ 2 , which
we set to discover. To that end, we minimize minz kyy ‚àí
Xzz k2 and have
Œ≤ = (X T X)‚àí1 X Ty = (X T X)‚àí1 X T (XŒ≤Œ≤ + e) = Œ≤ + X +e (1)
Œ≤ÃÇ
Œ∂ = y ‚àí XŒ≤ÃÇŒ≤ = (XŒ≤Œ≤ + e ) ‚àí X(Œ≤Œ≤ + X +e ) = (I ‚àí XX + )ee (2)
And then for any coordinate j the t-value, which
def
Œ≤ÃÇj ‚àíŒ≤j
q
is the quantity t(Œ≤j )
=
, is
Œ∂k
kŒ∂
‚àí1
T
(X X)j,j ¬∑ ‚àön‚àíp

distributed
according to Tn‚àíp
I.e.,
h
i -distribution.
R
Œ≤ and Œ∂ satisfying t(Œ≤j ) ‚àà S =
Pr Œ≤ÃÇ
PDFTn‚àíp (x)dx
S

for any measurable S ‚äÇ R. Thus t(Œ≤j ) describes the
likelihood of any Œ≤j ‚Äî for any z ‚àà R we can now give
an estimation of how likely it is to have Œ≤j = z (which
is PDFTn‚àíp (t(z))), and this is known as t-test for the
value z. In particular, given 0 < Œ± < 1, we denote cŒ±
as the number for which the interval (‚àícŒ± , cŒ± ) contains
a probability mass of 1 ‚àí Œ± from the Tn‚àíp -distribution.
And so we derive a corresponding confidence interval IŒ±
centered at Œ≤ÃÇj where Œ≤j ‚àà IŒ± with confidence of level of
1 ‚àí Œ±.
def

Of particular importance is the quantity t0 = t(0) =
‚àö

Œ≤ÃÇj n‚àíp
q
,since
(X T X)‚àí1
j,j

kŒ∂Œ∂ k

if there is no correlation between xj

and y then the likelihood of seeing Œ≤ÀÜj depends on the ratio
of its magnitude to its standard deviation. As mentioned
k‚Üí‚àû
earlier, since Tk ‚Üí N (0, 1), then rather than viewing

Differentially Private Ordinary Least Squares

this t0 as sampled from a Tn‚àíp -distribution, it is common
to think of t0 as a sample from a normal Gaussian N (0, 1).
This allows us to associate t0 with a p-value, estimating the
event ‚ÄúŒ≤j and Œ≤ÃÇj have different signs.‚Äù Specifically, given
Œ± ‚àà (0, 1/2), we Œ±-reject the null hypothesis if p0 < Œ±.
R‚àû
2
Let œÑŒ± be the number s.t. Œ¶(œÑŒ± ) = œÑŒ± ‚àö12œÄ e‚àíx /2 dx = Œ±.
This means we Œ±-reject the null hypothesis when |t0 | > œÑŒ± .
We now lower bound the number of i.i.d sample points
needed in order to Œ±-reject the null hypothesis. This bound
is our basis for comparison between standard OLS and the
differentially private version.5
Theorem 2.2. Fix any positive definite matrix Œ£ ‚àà Rp√óp
and any ŒΩ ‚àà (0, 21 ). Fix parameters Œ≤ ‚àà Rp and œÉ 2
and a coordinate j s.t. Œ≤j 6= 0. Let X be a matrix whose n rows are i.i.d samples from N (00, Œ£), and
Œ≤ )i is sampled i.i.d from
y be a vector where yi ‚àí (XŒ≤
N (0, œÉ 2 ). Fix Œ± ‚àà (0, 1). Then w.p. ‚â• 1 ‚àí Œ± ‚àí ŒΩ we
have p
that OLS‚Äôs (1 ‚àí Œ±)-confidence interval has length
O(cŒ± œÉ 2 /(nœÉmin (Œ£))) provided n ‚â• C1 (p + ln(1/ŒΩ))
for some sufficiently large constant C1 . Furthermore,
there exists a constant C2 such that w.p. ‚â• 1 ‚àí Œ± ‚àí ŒΩ
OLS n
(correctly) rejects the null hypothesis provided
n ‚â•
o
2
c2Œ± +œÑŒ±
œÉ2
max C1 (p + ln(1/ŒΩ)), p + C2 Œ≤ 2 ¬∑ œÉmin (Œ£) , where cŒ±
j
Rc
is the number for which ‚àícŒ±Œ± PDFTn‚àíp (x)dx = 1 ‚àí Œ±.

3. OLS over Projected Data
In this section we deal with the output of Algorithm 1
in the special case where Algorithm 1 outputs matrix
unaltered and so we work with RA.
To clarify, the setting is as follows. We denote A = [X; y ]
the column-wise concatenation of the (n √ó (d ‚àí 1))-matrix
X with the n-length vector y . (Clearly, we can denote
any column of A as y and any subset of the remaining
columns as the matrix X.) We therefore denote the output
RA = [RX; Ryy ] and for simplicity we denote M = RX
and p = d ‚àí 1. We denote the SVD decomposition of
X = U Œ£V T . So U is an orthonormal basis for the columnspan of X and as X is full-rank V is an orthonormal basis
for Rp . Finally, in our work we examine the linear regression problem derived from the projected data. That is, we
denote
Œ≤ = (X T RT RX)‚àí1 (RX)T (Ryy ) = Œ≤ + (RX)+ Ree
Œ≤ÃÉ
œÉÃÉ 2 =

r
kŒ∂ÃÉŒ∂ k2 , with Œ∂ÃÉŒ∂ =
r‚àíp

‚àö1 Ry
y
r

‚àí

‚àö1 (RX)Œ≤ÃÉ
Œ≤
r

(3)
(4)

We now give our main theorem, for estimating the t-values
Œ≤ and œÉÃÉ.
based on Œ≤ÃÉ
5

Theorem 2.2 also illustrates how we ‚Äúseparate‚Äù the two
sources of privacy. In this case, ŒΩ bounds the probability of bad
events that depend to sampling the rows of X, and Œ± bounds the
probability of a bad event that depends on the sampling of the y
coordinates.

Theorem 3.1. Let X be a (n √ó p)-matrix, and parameters Œ≤ ‚àà Rp and œÉ 2 are such that we generate the vector
Œ≤ + e with each coordinate of e sampled indepeny = XŒ≤
dently from N (0, œÉ 2 ). Assume Algorithm 1 projects the
matrix A = [X; y ] without altering it. Fix ŒΩ ‚àà (0, 1/2)
and r = p + ‚Ñ¶(ln(1/ŒΩ)). Fix coordinate j. Then we have
Œ≤ and œÉÃÉ 2 as in Equations (3)
that w.p. ‚â• 1 ‚àí ŒΩ deriving Œ≤ÃÉ
Œ≤ÃÉ ‚àíŒ≤
and (4), the pivot quantity tÃÉ(Œ≤j ) = q Tj T j ‚àí1 has a
œÉÃÉ

(X R RX)j,j

distribution D satisfying e‚àía PDFTr‚àíp (x) ‚â§ PDFD (x) ‚â§
ea PDFTr‚àíp (e‚àía x) for any x ‚àà R, where we denote a =
r‚àíp
n‚àíp .
The implications of Theorem 3.1 are immediate: all estimations one can do based on the t-values from the true data
X, y , we can now do based on tÃÉ modulo an approximation
r‚àíp
factor of exp( n‚àíp
). In particular, Theorem 3.1 enables us
Œ≤.
to deduce a corresponding confidence interval based on Œ≤ÃÉ
Corollary 3.2. In the same setting as in Theorem 3.1, w.p.
‚â• 1 ‚àí ŒΩ we have the following. Fix any Œ± ‚àà (0, 12 ). Let cÃÉŒ±
denote the number s.t. the interval (cÃÉŒ± , ‚àû) contains Œ±2 e‚àía
probability
mass q
of the Tr‚àíp -distribution.
Then Pr[Œ≤j ‚àà


Œ≤ÃÉj ¬± ea ¬∑ cÃÉŒ± ¬∑ œÉÃÉ

(X T RT RX)‚àí1
j,j ] ‚â• 1 ‚àí Œ±.

6

We compare the confidence interval of Corollary 3.2
to the confidence intervalqof the standard OLS model,
Œ∂k
(X T X)‚àí1
As R is a JLwhose length is cŒ± ‚àökŒ∂
j,j .
n‚àíp
matrix, known results regarding
the
JL
transform give
q

that kŒ∂ÃÉŒ∂ k = Œò (kŒ∂Œ∂ k), and that (r ‚àí p)(X T RT RX)‚àí1
j,j =

q
(X T X)‚àí1
We therefore have that
Œò
j,j .
q
‚àö q
kŒ∂ÃÉŒ∂ k r
‚àö
œÉÃÉ (X T RT RX)‚àí1
=
(X T RT RX)‚àí1
=
j,j
j,j
r‚àíp
q
q


kŒ∂Œ∂ k
r¬∑(n‚àíp)
‚àö
(X T X)‚àí1
j,j . So for values of r
(r‚àíp)2 ¬∑ Œò
n‚àíp
r
r‚àíp

= Œò(1) we get that the confidence interval

 q
of Theorem 3.1 is a factor of Œò ccÃÉŒ±Œ± n‚àíp
r‚àíp -larger than
the standard OLS confidence interval. Observe that when
Œ± = Œò(1),
p which is the common case, the dominating
factor is (n ‚àí p)/(r ‚àí p). This bound intuitively makes
sense: we have contracted n observations to r observations, hence our model is based on confidence intervals
derived from Tr‚àíp rather than Tn‚àíp .
for which

In the supplementary material we give further discussion,
in which we compare our work to the more straight-forward
bounds one gets by ‚Äúplugging in‚Äù Sarlos‚Äô work (2006); and
we also compare ourselves to the bounds derived from alternative works in differentially private linear regression.
6

Moreover,

this interval is essentially optimal:
r‚àíp

de-

note dÀúŒ± s.t the interval (dÀúŒ± , ‚àû) contains Œ±2 e n‚àíp probability
mass qof the Tr‚àíp -distribution.
Then Pr[Œ≤j ‚àà


‚àí1
T
T
Àú
Œ≤ÃÉj ¬± dŒ± ¬∑ œÉÃÉ (X R RX)
] ‚â§ 1 ‚àí Œ±.
j,j

Differentially Private Ordinary Least Squares

Rejecting the Null Hypothesis. Due to Theorem 3.1,
we can mimic OLS‚Äô technique for rejecting the null hyŒ≤ÃÉ
pothesis. I.e., we denote tÃÉ0 = q T jT
and re‚àí1
œÉÃÉ

(X R RX)j,j

ject the null-hypothesis if indeed the associated pÃÉ0 , denotr‚àíp
‚àí n‚àíp

ing p-value of the slightly truncated e
r‚àíp
‚àí n‚àíp

tÃÉ0 , is below

Œ±¬∑e
. Much like Theorem 2.2 we now establish a
lower bound on n so that w.h.p we end up (correctly) rejecting the null-hypothesis.
Theorem 3.3. Fix a positive definite matrix Œ£ ‚àà Rp√óp .
Fix parameters Œ≤ ‚àà Rp and œÉ 2 > 0 and a coordinate j
s.t. Œ≤j 6= 0. Let X be a matrix whose n rows are sampled
Œ≤ )i
i.i.d from N (00p , Œ£). Let y be a vector s.t. yi ‚àí (XŒ≤
is sampled i.i.d from N (0, œÉ 2 ). Fix ŒΩ ‚àà (0, 1/2) and
Œ± ‚àà (0, 1/2). Then there exist constants C1 , C2 , C3
and C4 such that when we run Algorithm 1 over [X; y ]
with parameter r w.p. ‚â• 1 ‚àí Œ± ‚àí ŒΩ we (correctly)
reject the null hypothesis using pÃÉ0 (i.e., Algorithm 1
returns matrix unaltered and we can estimate tÃÉ0
‚àí

r‚àíp

and verify thatnindeed pÃÉ0 < Œ± ¬∑ e n‚àíp
o ) provided
2
œÉ 2 (cÃÉ2Œ± +œÑÃÉŒ±
)
r ‚â• p + max C1 Œ≤ 2 œÉmin (Œ£) , C2 ln(1/ŒΩ) , and n ‚â•
j
n
o
w2
max r, C3 min{œÉmin
where
(Œ£),œÉ 2 } , C4 p ln(1/ŒΩ)
r‚àíp

cÃÉŒ± , œÑÃÉŒ± defined s.t.

PrX‚àºTr‚àíp [X > cÃÉŒ± /e n‚àíp ] =
r‚àíp

PrX‚àºN (0,1) [X > œÑÃÉŒ± /e n‚àíp ] =

r‚àíp
Œ± ‚àí n‚àíp
e
.
2

n 2 2
o
 œÉ
(Œ£A )
2
min n, B 4min
(n
‚àí
ln(1/Œ¥))
. This discussion culln(1/Œ¥)
minates in the following corollary.
2

2

+œÑÃÉŒ± )
, we
= œÉŒ≤ 2(cÃÉœÉŒ±min
(Œ£)
j 

^
thus conclude that if n ‚àí p ‚â• ‚Ñ¶ LB
and n =
2.2


q
B 2 ln(1/Œ¥)
^
‚Ñ¶ œÉ
¬∑ LB
2.2 , then the result of Theorem 3.3
min (Œ£A )
n 2 2
o
 œÉ
(Œ£A )
2
holds by setting r = min n, B 4min
(n
‚àí
ln(1/Œ¥))
.
ln(1/Œ¥)

^
Corollary 3.4. Denoting LB
2.2

It is interesting to note that when we know Œ£A , we also
have a bound on B. Recall Œ£A , the variance of the
x ‚ó¶ y). Since every sample is an independent
Gaussian (x
draw from N (00p+1 , Œ£A ) then we have an upper bound of
B 2 ‚â§ log(np)œÉmax (Œ£A ). So our lower bound on n (using
Œ∫(Œ£A ) to denote
number of Œ£Aq
) is given
by
 the condition
 
Œ∫(Œ£A ) ln(1/Œ¥)
^
^
n ‚â• max ‚Ñ¶ LB
¬∑ LB
.
2.2 , ‚Ñ¶ÃÉ
2.2

Observe, overall this result is similar in nature to many
other results in differentially private learning (Bassily et al.,
2014) which are of the form ‚Äúwithout privacy, in order to
achieve a total loss of ‚â§ Œ∑ we have a sample complexity
bound of some NŒ∑ ; and with differential
p privacy the sample complexity increases to NŒ∑ + ‚Ñ¶( NŒ∑ /).‚Äù However,
^
there‚Äôs a subtlety here worth noting. LB
2.2 is proportional
(Œ£A )
to œÉmin1(Œ£A ) but not to Œ∫(Œ£A ) = œÉœÉmax
. The additional
min (Œ£A )
dependence on œÉmax follows from the fact that differential
privacy adds noise proportional to the upper bound on the
norm of each row.

3.1. Setting the Value of r, Deriving a Bound on n
Comparing the lower bound on n given by Theorem 3.3 to
the bound ofTheorem 2.2,
 we have that the data-dependent
(cÃÉŒ± +œÑÃÉŒ± )2 œÉ 2
bound of ‚Ñ¶ Œ≤ 2 œÉmin (Œ£) should now hold for r rather than
j
n. Yet, Theorem 3.3 also introduces an additional depen2
w2
dency between n and r: we require n = ‚Ñ¶( w
œÉ 2 + œÉmin (Œ£) )
(since otherwise we do not have œÉmin (A)  w and Algorithm 1 might alter A before
projecting it) and by definition
p
w2 is proportional to r ln(1/Œ¥)/. This is precisely the
focus of our discussion in this subsection. We would like
to set r‚Äôs value as high as possible ‚Äî the larger r is, the
more observations we have in RA and the better our confidence bounds (that
depend on Tr‚àíp ) are ‚Äî while satisfying
‚àö
n = ‚Ñ¶( ¬∑min{œÉ2 ,œÉr min (Œ£)} ).
Recall that if each sample point is drawn i.i.d x ‚àº
xi ‚ó¶ yi ) is sampled from
N (00p , Œ£), then each sample (x
N (00p+1 , Œ£A ) for Œ£
defined
in
the proof of
A

 Theorem 3.3,
Œ≤
Œ£
Œ£Œ≤
that is: Œ£A =
. So, TheoŒ≤ T Œ£Œ≤
Œ≤
Œ≤ TŒ£
œÉ 2 +Œ≤


2

2

+œÑÃÉŒ± )
rem 3.3 gives the lower bound r ‚àí p = ‚Ñ¶ œÉŒ≤ 2(cÃÉœÉŒ±min
(Œ£)
j
and the following
lower
bounds
on
n:
n
‚â•
r
and
 ‚àö

B 2 ( r ln(1/Œ¥)+ln(1/Œ¥))
n = ‚Ñ¶
, which means r =
œÉmin (Œ£A )

4. Projected Ridge Regression
We now turn to deal with the case that our matrix does not
pass the if-condition of Algorithm 1. In this case, the matrix is appended
 with a d √ód-matrix which is wId√ód . DeA
noting A0 =
we have that the algorithm‚Äôs
w ¬∑ Id√ód
output is RA0 . Similarly to before, we are going to denote
d = p + 1 and decompose A = [X; y ] with X ‚àà Rn√óp and
Œ≤ + e and
y ‚àà Rn , with the standard assumption of y = XŒ≤
ei sampled i.i.d from N (0, œÉ 2 ). We now need to introduce
some additional notation. We denote the appended matrix
and vectors X 0 and y 0 s.t. A0 = [X 0 ; y 0 ]. And so, using the
output RA0 of Algorithm 1, we solve the linear regression
problem derived from ‚àö1r RX 0 and ‚àö1r Ryy 0 . I.e., we set
Œ≤ 0 = (X 0T RT RX 0 )‚àí1 (RX 0 )T (Ryy 0 )
Œ∂ 0 = ‚àö1r (Ryy 0 ‚àí RX 0Œ≤ 0 )

(5)

Sarlos‚Äô results (2006) regarding the Johnson Lindenstrauss transform give that, when R has sufficiently
many rows, solving the latter optimization problem gives
a good approximation for the solution of the optimization problem Œ≤ R = argminz kyy 0 ‚àí X 0z k2 =
arg minz kyy ‚àí Xzz k2 + w2 kzz k2 . The latter problem is

Differentially Private Ordinary Least Squares

known as the Ridge Regression problem. Invented in the
60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large. It is also
often applied in the case where X doesn‚Äôt have full rank
or is close to not having full-rank: one can show that the
minimizer Œ≤ R = (X T X + w2 Ip√óp )‚àí1 X Ty is the unique
solution of the Ridge Regression problem and that the RHS
is always well-defined.
While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution, it is not known
how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate
Œ≤ = (X T X)‚àí1 X Ty and relying on OLS).
Œ≤ R back into Œ≤ÃÇ
In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard
OLS, because access to X and y was given.
Therefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.7 Clearly,
there are situations where such confidence bounds simply
cannot be derived.Nonetheless, under additional assumptions about the data, our work can give confidence intervals
for Œ≤j , and in the case where the interval doesn‚Äôt intersect
the origin ‚Äî assure us that sign(Œ≤j0 ) = sign(Œ≤j ) w.h.p.
This is detailed in the supplementary material.
To give an overview of our analysis, we first discuss a
Œ≤ is fixed (i.e., the data is fixed
model where e = y ‚àí XŒ≤
and the algorithm is the sole source of randomness), and
Œ≤.
prove that in this model Œ≤ 0 is as an approximation to Œ≤ÃÇ

so, in summary, in Section C we give conditions under
which the
q length of the interval I is dominated by the
Œ∂0k
c0Œ± ‚àökŒ∂r‚àíp
r(M 0T M 0 )‚àí1
j,j factor derived from Theorem 4.1.

5. Confidence Intervals for ‚ÄúAnalyze Gauss‚Äù
In this section we analyze the ‚ÄúAnalyze Gauss‚Äù algorithm
of Dwork et al (2014). Algorithm 2 works by adding random Gaussian noise to AT A, where the noise is symmetric
with each coordinate above
 the diagonal
 sampled i.i.d from
2
2
4 log(1/Œ¥)
N (0, ‚àÜ ) with ‚àÜ = O B
. Using the same no2
tation for a sub-matrix of A as Ô£´
[X; y ] as before, we
Ô£∂ denote
the output of Algorithm 2 as

where c0Œ± denotes the number such that (‚àíc0Œ± , c0Œ± ) contains
1 ‚àí Œ± mass of the Tr‚àíp -distribution.
However, our goal remains to argue that Œ≤j0 serves as a good
approximation for Œ≤j . To that end, we combine the standard OLS confidence interval ‚Äî which says that w.p. ‚â•
1 ‚àí Œ± over the randomness of picking er
in the homoscedastic model we have |Œ≤j ‚àí Œ≤ÃÇj | ‚â§ cŒ± kŒ∂Œ∂ k

(X T X)‚àí1
j,j
n‚àíp

^
TX
X
TX
yg

we approximate Œ≤ and kŒ∂Œ∂ k by Œ≤e =



g
Ty Ô£∑
X
Ô£∑
Ô£∑.
Ô£∏
Ty
yg

^
TX
X

‚àí1

Thus,

g
Ty and
X

g
^
]
e + Œ≤e T X
e resp. We now argue
Ty ‚àí 2 y
TXŒ≤
TX Œ≤
Œ∂ k2 = yg
kŒ∂
g
that it is possible to use Œ≤ej and kŒ∂Œ∂ k2 to get a confidence
interval for Œ≤j under certain conditions.
Theorem 5.1. Fix Œ±, ŒΩ ‚àà (0, 12 ). Assume that there exists
p
Œ∑ ‚àà (0, 21 ) s.t. œÉmin (X T X) > ‚àÜ p ln(1/ŒΩ)/Œ∑. Under the
homoscedastic model, given Œ≤ and œÉ 2 , if we assume also
T
‚àí1 T
Œ≤ k ‚â§ B and kŒ≤ÃÇ
Œ≤ k = k(X
that kŒ≤
 X) X y k ‚â§ B, then


w.p. ‚â• 1 ‚àí Œ± ‚àí ŒΩ it holds that Œ≤j ‚àí Œ≤ej  is at most
s


‚àí1
‚àí2
p
^
^
TX
TX
O œÅ¬∑
+ ‚àÜ p ln(1/ŒΩ) ¬∑ X
X
ln(1/Œ±)
j,j

n√óp

Œ≤ =
Theorem 4.1. Fix X ‚àà R
and y ‚àà R. Define Œ≤ÃÇ
X +y and Œ∂ = (I ‚àí XX + )yy . Let RX 0 = M 0 and Ryy 0
denote the result of applying Algorithm 1 to the matrix A =
[X; y ] when the algorithm appends the data with a w ¬∑ I
matrix. Fix a coordinate j and any Œ± ‚àà (0, 1/2). When
computing Œ≤ 0 and Œ∂ 0 as
have that w.p. ‚â• 1 ‚àí
 in (5), we q

r
0
0 Œ∂0
Œ± it holds that Œ≤ÃÇj ‚àà Œ≤j ¬± cŒ± kŒ∂ k r‚àíp
¬∑ (M 0T M 0 )‚àí1
j,j

Ô£¨
Ô£¨
Ô£¨
Ô£≠

j,j

r
+‚àÜ

‚àí2

‚àö
^
TX
X
j,j ¬∑ ln(1/ŒΩ) ¬∑ (B p + 1)



where œÅ is w.h.p an upper bound on œÉ (details appear in
the Supplementary material).
Œ≤ k ‚â§ B and kŒ≤ÃÇ
Œ≤k ‚â§ B
Note that the assumptions that kŒ≤
are fairly benign once we assume each row has bounded
l2 -norm. The key assumption is that X T X is well-spread.
Yet in the model where each row in X is sampled i.i.d
from N (00, Œ£), this assumption merely
‚àö means that n is large
enough ‚Äî namely, that n = ‚Ñ¶ÃÉ(

‚àÜ p ln(1/ŒΩ)
Œ∑¬∑œÉmin (Œ£) ).

6. Experiment: t-Values of Output

‚Äî with

the confidence interval
q of Theorem 4.10 above,
q and denotkŒ∂Œ∂ k
kŒ∂Œ∂ k
‚àí1
0
T
ing I = cŒ± ‚àön‚àíp (X X)j,j + cŒ± ‚àör‚àíp r(M 0T M 0 )‚àí1
j,j
we have that Pr[|Œ≤j0 ‚àí Œ≤j | = O(I)] ‚â• 1 ‚àí Œ±. And
7
Note: The naƒ±Ãàve approach of using RX 0 and Ryy 0 to interpolate RX and Ryy and then apply Theorem 3.1 using these estimations of RX and Ryy ignores the noise added from appending the
matrix A into A0 , and therefore leads to inaccurate estimations of
the t-values.

Goal. We set to experiment with the outputs of Algorithms 1 and 2. While Theorem 3.1 guarantees that computing the t-value from the output of Algorithm 1 in the
matrix unaltered case does give a good approximation of the t-value ‚Äì we were wondering if by computing
the t-value directly from the output we can (a) get a good
approximation of the true (non-private) t-value and (b) get
the same ‚Äúhigher-level conclusion‚Äù of rejecting the nullhypothesis. The answers are, as ever, mixed. The two main

Differentially Private Ordinary Least Squares

observations we do notice is that both algorithms improve
as the number of examples increases, and that Algorithm 1
is more conservative then Algorithm 2.
Setting. We tested both algorithms in two settings. The
first is over synthetic data. Much like the setting in Theorems 2.2 and 3.3, X was generated using p = 3 independent normal Gaussian features, and y was generated using
the homoscedastic model. We chose Œ≤ = (0.5, ‚àí0.25, 0)
so the first coordinate is twice as big a the second but
of opposite sign, and moreover, y is independent of the
3rd feature. The variance of the label is also set to 1,
and so the variance of the homosedastic noise equals to
œÉ 2 = 1 ‚àí (0.5)2 ‚àí (‚àí0.25)2 . The number of observations
n ranges from n = 1000 to n = 100000.
The second setting is over real-life data. We ran the two
algorithms over diabetes dataset collected over ten years
(1999-2008) taken from the UCI repository (Strack et al.,
2014). We truncated the data to 4 attributes: sex (binary),
age (in buckets of 10 years), number medications (numeric,
0-100), and a diagnosis (numeric, 0-1000). Naturally, we
added a 5th column of all-1 (intercept). Omitting any entry with missing or non-numeric values on these nine attributes we were left with N = 91842 entries, which we
shuffled and fed to the algorithm in varying sizes ‚Äî from
n = 30, 000 to n = 90, 000. Running OLS over the entire
N observation yields Œ≤ ‚âà (14.07, 0.54, ‚àí0.22, 482.59),
and t-Values of (10.48, 1.25, ‚àí2.66, 157.55).

get fairly high (in magnitude) t-values. As the result, we
falsely reject the null-hypothesis based on the t-value of
Analyze Gauss quite often, even for large values of n. This
is shown in Figure 1b. Additional figures (including plotting the distribution of the t-value approximations) appear
in the supplementary material.
The results show that t-value approximations that do not
take into account the inherent randomness in the DPalgorithms lead to erroneous conclusions. One approach
would be to follow the more conservative approach we advocate in this paper, where Algorithm 1 may allow you to
get true approximation of the t-values and otherwise reject the null-hypothesis only based on the confidence interval (of Algorithm 1 or 2) not intersecting the origin. Another approach, which we leave as future work, is to replace the T -distribution with a new distribution, one that
takes into account the randomness in the estimator as well.
This, however, has been an open and long-standing challenge since the first works on DP and statistics (see (Vu
& Slavkovic, 2009; Dwork & Lei, 2009)) and requires we
move into non-asymptotic hypothesis testing.

The Algorithms. We ran a version of Algorithm 1 that uses
a DP-estimation of œÉmin , and finds the largest r the we can
use without altering the input, yet if this r is below 25 then
it does alter the input and approximates Ridge regression.
We ran Algorithm 2 verbatim. We set  = 0.25 and Œ¥ =
10‚àí6 . We repeated each algorithm 100 times.
(a) Synthetic data, coordinate Œ≤1

Results. We plot the t-values we get from Algorithms 1
and 2 and decide to reject the null-hypothesis based on tvalue larger than 2.8 (which corresponds to a fairly conservative p-value of 0.005). Not surprisingly, as n increases,
the t-values become closer to their expected value ‚Äì the tvalue of Analyze Gauss is close to the non-private
p t-value
and the t-value from Algorithm 1 is a factor of nr smaller
as detailed above (see after Corollary 3.2). As a result,
when the null-hypothesis is false, Analyze Gauss tends to
produce larger t-values (and thus reject the null-hypothesis)
for values of n under which Algorithm 1 still does not reject, as shown in Figure 1a. This is exacerbated in real
data setting, where its actual least singular value (‚âà 500) is
fairly small in comparison to its size (N = 91842).
However, what is fairly surprising is the case where the
null-hypothesis should not be rejected ‚Äî since Œ≤j = 0
(in the synthetic case) or its non-private t-value is close
to 0 (in the real-data case). Here, the Analyze Gauss‚Äô tvalue approximation has fairly large variance, and we still

(b) Synthetic data, coordinate Œ≤3
Figure 1. Correctly and Wrongly Rejecting the Null-Hypothesis

Differentially Private Ordinary Least Squares

Acknowledgements
The bulk of this work was done when the author was a
postdoctoral fellow at Harvard University, supported by
NSF grant CNS-123723; and also an unpaid collaborator
on NSF grant 1565387. The author wishes to wholeheartedly thank Prof. Salil Vadhan, for his tremendous help in
shaping this paper. The author would also like to thank
Prof. Jelani Nelson and the members of the ‚ÄúPrivacy Tools
for Sharing Research Data‚Äù project at Harvard University (especially James Honaker, Vito D‚ÄôOrazio, Vishesh
Karwa, Prof. Kobbi Nissim and Prof. Gary King) for many
helpful discussions and suggestions; as well as Abhradeep
Thakurta for clarifying the similarity between our result.
Lastly the author thanks the anonymous referees for many
helpful suggestions in general and for a reference to (Ullman, 2015) in particular.

References
Agresti, A. and Finlay, B. Statistical Methods for the Social
Sciences. Pearson P. Hall, 2009.
Bassily, R., Smith, A., and Thakurta, A. Private empirical
risk minimization: Efficient algorithms and tight error
bounds. In FOCS, 2014.
Blocki, J., Blum, A., Datta, A., and Sheffet, O. The
Johnson-Lindenstrauss transform itself preserves differential privacy. In FOCS, 2012.
Chaudhuri, Kamalika and Hsu, Daniel J. Convergence rates
for differentially private statistical estimation. In ICML,
2012.

Dwork, Cynthia, Su, Weijie, and Zhang, Li. Private false
discovery rate control. CoRR, abs/1511.03803, 2015.
Hoerl, A. E. and Kennard, R. W. Ridge regression: Biased
estimation for nonorthogonal problems. Technometrics,
12:55‚Äì67, 1970.
Kasiviswanathan, S., Lee, H., Nissim, K., Raskhodnikova,
S., and Smith, A. What can we learn privately? In FOCS,
2008.
Kifer, Daniel, Smith, Adam D., and Thakurta, Abhradeep.
Private convex optimization for empirical risk minimization with applications to high-dimensional regression. In
COLT, 2012.
Laurent, B. and Massart, P. Adaptive estimation of a
quadratic functional by model selection. The Annals of
Statistics, 28(5), 10 2000.
Ma, E. M. and Zarowski, Christopher J. On lower bounds
for the smallest eigenvalue of a hermitian positivedefinite matrix. IEEE Transactions on Information Theory, 41(2), 1995.
Muller, Keith E. and Stewart, Paul W. Linear Model Theory: Univariate, Multivariate, and Mixed Models. John
Wiley & Sons, Inc., 2006.
Pilanci, M. and Wainwright, M. Randomized sketches of
convex programs with sharp guarantees. In ISIT, 2014a.
Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. CoRR, abs/1411.0347, 2014b.

Chaudhuri, Kamalika, Monteleoni, Claire, and Sarwate,
Anand D. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12, 2011.

Rao, C. Radhakrishna. Linear statistical inference and its
applications. Wiley, 1973.

Duchi, John C., Jordan, Michael I., and Wainwright, Martin J. Local privacy and statistical minimax rates. In
FOCS, pp. 429‚Äì438, 2013.

Rogers, Ryan M., Vadhan, Salil P., Lim, Hyun-Woo, and
Gaboardi, Marco. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In ICML, pp. 2111‚Äì2120, 2016.

Dwork, C. and Lei, J. Differential privacy and robust statistics. In STOC, 2009.
Dwork, Cynthia, Kenthapadi, Krishnaram, McSherry,
Frank, Mironov, Ilya, and Naor, Moni. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT, 2006a.
Dwork, Cynthia, Mcsherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In TCC, 2006b.
Dwork, Cynthia, Talwar, Kunal, Thakurta, Abhradeep, and
Zhang, Li. Analyze gauss - optimal bounds for privacy preserving principal component analysis. In STOC,
2014.

Rudelson, Mark and Vershynin, Roman. Smallest singular
value of a random rectangular matrix. Comm. Pure Appl.
Math, pp. 1707‚Äì1739, 2009.
SarloÃÅs, T. Improved approx. algs for large matrices via random projections. In FOCS, 2006.
Sheffet, O. Private approximations of the 2nd-moment matrix using existing techniques in linear regression. CoRR,
abs/1507.00056, 2015. URL http://arxiv.org/
abs/1507.00056.
Smith, Adam D. Privacy-preserving statistical estimation
with optimal convergence rates. In STOC, pp. 813‚Äì822,
2011.

Differentially Private Ordinary Least Squares

Strack, B., DeShazo, J., Gennings, C., Olmo, J., Ventura,
S., Cios, K., and Clore, J. Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000
clinical database patient records. BioMed Research International, 2014:11 pages, 2014.
Tao, T. Topics in Random Matrix Theory. American Mathematical Soc., 2012.
Thakurta, Abhradeep and Smith, Adam. Differentially private feature selection via stability arguments, and the robustness of the lasso. In COLT, 2013.
Tikhonov, A. N. Solution of incorrectly formulated problems and the regularization method. Soviet Math. Dokl.,
4, 1963.
Uhler, Caroline, Slavkovic, Aleksandra B., and Fienberg, Stephen E. Privacy-preserving data sharing for
genome-wide association studies. Journal of Privacy
and Confidentiality, 2013. Available at: http://
repository.cmu.edu/jpc/vol5/iss1/6.
Ullman, J. Private multiplicative weights beyond linear
queries. In PODS, 2015.
Vu, D. and Slavkovic, A. Differential privacy for clinical
trial data: Preliminary evaluations. In ICDM, 2009.
Wang, Yue, Lee, Jaewoo, and Kifer, Daniel.
entially private hypothesis testing, revisited.
abs/1511.03376, 2015.

DifferCoRR,

Xi, B., Kantarcioglu, M., and Inan, A. Mixture of gaussian models and bayes error under differential privacy.
In CODASPY. ACM, 2011.
Zhou, S., Lafferty, J., and Wasserman, L. Compressed regression. In NIPS, 2007.

