Tensor Decomposition via Simultaneous Power Iteration

Po-An Wang 1 Chi-Jen Lu 1

Abstract
Tensor decomposition is an important problem
with many applications across several disciplines, and a popular approach for this problem
is the tensor power method. However, previous
works with theoretical guarantee based on this
approach can only find the top eigenvectors one
after one, unlike the case for matrices. In this
paper, we show how to find the eigenvectors simultaneously with the help of a new initialization
procedure. This allows us to achieve a better running time in the batch setting, as well as a lower
sample complexity in the streaming setting.

1. Introduction
Tensors have long been successfully used in several disciplines, including neuroscience, phylogenetics, statistics,
signal processing, computer vision, and data mining. They
are used to model multi-relational or multi-modal data, and
their decompositions often reveal some underlying structures behind the observed data. See (Kolda & Bader, 2009)
for a survey of such results. Recently, they have found
applications in machine learning, particularly for learning
various latent variable models (Anandkumar et al., 2012;
Chaganty & Liang, 2014; Anandkumar et al., 2014a).
One popular decomposition method in such applications
is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components. This is similar to the singular value decomposition
(SVD) of matrices, and a popular approach for SVD is the
power method, which is well-understood and has nice theoretical guarantee. As tensors can be seen as generalization
of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could
inherit the success from the matrix case. However, the situation turns out to be much more complicated for tensors
(see e.g. the discussion in (Anandkumar et al., 2014a)),
1
Academia Sinica, Taiwan. Correspondence to: Po-An Wang
<poanwang@iis.sinica.edu.tw>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

and in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013). Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable
again. In particular, for tensors having orthogonal decomposition, Anandkumar et al. (2014a) provided an efficient
algorithm based on the tensor power method with theoretical guarantee. Still, as we will discuss later in Section 2, the
seemingly subtle change of going from matrices to tensors
makes some significant differences for the power method.
The first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely
converge to the top singular vector, we have much less control of where the convergence goes in the tensor case. Consequently, most previous works based on the tensor power
method with theoretical guarantee, such as (Anandkumar
et al., 2014a;b; Wang & Anandkumar, 2016), require much
more complicated procedures. In particular, they can only
find the top k eigenvectors one by one, each time with the
power method applied to a modified tensor, deflated from
the original tensor according to the previously found vectors. Moreover, to find each vector, they need to sample
several initial vectors and apply the power method on all
of them, before selecting just one from them. In contrast,
algorithms for matrices such as (Mitliagkas et al., 2013;
Hardt & Price, 2014) are much simpler, as they can find
the k vectors simultaneously by applying the power method
only on k random initial vectors. The second difference, on
the other hand, has a beneficial effect, which allows the tensor power method to converge exponentially faster than the
matrix one when starting from good initial vectors. Then a
natural question is: can we inherit the best of both worlds?
Namely, is it possible to have a simple algorithm which can
find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?
Our Results. As in previous works, we consider the
slightly harder scenario in which we only have access to
a noisy version of the tensor we want to decompose. This
arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from
some empirical average of the observed data. Our main
contribution is to answer the above question affirmatively.
First, we consider the batch setting in which we assume

Tensor Decomposition via Simultaneous Power Iteration

that the given noisy tensor is stored somewhere and can be
accessed whenever we want to. In this setting, we identify a sufficient condition such that if we have k initial vectors satisfying this condition, then we can apply the tensor
power method on them simultaneously, which will come
within some distance Œµ to the eigenvectors in O(log log 1Œµ )
iterations, with parameters related to eigenvalues considered as constant. To apply such a result, we need an efficient way to find such initial vectors. We show how to
do this by choosing a good direction to project the tensor
down to a matrix while preserving the eigengaps, and then
applying the matrix power method for only a few iterations
just to obtain vectors meeting that sufficient condition. The
number of iterations needed here is only O(log d), independent of Œµ, where d is the dimension of the eigenvectors.
The result stated above is for orthogonal tensors. On the
other hand, it is known that an nonorthogonal tensor with
linearly independent eigenvectors can be converted into an
orthogonal one with the help of some whitening matrix.
However, previous works usually pay little attention on
how to find such a whitening matrix efficiently. According to (Anandkumar et al., 2014a), one way is via SVD on
some second moment matrix, but doing this using the matrix power method would take longer to converge compared
to the tensor power method which would then be applied on
the whitened tensor. Our second contribution is to provide
an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.
While most previous works on tensor decomposition focus
on the batch setting, storing even a tensor of order three requires ‚Ñ¶(d3 ) space, which is infeasible for a large d. We
show to avoid this in the streaming setting, with a stream
of data arriving one at a time, which is the only source of
information about the tensor. We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension
d. To achieve an approximation error Œµ, the total number of
samples we need is O(kd log d + Œµ12 log(d log 1Œµ )).
Related Works There is a huge literature on tensor decomposition, and it is beyond the scope of this paper to
give a comprehensive survey. Thus, we only compare our
results to the most related ones, particularly those based
on the power method. While different works may focus
on different aspects, we are most interested in understanding how the error parameter Œµ affects various performance
measures, having in mind a small Œµ.
First, the batch algorithm of Anandkumar et al. (2014a),
using a better analysis in (Wang & Anandkumar, 2016),
runs in time about O((k 2 log k)(log log 1Œµ )), which can be
made to run in O(k(log log 1Œµ )) iterations in parallel, while
ours are O(k log log 1Œµ ) and O(log log 1Œµ ), respectively. On

the other hand, one advantage of their algorithm is that its
running time does not depend on the eigengaps, while ours
has the dependence hidden above as some constant.
In the streaming setting, Wang & Anandkumar (2016)
provided an algorithm using O(dk log k) memory and
O( Œµk2 log( dŒµ )) samples, while ours only uses O(dk) memory and O( Œµ12 log(d log log 1Œµ )) samples.1 Nevertheless, the
sample complexity of Wang & Anandkumar (2016) is also
independent of the eigengaps, while ours has the dependence hidden above as a constant factor.
As one can see, our algorithms, which find the k eigenvectors simultaneously, allow us to save a factor of k in the
time complexity and the sample complexity, although our
bounds may become worse when the eigengaps are small.
Thus, our algorithms can be seen as new options for users
to choose from, depending on the data they are given.
Although not directed related, let us also compare to previous works on SVD. Two related ones, both based on the
simultaneous matrix power method, are the batch algorithm
of (Hardt & Price, 2014) which converges in O(log dŒµ ) iterations, and the streaming algorithm of (Li et al., 2016)
which requires O( Œµ12 log(d log 1Œµ )) samples. Both bounds
are worse than ours and also depend on the eigengaps.
Thus, although one approach for orthogonal tensor decomposition is to reduce it to a matrix SVD problem, this does
not appear to result in better performance than ours.
Finally, comparisons of the tensor power method with other
approaches can be found in works such as (Anandkumar
et al., 2014a; Wang & Anandkumar, 2016). For example,
the online SGD approach of (Ge et al., 2015) works only
for tensors of even orders and its sample complexity has a
poor dependency on the dimension d.

Organization of the paper. First, we provide some preliminaries in Section 2. Then we present our batch algorithm for orthogonal and symmetric tensors of order three
in Section 3, and then for general orthogonal tensors in Section 4. In Section 5, we introduce our whitening procedure
for nonorthogonal but symmetry tensors. Finally, in Section 6, we present our algorithm for the streaming setting.
Due to the space limitation, we will move all our proofs to
the appendix in the supplementary material.

2. Preliminaries
Let us first introduce some notations and definitions which
we will use later. Let R denote the set of real numbers
and N the set of positive integers. Let N (0, 1) denote the
standard normal distribution with mean 0 and variance 1,
1
We use a different input distribution from theirs. The bound
listed here is modified from theirs according to our distribution.

Tensor Decomposition via Simultaneous Power Iteration

and let N d (0, 1), for d ‚àà N, denote the d-variate one which
has each of its d dimensions sampled independently from
N (0, 1). For d ‚àà N, let [d] denote the set {1, . . . , d}. For
a vector x, let kxk denote its L2 norm. For d ‚àà N, let Id
denote the d √ó d identity matrix. For a matrix A ‚àà Rd√ók ,
let Ai , for i ‚àà [k], denote its i-th column, and let Ai,j , for
j ‚àà [d], be the j-th entry of Ai . Moreover, for a matrix A,
let A> denote its transpose, and define its norm as kAk =
0
maxx‚ààRk kA¬∑xk
kxk , using the convention that 0 = 0.

method, which works P
as follows. Suppose we are given
a d √ó d matrix M = i‚àà[d] Œªi ¬∑ ui ‚äó ui , with nonnegative Œª1 > Œª2 ‚â• ¬∑ ¬∑ ¬∑ and orthonormal vectors
P u1 , . . . , u d .
The power method starts with some q (0) = i‚àà[d] ci ¬∑ ui ,
usually chosen randomly, and then repeatedly performs the
update q (t) = M ¬∑ q (t‚àí1) , which results in

Tensors are the focus of our paper, which can be seen as
generalization of matrices to higher orders. For simplicity of presentation, we will use symmetric tensors of order
three as examples in the following definitions. A real tensor T of order three can be seen as an three-dimensional
array in Rd√ód√ód , for some d ‚àà N, with its (i, j, k)-th
entry denoted as Ti,j,k . For such a tensor T and three
matrices A ‚àà Rd√óm1 , B ‚àà Rd√óm2 , C ‚àà Rd√óm3 , let
T (A, B, C) be the P
tensor in Rm1 √óm2 √óm3 , with its (a, b, c)th entry defined as i,j,k‚àà[d] Ti,j,k Aa,i Bb,j Cc,k . The norm
of a tensor T we will use is the operator norm: kT k =
|T (x,y,z)|
maxx,y,z‚ààRd kxkkykkzk
.

Note that for any i 6= 1, as Œªi < Œª1 , the coefficient Œªti ci will
soon become much smaller than the coefficient Œªt1 c1 if c1
is not too small, which is likely to happen for a randomly
chosen q (0) . This has the effect that after normalization,
q (t) /kq (t) k approaches u1 quickly.
P
Now consider a tensor T =
i‚àà[d] Œªi ¬∑ ui ‚äó ui ‚äó ui ,
with nonnegative Œª1 > Œª2 ‚â• ¬∑ ¬∑ ¬∑ and orthonormal
vectors u1 , . . . , ud . The tensor version of the power
method
again starts from a randomly chosen q (0) =
P
c
i‚àà[d] i ¬∑ ui , but now repeatedly performs the update
(t)
q = T (Id , q (t‚àí1) , q (t‚àí1) ), which in turn results in

The tensor decomposition problem. In this problem,
there is a tensor T with some unknown decomposition
X
T =
Œªi ¬∑ ui ‚äó ui ‚äó ui ,

q (t) =

i‚àà[d]

with Œªi ‚â• 0 and ui ‚àà Rd for any i ‚àà [d]. Then given some
k ‚àà [d] and Œµ ‚àà (0, 1), our goal is to find ŒªÃÇi and uÃÇi with
|ŒªÃÇi ‚àí Œªi | ‚â§ Œµ and kuÃÇi ‚àí ui k ‚â§ Œµ, for every i ‚àà [k].
We will assume that
X
Œªi ‚â§ 1 and ‚àÄi ‚àà [k] : Œªi > Œªi+1 .

(1)

X

q (t) =



X

(t‚àí1)
Œªi u>
q
¬∑ ui =
Œªti ci ¬∑ ui .
i

i‚àà[d]

X

i‚àà[d]


2
X
2t
(t‚àí1)
Œªi u>
q
¬∑ ui =
Œª‚àí1
i
i (Œªi ci ) ¬∑ ui .

i‚àà[d]

i‚àà[d]

The coefficient of each ui now has a different form from
the matrix case, and this leads to the following two effects.
First, one now has much less control on what q (t) /kq (t) k
converges to. In fact, it can converge to any ui 6= u1 if
ui has the largest value of Œªi |ci |, which happens with a
good probability if Œªi is not much smaller than Œª1 . Consequently, to find the top k vectors u1 , . . . , uk , previous
works based on the power method all need much more
complicated procedures (Anandkumar et al., 2014a), compared to those for matrices, as discussed in the introduction.

i‚àà[d]

As in previous works, we consider a slightly harder version of the problem, in which we only have access to some
noisy version of T , instead of the noiseless T . We will
consider the following two settings. In the batch setting,
we have access to some TÃÑ = T + Œ¶ for the whole time,
for some perturbation tensor Œ¶. In the streaming setting,
we have a stream of data points x1 , x2 , . . . arriving one by
one, which provide the only information we have about T ,
with each xœÑ ‚àà Rd allowing us to compute some TÃÑœÑ with
mean E[TÃÑ ] = T . In this streaming setting, we are particularly interested in the case of a large d which prohibits one
to store a tensor of size d3 in memory.
Power Method: Matrices versus Tensors. Note that a
tensor of order two is just a matrix, and a popular approach for decomposing matrices is the so-called power

On the other hand, the different form of q (t) has the beneficial effect that the convergence is now exponentially faster
than in the matrix case. More precisely, if Œªi |ci | < Œªj |cj |,
t
t
than the gap between the coefficients (Œªi ci )2 and (Œªj cj )2
is now amplified much faster. We will show how to inherit
this nice property of faster convergence but at the same time
avoid the difficulty discussed above.

3. Orthogonal and Symmetric Tensors of
Order Three
In this section, we focus on the special case in which the
tensors to be decomposed are orthogonal, symmetric, and
of order three. Formally, there is an underlying tensor
T =

X
i‚àà[d]

Œªi ¬∑ ui ‚äó ui ‚äó ui ,

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 1 Robust tensor power method
Input: Tensor TÃÑ ‚àà Rd√ód√ód and parameters k, L, S, N .
Initialization Phase:
(0)
(0)
Sample w1 , . . . , wP
‚àº N d (0, 1).
L , Y 1 , . . . , Yk
1
Compute wÃÑ = L j‚àà[L] TÃÑ (Id , wj , wj ).
Compute MÃÑ = TÃÑ (Id , Id , wÃÑ).
Factorize Y (0) as Z (0) ¬∑ R(0) by QR decomposition.
for s = 1 to S do
Compute Y (s) = MÃÑ ¬∑ Z (s‚àí1) .
Factorize Y (s) as Z (s) ¬∑ R(s) by QR decomposition.
end for
Tensor Power Phase:
Let Q(0) = Z (S) .
for t = 1 to N do
(t)
(t‚àí1)
(t‚àí1)
Compute Yj = TÃÑ (Id , Qj
, Qj
), ‚àÄj ‚àà [k].
Factorize Y (t) as Q(t) ¬∑ R(t) by QR decomposition.
end for
(N )
Output: uÃÇj = Qj and ŒªÃÇj = TÃÑ (uÃÇj , uÃÇj , uÃÇj ), ‚àÄj ‚àà [k].

with orthonormal vectors ui ‚Äôs and real Œªi ‚Äôs satisfying the
condition (1). Then given k ‚àà [d] and Œµ ‚àà (0, 1), our goal
is to find approximates to those Œªi and ui within distance Œµ,
but we only have access to some noisy tensor TÃÑ = T + Œ¶
for some symmetric perturbation tensor Œ¶.
Our algorithm is given in Algorithm 1, which consists of
two phases: the initialization phase and the tensor power
phase. The main phase is the tensor power phase, which
we will discuss in detail in Subsection 3.1. For our tensor power phase to work, it needs to have a good starting
point. This is provided by the initialization phase, which
we will discuss in detail in Subsection 3.2. Through these
two subsections, we will prove Theorem 1 below, which
summarizes the performance of our algorithm, according
to the following parameters of the tensor:
Œª2i ‚àí Œª2i+1
Œªi ‚àí Œªi+1
and ‚àÜ = min
.
2
Œªi
4
i‚àà[k]
i‚àà[k]

Œ≥ = min

Theorem 1. Suppose Œµ ‚â§ Œª2k and the perturbation tensor
2
0‚àÜ
‚àö , ‚àÜ , Œ±‚àö
has the bound kŒ¶k ‚â§ min{ 2‚àÜŒµ
} for a small
k 3d
dk
enough constant Œ±0 . Then for some L = O( Œ≥12 log d), S =
O( Œ≥1 log d), and N = O(log( Œ≥1 log 1Œµ )), our Algorithm 1
with high probability will output uÃÇi and ŒªÃÇi with kuÃÇi ‚àíui k ‚â§
Œµ and |ŒªÃÇi ‚àí Œªi | ‚â§ Œµ for every i ‚àà [k].
Let us make some remarks about the theorem. First, the L
samples are used to compute wÃÑ and MÃÑ , which can be done
in a parallel way. Second, our parameter Œ≥ is related to a
i+1
parameter Œ≥ 0 = mini‚àà[k] Œªi ‚àíŒª
used in (Hardt & Price,
Œªi
2014), and it is easy to verify that Œ≥ ‚â• Œ≥ 0 . Thus, our algorithm for tensors converges in O( Œ≥1 log d + log( Œ≥1 log 1Œµ ))
rounds, which is faster than the O( Œ≥10 log d + Œ≥10 log 1Œµ )

rounds of (Hardt & Price, 2014) for matrices. Note that
our dependence on the error parameter Œµ is exponentially
smaller than that of (Hardt & Price, 2014), which means
that for a small Œµ, we can decompose tensors much faster
than matrices. Finally, compared to previous works on
tensors, our convergence time, for a small Œµ is about
O(log log 1Œµ ) while those in (Anandkumar et al., 2014a;
Wang & Anandkumar, 2016) are at least ‚Ñ¶(k log log 1Œµ ).
3.1. Our Robust Tensor Power Method
The tensor power phase of our Algorithm 1 is based on
our version of the tensor power method, which works as
follows. At each step t, we maintain a d √ó k matrix Q(t)
(t)
(t)
with columns Q1 , . . . , Qk as our current estimators for
u1 , . . . , uk , which is obtained by updating the previous estimators with the following two operations.
The main operation is to apply the noisy tensor TÃÑ on them
simultaneously to get a d √ó k matrix Y (t) with its j-th col(t)
(t‚àí1)
(t‚àí1)
umn computed as Yj = TÃÑ (Id , Qj
, Qj
), which
equals
2
X 
(t‚àí1)
(t)
Œªi u>
Q
¬∑ ui + Œ¶ÃÇj ,
i
j
i‚àà[d]

for

(t)
Œ¶ÃÇj

(t‚àí1)

= Œ¶(Id , Qj

(t)

‚àÄi ‚àà [d] : u>
i Yj

(t‚àí1)

, Qj
). This implies that

2
(t‚àí1)
(t)
= Œªi u>
+ u>
i Qj
i Œ¶ÃÇj ,

(2)

which shows the progress made by this operation.
The second operation is to orthogonalize Y (t) as
Y (t) = Q(t) ¬∑ R(t) ,
by the QR decomposition via the Gram-Schmidt process,
(t)
(t)
to obtain a d √ó k matrix Q(t) with columns Q1 , . . . , Qk ,
which then become our new estimators. As we will show
in Lemma 1 below, given a small enough kŒ¶k, if we start
with a full-rank Q(0) , then each Q(t) also has full rank and
consists of orthonormal columns, and each R(t) is invertible. Moreover, although we apply the QR decomposition
on the whole matrix Y (t) to obtain the matrix Q(t) , it has
the effect that for any m ‚àà [k], the first m columns of Q(t)
can be seen as obtained from the first m columns of Y (t)
by a QR decomposition. This property is needed in our
Lemma 1 and Theorem 2 below to guarantee the simulta(t)
neous convergence of Qi to ui for every i ‚àà [k].
Before stating Lemma 1 which guarantees the progress we
make at each step, let us prepare some notations first. For
a d √ó k matrix A and some m ‚àà [k], let A[m] denote the
d √ó m matrix containing the first m columns of A. Let U
denote the d √ó k matrix with the target vector ui as its i‚Äôth
column. For a d √ó k matrix Q and some m ‚àà [k], define


 >

cosm (Q) = minm U[m]
¬∑ Q[m] ¬∑ y  /kQ[m] ¬∑ yk,
y‚ààR

Tensor Decomposition via Simultaneous Power Iteration

which equals the cosine of the m‚Äôth principal angle between the column spaces of U[m] and Q[m] , let sinm (Q) =
p
1 ‚àí cos2m (Q), and let us use as the error measure
tanm (Q) = sinm (Q)/ cosm (Q).
More information about the principal angles can be found
in, e.g., (Golub & Van Loan, 1996). Then we have the
following lemma, which we prove in Appendix B.1.
(t)

Lemma 1. Fix any m ‚àà [k] and t ‚â• 0. Let Œ¶ÃÇ[m] denote
(t)

(t‚àí1)

(t‚àí1)

the d √ó m matrix with Œ¶ÃÇj = Œ¶(Id , Qj
, Qj
) as its
j‚Äôth column, and suppose


n
o
 (t) 
(3)
Œ¶ÃÇ[m]  < ‚àÜ ¬∑ min Œ≤, cos2m (Q(t‚àí1) ) ,
for some Œ≤ > 0. Then for œÅ = maxi‚àà[k] ( ŒªŒªi+1
) 4 , we have
i
1

n
o
tanm (Q(t) ) ‚â§ max Œ≤, max{Œ≤, œÅ} ¬∑ tan2m (Q(t‚àí1) ) .
Observe that the guarantee provided by the lemma above
has a similar form as that in (Hardt & Price, 2014) for
matrices. The main difference is that here in the tensor
case, we have the error measure essentially squared after each step, which has the following two implications.
First, to guarantee that the error is indeed reduced, we need
tanm (Q(t‚àí1) ) to be small enough (say, less than one), unlike in the matrix case. Next, if we indeed have a small
enough tanm (Q(t‚àí1) ), then the error can be reduced in a
much faster rate than in the matrix case. Another difference is that here we provide the guarantee for all the k sub(t)
matrices Q[m] , for m ‚àà [k], instead of just one matrix Q(t) .
This allows us to show the simultaneous convergence of
(t)
each column Qi to the target vector ui for every i ‚àà [k],
as given in the following, which we prove in Appendix B.2.
Theorem 2. For any Œµ ‚àà (0, Œª2k ), there exists some
N ‚â§ O(log( Œ≥1 log 1Œµ )) such that the following holds. Sup‚àö and we
pose the perturbation is bounded by kŒ¶k ‚â§ 2‚àÜŒµ
k
start from some initial Q(0) with tanm Q(0) ‚â§ 1 for ev(t)
ery m ‚àà [k]. Then for any t ‚â• N , with uÃÇi = Qi
and ŒªÃÇi = TÃÑ (uÃÇi , uÃÇi , uÃÇi ), we have kui ‚àí uÃÇi k ‚â§ Œµ and
|Œªi ‚àí ŒªÃÇi | ‚â§ Œµ, for every i ‚àà [k].
Note that the convergence rate guaranteed by the theorem
above is exponentially faster than that in (Hardt & Price,
2014) for matrices, assuming that we indeed can have such
a good initial Q(0) to start with. In the next subsection, we
show how it can be found efficiently.
3.2. Initialization Procedure
Our approach for finding a good initialization is to project
the tensor down to a matrix and apply the matrix power

method for only a few steps just to make the tangents less
than one. Although we could continue applying the matrix
power method till reaching the much smaller target bound
Œµ, this would take exponentially longer than by switching
to the tensor power method as we actually do.
As mentioned above, we would first like to project the tensor TÃÑ down to a matrix. A naive approach is to sample
a random vectorPwÃÑ and take the matrix TÃÑ (Id , Id , wÃÑ) ‚âà
T (Id , Id , wÃÑ) = i‚àà[d] Œªi (u>
i wÃÑ) ¬∑ ui ‚äó ui . However, this
may mess up the gaps between eigenvalues, which are
needed to guarantee the convergence rate of the matrix
power method. The reason is that as each u>
i wÃÑ has mean
zero, the coefficient Œªi (u>
wÃÑ)
also
has
mean
zero and thus
i
has a good chance of coming very close to others. To pre>
serve the gaps, we would like to have u>
i wÃÑ ‚â• ui+1 wÃÑ for
each i with high probability. To achieve this, let us first
imagine sampling a random w ‚àà Rd from N d (0, 1), and
computing the vector wÃÑ = TÃÑ (Id , w, w), which is close to
X
2
T (Id , w, w) =
Œªi (u>
i w) ¬∑ ui .
i‚àà[d]
2
Then one can show that forP
every i, E[(u>
i w) ] = 1, so that
E[wÃÑ] ‚âà E[T (Id , w, w)] = i‚àà[d] Œªi ui , and
>
u>
i E[wÃÑ] ‚âà Œªi > Œªi+1 ‚âà ui+1 E[wÃÑ].

However, we want the gap-preserving guarantee to be in
high probability, instead in expectation. Thus we go further
by sampling not just one, but some number L of vectors
w1 , . . . , wL independently from the distribution N d (0, 1),
and then taking the average
wÃÑ =

1 X
TÃÑ (Id , wj , wj ).
L

(4)

j‚àà[L]

The following lemma shows that such a wÃÑ is likely to have
u>
i wÃÑ ‚âà Œªi , which we prove in Appendix B.3.
‚àÜ
Lemma 2. Suppose we have TÃÑ = T + Œ¶ with kŒ¶k ‚â§ 3d
.
1
Then for some L ‚â§ O( Œ≥ 2 log d), the vector wÃÑ computed
according to (4) with high probability satisfies

 >

ui wÃÑ ‚àí Œªi  ‚â§ 1 (Œªi Œ≥ + 2‚àÜ) for every i ‚àà [k].
4

(5)

With this wÃÑ, we compute the matrix MÃÑ = TÃÑ (Id , Id , wÃÑ).
As shown by the following lemma, which we prove in Appendix B.4, MÃÑ is close to a matrix with ui ‚Äôs as eigenvectors
and good gaps between eigenvalues.
Lemma 3. Suppose we have TÃÑ = T + Œ¶. Then for any wÃÑ
satisfying the condition (5) in Lemma 2, the matrix MÃÑ =
TÃÑ (Id , Id , wÃÑ) can be decomposed as
X
MÃÑ =
ŒªÃÑi ¬∑ ui ‚äó ui + Œ¶ÃÑ,
i‚àà[d]

Tensor Decomposition via Simultaneous Power Iteration

for some ŒªÃÑi ‚Äôs with ŒªÃÑi ‚àí ŒªÃÑi+1 ‚â• ‚àÜ2 , for i ‚àà [k], and Œ¶ÃÑ =
Œ¶(Id , Id , wÃÑ) with kŒ¶ÃÑk ‚â§ 2kŒ¶k.
With such a matrix MÃÑ , we next apply the matrix power
method of (Hardt & Price, 2014) to find good approximates to its eigenvectors. More precisely, we sample an
initial matrix Y (0) ‚àà Rd√ók by choosing each of its column independently according to the distribution N d (0, 1),
and factorize it as Y (0) = Z (0) ¬∑ R(0) by QR decomposition via the Gram-Schmidt process. Then at step s ‚â• 1,
we multiply the previous estimate Z (s‚àí1) by MÃÑ to obtain
Y (s) = MÃÑ ¬∑ Z (s‚àí1) , factorize it as Y (s) = Z (s) ¬∑ R(s)
by QR decomposition via the Gram-Schmidt process, and
then take the orthonormal Z (s) as the new estimate. The
following lemma shows the number of steps needed to find
a good enough Z (s) .
Lemma 4. Suppose we are given a matrix MÃÑ having the
‚àÜ2
decomposition described in Lemma 3, with kŒ¶ÃÑk ‚â§ 2Œ±‚àö0dk
for a small enough constant Œ±0 . Then there exists some
S ‚â§ O( Œ≥1 log d) such that with high probability, we have
tanm (Z (s) ) ‚â§ 1 for every m ‚àà [k] whenever s ‚â• S.
This together with the previous two lemmas guarantee that
‚àÜ2
‚àÜ Œ±‚àö
given TÃÑ = T + Œ¶, with kŒ¶k ‚â§ min{ 3d
, 0dk
}, for a small
enough constant Œ±0 , we can obtain with high probability a
good Z (S) which can be used as the initial Q(0) for our
tensor power phase. Combining this with Theorem 2 in the
previous subsection, we then have our Theorem 1 given at
the beginning of the section.

4. General Orthogonal Tensors
In the previous section, we consider tensors which are orthogonal, symmetric, and of order three. In this section, we
show how to extend our results for general orthogonal tensors, to deal with higher orders first and then asymmetry.
4.1. Higher-Order Tensors
To handle orthogonal and symmetric tensors of any order, only the initialization procedure needs to be modified.
First, for tensors of any odd order, a straightforward modification is as follows. Take for example a tensor of order
2r + 1. Now we simply compute
1 X
wÃÑ =
TÃÑ (Id , wj , . . . , wj ),
L
j‚àà[L]

with 2r copies of wj , and similarly to Lemma
P 2, one can
show that wÃÑ is likely to be close to the vector i‚àà[d] Œªi ¬∑ ui .
With such a vector wÃÑ, one can show that the matrix
MÃÑ = TÃÑ (Id , Id , wÃÑ, . . . , wÃÑ),
P
with 2r ‚àí 1 copies of wÃÑ, is close to the matrix i‚àà[d] Œª2r
i ¬∑
ui ‚äó ui , similarly to Lemma 3. Then the rest is the same

as that in the previous section. Note that this approach has
the eigenvalues decreased exponentially in r. A different
approach avoiding this is to compute MÃÑ directly as
MÃÑ =

2
1 X
TÃÑ (Id , Id , wj , . . . , wj ) ,
L
j‚àà[L]

which is close to
1 X
2
(T (Id , Id , wj , . . . , wj ))
L
j‚àà[L]

=

1 X X 2 > 4r‚àí2
Œªi ui wj
¬∑ ui ‚äó ui
L

=

X

j‚àà[L] i‚àà[d]

Œª2i

i‚àà[d]

1 X > 4r‚àí2
ui wj
¬∑ ui ‚äó ui .
L
j‚àà[L]

Then one can again show
P that such a matrix MÃÑ is likely to
be close to the matrix i‚àà[d] Œª2i ¬∑ ui ‚äó ui .
To handle tensors of even orders, the initialization is
slightly different but the idea is similar. Given a tensor of
order 2r, we again sample vectors w1 , . . . , wL as before,
but now we compute the matrix directly as
MÃÑ =

1 X
TÃÑ (Id , Id , wj , . . . , wj ),
L
j‚àà[L]

with 2r ‚àí 2 copies of wj . As before, one canP
show that
the matrix MÃÑ is likely to be close to the matrix i‚àà[d] Œªi ¬∑
ui ‚äó ui . Then again we can apply the matrix power method
on MÃÑ and obtain a good initialization for the tensor power
method as before. Note that now the eigenvalues are no
longer squared, and the previous requirement on kŒ¶k can
be slightly relaxed, with the dependence on ‚àÜ2 being replaced by ‚àÜ.
4.2. Asymmetric Tensors
For simplicity of presentation, let us focus on the third order case; the extension to higher orders is straightforward.
That is, now the underlying tensor has the form
X
T =
Œªi ¬∑ ai ‚äó bi ‚äó ci ,
i‚àà[d]

with nonnegative Œªi ‚Äôs satisfying the condition (1), together
with three sets of orthonormal vectors of ai ‚Äôs, bi ‚Äôs, and ci ‚Äôs.
As before, we only have access to a noisy version TÃÑ of T .
The main modification of our algorithm is again to the initialization procedure, but the idea is also similar. To find a
good initial matrix A for ai ‚Äôs, we sample w1 , . . . , wL independently from N d (0, 1), and now compute the matrix
MÃÑ =


>
1 X
TÃÑ (Id , Id , wj ) TÃÑ (Id , Id , wj ) .
L
j‚àà[L]

Tensor Decomposition via Simultaneous Power Iteration

As before, it is not hard to show that
X 1 X
2
MÃÑ ‚âà
Œª2i c>
¬∑ ai ‚äó ai ,
i wj
L
i‚àà[d]

i‚àà[k]

j‚àà[L]

P
which is close to the matrix i‚àà[d] Œª2i ¬∑ ai ‚äó ai with high
probability. From the matrix MÃÑ , we can again apply the
matrix power method to find a good initial matrix A. Similarly, we can find good initial matrices B and C for bi ‚Äôs
and ci ‚Äôs, respectively.
Next, with such matrices, we would like to apply the tensor
power method, which we modify as follows. Now at each
step t, we take previous estimates A(t‚àí1) , B (t‚àí1) , C (t‚àí1) ,
(t)
(t‚àí1)
(t‚àí1)
(t)
and compute Xi
= TÃÑ (Id , Bi
, Ci
), Yi
=
(t‚àí1)
(t‚àí1)
(t)
(t‚àí1)
(t‚àí1)
TÃÑ (Ai
, Id , Ci
), Zi = TÃÑ (Ai
, Bi
, Id ), for
i ‚àà [k], followed by orthonormalizing X (t) , Y (t) , Z (t)
to obtain the new estimates A(t) , B (t) , C (t) via QRdecomposition. It is not hard to show that the resulting algorithm has a similar convergence rate as our Algorithm 1.

5. Nonorthogonal but Symmetric Tensors
In the previous section, we consider general orthogonal tensors, which can be asymmetric. In this section, we consider non-orthogonal tensors which are symmetric. We
remark that for some latent variable models such as the
multi-view model, the corresponding asymmetric tensors
can be converted into symmetric ones (Anandkumar et al.,
2014a), so that our result here can still be applied. For
simplicity of exposition, let us again focus on the case of
order
P three, so that the given tensor has the form T =
i‚àà[d] Œªi ¬∑vi ‚äóvi ‚äóvi , but the vectors vi ‚Äôs are no longer assumed to be orthogonal to each other. Still we assume them
to be linearly independent, and we again assume without
loss of generality that kT k ‚â§ 1 and kvi k = 1 for each i. In
addition, let us assume, as in previous works, that Œªj = 0
for j ‚â• k + 1.2
Following (Anandkumar et al., 2014a), we would like to
whiten such a tensor T into an orthogonal one, so that we
can then apply our Algorithm 1. More precisely, our goal is
to find a d √ó k matrix W such that the tensor T (W, W, W )
becomes orthogonal. As in (Anandkumar et al., 2014a),
assume that we also have available a matrix
X
M=
Œªi ¬∑ vi ‚äó vi .3
i‚àà[k]

Then for a whitening matrix, it suffices to find some W
2

such that W > M W = Ik . The reason is that
X


Ik = W > M W =
Œª i ¬∑ W > vi ‚äó W > v i ,

This assumption is not necessary. We assume it just to simplify the first step of our algorithm given below. Without it, we
can simply replace that step by the matrix power method used in
our Algorithm 1, which takes more steps but can still do the job.
3
More generally, the weights Œªi in M are allowed to differ
from those in T , but for simplicity we assume they are the same.

‚àö
which implies that the vectors Œªi W > vi , for i ‚àà [k], are
orthonormal. Then the tensor T (W, W, W ) equals
‚äó3
X
X 1 p
‚àö ¬∑
Œªi ¬∑ (W > vi )‚äó3 =
Œª i W > vi
,
Œªi
i‚àà[k]
i‚àà[k]
which has an orthogonal decomposition.
According to (Anandkumar et al., 2014a), one way to
find such a W is to do the spectral decomposition of M
as U ŒõU > , with eigenvectors as columns of U , and let
1
W = U Œõ‚àí 2 . However, we will not take this approach,
because finding a good approximate to U by the matrix
power method would take longer to converge than the tensor power method which we will later apply to the whitened
tensor. Our key observation is that it suffices to find a d √ó k
matrix Q such that the matrix P = Q> M Q is invertible,
1
since we can then let W = QP ‚àí 2 and have
1

1

W > M W = P ‚àí 2 Q> M QP ‚àí 2 = Ik .
With such a W , the tensor T (W, W, W ) becomes orthogonal, so that we can decompose it4 to obtain œÉi = ‚àö1Œª and
i
‚àö
ui = Œªi W > vi , from which we can recover Œªi = œÉ12 and
1

i

vi = œÉi QP 2 ui if Q has orthonormal columns.
As before, we consider a similar setting in which we only
have access to a noisy MÃÑ = M + Œ¶ÃÑ, for some symmetric
perturbation matrix Œ¶ÃÑ, in addition to the noisy tensor TÃÑ =
T +Œ¶. Then our algorithm for finding the whitening matrix
consists of the following two steps:
1. Sample a random matrix Z ‚àà Rd√ók with orthonormal
columns, compute YÃÑ = MÃÑ Z, and factorize it as YÃÑ =
QRÃÑ by a QR decomposition.
1

2. Compute PÃÑ = Q> MÃÑ Q and output WÃÑ = QPÃÑ ‚àí 2 as
the whitening matrix.
We analyze our algorithm in the following. First note that
Q is computed in the same way as we compute Z (1) in
Algorithm 1, and with Œªk+1 = 0 we are likely to have
tank (Q) ‚âà 0 so that the matrix P = Q> M Q is invertible. Formally, we have the following, which we prove in
Appendix C.1.
Œªk
Lemma 5. Suppose kŒ¶ÃÑk ‚â§ Œ±‚àö0dk
for a small enough constant Œ±0 . Then with high probability we have œÉmax (P ) ‚â§
Œª1 and œÉmin (P ) ‚â• Œª2k .
4
To apply ‚àö
our Algorithm 1, we need to scale it properly, say
by a factor of Œªk /k to make its norm at most one.

Tensor Decomposition via Simultaneous Power Iteration

Next, with a small enough kŒ¶ÃÑk, if P is invertible, then so
1
1
is PÃÑ , and moreover, we have PÃÑ ‚àí 2 ‚âà P ‚àí 2 . This is shown
in the following, which we prove in Appendix C.2.
Lemma 6. Fix any  ‚àà (0, 1) and suppose we have
œÉmin (P ) ‚â• 2 and kŒ¶ÃÑk ‚â§ . Then PÃÑ is invertible and
1
1
1
kPÃÑ ‚àí 2 ‚àí P ‚àí 2 k ‚â§ 2(œÉmin (P ))‚àí2 (œÉmax (P )) 2 .
1

Then, with a good PÃÑ ‚àí 2 , we can obtain a good WÃÑ and have
TÃÑ (WÃÑ , WÃÑ , WÃÑ ) close to T (W, W, W ) which has an orthogonal decomposition. This is shown in the following, which
we prove in Appendix C.3.
Theorem 3. Fix any Œµ ‚àà (0, Œª4k ) and suppose we have
3

Œª3

k
kŒ¶k ‚â§ Œ±0 Œªk2 Œµ and kŒ¶ÃÑk ‚â§ Œ±0 Œµ min{ ‚àöŒªdk
, ‚àöŒªk }, for a
1
small enough constant Œ±0 . Then with high probability we
have kTÃÑ (WÃÑ , WÃÑ , WÃÑ ) ‚àí T (W, W, W )k ‚â§ Œµ.

6. Streaming setting
In the previous sections, we consider the batch setting in
which the tensor TÃÑ is assumed to be stored somewhere
which can be accessed whenever we want to. However,
storing such a tensor, say of order three, requires a space
complexity of ‚Ñ¶(d3 ), which becomes impractical even for
a moderate value of d. In this section, we study the possibility of achieving a space complexity of O(kd), which is the
least amount of memory needed just to store the k vectors
in Rd . More precisely, we consider the streaming setting,
in which there is a stream of vectors x1 , x2 , . . . arriving
one at a time. We assume that each vector x is sampled independently from some distribution over Rd , with kxk ‚â§ 1
and some function g : Rd ‚Üí Rd√ód√ód such that

we can choose a proper size for J. In fact, to save the total number of samples, we can follow the approach of (Li
et al., 2016) by choosing different sizes in different iterations of the matrix or tensor power method.
Following (Wang & Anandkumar, 2016), let us take the
specific case with g(x) = x ‚äó x ‚äó x as a concrete example
and focus on the orthogonal case studied in Section 3; it is
not hard to convert other algorithms of ours to the streaming setting.
P One can show that in this specific case, we have
E[x] = i‚àà[d] Œªi ui so that there is a more efficient way to
find a vector wÃÑ for producing the matrix MÃÑ in the initialization phase. Formally, we have the following lemma, which
we prove in Appendix D.1.
Lemma 7. There is an algorithm using O(d) space and
k
d
O( log
‚àÜ2 ) samples to find some wÃÑ ‚àà R satisfying the condition (5) in Lemma 2 with high probability.
With such a vector wÃÑ, we can then use the streaming algorithm of (Li et al., 2016) to find a good initial matrix Z for
the later tensor power phase. Formally, we have the following lemma, which we prove in Appendix D.2.
Lemma 8. Given wÃÑ from Lemma 7, we can use O(kd)
kd log

d

space and O( ‚àÜ4 Œ≥ Œ≥ ) samples to find some Z ‚àà Rd√ók
with tanm (Z) < 1, for any m ‚àà [k], with high probability.
Having such a matrix Z, we can proceed to the tensor
power phase. Borrowing again the idea from (Li et al.,
2016), let us partition the incoming data into blocks of increasing sizes, with the t‚Äôth block Jt used to carry out one
(t)
(t‚àí1)
(t‚àí1)
tensor power iteration Yi = TÃÑ (t) (Id , QiP , Qi
), for
1
(t)
i ‚àà [k], of Algorithm 1, with TÃÑ = |Jt | œÑ ‚ààJt g(xœÑ ). In(t)

‚Ä¢ E[g(x)] = T , and given x, u, v ‚àà Rd , g(x)(Id , u, v)
can be computed in O(d) space.

stead of preparing this TÃÑ (t) and then computing each Yi ,
we now go through |Jt | steps of updates:
(t)

Such a function g is known to exist for some latent variable models (Ge et al., 2015; Wang & Anandkumar, 2016).
Given such a function, our algorithms in previous sections
can all be converted to work in the streaming setting using
O(kd) space. This is because all our operations involving tensors have the form TÃÑ (Id , u, v), for some u, v ‚àà Rd ,
which can be realized as
!
1 X
1 X
g(xt ) (Id , u, v) =
(g(xt ) (Id , u, v)) ,
|J|
|J|
t‚ààJ

t‚ààJ

for a collection J of samples, with the righthand side above
clearly computable in O(kd) space.5 Then
P depending on
1
the distance we want between TÃÑ = |J|
t‚ààJ g(xt ) and T ,
5

This also includes the initialization phase in which we now do
not store the matrix MÃÑ explicitly but instead replace the operation
MÃÑ Zi by TÃÑ (Id , Zi , wÃÑ).

‚Ä¢ For œÑ ‚àà Jt do: Yi

(t)

= Yi

+

1
> (t‚àí1) 2
) xœÑ .
|Jt | (xœÑ Qi

The block sizes are chosen carefully to keep kTÃÑ (t) ‚àí T k
small enough so that we can have tanm (Q(t) ) decreased in
a desirable rate. Here, we choose the parameters
n t
Œµo
c0 log(dt)
Œ≤t = max œÅ2 ‚àí1 ,
and |Jt | =
,
2
‚àÜ2 Œ≤t2

(6)

for a large enough constant c0 , to make the condition (3)
in Lemma 1 hold with high probability so that we have
tanm (Q(t) ) ‚â§ Œ≤t . In Appendix D.3, we summarize our
algorithm and prove the following theorem.
Theorem 4. Given Œµ ‚àà (0, Œª2k ), with high probability we
can find ŒªÃÇi , uÃÇi with |ŒªÃÇi ‚àí Œªi |, kuÃÇi ‚àí ui k ‚â§ Œµ, for any i ‚àà
[k], using O(kd) space and O(
samples.

d
kd log Œ≥
‚àÜ4 Œ≥

+

log(d log( Œ≥1 log
‚àÜ2 Œ≥Œµ2

1
Œµ ))

)

Tensor Decomposition via Simultaneous Power Iteration

References
Anandkumar, Animashree, Hsu, Daniel J, and Kakade,
Sham M. A method of moments for mixture models and
hidden markov models. In COLT, volume 1, pp. 4, 2012.
Anandkumar, Animashree, Ge, Rong, Hsu, Daniel,
Kakade, Sham M, and Telgarsky, Matus. Tensor decompositions for learning latent variable models. Journal of
Machine Learning Research, 15(1):2773‚Äì2832, 2014a.
Anandkumar, Animashree, Ge, Rong, and Janzamin, Majid. Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates. arXiv preprint
arXiv:1402.5180, 2014b.
Chaganty, Arun Tejasvi and Liang, Percy. Estimating
latent-variable graphical models using moments and
likelihoods. In ICML, pp. 1872‚Äì1880, 2014.
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Escaping from saddle pointsonline stochastic gradient for
tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pp. 797‚Äì842, 2015.
Golub, Gene H. and Van Loan, Charles F. Matrix Computation. John Hopkins University Press, 3rd edition,
1996.
Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861‚Äì2869, 2014.
Hillar, Christopher J and Lim, Lek-Heng. Most tensor
problems are NP-hard. Journal of the ACM (JACM), 60
(6), 2013.
Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51(3):455‚Äì500,
2009.
Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 473‚Äì481, 2016.
Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Prateek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886‚Äì2894,
2013.
Wang, Yining and Anandkumar, Anima. Online and
differentially-private tensor decomposition.
In Advances in Neural Information Processing Systems, pp.
3531‚Äì3539, 2016.

