Tensor Decomposition via Simultaneous Power Iteration

Po-An Wang 1 Chi-Jen Lu 1

Abstract
Tensor decomposition is an important problem
with many applications across several disciplines, and a popular approach for this problem
is the tensor power method. However, previous
works with theoretical guarantee based on this
approach can only find the top eigenvectors one
after one, unlike the case for matrices. In this
paper, we show how to find the eigenvectors simultaneously with the help of a new initialization
procedure. This allows us to achieve a better running time in the batch setting, as well as a lower
sample complexity in the streaming setting.

1. Introduction
Tensors have long been successfully used in several disciplines, including neuroscience, phylogenetics, statistics,
signal processing, computer vision, and data mining. They
are used to model multi-relational or multi-modal data, and
their decompositions often reveal some underlying structures behind the observed data. See (Kolda & Bader, 2009)
for a survey of such results. Recently, they have found
applications in machine learning, particularly for learning
various latent variable models (Anandkumar et al., 2012;
Chaganty & Liang, 2014; Anandkumar et al., 2014a).
One popular decomposition method in such applications
is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components. This is similar to the singular value decomposition
(SVD) of matrices, and a popular approach for SVD is the
power method, which is well-understood and has nice theoretical guarantee. As tensors can be seen as generalization
of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could
inherit the success from the matrix case. However, the situation turns out to be much more complicated for tensors
(see e.g. the discussion in (Anandkumar et al., 2014a)),
1
Academia Sinica, Taiwan. Correspondence to: Po-An Wang
<poanwang@iis.sinica.edu.tw>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

and in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013). Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable
again. In particular, for tensors having orthogonal decomposition, Anandkumar et al. (2014a) provided an efficient
algorithm based on the tensor power method with theoretical guarantee. Still, as we will discuss later in Section 2, the
seemingly subtle change of going from matrices to tensors
makes some significant differences for the power method.
The first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely
converge to the top singular vector, we have much less control of where the convergence goes in the tensor case. Consequently, most previous works based on the tensor power
method with theoretical guarantee, such as (Anandkumar
et al., 2014a;b; Wang & Anandkumar, 2016), require much
more complicated procedures. In particular, they can only
find the top k eigenvectors one by one, each time with the
power method applied to a modified tensor, deflated from
the original tensor according to the previously found vectors. Moreover, to find each vector, they need to sample
several initial vectors and apply the power method on all
of them, before selecting just one from them. In contrast,
algorithms for matrices such as (Mitliagkas et al., 2013;
Hardt & Price, 2014) are much simpler, as they can find
the k vectors simultaneously by applying the power method
only on k random initial vectors. The second difference, on
the other hand, has a beneficial effect, which allows the tensor power method to converge exponentially faster than the
matrix one when starting from good initial vectors. Then a
natural question is: can we inherit the best of both worlds?
Namely, is it possible to have a simple algorithm which can
find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?
Our Results. As in previous works, we consider the
slightly harder scenario in which we only have access to
a noisy version of the tensor we want to decompose. This
arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from
some empirical average of the observed data. Our main
contribution is to answer the above question affirmatively.
First, we consider the batch setting in which we assume

Tensor Decomposition via Simultaneous Power Iteration

that the given noisy tensor is stored somewhere and can be
accessed whenever we want to. In this setting, we identify a sufficient condition such that if we have k initial vectors satisfying this condition, then we can apply the tensor
power method on them simultaneously, which will come
within some distance ε to the eigenvectors in O(log log 1ε )
iterations, with parameters related to eigenvalues considered as constant. To apply such a result, we need an efficient way to find such initial vectors. We show how to
do this by choosing a good direction to project the tensor
down to a matrix while preserving the eigengaps, and then
applying the matrix power method for only a few iterations
just to obtain vectors meeting that sufficient condition. The
number of iterations needed here is only O(log d), independent of ε, where d is the dimension of the eigenvectors.
The result stated above is for orthogonal tensors. On the
other hand, it is known that an nonorthogonal tensor with
linearly independent eigenvectors can be converted into an
orthogonal one with the help of some whitening matrix.
However, previous works usually pay little attention on
how to find such a whitening matrix efficiently. According to (Anandkumar et al., 2014a), one way is via SVD on
some second moment matrix, but doing this using the matrix power method would take longer to converge compared
to the tensor power method which would then be applied on
the whitened tensor. Our second contribution is to provide
an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.
While most previous works on tensor decomposition focus
on the batch setting, storing even a tensor of order three requires Ω(d3 ) space, which is infeasible for a large d. We
show to avoid this in the streaming setting, with a stream
of data arriving one at a time, which is the only source of
information about the tensor. We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension
d. To achieve an approximation error ε, the total number of
samples we need is O(kd log d + ε12 log(d log 1ε )).
Related Works There is a huge literature on tensor decomposition, and it is beyond the scope of this paper to
give a comprehensive survey. Thus, we only compare our
results to the most related ones, particularly those based
on the power method. While different works may focus
on different aspects, we are most interested in understanding how the error parameter ε affects various performance
measures, having in mind a small ε.
First, the batch algorithm of Anandkumar et al. (2014a),
using a better analysis in (Wang & Anandkumar, 2016),
runs in time about O((k 2 log k)(log log 1ε )), which can be
made to run in O(k(log log 1ε )) iterations in parallel, while
ours are O(k log log 1ε ) and O(log log 1ε ), respectively. On

the other hand, one advantage of their algorithm is that its
running time does not depend on the eigengaps, while ours
has the dependence hidden above as some constant.
In the streaming setting, Wang & Anandkumar (2016)
provided an algorithm using O(dk log k) memory and
O( εk2 log( dε )) samples, while ours only uses O(dk) memory and O( ε12 log(d log log 1ε )) samples.1 Nevertheless, the
sample complexity of Wang & Anandkumar (2016) is also
independent of the eigengaps, while ours has the dependence hidden above as a constant factor.
As one can see, our algorithms, which find the k eigenvectors simultaneously, allow us to save a factor of k in the
time complexity and the sample complexity, although our
bounds may become worse when the eigengaps are small.
Thus, our algorithms can be seen as new options for users
to choose from, depending on the data they are given.
Although not directed related, let us also compare to previous works on SVD. Two related ones, both based on the
simultaneous matrix power method, are the batch algorithm
of (Hardt & Price, 2014) which converges in O(log dε ) iterations, and the streaming algorithm of (Li et al., 2016)
which requires O( ε12 log(d log 1ε )) samples. Both bounds
are worse than ours and also depend on the eigengaps.
Thus, although one approach for orthogonal tensor decomposition is to reduce it to a matrix SVD problem, this does
not appear to result in better performance than ours.
Finally, comparisons of the tensor power method with other
approaches can be found in works such as (Anandkumar
et al., 2014a; Wang & Anandkumar, 2016). For example,
the online SGD approach of (Ge et al., 2015) works only
for tensors of even orders and its sample complexity has a
poor dependency on the dimension d.

Organization of the paper. First, we provide some preliminaries in Section 2. Then we present our batch algorithm for orthogonal and symmetric tensors of order three
in Section 3, and then for general orthogonal tensors in Section 4. In Section 5, we introduce our whitening procedure
for nonorthogonal but symmetry tensors. Finally, in Section 6, we present our algorithm for the streaming setting.
Due to the space limitation, we will move all our proofs to
the appendix in the supplementary material.

2. Preliminaries
Let us first introduce some notations and definitions which
we will use later. Let R denote the set of real numbers
and N the set of positive integers. Let N (0, 1) denote the
standard normal distribution with mean 0 and variance 1,
1
We use a different input distribution from theirs. The bound
listed here is modified from theirs according to our distribution.

Tensor Decomposition via Simultaneous Power Iteration

and let N d (0, 1), for d ∈ N, denote the d-variate one which
has each of its d dimensions sampled independently from
N (0, 1). For d ∈ N, let [d] denote the set {1, . . . , d}. For
a vector x, let kxk denote its L2 norm. For d ∈ N, let Id
denote the d × d identity matrix. For a matrix A ∈ Rd×k ,
let Ai , for i ∈ [k], denote its i-th column, and let Ai,j , for
j ∈ [d], be the j-th entry of Ai . Moreover, for a matrix A,
let A> denote its transpose, and define its norm as kAk =
0
maxx∈Rk kA·xk
kxk , using the convention that 0 = 0.

method, which works P
as follows. Suppose we are given
a d × d matrix M = i∈[d] λi · ui ⊗ ui , with nonnegative λ1 > λ2 ≥ · · · and orthonormal vectors
P u1 , . . . , u d .
The power method starts with some q (0) = i∈[d] ci · ui ,
usually chosen randomly, and then repeatedly performs the
update q (t) = M · q (t−1) , which results in

Tensors are the focus of our paper, which can be seen as
generalization of matrices to higher orders. For simplicity of presentation, we will use symmetric tensors of order
three as examples in the following definitions. A real tensor T of order three can be seen as an three-dimensional
array in Rd×d×d , for some d ∈ N, with its (i, j, k)-th
entry denoted as Ti,j,k . For such a tensor T and three
matrices A ∈ Rd×m1 , B ∈ Rd×m2 , C ∈ Rd×m3 , let
T (A, B, C) be the P
tensor in Rm1 ×m2 ×m3 , with its (a, b, c)th entry defined as i,j,k∈[d] Ti,j,k Aa,i Bb,j Cc,k . The norm
of a tensor T we will use is the operator norm: kT k =
|T (x,y,z)|
maxx,y,z∈Rd kxkkykkzk
.

Note that for any i 6= 1, as λi < λ1 , the coefficient λti ci will
soon become much smaller than the coefficient λt1 c1 if c1
is not too small, which is likely to happen for a randomly
chosen q (0) . This has the effect that after normalization,
q (t) /kq (t) k approaches u1 quickly.
P
Now consider a tensor T =
i∈[d] λi · ui ⊗ ui ⊗ ui ,
with nonnegative λ1 > λ2 ≥ · · · and orthonormal
vectors u1 , . . . , ud . The tensor version of the power
method
again starts from a randomly chosen q (0) =
P
c
i∈[d] i · ui , but now repeatedly performs the update
(t)
q = T (Id , q (t−1) , q (t−1) ), which in turn results in

The tensor decomposition problem. In this problem,
there is a tensor T with some unknown decomposition
X
T =
λi · ui ⊗ ui ⊗ ui ,

q (t) =

i∈[d]

with λi ≥ 0 and ui ∈ Rd for any i ∈ [d]. Then given some
k ∈ [d] and ε ∈ (0, 1), our goal is to find λ̂i and ûi with
|λ̂i − λi | ≤ ε and kûi − ui k ≤ ε, for every i ∈ [k].
We will assume that
X
λi ≤ 1 and ∀i ∈ [k] : λi > λi+1 .

(1)

X

q (t) =



X

(t−1)
λi u>
q
· ui =
λti ci · ui .
i

i∈[d]

X

i∈[d]


2
X
2t
(t−1)
λi u>
q
· ui =
λ−1
i
i (λi ci ) · ui .

i∈[d]

i∈[d]

The coefficient of each ui now has a different form from
the matrix case, and this leads to the following two effects.
First, one now has much less control on what q (t) /kq (t) k
converges to. In fact, it can converge to any ui 6= u1 if
ui has the largest value of λi |ci |, which happens with a
good probability if λi is not much smaller than λ1 . Consequently, to find the top k vectors u1 , . . . , uk , previous
works based on the power method all need much more
complicated procedures (Anandkumar et al., 2014a), compared to those for matrices, as discussed in the introduction.

i∈[d]

As in previous works, we consider a slightly harder version of the problem, in which we only have access to some
noisy version of T , instead of the noiseless T . We will
consider the following two settings. In the batch setting,
we have access to some T̄ = T + Φ for the whole time,
for some perturbation tensor Φ. In the streaming setting,
we have a stream of data points x1 , x2 , . . . arriving one by
one, which provide the only information we have about T ,
with each xτ ∈ Rd allowing us to compute some T̄τ with
mean E[T̄ ] = T . In this streaming setting, we are particularly interested in the case of a large d which prohibits one
to store a tensor of size d3 in memory.
Power Method: Matrices versus Tensors. Note that a
tensor of order two is just a matrix, and a popular approach for decomposing matrices is the so-called power

On the other hand, the different form of q (t) has the beneficial effect that the convergence is now exponentially faster
than in the matrix case. More precisely, if λi |ci | < λj |cj |,
t
t
than the gap between the coefficients (λi ci )2 and (λj cj )2
is now amplified much faster. We will show how to inherit
this nice property of faster convergence but at the same time
avoid the difficulty discussed above.

3. Orthogonal and Symmetric Tensors of
Order Three
In this section, we focus on the special case in which the
tensors to be decomposed are orthogonal, symmetric, and
of order three. Formally, there is an underlying tensor
T =

X
i∈[d]

λi · ui ⊗ ui ⊗ ui ,

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 1 Robust tensor power method
Input: Tensor T̄ ∈ Rd×d×d and parameters k, L, S, N .
Initialization Phase:
(0)
(0)
Sample w1 , . . . , wP
∼ N d (0, 1).
L , Y 1 , . . . , Yk
1
Compute w̄ = L j∈[L] T̄ (Id , wj , wj ).
Compute M̄ = T̄ (Id , Id , w̄).
Factorize Y (0) as Z (0) · R(0) by QR decomposition.
for s = 1 to S do
Compute Y (s) = M̄ · Z (s−1) .
Factorize Y (s) as Z (s) · R(s) by QR decomposition.
end for
Tensor Power Phase:
Let Q(0) = Z (S) .
for t = 1 to N do
(t)
(t−1)
(t−1)
Compute Yj = T̄ (Id , Qj
, Qj
), ∀j ∈ [k].
Factorize Y (t) as Q(t) · R(t) by QR decomposition.
end for
(N )
Output: ûj = Qj and λ̂j = T̄ (ûj , ûj , ûj ), ∀j ∈ [k].

with orthonormal vectors ui ’s and real λi ’s satisfying the
condition (1). Then given k ∈ [d] and ε ∈ (0, 1), our goal
is to find approximates to those λi and ui within distance ε,
but we only have access to some noisy tensor T̄ = T + Φ
for some symmetric perturbation tensor Φ.
Our algorithm is given in Algorithm 1, which consists of
two phases: the initialization phase and the tensor power
phase. The main phase is the tensor power phase, which
we will discuss in detail in Subsection 3.1. For our tensor power phase to work, it needs to have a good starting
point. This is provided by the initialization phase, which
we will discuss in detail in Subsection 3.2. Through these
two subsections, we will prove Theorem 1 below, which
summarizes the performance of our algorithm, according
to the following parameters of the tensor:
λ2i − λ2i+1
λi − λi+1
and ∆ = min
.
2
λi
4
i∈[k]
i∈[k]

γ = min

Theorem 1. Suppose ε ≤ λ2k and the perturbation tensor
2
0∆
√ , ∆ , α√
has the bound kΦk ≤ min{ 2∆ε
} for a small
k 3d
dk
enough constant α0 . Then for some L = O( γ12 log d), S =
O( γ1 log d), and N = O(log( γ1 log 1ε )), our Algorithm 1
with high probability will output ûi and λ̂i with kûi −ui k ≤
ε and |λ̂i − λi | ≤ ε for every i ∈ [k].
Let us make some remarks about the theorem. First, the L
samples are used to compute w̄ and M̄ , which can be done
in a parallel way. Second, our parameter γ is related to a
i+1
parameter γ 0 = mini∈[k] λi −λ
used in (Hardt & Price,
λi
2014), and it is easy to verify that γ ≥ γ 0 . Thus, our algorithm for tensors converges in O( γ1 log d + log( γ1 log 1ε ))
rounds, which is faster than the O( γ10 log d + γ10 log 1ε )

rounds of (Hardt & Price, 2014) for matrices. Note that
our dependence on the error parameter ε is exponentially
smaller than that of (Hardt & Price, 2014), which means
that for a small ε, we can decompose tensors much faster
than matrices. Finally, compared to previous works on
tensors, our convergence time, for a small ε is about
O(log log 1ε ) while those in (Anandkumar et al., 2014a;
Wang & Anandkumar, 2016) are at least Ω(k log log 1ε ).
3.1. Our Robust Tensor Power Method
The tensor power phase of our Algorithm 1 is based on
our version of the tensor power method, which works as
follows. At each step t, we maintain a d × k matrix Q(t)
(t)
(t)
with columns Q1 , . . . , Qk as our current estimators for
u1 , . . . , uk , which is obtained by updating the previous estimators with the following two operations.
The main operation is to apply the noisy tensor T̄ on them
simultaneously to get a d × k matrix Y (t) with its j-th col(t)
(t−1)
(t−1)
umn computed as Yj = T̄ (Id , Qj
, Qj
), which
equals
2
X 
(t−1)
(t)
λi u>
Q
· ui + Φ̂j ,
i
j
i∈[d]

for

(t)
Φ̂j

(t−1)

= Φ(Id , Qj

(t)

∀i ∈ [d] : u>
i Yj

(t−1)

, Qj
). This implies that

2
(t−1)
(t)
= λi u>
+ u>
i Qj
i Φ̂j ,

(2)

which shows the progress made by this operation.
The second operation is to orthogonalize Y (t) as
Y (t) = Q(t) · R(t) ,
by the QR decomposition via the Gram-Schmidt process,
(t)
(t)
to obtain a d × k matrix Q(t) with columns Q1 , . . . , Qk ,
which then become our new estimators. As we will show
in Lemma 1 below, given a small enough kΦk, if we start
with a full-rank Q(0) , then each Q(t) also has full rank and
consists of orthonormal columns, and each R(t) is invertible. Moreover, although we apply the QR decomposition
on the whole matrix Y (t) to obtain the matrix Q(t) , it has
the effect that for any m ∈ [k], the first m columns of Q(t)
can be seen as obtained from the first m columns of Y (t)
by a QR decomposition. This property is needed in our
Lemma 1 and Theorem 2 below to guarantee the simulta(t)
neous convergence of Qi to ui for every i ∈ [k].
Before stating Lemma 1 which guarantees the progress we
make at each step, let us prepare some notations first. For
a d × k matrix A and some m ∈ [k], let A[m] denote the
d × m matrix containing the first m columns of A. Let U
denote the d × k matrix with the target vector ui as its i’th
column. For a d × k matrix Q and some m ∈ [k], define


 >

cosm (Q) = minm U[m]
· Q[m] · y  /kQ[m] · yk,
y∈R

Tensor Decomposition via Simultaneous Power Iteration

which equals the cosine of the m’th principal angle between the column spaces of U[m] and Q[m] , let sinm (Q) =
p
1 − cos2m (Q), and let us use as the error measure
tanm (Q) = sinm (Q)/ cosm (Q).
More information about the principal angles can be found
in, e.g., (Golub & Van Loan, 1996). Then we have the
following lemma, which we prove in Appendix B.1.
(t)

Lemma 1. Fix any m ∈ [k] and t ≥ 0. Let Φ̂[m] denote
(t)

(t−1)

(t−1)

the d × m matrix with Φ̂j = Φ(Id , Qj
, Qj
) as its
j’th column, and suppose


n
o
 (t) 
(3)
Φ̂[m]  < ∆ · min β, cos2m (Q(t−1) ) ,
for some β > 0. Then for ρ = maxi∈[k] ( λλi+1
) 4 , we have
i
1

n
o
tanm (Q(t) ) ≤ max β, max{β, ρ} · tan2m (Q(t−1) ) .
Observe that the guarantee provided by the lemma above
has a similar form as that in (Hardt & Price, 2014) for
matrices. The main difference is that here in the tensor
case, we have the error measure essentially squared after each step, which has the following two implications.
First, to guarantee that the error is indeed reduced, we need
tanm (Q(t−1) ) to be small enough (say, less than one), unlike in the matrix case. Next, if we indeed have a small
enough tanm (Q(t−1) ), then the error can be reduced in a
much faster rate than in the matrix case. Another difference is that here we provide the guarantee for all the k sub(t)
matrices Q[m] , for m ∈ [k], instead of just one matrix Q(t) .
This allows us to show the simultaneous convergence of
(t)
each column Qi to the target vector ui for every i ∈ [k],
as given in the following, which we prove in Appendix B.2.
Theorem 2. For any ε ∈ (0, λ2k ), there exists some
N ≤ O(log( γ1 log 1ε )) such that the following holds. Sup√ and we
pose the perturbation is bounded by kΦk ≤ 2∆ε
k
start from some initial Q(0) with tanm Q(0) ≤ 1 for ev(t)
ery m ∈ [k]. Then for any t ≥ N , with ûi = Qi
and λ̂i = T̄ (ûi , ûi , ûi ), we have kui − ûi k ≤ ε and
|λi − λ̂i | ≤ ε, for every i ∈ [k].
Note that the convergence rate guaranteed by the theorem
above is exponentially faster than that in (Hardt & Price,
2014) for matrices, assuming that we indeed can have such
a good initial Q(0) to start with. In the next subsection, we
show how it can be found efficiently.
3.2. Initialization Procedure
Our approach for finding a good initialization is to project
the tensor down to a matrix and apply the matrix power

method for only a few steps just to make the tangents less
than one. Although we could continue applying the matrix
power method till reaching the much smaller target bound
ε, this would take exponentially longer than by switching
to the tensor power method as we actually do.
As mentioned above, we would first like to project the tensor T̄ down to a matrix. A naive approach is to sample
a random vectorPw̄ and take the matrix T̄ (Id , Id , w̄) ≈
T (Id , Id , w̄) = i∈[d] λi (u>
i w̄) · ui ⊗ ui . However, this
may mess up the gaps between eigenvalues, which are
needed to guarantee the convergence rate of the matrix
power method. The reason is that as each u>
i w̄ has mean
zero, the coefficient λi (u>
w̄)
also
has
mean
zero and thus
i
has a good chance of coming very close to others. To pre>
serve the gaps, we would like to have u>
i w̄ ≥ ui+1 w̄ for
each i with high probability. To achieve this, let us first
imagine sampling a random w ∈ Rd from N d (0, 1), and
computing the vector w̄ = T̄ (Id , w, w), which is close to
X
2
T (Id , w, w) =
λi (u>
i w) · ui .
i∈[d]
2
Then one can show that forP
every i, E[(u>
i w) ] = 1, so that
E[w̄] ≈ E[T (Id , w, w)] = i∈[d] λi ui , and
>
u>
i E[w̄] ≈ λi > λi+1 ≈ ui+1 E[w̄].

However, we want the gap-preserving guarantee to be in
high probability, instead in expectation. Thus we go further
by sampling not just one, but some number L of vectors
w1 , . . . , wL independently from the distribution N d (0, 1),
and then taking the average
w̄ =

1 X
T̄ (Id , wj , wj ).
L

(4)

j∈[L]

The following lemma shows that such a w̄ is likely to have
u>
i w̄ ≈ λi , which we prove in Appendix B.3.
∆
Lemma 2. Suppose we have T̄ = T + Φ with kΦk ≤ 3d
.
1
Then for some L ≤ O( γ 2 log d), the vector w̄ computed
according to (4) with high probability satisfies

 >

ui w̄ − λi  ≤ 1 (λi γ + 2∆) for every i ∈ [k].
4

(5)

With this w̄, we compute the matrix M̄ = T̄ (Id , Id , w̄).
As shown by the following lemma, which we prove in Appendix B.4, M̄ is close to a matrix with ui ’s as eigenvectors
and good gaps between eigenvalues.
Lemma 3. Suppose we have T̄ = T + Φ. Then for any w̄
satisfying the condition (5) in Lemma 2, the matrix M̄ =
T̄ (Id , Id , w̄) can be decomposed as
X
M̄ =
λ̄i · ui ⊗ ui + Φ̄,
i∈[d]

Tensor Decomposition via Simultaneous Power Iteration

for some λ̄i ’s with λ̄i − λ̄i+1 ≥ ∆2 , for i ∈ [k], and Φ̄ =
Φ(Id , Id , w̄) with kΦ̄k ≤ 2kΦk.
With such a matrix M̄ , we next apply the matrix power
method of (Hardt & Price, 2014) to find good approximates to its eigenvectors. More precisely, we sample an
initial matrix Y (0) ∈ Rd×k by choosing each of its column independently according to the distribution N d (0, 1),
and factorize it as Y (0) = Z (0) · R(0) by QR decomposition via the Gram-Schmidt process. Then at step s ≥ 1,
we multiply the previous estimate Z (s−1) by M̄ to obtain
Y (s) = M̄ · Z (s−1) , factorize it as Y (s) = Z (s) · R(s)
by QR decomposition via the Gram-Schmidt process, and
then take the orthonormal Z (s) as the new estimate. The
following lemma shows the number of steps needed to find
a good enough Z (s) .
Lemma 4. Suppose we are given a matrix M̄ having the
∆2
decomposition described in Lemma 3, with kΦ̄k ≤ 2α√0dk
for a small enough constant α0 . Then there exists some
S ≤ O( γ1 log d) such that with high probability, we have
tanm (Z (s) ) ≤ 1 for every m ∈ [k] whenever s ≥ S.
This together with the previous two lemmas guarantee that
∆2
∆ α√
given T̄ = T + Φ, with kΦk ≤ min{ 3d
, 0dk
}, for a small
enough constant α0 , we can obtain with high probability a
good Z (S) which can be used as the initial Q(0) for our
tensor power phase. Combining this with Theorem 2 in the
previous subsection, we then have our Theorem 1 given at
the beginning of the section.

4. General Orthogonal Tensors
In the previous section, we consider tensors which are orthogonal, symmetric, and of order three. In this section, we
show how to extend our results for general orthogonal tensors, to deal with higher orders first and then asymmetry.
4.1. Higher-Order Tensors
To handle orthogonal and symmetric tensors of any order, only the initialization procedure needs to be modified.
First, for tensors of any odd order, a straightforward modification is as follows. Take for example a tensor of order
2r + 1. Now we simply compute
1 X
w̄ =
T̄ (Id , wj , . . . , wj ),
L
j∈[L]

with 2r copies of wj , and similarly to Lemma
P 2, one can
show that w̄ is likely to be close to the vector i∈[d] λi · ui .
With such a vector w̄, one can show that the matrix
M̄ = T̄ (Id , Id , w̄, . . . , w̄),
P
with 2r − 1 copies of w̄, is close to the matrix i∈[d] λ2r
i ·
ui ⊗ ui , similarly to Lemma 3. Then the rest is the same

as that in the previous section. Note that this approach has
the eigenvalues decreased exponentially in r. A different
approach avoiding this is to compute M̄ directly as
M̄ =

2
1 X
T̄ (Id , Id , wj , . . . , wj ) ,
L
j∈[L]

which is close to
1 X
2
(T (Id , Id , wj , . . . , wj ))
L
j∈[L]

=

1 X X 2 > 4r−2
λi ui wj
· ui ⊗ ui
L

=

X

j∈[L] i∈[d]

λ2i

i∈[d]

1 X > 4r−2
ui wj
· ui ⊗ ui .
L
j∈[L]

Then one can again show
P that such a matrix M̄ is likely to
be close to the matrix i∈[d] λ2i · ui ⊗ ui .
To handle tensors of even orders, the initialization is
slightly different but the idea is similar. Given a tensor of
order 2r, we again sample vectors w1 , . . . , wL as before,
but now we compute the matrix directly as
M̄ =

1 X
T̄ (Id , Id , wj , . . . , wj ),
L
j∈[L]

with 2r − 2 copies of wj . As before, one canP
show that
the matrix M̄ is likely to be close to the matrix i∈[d] λi ·
ui ⊗ ui . Then again we can apply the matrix power method
on M̄ and obtain a good initialization for the tensor power
method as before. Note that now the eigenvalues are no
longer squared, and the previous requirement on kΦk can
be slightly relaxed, with the dependence on ∆2 being replaced by ∆.
4.2. Asymmetric Tensors
For simplicity of presentation, let us focus on the third order case; the extension to higher orders is straightforward.
That is, now the underlying tensor has the form
X
T =
λi · ai ⊗ bi ⊗ ci ,
i∈[d]

with nonnegative λi ’s satisfying the condition (1), together
with three sets of orthonormal vectors of ai ’s, bi ’s, and ci ’s.
As before, we only have access to a noisy version T̄ of T .
The main modification of our algorithm is again to the initialization procedure, but the idea is also similar. To find a
good initial matrix A for ai ’s, we sample w1 , . . . , wL independently from N d (0, 1), and now compute the matrix
M̄ =


>
1 X
T̄ (Id , Id , wj ) T̄ (Id , Id , wj ) .
L
j∈[L]

Tensor Decomposition via Simultaneous Power Iteration

As before, it is not hard to show that
X 1 X
2
M̄ ≈
λ2i c>
· ai ⊗ ai ,
i wj
L
i∈[d]

i∈[k]

j∈[L]

P
which is close to the matrix i∈[d] λ2i · ai ⊗ ai with high
probability. From the matrix M̄ , we can again apply the
matrix power method to find a good initial matrix A. Similarly, we can find good initial matrices B and C for bi ’s
and ci ’s, respectively.
Next, with such matrices, we would like to apply the tensor
power method, which we modify as follows. Now at each
step t, we take previous estimates A(t−1) , B (t−1) , C (t−1) ,
(t)
(t−1)
(t−1)
(t)
and compute Xi
= T̄ (Id , Bi
, Ci
), Yi
=
(t−1)
(t−1)
(t)
(t−1)
(t−1)
T̄ (Ai
, Id , Ci
), Zi = T̄ (Ai
, Bi
, Id ), for
i ∈ [k], followed by orthonormalizing X (t) , Y (t) , Z (t)
to obtain the new estimates A(t) , B (t) , C (t) via QRdecomposition. It is not hard to show that the resulting algorithm has a similar convergence rate as our Algorithm 1.

5. Nonorthogonal but Symmetric Tensors
In the previous section, we consider general orthogonal tensors, which can be asymmetric. In this section, we consider non-orthogonal tensors which are symmetric. We
remark that for some latent variable models such as the
multi-view model, the corresponding asymmetric tensors
can be converted into symmetric ones (Anandkumar et al.,
2014a), so that our result here can still be applied. For
simplicity of exposition, let us again focus on the case of
order
P three, so that the given tensor has the form T =
i∈[d] λi ·vi ⊗vi ⊗vi , but the vectors vi ’s are no longer assumed to be orthogonal to each other. Still we assume them
to be linearly independent, and we again assume without
loss of generality that kT k ≤ 1 and kvi k = 1 for each i. In
addition, let us assume, as in previous works, that λj = 0
for j ≥ k + 1.2
Following (Anandkumar et al., 2014a), we would like to
whiten such a tensor T into an orthogonal one, so that we
can then apply our Algorithm 1. More precisely, our goal is
to find a d × k matrix W such that the tensor T (W, W, W )
becomes orthogonal. As in (Anandkumar et al., 2014a),
assume that we also have available a matrix
X
M=
λi · vi ⊗ vi .3
i∈[k]

Then for a whitening matrix, it suffices to find some W
2

such that W > M W = Ik . The reason is that
X


Ik = W > M W =
λ i · W > vi ⊗ W > v i ,

This assumption is not necessary. We assume it just to simplify the first step of our algorithm given below. Without it, we
can simply replace that step by the matrix power method used in
our Algorithm 1, which takes more steps but can still do the job.
3
More generally, the weights λi in M are allowed to differ
from those in T , but for simplicity we assume they are the same.

√
which implies that the vectors λi W > vi , for i ∈ [k], are
orthonormal. Then the tensor T (W, W, W ) equals
⊗3
X
X 1 p
√ ·
λi · (W > vi )⊗3 =
λ i W > vi
,
λi
i∈[k]
i∈[k]
which has an orthogonal decomposition.
According to (Anandkumar et al., 2014a), one way to
find such a W is to do the spectral decomposition of M
as U ΛU > , with eigenvectors as columns of U , and let
1
W = U Λ− 2 . However, we will not take this approach,
because finding a good approximate to U by the matrix
power method would take longer to converge than the tensor power method which we will later apply to the whitened
tensor. Our key observation is that it suffices to find a d × k
matrix Q such that the matrix P = Q> M Q is invertible,
1
since we can then let W = QP − 2 and have
1

1

W > M W = P − 2 Q> M QP − 2 = Ik .
With such a W , the tensor T (W, W, W ) becomes orthogonal, so that we can decompose it4 to obtain σi = √1λ and
i
√
ui = λi W > vi , from which we can recover λi = σ12 and
1

i

vi = σi QP 2 ui if Q has orthonormal columns.
As before, we consider a similar setting in which we only
have access to a noisy M̄ = M + Φ̄, for some symmetric
perturbation matrix Φ̄, in addition to the noisy tensor T̄ =
T +Φ. Then our algorithm for finding the whitening matrix
consists of the following two steps:
1. Sample a random matrix Z ∈ Rd×k with orthonormal
columns, compute Ȳ = M̄ Z, and factorize it as Ȳ =
QR̄ by a QR decomposition.
1

2. Compute P̄ = Q> M̄ Q and output W̄ = QP̄ − 2 as
the whitening matrix.
We analyze our algorithm in the following. First note that
Q is computed in the same way as we compute Z (1) in
Algorithm 1, and with λk+1 = 0 we are likely to have
tank (Q) ≈ 0 so that the matrix P = Q> M Q is invertible. Formally, we have the following, which we prove in
Appendix C.1.
λk
Lemma 5. Suppose kΦ̄k ≤ α√0dk
for a small enough constant α0 . Then with high probability we have σmax (P ) ≤
λ1 and σmin (P ) ≥ λ2k .
4
To apply √
our Algorithm 1, we need to scale it properly, say
by a factor of λk /k to make its norm at most one.

Tensor Decomposition via Simultaneous Power Iteration

Next, with a small enough kΦ̄k, if P is invertible, then so
1
1
is P̄ , and moreover, we have P̄ − 2 ≈ P − 2 . This is shown
in the following, which we prove in Appendix C.2.
Lemma 6. Fix any  ∈ (0, 1) and suppose we have
σmin (P ) ≥ 2 and kΦ̄k ≤ . Then P̄ is invertible and
1
1
1
kP̄ − 2 − P − 2 k ≤ 2(σmin (P ))−2 (σmax (P )) 2 .
1

Then, with a good P̄ − 2 , we can obtain a good W̄ and have
T̄ (W̄ , W̄ , W̄ ) close to T (W, W, W ) which has an orthogonal decomposition. This is shown in the following, which
we prove in Appendix C.3.
Theorem 3. Fix any ε ∈ (0, λ4k ) and suppose we have
3

λ3

k
kΦk ≤ α0 λk2 ε and kΦ̄k ≤ α0 ε min{ √λdk
, √λk }, for a
1
small enough constant α0 . Then with high probability we
have kT̄ (W̄ , W̄ , W̄ ) − T (W, W, W )k ≤ ε.

6. Streaming setting
In the previous sections, we consider the batch setting in
which the tensor T̄ is assumed to be stored somewhere
which can be accessed whenever we want to. However,
storing such a tensor, say of order three, requires a space
complexity of Ω(d3 ), which becomes impractical even for
a moderate value of d. In this section, we study the possibility of achieving a space complexity of O(kd), which is the
least amount of memory needed just to store the k vectors
in Rd . More precisely, we consider the streaming setting,
in which there is a stream of vectors x1 , x2 , . . . arriving
one at a time. We assume that each vector x is sampled independently from some distribution over Rd , with kxk ≤ 1
and some function g : Rd → Rd×d×d such that

we can choose a proper size for J. In fact, to save the total number of samples, we can follow the approach of (Li
et al., 2016) by choosing different sizes in different iterations of the matrix or tensor power method.
Following (Wang & Anandkumar, 2016), let us take the
specific case with g(x) = x ⊗ x ⊗ x as a concrete example
and focus on the orthogonal case studied in Section 3; it is
not hard to convert other algorithms of ours to the streaming setting.
P One can show that in this specific case, we have
E[x] = i∈[d] λi ui so that there is a more efficient way to
find a vector w̄ for producing the matrix M̄ in the initialization phase. Formally, we have the following lemma, which
we prove in Appendix D.1.
Lemma 7. There is an algorithm using O(d) space and
k
d
O( log
∆2 ) samples to find some w̄ ∈ R satisfying the condition (5) in Lemma 2 with high probability.
With such a vector w̄, we can then use the streaming algorithm of (Li et al., 2016) to find a good initial matrix Z for
the later tensor power phase. Formally, we have the following lemma, which we prove in Appendix D.2.
Lemma 8. Given w̄ from Lemma 7, we can use O(kd)
kd log

d

space and O( ∆4 γ γ ) samples to find some Z ∈ Rd×k
with tanm (Z) < 1, for any m ∈ [k], with high probability.
Having such a matrix Z, we can proceed to the tensor
power phase. Borrowing again the idea from (Li et al.,
2016), let us partition the incoming data into blocks of increasing sizes, with the t’th block Jt used to carry out one
(t)
(t−1)
(t−1)
tensor power iteration Yi = T̄ (t) (Id , QiP , Qi
), for
1
(t)
i ∈ [k], of Algorithm 1, with T̄ = |Jt | τ ∈Jt g(xτ ). In(t)

• E[g(x)] = T , and given x, u, v ∈ Rd , g(x)(Id , u, v)
can be computed in O(d) space.

stead of preparing this T̄ (t) and then computing each Yi ,
we now go through |Jt | steps of updates:
(t)

Such a function g is known to exist for some latent variable models (Ge et al., 2015; Wang & Anandkumar, 2016).
Given such a function, our algorithms in previous sections
can all be converted to work in the streaming setting using
O(kd) space. This is because all our operations involving tensors have the form T̄ (Id , u, v), for some u, v ∈ Rd ,
which can be realized as
!
1 X
1 X
g(xt ) (Id , u, v) =
(g(xt ) (Id , u, v)) ,
|J|
|J|
t∈J

t∈J

for a collection J of samples, with the righthand side above
clearly computable in O(kd) space.5 Then
P depending on
1
the distance we want between T̄ = |J|
t∈J g(xt ) and T ,
5

This also includes the initialization phase in which we now do
not store the matrix M̄ explicitly but instead replace the operation
M̄ Zi by T̄ (Id , Zi , w̄).

• For τ ∈ Jt do: Yi

(t)

= Yi

+

1
> (t−1) 2
) xτ .
|Jt | (xτ Qi

The block sizes are chosen carefully to keep kT̄ (t) − T k
small enough so that we can have tanm (Q(t) ) decreased in
a desirable rate. Here, we choose the parameters
n t
εo
c0 log(dt)
βt = max ρ2 −1 ,
and |Jt | =
,
2
∆2 βt2

(6)

for a large enough constant c0 , to make the condition (3)
in Lemma 1 hold with high probability so that we have
tanm (Q(t) ) ≤ βt . In Appendix D.3, we summarize our
algorithm and prove the following theorem.
Theorem 4. Given ε ∈ (0, λ2k ), with high probability we
can find λ̂i , ûi with |λ̂i − λi |, kûi − ui k ≤ ε, for any i ∈
[k], using O(kd) space and O(
samples.

d
kd log γ
∆4 γ

+

log(d log( γ1 log
∆2 γε2

1
ε ))

)

Tensor Decomposition via Simultaneous Power Iteration

References
Anandkumar, Animashree, Hsu, Daniel J, and Kakade,
Sham M. A method of moments for mixture models and
hidden markov models. In COLT, volume 1, pp. 4, 2012.
Anandkumar, Animashree, Ge, Rong, Hsu, Daniel,
Kakade, Sham M, and Telgarsky, Matus. Tensor decompositions for learning latent variable models. Journal of
Machine Learning Research, 15(1):2773–2832, 2014a.
Anandkumar, Animashree, Ge, Rong, and Janzamin, Majid. Guaranteed non-orthogonal tensor decomposition via alternating rank-1 updates. arXiv preprint
arXiv:1402.5180, 2014b.
Chaganty, Arun Tejasvi and Liang, Percy. Estimating
latent-variable graphical models using moments and
likelihoods. In ICML, pp. 1872–1880, 2014.
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Escaping from saddle pointsonline stochastic gradient for
tensor decomposition. In Proceedings of The 28th Conference on Learning Theory, pp. 797–842, 2015.
Golub, Gene H. and Van Loan, Charles F. Matrix Computation. John Hopkins University Press, 3rd edition,
1996.
Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861–2869, 2014.
Hillar, Christopher J and Lim, Lek-Heng. Most tensor
problems are NP-hard. Journal of the ACM (JACM), 60
(6), 2013.
Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009.
Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 473–481, 2016.
Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Prateek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886–2894,
2013.
Wang, Yining and Anandkumar, Anima. Online and
differentially-private tensor decomposition.
In Advances in Neural Information Processing Systems, pp.
3531–3539, 2016.

