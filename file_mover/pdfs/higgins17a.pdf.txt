DARLA: Improving Zero-Shot Transfer in Reinforcement Learning
Irina Higgins * 1 Arka Pal * 1 Andrei Rusu 1 Loic Matthey 1 Christopher Burgess 1 Alexander Pritzel 1
Matthew Botvinick 1 Charles Blundell 1 Alexander Lerchner 1

Abstract
Domain adaptation is an important open problem in deep reinforcement learning (RL). In
many scenarios of interest data is hard to obtain, so agents may learn a source policy in a
setting where data is readily available, with the
hope that it generalises well to the target domain. We propose a new multi-stage RL agent,
DARLA (DisentAngled Representation Learning
Agent), which learns to see before learning to act.
DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire
source policies that are robust to many domain
shifts - even with no access to the target domain.
DARLA significantly outperforms conventional
baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base
RL algorithms (DQN, A3C and EC).

1. Introduction
Autonomous agents can learn how to maximise future
expected rewards by choosing how to act based on incoming sensory observations via reinforcement learning
(RL). Early RL approaches did not scale well to environments with large state spaces and high-dimensional
raw observations (Sutton & Barto, 1998). A commonly
used workaround was to embed the observations in a
lower-dimensional space, typically via hand-crafted and/or
privileged-information features. Recently, the advent of
deep learning and its successful combination with RL has
enabled end-to-end learning of such embeddings directly
from raw inputs, sparking success in a wide variety of previously challenging RL domains (Mnih et al., 2015; 2016;
Jaderberg et al., 2017). Despite the seemingly universal
*

Equal contribution 1 DeepMind, 6 Pancras Square, Kings
Cross, London, N1C 4AG, UK. Correspondence to: Irina Higgins
<irinah@google.com>, Arka Pal <arkap@google.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

efficacy of deep RL, however, fundamental issues remain.
These include data inefficiency, the reactive nature and general brittleness of learnt policies to changes in input data
distribution, and lack of model interpretability (Garnelo
et al., 2016; Lake et al., 2016). This paper focuses on one
of these outstanding issues: the ability of RL agents to deal
with changes to the input distribution, a form of transfer
learning known as domain adaptation (Bengio et al., 2013).
In domain adaptation scenarios, an agent trained on a particular input distribution with a specified reward structure
(termed the source domain) is placed in a setting where the
input distribution is modified but the reward structure remains largely intact (the target domain). We aim to develop
an agent that can learn a robust policy using observations
and rewards obtained exclusively within the source domain.
Here, a policy is considered as robust if it generalises with
minimal drop in performance to the target domain without
extra fine-tuning.
Past attempts to build RL agents with strong domain adaptation performance highlighted the importance of learning good internal representations of raw observations (Finn
et al., 2015; Raffin et al., 2017; Pan & Yang, 2009; Barreto et al., 2016; Littman et al., 2001). Typically, these approaches tried to align the source and target domain representations by utilising observation and reward signals
from both domains (Tzeng et al., 2016; Daftry et al., 2016;
Parisotto et al., 2015; Guez et al., 2012; Talvitie & Singh,
2007; Niekum et al., 2013; Gupta et al., 2017; Finn et al.,
2017; Rajendran et al., 2017). In many scenarios, such as
robotics, this reliance on target domain information can be
problematic, as the data may be expensive or difficult to
obtain (Finn et al., 2017; Rusu et al., 2016). Furthermore,
the target domain may simply not be known in advance.
On the other hand, policies learnt exclusively on the source
domain using existing deep RL approaches that have few
constraints on the nature of the learnt representations often overfit to the source input distribution, resulting in poor
domain adaptation performance (Lake et al., 2016; Rusu
et al., 2016).
We propose tackling both of these issues by focusing instead on learning representations which capture an underlying low-dimensional factorised representation of the world
and are therefore not task or domain specific. Many nat-

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

Figure 1. Schematic representation of DARLA. Yellow represents
the denoising autoencoder part of the model, blue represents the
-VAE part of the model, and grey represents the policy learning
part of the model.

uralistic domains such as video game environments, simulations and our own world are well described in terms of
such a structure. Examples of such factors of variation are
object properties like colour, scale, or position; other examples correspond to general environmental factors, such as
geometry and lighting. We think of these factors as a set of
high-level parameters that can be used by a world graphics
engine to generate a particular natural visual scene (Kulkarni et al., 2015). Learning how to project raw observations
into such a factorised description of the world is addressed
by the large body of literature on disentangled representation learning (Schmidhuber, 1992; Desjardins et al., 2012;
Cohen & Welling, 2014; 2015; Kulkarni et al., 2015; Hinton et al., 2011; Rippel & Adams, 2013; Reed et al., 2014;
Yang et al., 2015; Goroshin et al., 2015; Kulkarni et al.,
2015; Cheung et al., 2015; Whitney et al., 2016; Karaletsos et al., 2016; Chen et al., 2016; Higgins et al., 2017).
Disentangled representations are defined as interpretable,
factorised latent representations where either a single latent
or a group of latent units are sensitive to changes in single
ground truth factors of variation used to generate the visual world, while being invariant to changes in other factors
(Bengio et al., 2013). The theoretical utility of disentangled
representations for supervised and reinforcement learning
has been described before (Bengio et al., 2013; Higgins
et al., 2017; Ridgeway, 2016); however, to our knowledge,
it has not been empirically validated to date.
We demonstrate how disentangled representations can improve the robustness of RL algorithms in domain adaptation scenarios by introducing DARLA (DisentAngled Representation Learning Agent), a new RL agent capable
of learning a robust policy on the source domain that
achieves significantly better out-of-the-box performance in
domain adaptation scenarios compared to various baselines. DARLA relies on learning a latent state representation that is shared between the source and target domains,
by learning a disentangled representation of the environment’s generative factors. Crucially, DARLA does not require target domain data to form its representations. Our
approach utilises a three stage pipeline: 1) learning to
see, 2) learning to act, 3) transfer. During the first stage,

DARLA develops its vision, learning to parse the world in
terms of basic visual concepts, such as objects, positions,
colours, etc. by utilising a stream of raw unlabelled observations – not unlike human babies in their first few months
of life (Leat et al., 2009; Candy et al., 2009). In the second
stage, the agent utilises this disentangled visual representation to learn a robust source policy. In stage three, we
demonstrate that the DARLA source policy is more robust
to domain shifts, leading to a significantly smaller drop in
performance in the target domain even when no further policy finetuning is allowed (median 270.3% improvement).
These effects hold consistently across a number of different RL environments (DeepMind Lab and Jaco/MuJoCo:
Beattie et al., 2016; Todorov et al., 2012) and algorithms
(DQN, A3C and Episodic Control: Mnih et al., 2015; 2016;
Blundell et al., 2016).

2. Framework
2.1. Domain adaptation in Reinforcement Learning
We now formalise domain adaptation scenarios in a reinforcement learning (RL) setting. We denote the source
and target domains as DS and DT , respectively. Each
domain corresponds to an MDP defined as a tuple DS ⌘
(SS , AS , TS , RS ) or DT ⌘ (ST , AT , TT , RT ) (we assume
a shared fixed discount factor ), each with its own state
space S, action space A, transition function T and reward
function R.1 In domain adaptation scenarios the states S
of the source and the target domains can be quite different,
while the action spaces A are shared and the transitions T
and reward functions R have structural similarity. For example, consider a domain adaptation scenario for the Jaco
robotic arm, where the MuJoCo (Todorov et al., 2012) simulation of the arm is the source domain, and the real world
setting is the target domain. The state spaces (raw pixels)
of the source and the target domains differ significantly due
to the perceptual-reality gap (Rusu et al., 2016); that is to
say, SS 6= ST . Both domains, however, share action spaces
(AS = AT ), since the policy learns to control the same set
of actuators within the arm. Finally, the source and target domain transition and reward functions share structural
similarity (TS ⇡ TT and RS ⇡ RT ), since in both domains
transitions between states are governed by the physics of
the world and the performance on the task depends on the
relative position of the arm’s end effectors (i.e. fingertips)
with respect to an object of interest.
2.2. DARLA
In order to describe our proposed DARLA framework, we
assume that there exists a set M of MDPs that is the set
1

For further background on the notation relating to the RL
paradigm, see Section A.1 in the Supplementary Materials.

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

of all natural world MDPs, and each MDP Di is sampled
from M. We define M in terms of the state space Ŝ that
contains all possible conjunctions of high-level factors of
variation necessary to generate any naturalistic observation
in any Di 2 M. A natural world MDP Di is then one
whose state space S corresponds to some subset of Ŝ. In
simple terms, we assume that there exists some shared underlying structure between the MDPs Di sampled from M.
We contend that this is a reasonable assumption that permits inclusion of many interesting problems, including being able to characterise our own reality (Lake et al., 2016).
We now introduce notation for two state space variables
that may in principle be used interchangeably within the
source and target domain MDPs DS and DT – the agent
observation state space S o , and the agent’s internal latent
state space S z .2 Sio in Di consists of raw (pixel) observations soi generated by the true world simulator from a sampled set of data generative factors ŝi , i.e. soi ⇠ Sim(ŝi ).
ŝi is sampled by some distribution or process Gi on Ŝ,
ŝi ⇠ Gi (Ŝ).

Using the newly introduced notation, domain adaptation
scenarios can be described as having different sampling
processes GS and GT such that ŝS ⇠ GS (Ŝ) and ŝT ⇠
GT (Ŝ) for the source and target domains respectively, and
then using these to generate different agent observation
states soS ⇠ Sim(ŝS ) and soT ⇠ Sim(ŝT ). Intuitively, consider a source domain where oranges appear in blue rooms
and apples appear in red rooms, and a target domain where
the object/room conjunctions are reversed and oranges appear in red rooms and apples appear in blue rooms. While
the true data generative factors of variation Ŝ remain the
same - room colour (blue or red) and object type (apples
and oranges) - the particular source and target distributions
GS and GT differ.

Typically deep RL agents (e.g. Mnih et al., 2015; 2016)
operating in an MDP Di 2 M learn an end-to-end mapping from raw (pixel) observations soi 2 Sio to actions
ai 2 Ai (either directly or via a value function Qi (soi , ai )
from which actions can be derived). In the process of doing so, the agent implicitly learns a function F : Sio ! Siz
that maps the typically high-dimensional raw observations
soi to typically low-dimensional latent states szi ; followed
by a policy function ⇡i : Siz ! Ai that maps the latent
states szi to actions ai 2 Ai . In the context of domain
adaptation, if the agent learns a naive latent state mapping function FS : SSo ! SSz on the source domain using reward signals to shape the representation learning, it
is likely that FS will overfit to the source domain and will
not generalise well to the target domain. Returning to our
2
Note that we do not assume these to be Markovian i.e. it is not
necessarily the case that p(sot+1 |sot ) = p(sot+1 |sot , sot 1 , . . . , so1 ),
and similarly for sz . Note the index t here corresponds to time.

intuitive example, imagine an agent that has learnt a policy to pick up oranges and avoid apples on the source domain. Such a source policy ⇡S is likely to be based on
an entangled latent state space SSz of object/room conjunctions: oranges/blue ! good, apples/red ! bad, since this
is arguably the most efficient representation for maximising expected rewards on the source task in the absence of
extra supervision signals suggesting otherwise. A source
policy ⇡S (a|szS ; ✓) based on such an entangled latent representation szS will not generalise well to the target domain
without further fine-tuning, since FS (soS ) 6= FS (soT ) and
therefore crucially SSz 6= STz .
On the other hand, since both ŝS ⇠ GS (Ŝ) and ŝT ⇠
GT (Ŝ) are sampled from the same natural world state
space Ŝ for the source and target domains respectively, it
should be possible to learn a latent state mapping function
F̂ : S o ! SŜz , which projects the agent observation state
space S o to a latent state space SŜz expressed in terms of
factorised data generative factors that are representative of
the natural world i.e. SŜz ⇡ Ŝ. Consider again our intuitive

example, where F̂ maps agent observations (soS : orange
in a blue room) to a factorised or disentangled representation expressed in terms of the data generative factors (szŜ :
room type = blue; object type = orange). Such a disentangled latent state mapping function should then directly
generalise to both the source and the target domains, so that
F̂(soS ) = F̂(soT ) = szŜ . Since SŜz is a disentangled representation of object and room attributes, the source policy
⇡S can learn a decision boundary that ignores the irrelevant room attributes: oranges ! good, apples ! bad. Such
a policy would then generalise well to the target domain
out of the box, since ⇡S (a|F̂(soS ); ✓) = ⇡T (a|F̂(soT ); ✓) =
⇡T (a|szŜ ; ✓). Hence, DARLA is based on the idea that a

good quality F̂ learnt exclusively on the source domain
DS 2 M will zero-shot-generalise to all target domains
Di 2 M, and therefore the source policy ⇡(a|SŜz ; ✓) will
also generalise to all target domains Di 2 M out of the
box.
Next we describe each of the stages of the DARLA pipeline
that allow it to learn source policies ⇡S that are robust to
domain adaptation scenarios, despite being trained with no
knowledge of the target domains (see Fig. 1 for a graphical
representation of these steps):
1) Learn to see (unsupervised learning of FU ) – the task
of inferring a factorised set of generative factors SŜz = Ŝ
from observations S o is the goal of the extensive disentangled factor learning literature (e.g. Chen et al., 2016; Higgins et al., 2017). Hence, in stage one we learn a mapping
FU : SUo ! SUz , where SUz ⇡ SŜz (U stands for ‘unsupervised’) using an unsupervised model for learning disentangled factors that utilises observations collected by an
agent with a random policy ⇡U from a visual pre-training

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

MDP DU 2 M. Note that we require sufficient variability of factors and their conjunctions in DU in order to have
SUz ⇡ SŜz ;

2) Learn to act (reinforcement learning of ⇡S in the source
domain DS utilising previously learned FU ) – an agent
that has learnt to see the world in stage one in terms of the
natural data generative factors is now exposed to a source
domain DS 2 M. The agent is tasked with learning the
source policy ⇡S (a|szS ; ✓), where szS = FU (soS ) ⇡ szŜ , via
a standard reinforcement learning algorithm. Crucially, we
do not allow FU to be modified (e.g. by gradient updates)
during this phase;
3) Transfer (to a target domain DT ) – in the final step, we
test how well the policy ⇡S learnt on the source domain
generalises to the target domain DT 2 M in a zero-shot
domain adaptation setting, i.e. the agent is evaluated on the
target domain without retraining. We compare the performance of policies learnt with a disentangled latent state SŜz
to various baselines where the latent state mapping function FU projects agent observations so to entangled latent
state representations sz .
2.3. Learning disentangled representations
In order to learn FU , DARLA utilises -VAE (Higgins
et al., 2017), a state-of-the-art unsupervised model for automated discovery of factorised latent representations from
raw image data. -VAE is a modification of the variational autoencoder framework (Kingma & Welling, 2014;
Rezende et al., 2014) that controls the nature of the learnt
latent representations by introducing an adjustable hyperparameter to balance reconstruction accuracy with latent
channel capacity and independence constraints. It maximises the objective:
L(✓, ; x, z, ) =Eq

(z|x) [log p✓ (x|z)]

DKL (q (z|x)||p(z))

(1)

where , ✓ parametrise the distributions of the encoder and
the decoder respectively. Well-chosen values of - usually
larger than one ( > 1) - typically result in more disentangled latent representations z by limiting the capacity of the
latent information channel, and hence encouraging a more
efficient factorised encoding through the increased pressure
to match the isotropic unit Gaussian prior p(z) (Higgins
et al., 2017).
2.3.1. P ERCEPTUAL S IMILARITY L OSS
The cost of increasing is that crucial information about
the scene may be discarded in the latent representation z,
particularly if that information takes up a small proportion
of the observations x in pixel space. We encountered this
issue in some of our tasks, as discussed in Section 3.1.
The shortcomings of calculating the log-likelihood term

Eq (z|x) [log p✓ (x|z)] on a per-pixel basis are known and
have been addressed in the past by calculating the reconstruction cost in an abstract, high-level feature space given
by another neural network model, such as a GAN (Goodfellow et al., 2014) or a pre-trained AlexNet (Krizhevsky
et al., 2012; Larsen et al., 2016; Dosovitskiy & Brox,
2016; Warde-Farley & Bengio, 2017). In practice we found
that pre-training a denoising autoencoder (Vincent et al.,
2010) on data from the visual pre-training MDP DU 2 M
worked best as the reconstruction targets for -VAE to
match (see Fig. 1 for model architecture and Sec. A.3.1 in
Supplementary Materials for implementation details). The
new -VAEDAE model was trained according to Eq. 2:
L(✓, ; x, z, ) =Eq

(z|x)

kJ(x̂)

2

J(x)k2

DKL (q (z|x)||p(z))

(2)

where x̂ ⇠ p✓ (x|z) and J : RW ⇥H⇥C ! RN is the function that maps images from pixel space with dimensionality
W ⇥ H ⇥ C to a high-level feature space with dimensionality N given by a stack of pre-trained DAE layers up to a
certain layer depth. Note that by replacing the pixel based
reconstruction loss in Eq. 1 with high-level feature reconstruction loss in Eq. 2 we are no longer optimising the variational lower bound, and -VAEDAE with = 1 loses its
equivalence to the Variational Autoencoder (VAE) framework as proposed by (Kingma & Welling, 2014; Rezende
et al., 2014). In this setting, the only way to interpret is as
a mixing coefficient that balances the capacity of the latent
channel z of -VAEDAE against the pressure to match the
high-level features within the pre-trained DAE.
2.4. Reinforcement Learning Algorithms
We used various RL algorithms (DQN, A3C and Episodic
Control: Mnih et al., 2015; 2016; Blundell et al., 2016) to
learn the source policy ⇡ S during stage two of the pipeline
using the latent states sz acquired by -VAE based models
during stage one of the DARLA pipeline.
Deep Q Network (DQN) (Mnih et al., 2015) is a variant of
the Q-learning algorithm (Watkins, 1989) that utilises deep
learning. It uses a neural network to parametrise an approximation for the action-value function Q(s, a; ✓) using
parameters ✓.
Asynchronous Advantage Actor-Critic (A3C) (Mnih et al.,
2016) is an asynchronous implementation of the advantage
actor-critic paradigm (Sutton & Barto, 1998; Degris & Sutton, 2012), where separate threads run in parallel and perform updates to shared parameters. The different threads
each hold their own instance of the environment and have
different exploration policies, thereby decorrelating parameter updates without the need for experience replay. Therefore, A3C is an online algorithm, whereas DQN learns its
policy offline, resulting in different learning dynamics be-

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

tween the two algorithms.
Model-Free Episodic Control (EC) (Blundell et al., 2016)
was proposed as a complementary learning system to the
other RL algorithms described above. The EC algorithm
relies on near-determinism of state transitions and rewards
in RL environments; in settings where this holds, it can exploit these properties to memorise which action led to high
returns in similar situations in the past. Since in its simplest
form EC relies on a lookup table, it learns good policies
much faster than value-function-approximation based deep
RL algorithms like DQN trained via gradient descent - at
the cost of generality (i.e. potentially poor performance in
non-deterministic environments).
We also compared our approach to that of UNREAL (Jaderberg et al., 2017), a recently proposed RL algorithm which
also attempts to utilise unsupervised data in the environment. The UNREAL agent takes as a base an LSTM A3C
agent (Mnih et al., 2016) and augments it with a number of
unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent besides the (sometimes
very sparse) extrinsic reward signals. This auxiliary learning tends to improve the representation learnt by the agent.
See Sec. A.6 in Supplementary Materials for further details
of the algorithms above.

3. Tasks
We evaluate the performance of DARLA on different task
and environment setups that probe subtly different aspects
of domain adaptation. As a reminder, in Sec. 2.2 we defined
Ŝ as a state space that contains all possible conjunctions
of high-level factors of variation necessary to generate any
naturalistic observation in any Di 2 M. During domain
adaptation scenarios agent observation states are generated
according to soS ⇠ SimS (ŝS ) and soT ⇠ SimT (ŝT ) for the
source and target domains respectively, where ŝS and ŝT
are sampled by some distributions or processes GS and GT
according to ŝS ⇠ GS (Ŝ) and ŝT ⇠ GT (Ŝ).
We use DeepMind Lab (Beattie et al., 2016) to test a version of domain adaptation setup where the source and target
domain observation simulators are equal (SimS = SimT ),
but the processes used to sample ŝS and ŝT are different (GS 6= GT ). We use the Jaco arm with a matching
MuJoCo simulation environment (Todorov et al., 2012) in
two domain adaptation scenarios: simulation to simulation (sim2sim) and simulation to reality (sim2real). The
sim2sim domain adaptation setup is relatively similar to
DeepMind Lab i.e. the source and target domains differ
in terms of processes GS and GT . However, there is a significant point of difference. In DeepMind Lab, all values of
factors in the target domain, ŝT , are previously seen in the
source domain; however, ŝS 6= ŝT as the conjunctions of

Figure 2. A: DeepMind Lab (Beattie et al., 2016) transfer task
setup. Different conjunctions of {room, object1 , object2 } were
used during different parts of the domain adaptation curriculum.
During stage one, DU (shown in yellow), we used a minimal set
spanning all objects and all rooms whereby each object is seen
in each room. Note there is no extrinsic reward signal or notion
of ‘task’ in this phase. During stage two, DS (shown in green),
the RL agents were taught to pick up cans and balloons and avoid
hats and cakes. The objects were always presented in pairs hat/can
and cake/balloon. The agent never saw the hat/can pair in the pink
room. This novel room/object conjunction was presented as the
target domain adaptation condition DT (shown in red) where the
ability of the agent to transfer knowledge of the objects’ value to
a novel environment was tested. B: -VAE reconstructions (bottom row) using frames from DeepMind Lab (top row). Due to
the increased
> 1 necessary to disentangle the data generative factors of variations the model lost information about objects.
See Fig. 3 for a model appropriately capturing objects. C: left –
sample frames from MuJoCo simulation environments used for
vision (phase 1, SU ) and source policy training (phase 2, SS );
middle – sim2sim domain adaptation test (phase 3, ST ); and right
– sim2real domain adaptation test (phase 3, ST ).

these factor values are different. In sim2sim, by contrast,
novel factor values are experienced in the target domain
(this accordingly also leads to novel factor conjunctions).
Hence, DeepMind Lab may be considered to be assessing
domain interpolation performance, whereas sim2sim tests
domain extrapolation.
The sim2real setup, on the other hand, is based on identical
processes GS = GT , but different observation simulators
SimS 6= SimT corresponding to the MuJoCo simulation
and the real world, which results in the so-called ‘perceptual reality gap’ (Rusu et al., 2016). More details of the
tasks are given below.
3.1. DeepMind Lab
DeepMind Lab is a first person 3D game environment with
rich visuals and realistic physics. We used a standard seekavoid object gathering setup, where a room is initialised
with an equal number of randomly placed objects of two
different types. One of the object varieties is ‘good’ (its collection is rewarded +1), while the other is ‘bad’ (its collection is punished -1). The full state space Ŝ consisted of all
conjunctions of two room types (pink and green based on
the colour of the walls) and four object types (hat, can, cake
and balloon) (see Fig. 2A). The source domain DS con-

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

tained environments with hats/cans presented in the green
room, and balloons/cakes presented in either the green or
the pink room. The target domain DT contained hats/cans
presented in the pink room. In both domains cans and balloons were the rewarded objects.
1) Learn to see: we used -VAEDAE to learn the disentangled latent state representation sz that includes both the
room and the object generative factors of variation within
DeepMind Lab. We had to use the high-level feature space
of a pre-trained DAE within the -VAEDAE framework
(see Section 2.3.1), instead of the pixel space of vanilla VAE , because we found that objects failed to reconstruct
when using the values of necessary to disentangle the
generative factors of variation within DeepMind Lab (see
Fig. 2B).
-VAEDAE was trained on observations soU collected by
an RL agent with a simple wall-avoiding policy ⇡U (otherwise the training data was dominated by close up images of walls). In order to enable the model to learn
F(soU ) ⇡ Ŝ, it is important to expose the agent to at least
a minimal set of environments that span the range of values for each factor, and where no extraneous correlations
are added between different factors3 (see Fig. 2A, yellow).
See Section A.3.1 in Supplementary Materials for details
of -VAEDAE training.
2) Learn to act: the agent was trained with the algorithms detailed in Section 2.4 on a seek-avoid task using the source domain (DS ) conjunctions of object/room
shown in Fig. 2A (green). Pre-trained -VAEDAE from
stage one was used as the ‘vision’ part of various RL algorithms (DQN, A3C and Episodic Control: Mnih et al.,
2015; 2016; Blundell et al., 2016) to learn a source policy
⇡S that picks up balloons and avoids cakes in both the green
and the pink rooms, and picks up cans and avoids hats in
the green rooms. See Section A.3.1 in Supplementary Materials for more details of the various versions of DARLA
we have tried, each based on a different base RL algorithm.
3) Transfer: we tested the ability of DARLA to transfer the
seek-avoid policy ⇡S it had learnt on the source domain in
stage two using the domain adaptation condition DT illustrated in Figure 2A (red). The agent had to continue picking
up cans and avoid hats in the pink room, even though these
objects had only been seen in the green room during source
policy training. The optimal policy ⇡T is one that maintains
the reward polarity from the source domain (cans are good
and hats are bad). For further details, see Appendix A.2.1.
3
In our setup of DeepMind Lab domain adaptation task, the
object and environment factors are supposed to be independent. In
order to ensure that -VAEDAE learns a factorised representation
that reflects this ground truth independence, we present observations of all possible conjunctions of room and individual object
types.

3.2. Jaco Arm and MuJoCo
We used frames from an RGB camera facing a robotic
Jaco arm, or a matching rendered camera view from a
MuJoCo physics simulation environment (Todorov et al.,
2012) to investigate the performance of DARLA in two
domain adaptation scenarios: 1) simulation to simulation (sim2sim), and 2) simulation to reality (sim2real).
The sim2real setup is of particular importance, since the
progress that deep RL has brought to control tasks in simulation (Schulman et al., 2015; Mnih et al., 2016; Levine
& Abbeel, 2014; Heess et al., 2015; Lillicrap et al., 2015;
Schulman et al., 2016) has not yet translated as well to reality, despite various attempts (Tobin et al., 2017; Tzeng
et al., 2016; Daftry et al., 2016; Finn et al., 2015; Rusu
et al., 2016). Solving control problems in reality is hard due
to sparse reward signals, expensive data acquisition and the
attendant danger of breaking the robot (or its human minders) during exploration.
In both sim2sim and sim2real, we trained the agent to perform an object reaching policy where the goal is to place
the end effector as close to the object as possible. While
conceptually the reaching task is simple, it is a hard control
problem since it requires correct inference of the arm and
object positions and velocities from raw visual inputs.
1) Learn to see: -VAE was trained on observations collected in MuJoCo simulations with the same factors of
variation as in DS . In order to enable the model to learn
F(soU ) ⇡ ŝ, a reaching policy was applied to phantom objects placed in random positions - therefore ensuring that
the agent learnt the independent nature of the arm position
and object position (see Fig. 2C, left);
2) Learn to act: a feedforward-A3C based agent with the
vision module pre-trained in stage one was taught a source
reaching policy ⇡S towards the real object in simulation
(see Fig. 2C (left) for an example frame, and Sec. A.4
in Supplementary Materials for a fuller description of the
agent). In the source domain DS the agent was trained on
a distribution of camera angles and positions. The colour
of the tabletop on which the arm rests and the object colour
were both sampled anew every episode.
3) Transfer: sim2sim: in the target domain, DT , the agent
was faced with a new distribution of camera angles and positions with little overlap with the source domain distributions, as well as a completely held out set of object colours
(see Fig. 2C, middle). sim2real: in the target domain DT
the camera position and angle as well as the tabletop colour
and object colour were sampled from the same distributions as seen in the source domain DS , but the target domain DT was now the real world. Many details present
in the real world such as shadows, specularity, multiple
light sources and so on are not modelled in the simulation;

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning
Disentangled

Environment

room id

z1

turn left

Object

turn right

z2

distance left object rotation

z3

z4

z5

Entangled

object id

z6

-3

-3

0

0

3

z7

z1

z2

z3

z4

3

Figure 3. Plot of traversals of various latents of an entangled and
a disentangled version of -VAEDAE using frames from DeepMind Lab (Beattie et al., 2016).

Object

Disentangled

close/far left/right

z1

z2

Entangled

Arm

wrist turn close/far up/down right/left

z3

z4

z5

z6

-3

-3

0

0

3

z1

z2

z3

z4

3

Figure 4. Plot of traversals of -VAE on MuJoCo. Using a disentangled -VAE model, single latents directly control for the factors responsible for the object or arm placements.

the physics engine is also not a perfect model of reality.
Thus sim2real tests the ability of the agent to cross the
perceptual-reality gap and generalise its source policy ⇡S
to the real world (see Fig. 2C, right). For further details,
see Appendix A.2.2.

4. Results
We evaluated the robustness of DARLA’s policy ⇡S learnt
on the source domain to various shifts in the input data distribution. In particular, we used domain adaptation scenarios based on the DeepMind Lab seek-avoid task and
the Jaco arm reaching task described in Sec. 3. On each
task we compared DARLA’s performance to that of various baselines. We evaluated the importance of learning
‘good’ vision during stage one of the pipeline, i.e one that
maps the input observations so to disentangled representations sz ⇡ ŝ. In order to do this, we ran the DARLA
pipeline with different vision models: the encoders of a

disentangled -VAE 4 (the original DARLA), an entangled -VAE (DARLAENT ), and a denoising autoencoder
(DARLADAE ). Apart from the nature of the learnt representations sz , DARLA and all versions of its baselines
were equivalent throughout the three stages of our proposed pipeline in terms of architecture and the observed
data distribution (see Sec. A.3 in Supplementary Materials
for more details).
Figs. 3-4 display the degree of disentanglement learnt by
the vision modules of DARLA and DARLAENT on DeepMind Lab and MuJoCo. DARLA’s vision learnt to independently represent environment variables (such as room
colour-scheme and geometry) and object-related variables
(change of object type, size, rotation) on DeepMind Lab
(Fig. 3, left). Disentangling was also evident in MuJoCo.
Fig. 4, left, shows that DARLA’s single latent units zi learnt
to represent different aspects of the Jaco arm, the object,
and the camera. By contrast, in the representations learnt
by DARLAENT , each latent is responsible for changes to
both the environment and objects (Fig. 3, right) in DeepMind Lab, or a mixture of camera, object and/or arm movements (Fig. 4, right) in MuJoCo.
The table in Fig. 5 shows the average performance (across
different seeds) in terms of rewards per episode of the various agents on the target domain with no fine-tuning of the
source policy ⇡S . It can be seen that DARLA is able to
zero-shot-generalise significantly better than DARLAENT
or DARLADAE , highlighting the importance of learning a
disentangled representation sz = szŜ during the unsupervised stage one of the DARLA pipeline. In particular, this
also demonstrates that the improved domain transfer performance is not simply a function of increased exposure to
training observations, as both DARLAENT and DARLADAE
were exposed to the same data. The results are mostly consistent across target domains and in most cases DARLA is
significantly better than the second-best-performing agent.
This holds in the sim2real task 5 , where being able to perform zero-shot policy transfer is highly valuable due to the
particular difficulties of gathering data in the real world.
DARLA’s performance is particularly surprising as it actually preserves less information about the raw observations
so than DARLAENT and DARLADAE . This is due to the
nature of the -VAE and how it achieves disentangling; the
disentangled model utilised a significantly higher value of
the hyperparameter than the entangled model (see Appendix A.3 for further details), which constrains the ca4
In this section of the paper, we use the term -VAE to refer to a standard -VAE for the MuJoCo experiments, and a
-VAEDAE for the DeepMind Lab experiments (as described in
stage 1 of Sec. 3.1).
5
See https://youtu.be/sZqrWFl0wQ4 for example sim2sim
and sim2real zero-shot transfer policies of DARLA and baseline
A3C agent.

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning
Table 1. Transfer performance

DQN

D EEP M IND L AB
A3C

EC

SIM 2 SIM

BASELINE AGENT
UNREAL
DARLA FT
DARLA ENT
DARLA DAE

1.86 ± 3.91
13.36 ± 5.8
3.45 ± 4.47
7.83 ± 4.47

5.32 ± 3.36
4.13 ± 3.95
1.4 ± 2.16
15.66 ± 5.19
6.74 ± 2.81

-0.41 ± 4.21
5.69 ± 3.73
5.59 ± 3.37

97.64 ± 9.02
86.59 ± 5.53
84.77 ± 4.42
85.15 ± 7.43

94.56 ± 3.55
99.25 ± 2.3
59.99 ± 15.05
100.72 ± 4.7

DARLA

10.25 ± 5.46

19.7 ± 5.43

11.41 ± 3.52

100.85 ± 2.92

108.2 ± 5.97

V ISION T YPE

JACO (A3C)

SIM 2 REAL

DARLA’ S PERFORMANCE IS SIGNIFICANTLY DIFFERENT FROM ALL BASELINES UNDER W ELCH ’ S UNEQUAL VARIANCES T- TEST WITH p < 0.01 (N 2 [60, 150]).

165
166
167
168
169
170
171
172
173
174
175
176
177
178

Figure 5. Table: Zero-shot performance (avg. reward per episode) of the source policy ⇡S in target domains within DeepMind Lab and
Jaco/MuJoCo environments. Baseline agent refers to vanilla DQN/A3C/EC (DeepMind Lab) or A3C (Jaco) agents. See main text for
more detailed model descriptions. Figure: Correlation between zero-shot performance transfer performance on the DeepMind Lab task
obtained by EC based DARLA and the level of disentanglement as measured by the transfer/disentanglement score (r = 0.6, p < 0.001)

pacity of the latent channel. Indeed, DARLA’s -VAE
only utilises 8 of its possible 32 Gaussian latents to store
observation-specific information for MuJoCo/Jaco (and 20
in DeepMind Lab), whereas DARLAENT utilises all 32 for
both environments (as does DARLADAE ).
Furthermore, we examined what happens if DARLA’s vision (i.e. the encoder of the disentangled -VAE ) is allowed to be fine-tuned via gradient updates while learning
the source policy during stage two of the pipeline. This
is denoted by DARLAFT in the table in Fig. 5. We see
that it exhibits significantly worse performance than that
of DARLA in zero-shot domain adaptation using an A3Cbased agent in all tasks. This suggests that a favourable
initialisation does not make up for subsequent overfitting
to the source domain for the on-policy A3C. However, the
off-policy DQN-based fine-tuned agent performs very well.
We leave further investigation of this curious effect for future work.
Finally, we compared the performance of DARLA to an
UNREAL (Jaderberg et al., 2017) agent with the same architecture. Despite also exploiting the unsupervised data
available in the source domain, UNREAL performed worse
than baseline A3C on the DeepMind Lab domain adaptation task. This further demonstrates that use of unsupervised data in itself is not a panacea for transfer performance; it must be utilised in a careful and structured
manner conducive to learning disentangled latent states
sz = szŜ .

In order to quantitatively evaluate our hypothesis that disentangled representations are essential for DARLA’s performance in domain adaptation scenarios, we trained various DARLAs with different degrees of learnt disentanglement in sz by varying (of -VAE ) during stage one of
the pipeline. We then calculated the correlation between
the performance of the EC-based DARLA on the DeepMind Lab domain adaptation task and the transfer metric,
which approximately measures the quality of disentanglement of DARLA’s latent representations sz (see Sec. A.5.2

in Supplementary Materials). This is shown in the chart in
Fig. 5; as can be seen, there is a strong positive correlation
between the level of disentanglement and DARLA’s zeroshot domain transfer performance (r = 0.6, p < 0.001).
Having shown the robust utility of disentangled representations in agents for domain adaptation, we note that there
is evidence that they can provide an important additional
benefit. We found significantly improved speed of learning
of ⇡S on the source domain itself, as a function of how disentangled the model was. The gain in data efficiency from
disentangled representations for source policy learning is
not the main focus of this paper so we leave it out of the
main text; however, we provide results and discussion in
Section A.7 in Supplementary Materials.

5. Conclusion
We have demonstrated the benefits of using disentangled
representations in a deep RL setting for domain adaptation.
In particular, we have proposed DARLA, a multi-stage RL
agent. DARLA first learns a visual system that encodes the
observations it receives from the environment as disentangled representations, in a completely unsupervised manner.
It then uses these representations to learn a robust source
policy that is capable of zero-shot domain adaptation.
We have demonstrated the efficacy of this approach in a
range of domains and task setups: a 3D naturalistic firstperson environment (DeepMind Lab), a simulated graphics
and physics engine (MuJoCo), and crossing the simulation
to reality gap (MuJoCo to Jaco sim2real). We have also
shown that the effect of disentangling is consistent across
very different RL algorithms (DQN, A3C, EC), achieving
significant improvements over the baseline algorithms (median 2.7 times improvement in zero-shot transfer across
tasks and algorithms). To the best of our knowledge, this
is the first comprehensive empirical demonstration of the
strength of disentangled representations for domain adaptation in a deep RL setting.

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning

References
Abadi, Martin, Agarwal, Ashish, and et al, Paul Barham. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. Preliminary White Paper, 2015.
Barreto, André, Munos, Rémi, Schaul, Tom, and Silver, David.
Successor features for transfer in reinforcement learning.
CoRR, abs/1606.05312, 2016. URL http://arxiv.org/
abs/1606.05312.

Garnelo, Marta, Arulkumaran, Kai, and Shanahan, Murray. Towards deep symbolic reinforcement learning. arXiv, 2016.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., WardeFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. NIPS, pp. 26722680, 2014.
Goroshin, Ross, Mathieu, Michael, and LeCun, Yann. Learning
to linearize under uncertainty. NIPS, 2015.

Beattie, Charles, Leibo, Joel Z., Teplyashin, Denis, Ward, Tom,
and et al, Marcus Wainwright. Deepmind lab. arxiv, 2016.

Guez, Arthur, Silver, David, and Dayan, Peter. Efficient bayesadaptive reinforcement learning using sample-based search.
NIPS, 2012.

Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. In IEEE Transactions on
Pattern Analysis & Machine Intelligence, 2013.

Gupta, Abhishek, Devin, Coline, Liu, YuXuan, Abbeel, Pieter,
and Levine, Sergey. Learning invariant feature spaces to transfer skills with reinforcement learning. ICLR, 2017.

Blundell, Charles, Uria, Benigno, Pritzel, Alexander, Li, Yazhe,
Ruderman, Avraham, Leibo, Joel Z, Rae, Jack, Wierstra, Daan,
and Hassabis, Demis. Model-free episodic control. arXiv,
2016.

Heess, Nicolas, Wayne, Gregory, Silver, David, Lillicrap, Timothy P., Erez, Tom, and Tassa, Yuval. Learning continuous
control policies by stochastic value gradients. NIPS, 2015.

Candy, T. Rowan, Wang, Jingyun, and Ravikumar, Sowmya. Retinal image quality and postnatal visual experience during infancy. Optom Vis Sci, 86(6):556–571, 2009.
Chen, Xi, Duan, Yan, Houthooft, Rein, Schulman, John,
Sutskever, Ilya, and Abbeel, Pieter. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv, 2016.
Cheung, Brian, Levezey, Jesse A., Bansal, Arjun K., and Olshausen, Bruno A. Discovering hidden factors of variation in
deep networks. In Proceedings of the International Conference
on Learning Representations, Workshop Track, 2015.
Cohen, T. and Welling, M. Transformation properties of learned
visual representations. In ICLR, 2015.
Cohen, Taco and Welling, Max. Learning the irreducible representations of commutative lie groups. arXiv, 2014.
Daftry, Shreyansh, Bagnell, J. Andrew, and Hebert, Martial.
Learning transferable policies for monocular reactive mav control. International Symposium on Experimental Robotics,
2016.

Higgins, Irina, Matthey, Loic, Pal, Arka, Burgess, Christopher,
Glorot, Xavier, Botvinick, Matthew, Mohamed, Shakir, and
Lerchner, Alexander. Beta-vae: Learning basic visual concepts
with a constrained variational framework. In ICLR, 2017.
Hinton, G., Krizhevsky, A., and Wang, S. D. Transforming autoencoders. International Conference on Artificial Neural Networks, 2011.
Jaderberg, Max, Mnih, Volodymyr, Czarnecki, Wojciech Marian,
Schaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu,
Koray. Reinforcement learning with unsupervised auxiliary
tasks. ICLR, 2017.
Karaletsos, Theofanis, Belongie, Serge, and Rtsch, Gunnar.
Bayesian representation learning with oracle constraints.
ICLR, 2016.
Kingma, D. P. and Ba, Jimmy. Adam: A method for stochastic
optimization. arXiv, 2014.
Kingma, D. P. and Welling, M. Auto-encoding variational bayes.
ICLR, 2014.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convolutional neural networks.
NIPS, 2012.

Degris, Thomas, Pilarski Patrick M and Sutton, Richard S.
Model-free reinforcement learning with continuous action in
practice. American Control Conference (ACC), 22:21772182,
2012.

Kulkarni, Tejas, Whitney, William, Kohli, Pushmeet, and Tenenbaum, Joshua. Deep convolutional inverse graphics network.
NIPS, 2015.

Desjardins, G., Courville, A., and Bengio, Y. Disentangling factors of variation via generative entangling. arXiv, 2012.

Lake, Brenden M., Ullman, Tomer D., Tenenbaum, Joshua B.,
and Gershman, Samuel J. Building machines that learn and
think like people. arXiv, 2016.

Dosovitskiy, Alexey and Brox, Thomas. Generating images with
perceptual similarity metrics based on deep networks. arxiv,
2016.

Larsen, Anders Boesen Lindbo, Snderby, Sren Kaae, Larochelle,
Hugo, and Winther, Ole. Autoencoding beyond pixels using a
learned similarity metric. ICML, 2016.

Finn, Chelsea, Tan, Xin Yu, Duan, Yan, Darrell, Trevor, Levine,
Sergey, and Abbeel, Pieter. Deep spatial autoencoders for visuomotor learning. arxiv, 2015.

Leat, Susan J., Yadav, Naveen K., and Irving, Elizabeth L. Development of visual acuity and contrast sensitivity in children.
Journal of Optometry, 2009.

Finn, Chelsea, Yu, Tianhe, Fu, Justin, Abbeel, Pieter, and Levine,
Sergey. Generalizing skills with semi-supervised reinforcement learning. ICLR, 2017.

Levine, Sergey and Abbeel, Pieter. Learning neural network
policies with guided policy search under unknown dynamics.
NIPS, 2014.

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning
Lillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander,
Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and
Wierstra, Daan. Continuous control with deep reinforcement
learning. CoRR, 2015.
Littman, Michael L., Sutton, Richard S., and Singh, Satinder. Predictive representations of state. NIPS, 2001.
Marr, D. Simple memory: A theory for archicortex. Philosophical
Transactions of the Royal Society of London, pp. 2381, 1971.
McClelland, James L, McNaughton, Bruce L, and OReilly, Randall C. Why there are complementary learning systems in the
hippocampus and neocortex: insights from the successes and
failures of connectionist models of learning and memory. Psychological review, 102:419, 1995.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David S, and
Rusu, Andrei A. et al. Human-level control through deep reinforcement learning. Nature, 2015.
Mnih, Volodymyr, Badia, Adri Puigdomnech, Mirza, Mehdi,
Graves, Alex, Lillicrap, Timothy P., Harley, Tim, Silver, David,
and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. ICML, 2016. URL https://arxiv.
org/pdf/1602.01783.pdf.
Niekum, Scott, Chitta, Sachin, Barto, Andrew G, Marthi,
Bhaskara, and Osentoski, Sarah. Incremental semantically
grounded learning from demonstration. Robotics: Science and
Systems, 2013.
Norman, Kenneth A and O’Reilly, Randall C. Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach. Psychological review, 110:611, 2003.
Pan, Sinno Jialin and Yang, Quiang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering,
2009.
Parisotto, Emilio, Ba, Jimmy, and Salakhutdinov, Ruslan. Actormimic: Deep multitask and transfer reinforcement learning.
CoRR, 2015.
Pathak, Deepak, Krähenbühl, Philipp, Donahue, Jeff, Darrell,
Trevor, and Efros, Alexei A. Context encoders: Feature learning by inpainting. CoRR, abs/1604.07379, 2016. URL http:
//arxiv.org/abs/1604.07379.
Peng, J. Efficient dynamic programming-based learning for control. PhD thesis, Northeastern University, Boston., 1993.
Peng, Jing and Williams, Ronald J. Incremental multi-step qlearning. Machine Learning, 22:283290, 1996.
Puterman, Martin L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons, Inc.,
New York, NY, USA, 1st edition, 1994. ISBN 0471619779.
Raffin, Antonin, Hfer, Sebastian, Jonschkowski, Rico, Brock,
Oliver, and Stulp, Freek. Unsupervised learning of state representations for multiple tasks. ICLR, 2017.
Rajendran, Janarthanan, Lakshminarayanan, Aravind, Khapra,
Mitesh M., P, Prasanna, and Ravindran, Balaraman. Attend,
adapt and transfer: Attentive deep architecture for adaptive
transfer from multiple sources in the same domain. ICLR,
2017.

Reed, Scott, Sohn, Kihyuk, Zhang, Yuting, and Lee, Honglak.
Learning to disentangle factors of variation with manifold interaction. ICML, 2014.
Rezende, Danilo J., Mohamed, Shakir, and Wierstra, Daan.
Stochastic backpropagation and approximate inference in deep
generative models. arXiv, 2014.
Ridgeway, Karl. A survey of inductive biases for factorial
Representation-Learning. arXiv, 2016. URL http://
arxiv.org/abs/1612.05299.
Rippel, Oren and Adams, Ryan Prescott. High-dimensional probability estimation with deep density models. arXiv, 2013.
Rusu, Andrei A., Vecerik, Matej, Rothrl, Thomas, Heess, Nicolas,
Pascanu, Razvan, and Hadsell, Raia. Sim-to-real robot learning
from pixels with progressive nets. arXiv, 2016.
Schmidhuber, Jürgen. Learning factorial codes by predictability
minimization. Neural Computation, 4(6):863–869, 1992.
Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan,
Michael I., and Abbeel, Pieter. Trust region policy optimization. ICML, 2015.
Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan,
Michael, and Abbeel, Pieter. High-dimensional continuous
control using generalized advantage estimation. ICLR, 2016.
Sutherland, Robert J and Rudy, Jerry W. Configural association
theory: The role of the hippocampal formation in learning,
memory, and amnesia. Psychobiology, 17:129144, 1989.
Sutton, Richard S. and Barto, Andrew G. Reinforcement Learning: An Introduction. MIT Press, 1998.
Talvitie, Erik and Singh, Satinder. An experts algorithm for transfer learning. In Proceedings of the 20th international joint conference on Artifical intelligence, 2007.
Tobin, Josh, Fong, Rachel, Ray, Alex, Schneider, Jonas, Zaremba,
Wojciech, and Abbeel, Pieter. Domain randomization for transferring deep neural networks from simulation to the real world.
arxiv, 2017.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for
model-based control. IROS, 2012.
Tulving, Endel, Hayman, CA, and Macdonald, Carol A. Longlasting perceptual priming and semantic learning in amnesia: a
case experiment. Journal of Experimental Psychology: Learning, Memory, and Cognition, 17:595, 1991.
Tzeng, Eric, Devin, Coline, Hoffman, Judy, Finn, Chelsea,
Abbeel, Pieter, Levine, Sergey, Saenko, Kate, and Darrell,
Trevor. Adapting deep visuomotor representations with weak
pairwise constraints. WAFR, 2016.
Vincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio,
Yoshua, and Manzagol, Pierre-Antoine. Stacked denoising autoencoders: Learning useful representations in a deep network
with a local denoising criterion. NIPS, 2010.
Warde-Farley, David and Bengio, Yoshua. Improving generative
adversarial networks with denoising feature matching. ICLR,
2017.

DARLA: Improving Zero-Shot Transfer in Reinforcement Learning
Watkins, Christopher John Cornish Hellaby. Learning from delayed rewards. PhD thesis, University of Cambridge, Cambridge, UK, 1989.
Whitney, William F., Chang, Michael, Kulkarni, Tejas, and
Tenenbaum, Joshua B. Understanding visual concepts with
continuation learning. arXiv, 2016. URL http://arxiv.
org/pdf/1602.06822.pdf.
Yang, Jimei, Reed, Scott, Yang, Ming-Hsuan, and Lee, Honglak.
Weakly-supervised disentangling with recurrent transformations for 3d view synthesis. NIPS, 2015.

