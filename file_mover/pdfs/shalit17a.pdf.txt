Estimating individual treatment effect: generalization bounds and algorithms

Uri Shalit * 1 Fredrik D. Johansson * 2 David Sontag 2 3

Abstract
There is intense interest in applying machine
learning to problems of causal inference in fields
such as healthcare, economics and education.
In particular, individual-level causal inference
has important applications such as precision
medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational
data, under the assumption known as strong ignorability. The algorithms learn a ‚Äúbalanced‚Äù
representation such that the induced treated and
control distributions look similar, and we give
a novel and intuitive generalization-error bound
showing the expected ITE estimation error of a
representation is bounded by a sum of the standard generalization-error of that representation
and the distance between the treated and control distributions induced by the representation.
We use Integral Probability Metrics to measure
distances between distributions, deriving explicit
bounds for the Wasserstein and Maximum Mean
Discrepancy (MMD) distances. Experiments on
real and simulated data show the new algorithms
match or outperform the state-of-the-art.

1. Introduction
Making predictions about causal effects of actions is a central problem in many domains. For example, a doctor deciding which medication will cause better outcomes for a
patient; a government deciding who would benefit most
from subsidized job training; or a teacher deciding which
study program would most benefit a specific student. In
this paper we focus on the problem of making these predictions based on observational data. Observational data is
*

Equal contribution 1 CIMS, New York University, New
York, NY 10003 2 IMES, MIT, Cambridge, MA 02142
3
CSAIL, MIT, Cambridge, MA 02139.
Correspondence
to: Uri Shalit <shalit@cs.nyu.edu>, Fredrik D. Johansson
<fredrikj@mit.edu>, David Sontag <dsontag@csail.mit.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

data which contains past actions, their outcomes, and possibly more context, but without direct access to the mechanism which gave rise to the action. For example we might
have access to records of patients (context), their medications (actions), and outcomes, but we do not have complete
knowledge of why a specific action was applied to a patient.
The hallmark of learning from observational data is that
the actions observed in the data depend on variables which
might also affect the outcome, resulting in confounding:
For example, richer patients might better afford certain
medications, and job training might only be given to those
motivated enough to seek it. The challenge is how to untangle these confounding factors and make valid predictions.
Specifically, we work under the common simplifying assumption of ‚Äúno-hidden confounding‚Äù, assuming that all
the factors determining which actions were taken are observed. In the examples above, it would mean that we have
measured a patient‚Äôs wealth or an employee‚Äôs motivation.
As a learning problem, estimating causal effects from observational data is different from classic learning in that in
our training data we never see the individual-level effect.
For each unit, we only see their response to one of the possible actions - the one they had actually received. This is
close to what is known in the machine learning literature as
‚Äúlearning from logged bandit feedback‚Äù (Strehl et al., 2010;
Swaminathan & Joachims, 2015), with the distinction that
we do not have access to the model generating the action.
Our work differs from much work in causal inference in
that we focus on the individual-level causal effect (‚Äúcspecific treatment effects‚Äù Shpitser & Pearl (2006); Pearl
(2015)), rather than the average or population level. Our
main contribution is to give what is, to the best of our
knowledge, the first generalization-error1 bound for estimating individual-level causal effect, where each individual is identified by its features x. The bound leads naturally to a new family of representation-learning based algorithms (Bengio et al., 2013), which we show to match or
outperform state-of-the-art methods on several causal effect inference tasks.
1

Our use of the term generalization is different from its use in
the study of transportability, where the goal is to generalize causal
conclusion across distributions (Bareinboim & Pearl, 2016).

Estimating individual treatment effect: generalization bounds and algorithms

We frame our results using the Neyman-Rubin potential
outcomes framework (Rubin, 2011), as follows. We assume that for a unit with features x ‚àà X , and an action
(also known as treatment or intervention) t ‚àà {0, 1}, there
are two potential outcomes: Y0 and Y1 . For each unit we
only observe one of the potential outcomes, according to
treatment assignment: if t = 0 we observe y = Y0 , if
t = 1, we observe y = Y1 ; this is known as the consistency assumption. For example, x can denote the set of lab
tests and demographic factors of a diabetic patient, t = 0
denote the standard medication for controlling blood sugar,
t = 1 denotes a new medication, and Y0 and Y1 indicate the
patient‚Äôs blood sugar level if they were to be given medications t = 0 and t = 1, respectively.
We will denote m1 (x) = E [Y1 |x], m0 (x) = E [Y0 |x].
We are interested in learning the function œÑ (x) :=
E [Y1 ‚àí Y0 |x] = m1 (x) ‚àí m0 (x). œÑ (x) is the expected
treatment effect of t = 1 relative to t = 0 on a unit with
characteristics x, or the Individual Treatment Effect (ITE)2 .
Our goal is to find
h an estimate
i œÑÃÇ of œÑ such that some loss
2
function, e.g. E (œÑÃÇ ‚àí œÑ ) , is small. For example, for a
patient with features x, we attempt to predict which of two
treatments will have a better outcome. The fundamental
problem of causal inference is that for any x in our data we
only observe Y1 or Y0 , but never both.
As mentioned above, we make an important ‚Äúno-hidden
confounders‚Äù assumption, in order to make the conditional causal effect identifiable. We formalize this assumption by using the standard strong ignorability condition:
(Y1 , Y0 ) ‚ä•
‚ä• t|x, and 0 < p(t = 1|x) < 1 for all x. Strong
ignorability is a sufficient condition for the ITE function
œÑ (x) to be identifiable (Imbens & Wooldridge, 2009; Pearl,
2015; Rolling, 2014): see proof in the supplement. The validity of strong ignorability cannot be assessed from data,
and must be determined by domain knowledge and understanding of the causal relationships between the variables.
One approach to the problem of estimating the function
œÑ (x) is by learning the two functions m0 (x) and m1 (x)
using samples from p(Yt |x, t). This is similar to a standard machine learning problem of learning from finite samples. However, there is an additional source of variance at
work here: For example, if mostly rich patients received
treatment t = 1, and mostly poor patients received treatment t = 0, we might have an unreliable estimation of
m1 (x) for poor patients. In this paper we upper bound this
additional source of variance using an Integral Probability Metric (IPM) measure of distance between two distributions p(x|t = 0), and p(x|t = 1), also known as the
control and treated distributions. In practice we use two
specific IPMs: the Maximum Mean Discrepancy (Gretton
2

Also known as Conditional Average Treatment Effect, CATE.

ùëñùëì	 ¬†ùë° = 1

ùë•

‚Ä¶

‚Ñé&
‚Ä¶

ùêø(‚Ñé&(Œ¶), ùë¶ = ùëå&)

‚Ä¶

ùêø ‚Ñé6 Œ¶ , ùë¶ = ùëå6

Œ¶
ùëñùëì	 ¬†ùë° = 0

ùë°

‚Ñé6

456 45&
IPM0 (ùëù23
, ùëù23 )

Figure 1. Neural network architecture for ITE estimation. L is
a loss function, IPMG is an integral probability metric. Note that
only one of h0 and h1 is updated for each sample during training.

et al., 2012), and the Wasserstein distance (Villani, 2008;
Cuturi & Doucet, 2014). We show that the expected error
in learning the individual treatment effect function œÑ (x) is
upper bounded by the error of learning Y1 and Y0 , plus the
IPM term. In the randomized controlled trial setting, where
t‚ä•
‚ä• x, the IPM term is 0, and our bound naturally reduces
to a standard learning problem of learning two functions.
The bound we derive points the way to a family of algorithms based on the idea of representation learning (Bengio et al., 2013): Jointly learn hypotheses for both treated
and control on top of a representation which minimizes a
weighted sum of the factual loss (the standard supervised
machine learning objective), and the IPM distance between
the control and treated distributions induced by the representation. This can be viewed as learning the functions m0
and m1 under a constraint that encourages better generalization across the treated and control populations. In the
Experiments section we apply algorithms based on neural
nets as representations and hypotheses, along with MMD
or Wasserstein distributional distances over the representation layer; see Figure 1 for the basic architecture.
In his foundational text on causality, Pearl (2009) writes:
‚ÄúWhereas in traditional learning tasks we attempt to generalize from one set of instances to another, the causal modeling task is to generalize from behavior under one set of
conditions to [...] another set. Causal models should therefore be chosen by a criterion that challenges their stability
against changing conditions‚Äù [emphasis ours]. We believe
our work points the way to one such stability criterion, for
causal inference in the strongly ignorable case.

2. Related work
Much recent work in machine learning for causal inference focuses on causal discovery, with the goal of discovering the underlying causal graph or causal direction from
data (Hoyer et al., 2009; Maathuis et al., 2010; Triantafillou & Tsamardinos, 2015; Mooij et al., 2016). We focus
on the case when the causal setup is simple and known
to be of the form (Y1 , Y0 ) ‚ä•
‚ä• x|t, with no hidden confounders. Under the causal model we assume, the most
common goal of causal effect inference as used in the ap-

Estimating individual treatment effect: generalization bounds and algorithms

plied sciences is to obtain the average treatment effect:
AT E = Ex‚àºp(x) [œÑ (x)]. We will briefly discuss how some
standard statistical causal effect inference methods relate to
our proposed method. Note that most of these approaches
assume some form of ignorability.
One of the most widely used approaches to estimating ATE
is covariate (or back-door) adjustment, also known as the
G-computation formula (Robins, 1986; Pearl, 2009). In
their basic version, covariate adjustment methods aim to
estimate the functions m1 (x), m0 (x), and are therefore
natural candidates for estimating ITE as well as ATE, using the estimates of mt (x). Most previous work on this
subject focused on asymptotic consistency (Belloni et al.,
2014; Athey et al., 2016; Chernozhukov et al., 2016), and
so far there has not been much work on the generalizationerror of such a procedure. One view of our results is that
we point out a previously unaccounted for source of variance when using covariate adjustment to estimate ITE. We
suggest a new type of regularization, by learning representations with reduced IPM distance between treated and control, enabling a new type of bias-variance trade-off.
Another widely used family of statistical methods used in
causal effect inference are weighting methods. Methods
such as inverse propensity score weighting (Austin, 2011)
re-weight the units in the observational data so as to make
the treated and control populations more comparable, and
have been used for estimating conditional effects as well
(Cole et al., 2003). The major challenge, especially in
high-dimensional cases, is controlling the variance of the
estimates (Swaminathan & Joachims, 2015). Doubly robust methods go further and combine propensity score reweighting and covariate adjustment in clever ways to reduce model bias (Funk et al., 2011).
Adapting machine learning methods for causal effect inference, and in particular for individual level treatment effect,
has gained much interest recently. For example Wager &
Athey (2015); Athey & Imbens (2016) discuss how treebased methods can be adapted to obtain a consistent estimator with semi-parametric asymptotic convergence rate.
Recent work has also looked into how machine learning
methods can help detect heterogeneous treatment effects
when some data from randomized experiments is available
(Taddy et al., 2016; Peysakhovich & Lada, 2016). Neural
nets have also been used for this purpose, exemplified in
early work by Beck et al. (2000), and more recently by
Hartford et al. (2016)‚Äôs work on deep instrumental variables. Our work differs from all the above by focusing
on the generalization-error aspects of estimating individual
treatment effect, as opposed to asymptotic consistency, and
by focusing solely on the observational study case, with no
randomized components or instrumental variables.
Our work has strong connections with work on domain

adaptation. In particular, estimating ITE requires prediction of outcomes over a different distribution from the observed one. Our ITE error upper bound has similarities with
generalization bounds in domain adaptation given by BenDavid et al. (2007); Mansour et al. (2009); Ben-David et al.
(2010); Cortes & Mohri (2014). These bounds employ distribution distance metrics such as the A-distance or the discrepancy metric, which are related to the IPM distance we
use. Our algorithm is similar to a recent algorithm for domain adaptation by Ganin et al. (2016), and in principle
other domain adaptation methods (e.g. DaumeÃÅ III (2007);
Pan et al. (2011); Sun et al. (2016)) could be adapted for
use in ITE estimation as presented here.
Finally, our paper builds on Johansson et al. (2016), where
we showed a connection between covariate shift and the
task of estimating counterfactuals. We proposed learning a
representation of the data that makes the treated and control
distributions more similar, fitting a linear ridge-regression
model on top of it. We bounded the relative error of fitting
a ridge-regression using the distribution with reverse treatment assignment versus fitting a ridge-regression using the
factual distribution. Unfortunately, the relative error bound
is not at all informative regarding the absolute quality of
the representation. In this paper we focus on a related but
more substantive task: estimating the individual treatment
effect, building on the counterfactual error term. We provide an informative bound on the absolute quality of the
representation. We also derive a much more flexible family
of algorithms, including non-linear hypotheses and much
more powerful distribution metrics in the form of IPMs
such as the Wasserstein and MMD distances. Finally, we
conduct significantly more thorough experiments including
a real-world dataset and out-of-sample performance, and
show our methods outperform previously proposed ones.

3. Estimating ITE: Error bounds
In this section we prove a bound on the expected error in
estimating the individual treatment effect for a given representation, and a hypothesis defined over that representation.
The bound is expressed in terms of (1) the expected loss
of the model when learning the observed outcomes y as a
function of x and t, denoted F , F standing for ‚ÄúFactual‚Äù;
(2) an Integral Probability Metric (IPM) distance between
the distribution of treated and control units. The term F
is the classic machine learning generalization-error, and in
turn can be upper bounded using the empirical error and
model complexity terms, applying standard machine learning theory (Shalev-Shwartz & Ben-David, 2014).
3.1. Problem setup
We will employ the following assumptions and notations.
The most important notations are in the Notation box in the

Estimating individual treatment effect: generalization bounds and algorithms

supplement. The space of covariates is a bounded subset
X ‚äÇ Rd . The outcome space is Y ‚äÇ R. Treatment t is a
binary variable. We assume there exists a joint distribution
p(x, t, Y0 , Y1 ), such that (Y1 , Y0 ) ‚ä•
‚ä• t|x and 0 < p(t =
1|x) < 1 for all x ‚àà X (strong ignorability). The treated
and control distributions are the distribution of the features
x conditioned on treatment: pt=1 (x) := p(x|t = 1), and
pt=0 (x) := p(x|t = 0), respectively.
Throughout this paper we will discuss representation functions of the form Œ¶ : X ‚Üí R, where R is the representation space. We make the following assumption about Œ¶:
Assumption 1. The representation Œ¶ is a twicedifferentiable, one-to-one function. Without loss of generality we will assume that R is the image of X under Œ¶.
We then have Œ® : R ‚Üí X as the inverse of Œ¶, such that
Œ®(Œ¶(x)) = x for all x ‚àà X .
The representation Œ¶ pushes forward the treated and control distributions into the new space R; we denote the induced distribution by pŒ¶ .
t=0
Definition 1. Define pt=1
Œ¶ (r) := pŒ¶ (r|t = 1), pŒ¶ (r) :=
pŒ¶ (r|t = 0), to be the treated and control distributions
induced over R. For a one-to-one Œ¶, the distributions
t=0
pt=1
Œ¶ (r) and pŒ¶ (r) can be obtained by the standard
change of variables formula, using the determinant of the
Jacobian of Œ®(r).

Let Œ¶ : X ‚Üí R be a representation function, and h :
R √ó {0, 1} ‚Üí Y be an hypothesis defined over the representation space R. Let L : Y √ó Y ‚Üí R+ be a loss function. We define two complimentary loss functions: one is
the standard machine learning loss, which we will call the
factual loss F , as it relates to observable quantities. The
other is the expected loss with respect to the distribution
where the treatment assignment is flipped, which we call
the counterfactual loss, CF .
Definition 2. The expected loss for the unit
and
treatment pair (x, t) is:
`h,Œ¶ (x, t)
=
R
L(Y
,
h(Œ¶(x),
t))p(Y
|x)dY
.
The
expected
factual
t
t
t
Y
and counterfactual losses of h and Œ¶ are:
Z
F (h, Œ¶) =
`h,Œ¶ (x, t) p(x, t) dxdt,
X √ó{0,1}
Z
CF (h, Œ¶) =
`h,Œ¶ (x, t) p(x, 1 ‚àí t) dxdt.
X √ó{0,1}

If x denotes patients‚Äô features, t a treatment, and Yt a potential outcome such as mortality, we think of F as measuring
how well do h and Œ¶ predict mortality for the patients and
doctors‚Äô actions sampled from the same distribution as our
data sample. CF measures how well our prediction with
h and Œ¶ would do in a ‚Äútopsy-turvy‚Äù world where the patients are the same but the doctors are inclined to prescribe

exactly the opposite treatment than the one the real-world
doctors would prescribe.
Definition 3. The expected factual treated and control
losses are:
Z
t=1
(h,
Œ¶)
=
`h,Œ¶ (x, 1) pt=1 (x) dx,
F
ZX
t=0
F (h, Œ¶) = `h,Œ¶ (x, 0) pt=0 (x) dx.
X

For u := p(t = 1), it is immediate to show that F (h, Œ¶) =
t=0
ut=1
F (h, Œ¶) + (1 ‚àí u)F (h, Œ¶).
Definition 4. The treatment effect (ITE) for unit x is:
œÑ (x) := E [Y1 ‚àí Y0 |x] .
Let f : X √ó {0, 1} ‚Üí Y by an hypothesis. For example,
we could have that f (x, t) = h(Œ¶(x), t).
Definition 5. The treatment effect estimate of the hypothesis f for unit x is:
œÑÃÇf (x) = f (x, 1) ‚àí f (x, 0).
Definition 6. The expected Precision in Estimation of Heterogeneous Effect (PEHE, Hill (2011)) loss of f is:
Z
2
PEHE (f ) =
(œÑÃÇf (x) ‚àí œÑ (x)) p(x) dx,
(1)
X

When f (x, t) = h(Œ¶(x), t), we will also use the notation
PEHE (h, Œ¶) = PEHE (f ).
Our proof relies on the notion of an Integral Probability
Metric (IPM), which is a class of metrics between probability distributions (Sriperumbudur et al., 2012; MuÃàller,
1997). For two probability density functions p, q defined
over S ‚äÜ Rd , and for a function family G of functions
g : S ‚Üí R, we have that
Z




IPMG (p, q) := sup  g(s)(p(s) ‚àí q(s)) ds .
g‚ààG

S

Integral probability metrics are always symmetric and obey
the triangle inequality, and trivially satisfy IPMG (p, p) =
0. For rich enough function families G, we also have that
IPMG (p, q) = 0 =‚áí p = q, and then IPMG is a true
metric. Examples of function families G for which IPMG
is a true metric are the family of bounded continuous functions, the family of 1-Lipschitz functions (Sriperumbudur
et al., 2012), and the unit-ball of functions in a universal
reproducing kernel Hilbert space (Gretton et al., 2012).
3.2. Bounds
We first state a Lemma bounding the counterfactual loss, a
key step in obtaining the bound on the error in estimating

Estimating individual treatment effect: generalization bounds and algorithms

individual treatment effect. We then give the main Theorem. The proofs and details are in the supplement.

have been used for various machine learning tasks in recent
years (Gretton et al., 2009; 2012; Cuturi & Doucet, 2014).

Let u := p(t = 1) be the marginal probability of treatment.
By the strong ignorability assumption, 0 < u < 1.

In order to explicitly evaluate the constant BŒ¶ in Theorem
1, we have to make some assumptions about the elements
of the problem. For the Wasserstein case these are the loss
L, the Lipschitz constants of p(Yt |x) and h, and the condition number of the Jacobian of Œ¶. For the MMD case,
we make assumptions about the RKHS representability and
RKHS norms of h , Œ¶, and the standard deviation of Yt |x.
The full details are given in the supplement, with the major
results stated in Theorems 2 and 3. In all cases we obtain
that making Œ¶ smaller increases the constant BŒ¶ precluding trivial solutions such as making Œ¶ arbitrarily small.

Lemma 1. Let Œ¶ : X ‚Üí R be a one-to-one representation
function, with inverse Œ®. Let h : R √ó {0, 1} ‚Üí Y be an
hypothesis. Let G be a family of functions g : R ‚Üí Y. Assume there exists a constant BŒ¶ > 0, such that for fixed t ‚àà
{0, 1}, the per-unit expected loss functions `h,Œ¶ (Œ®(r), t)
(Definition 2) obey B1Œ¶ ¬∑ `h,Œ¶ (Œ®(r), t) ‚àà G. We have:
CF (h, Œ¶) ‚â§
t=0
(1 ‚àí u)t=1
F (h, Œ¶) + uF (h, Œ¶)

t=0
+ BŒ¶ ¬∑ IPMG pt=1
,
Œ¶ , pŒ¶

where CF , t=0
and t=1
are as in Definitions 2 and 3.
F
F
Theorem 1. Under the conditions of Lemma 1, and assuming the loss L used to define `h,Œ¶ in Definitions 2 and 3 is
the squared loss, we have:
PEHE (h, Œ¶) ‚â§

2 CF (h, Œ¶) + F (h, Œ¶) ‚àí 2œÉY2 ‚â§
2

t=1
t=0
F (h, Œ¶)+F (h, Œ¶)+BŒ¶ IPMG

t=0
pt=1
Œ¶ , pŒ¶



(2)

‚àí2œÉY2 ,

where F and CF are defined w.r.t. the squared loss, and
œÉY2 is the variance of the outcomes Yt (see Definition A11
in Appendix for detailed definition).
The main idea of the proof is showing that PEHE is upper
bounded by the sum of the expected factual loss F and
expected counterfactual loss CF . However, we cannot estimate CF , since we only have samples relevant to F . We
therefore bound the difference CF ‚àí F using an IPM.
Choosing a small function family G makes the bound
tighter. However, choosing too small a family could result in an incomputable bound. For example, for the minimal choice G = {`h,Œ¶ (x, 0), `h,Œ¶ (x, 1)}, we will have to
evaluate an expectation term of Y1 over pt=0
Œ¶ , and of Y0
over pt=1
.
We
cannot
in
general
evaluate
these expectaŒ¶
tions, since by assumption when t = 0 we only observe
Y0 , and the same for t = 1 and Y1 . In addition, for some
function families there is no known way to efficiently compute the IPM distance or its gradients. Here, we use two
function families for which there are available optimization tools. The first is the family of 1-Lipschitz functions,
which leads to IPM being the Wasserstein distance (Villani, 2008), denoted Wass(p, q). The second is the family
of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric (Gretton et al., 2012), denoted MMD(p, q). Both the Wasserstein and MMD metrics
have consistent estimators which can be efficiently computed for finite samples (Sriperumbudur et al., 2012), and

For an empirical sample, and a family of representations
and hypotheses, we can further upper bound t=0
and t=1
F
F
by their respective empirical losses and a model complexity term using standard arguments (Shalev-Shwartz & BenDavid, 2014). The IPMs we use can be consistently estimated from finite samples (Sriperumbudur et al., 2012).
The negative variance term œÉY2 arises from the fact that,
following Hill (2011); Athey & Imbens (2016), we define
the error PEHE in terms of the conditional mean functions
mt (x), as opposed to fitting the random variables Yt .
Our results hold for any given h and Œ¶ obeying the Theorem conditions. This immediately suggest an algorithm
in which we minimize the upper bound in Eq. (2) with respect to Œ¶ and h and either the Wasserstein or MMD IPM,
in order to minimize the error in estimating the individual
treatment effect. This leads us to Algorithm 1 below.

4. Algorithm for estimating ITE
We propose a general framework called CFR (for Counterfactual Regression) for ITE estimation based on the theoretical results above. Our algorithm is an end-to-end, regularized minimization procedure which fits both a balanced
representation of the data and a hypothesis for the outcome.
CFR draws on the same intuition as our previous work (Johansson et al., 2016), but overcomes the following limitations: a) Our previous theory requires a two-step optimization procedure and is specific to linear hypotheses (it does
not support e.g. deep neural networks), b) The treatment indicator might be washed out in the old model, if the learned
representation is high-dimensional (see discussion below).
We assume there exists a distribution p(x, t, Y0 , Y1 ) over
X √ó {0, 1} √ó Y √ó Y, such that strong ignorability holds.
We further assume we have a sample from that distribution
(x1 , t1 , y1 ), . . . (xn , tn , yn ), where yi ‚àº p(Y1 |xi ) if ti = 1,
yi ‚àº p(Y0 |xi ) if ti = 0. This standard assumption means
that the treatment assignment determines which potential
outcome we see. Our goal is to find a representation Œ¶ :
X ‚Üí R and hypothesis h : X √ó {0, 1} ‚Üí Y that will

Estimating individual treatment effect: generalization bounds and algorithms

minimize PEHE (f ) for f (x, t) := h(Œ¶(x), t).
We parameterize Œ¶(x) and h(Œ¶, t) by deep neural networks
trained jointly, see Figure 1. This allows for learning complex non-linear representations and hypotheses with large
flexibility. In Johansson et al. (2016), we parameterized
h(Œ¶, t) with a single network, concatenating Œ¶ and t as input. In this case, if Œ¶ is high-dimensional, the influence of
t on h might be lost during training. To combat this, we parameterize h1 (Œ¶) = h(Œ¶, 1) and h0 (Œ¶) = h(Œ¶, 0) as two
separate ‚Äúheads‚Äù of the joint network, the former used to
estimate the outcome under treatment, and the latter under
control. This way, statistical power is shared in representation layers, while the effect of treatment is preserved in
the separate heads. Note that each sample is used to update
only the head corresponding to the observed treatment.
Our second contribution is to explicitly adjust for the bias
induced by treatment group imbalance. To this end, we
seek a representation Œ¶ and hypothesis h that minimizes a
trade-off between predictive accuracy and imbalance in the
representation space, using the following objective:
min

h,Œ¶
kŒ¶k=1

1
n

Pn

i=1

Algorithm 1 CFR: Counterfactual regression with integral
probability metrics
1: Input: Factual sample (x1 , t1 , y1 ), . . . , (xn , tn , yn ),

scaling parameter Œ± > 0, loss function L (¬∑, ¬∑), representation network Œ¶W with initial weights W, outcome network hV with initial weights V, function
family G for IPM.
Pn
Compute u = n1 i=1 ti
ti
1‚àíti
Compute wi = 2u
+ 2(1‚àíu)
for i = 1 . . . n
while not converged do
Sample mini-batch {i1 , i2 , . . . , im } ‚äÇ {1, 2, . . . , n}
Calculate the gradient of the IPM term:
g1 =‚àáW IPMG ({Œ¶W (xij )}tij =0 , {Œ¶W (xik )}tij =1 )
Calculate theP
gradients of the empirical loss: 
1
w ¬∑ L hV (Œ¶W (xij ), tij ), yij
g2 = ‚àáV m

Pj ij
1
g3 = ‚àáW m j wij ¬∑ L hV (Œ¶W (xij ), tij ), yij
Obtain step size scalar or matrix Œ∑ with standard
neural net methods e.g. Adam (Kingma & Ba, 2015)
[W, V] ‚Üê [W ‚àí Œ∑(Œ±g1 + g3 ), V ‚àí Œ∑(g2 + 2ŒªV)]
Check convergence criterion
end while

2:
3:
4:
5:
6:
7:

8:
9:
10:
11:

wi ¬∑ L (h(Œ¶(xi ), ti ) , yi ) + Œª ¬∑ R(h)

with

+Œ± ¬∑ IPMG ({Œ¶(xi )}i:ti =0 , {Œ¶(xi )}i:ti =1 ) ,
Pn
1‚àíti
ti
+ 2(1‚àíu)
, where u = n1 i=1 ti ,
wi = 2u

and

R is a model complexity term.

(3)

Note that u = p(t = 1) is simply the proportion of treated
units in the population. The weights wi compensate for the
difference in treatment group size in our sample, see Theorem 1. IPMG (¬∑, ¬∑) is the (empirical) integral probability
metric w.r.t. G. For most IPMs, we cannot compute the
factor BœÜ in (2), but treat it as part of the hyperparameter
Œ±. This makes our objective sensitive to the scaling of Œ¶,
even for a constant Œ±. We therefore normalize Œ¶ through
either projection or batch-normalization with fixed scale.
We refer to the model minimizing (3) with Œ± > 0 as Counterfactual Regression (CFR) and the variant without balance regularization (Œ± = 0) as Treatment-Agnostic Representation Network (TARNet). Both models are trained
by minimizing (3) using stochastic gradient descent, as described in Algorithm 1. Both the prediction loss and the
penalty term IPMG (¬∑, ¬∑) are computed for one mini-batch
at a time. Details of how to obtain the gradient g1 with
respect to the empirical IPMs are in the supplement.

5. Experiments
Evaluating causal inference algorithms is more difficult
than many machine learning tasks, since we rarely have access to the ground truth treatment effect. Existing literature
mostly deals with this in two ways. One is by using (semi-)

synthetic datasets, where the outcome or treatment assignment are fully known; we use the semi-synthetic IHDP
dataset from Hill (2011). The other is using real-world
data from randomized controlled trials (RCT). The problem
with using data from RCTs is that there is no imbalance between treatment groups, making our method redundant. We
partially overcome this problem by using the Jobs dataset
from LaLonde (1986), which includes both a randomized
and a non-randomized component. We use both components for training, but only use the randomized component
for evaluation. This alleviates, but does not solve, the issue
of a completely randomized and balanced dataset being unsuited for our method.
We evaluate our framework CFR, and its variant without balancing regularization (TARNet), in the task of estimating ITE and ATE. Both versions are implemented3
as feed-forward neural networks with 3 fully-connected
exponential-linear layers (Clevert et al., 2016) for the representation and 3 for the hypothesis. Layer sizes were 200 for
all layers used for Jobs and 200 and 100 for the representation and hypothesis used for IHDP. The model is trained
using Adam (Kingma & Ba, 2015). The hypothesis parameters are regularized with a small `2 weight decay. For continuous data we use mean squared loss and for binary data,
we use log-loss. While our theory does not immediately
apply to log-loss, we were curious to see how our model
performs with it. We use the Wasserstein (CFR WASS ) and
the squared linear MMD (CFR MMD ) distances to penalize
3

https://github.com/clinicalml/cfrnet

Estimating individual treatment effect: generalization bounds and algorithms

imbalance.

We consider two different estimation tasks. One is withinsample, where the task is to estimate ITE for all units in
a sample for which the (factual) outcome of one treatment
is observed. This corresponds to the common scenario in
which a cohort is selected once and not changed. This task
is non-trivial, as we never observe the ITE for any unit. The
other is out-of-sample, where the goal is to estimate ITE for
units with no observed outcomes. This corresponds to the
problem of selecting the best treatment for a new patient.
Within-sample error is computed over both the training and
validation sets, out-of-sample error over the test set.
5.1. Simulated outcome: IHDP
Hill (2011) compiled a dataset for causal effect estimation based on the Infant Health and Development Program
(IHDP), in which the covariates come from a randomized
experiment studying the effects of specialist home visits on
cognitive test scores. The treatment groups have been made
imbalanced by removing a biased subset of the treated population. The dataset comprises 747 units (139 treated, 608
control) and 25 covariates measuring aspects of children
and their mothers. We use the simulated outcome implemented as setting ‚ÄúA‚Äù in the NPCI package (Dorie, 2016).
Following Hill (2011), we use the noiseless outcome to
compute the true effect. We report the estimated (finitesample) PEHE loss PEHE (Eq. 1), andPthe absolute ern
ror in average treatment
effect ATE = | n1 i=1 (f (xi , 1) ‚àí
Pn
1
f (xi , 0)) ‚àí n i=1 (m1 (xi ) ‚àí m0 (xi ))|. The results of
the experiments on IHDP are presented in Table 1 (left).
We average over 1000 realizations of the outcomes with
63/27/10 train/validation/test splits.
We also investigate the effects of increasing imbalance between the original treatment groups by constructing biased subsamples of the IHDP dataset. A logistic-regression
propensity score model is fit to form estimates pÃÇ(t = 1|x)
of the conditional treatment probability. Then, repeatedly,

1.5

q = 0: 0

¬≤PEHE relative to ¬Æ = 0

1.4

q = 0: 5

1.3

q = 1: 0

1.2
1.1
1.0
0.9
0.8
0.7

0

10 -5

10 -4

10 -3

10 -2

10 -1

10 0

10 1

Imbalance penalty; ¬Æ
Figure 2. Out-of-sample ITE error versus IPM regularization for
CFR Wass, relative to the error at Œ± = 0, on 500 realizations of
IHDP, with high (q = 1), medium and low (artificial) imbalance
between control and treated.
0.32

Out of sample policy risk

We compare our method to Ordinary Least Squares with
treatment as a feature (OLS1 ), OLS with separate regressors for each treatment (OLS2 ), k-nearest neighbor
(k-NN), Targeted Maximum Likelihood (TMLE), which
is a doubly robust method (Gruber & van der Laan,
2011), Bayesian Additive Regression Trees (BART) (Chipman et al., 2010; Chipman & McCulloch, 2016), Random Forests (R. For.) (Breiman, 2001), Causal Forests
(C. For.) (Wager & Athey, 2015) as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) from Johansson et al. (2016). For classification tasks we substitute Logistic Regression (LR) for OLS.
Choosing hyperparameters for estimating PEHE is nontrivial; we detail our general procedure using a validation
set, in subsection C.1 of the supplement.

BART
Causal Forests
CFR MMD
TARNet
Random policy

0.30
0.28
0.26
0.24
0.22
0.20
0.18
0.16
0.0

0.2

0.4

0.6

0.8

1.0

Treatment inclusion rate
Figure 3. Policy risk on Jobs as a function of treatment inclusion
rate. Lower is better. Subjects are included in treatment in order
of their estimated treatment effect given by the various methods.
CFR Wass is similar to TARNet and is omitted to avoid clutter.

with probability q we remove the remaining control observation x that has pÃÇ(t = 1|x) closest to 1, and with probability 1 ‚àí q, we remove a random control observation. The
higher q, the more imbalance. For each value of q, we remove 347 observations from each set, leaving 400.
5.2. Real-world outcome: Jobs
The study by LaLonde (1986) is a widely used benchmark
in the causal inference community, where the treatment is
job training and the outcomes are income and employment
status after training. This dataset combines a randomized
study based on the National Supported Work program with
observational data to form a larger dataset (Smith & Todd,
2005). The presence of the randomized subgroup gives a
way to estimate the ‚Äúground truth‚Äù causal effect. The study
includes 8 covariates such as age and education, as well
as previous earnings. We construct a binary classification
task, called Jobs, where the goal is to predict unemployment, using the feature set of Dehejia & Wahba (2002).
Following Smith & Todd (2005), we use the LaLonde experimental sample (297 treated, 425 control) and the PSID
comparison group (2490 control). There were 482 (15%)
subjects unemployed by the end of the study. We average

Estimating individual treatment effect: generalization bounds and algorithms

Table 1. Results on IHDP and Jobs within-sample (left) and out-of-sample (right). Lower is better.
Within-sample
Out-of-sample
IHDP
J OBS
IHDP
‚àö
‚àö
PEHE
ATE
RP OL
ATT
PEHE
ATE
OLS/LR1
5.8 ¬± .3
.73 ¬± .04 .22 ¬± .00 .01 ¬± .00 OLS/LR1
5.8 ¬± .3
.94 ¬± .06
OLS/LR2
2.4 ¬± .1
.14 ¬± .01 .21 ¬± .00 .01 ¬± .01 OLS/LR2
2.5 ¬± .1
.31 ¬± .02
BLR
5.8 ¬± .3
.72 ¬± .04 .22 ¬± .01 .01 ¬± .01 BLR
5.8 ¬± .3
.93 ¬± .05
k-NN
2.1 ¬± .1
.14 ¬± .01 .02 ¬± .00 .21 ¬± .01 k-NN
4.1 ¬± .2
.79 ¬± .05
TMLE
5.0 ¬± .2
.30 ¬± .01 .22 ¬± .00 .02 ¬± .01 TMLE
‚Ä†
‚Ä†
BART
2.1 ¬± .1
.23 ¬± .01 .23 ¬± .00 .02 ¬± .00 BART
2.3 ¬± .1
.34 ¬± .02
R.F OR .
4.2 ¬± .2
.73 ¬± .05 .23 ¬± .01 .03 ¬± .01 R.F OR .
6.6 ¬± .3
.96 ¬± .06
3.8 ¬± .2
.18 ¬± .01 .19 ¬± .00 .03 ¬± .01 C.F OR .
3.8 ¬± .2
.40 ¬± .03
C.F OR .
BNN
2.2 ¬± .1
.37 ¬± .03 .20 ¬± .01 .04 ¬± .01 BNN
2.1 ¬± .1
.42 ¬± .03
TARN ET
.88 ¬± .02 .26 ¬± .01 .17 ¬± .01 .05 ¬± .02 TARN ET
.95 ¬± .02 .28 ¬± .01
CFR MMD
.73 ¬± .01 .30 ¬± .01 .18 ¬± .00 .04 ¬± .01 CFR MMD
.78 ¬± .02 .31 ¬± .01
CFR WASS .71 ¬± .02 .25 ¬± .01 .17 ¬± .01 .04 ¬± .01
CFR WASS .76 ¬± .02 .27 ¬± .01

over 10 train/validation/test splits with ratios 56/24/20.
Because all the treated subjects T were part of the original
randomized sample E, we can compute the true
average
‚àí1 P
treatment effect on the treated by ATT = |T |
i‚ààT yi ‚àí
‚àí1 P
|C ‚à© E|
group. We
i‚ààC‚à©E yi , where C is the control
P
report the error ATT = |ATT ‚àí |T1 | i‚ààT (f (xi , 1) ‚àí
f (xi , 0))|. We cannot evaluate PEHE on this dataset, since
there is no ground truth for the ITE. Instead, in order
to evaluate the quality of ITE estimation, we use a measure we call policy risk. The policy risk is defined as
the average loss in value when treating according to the
policy implied by an ITE estimator. In our case, for a
model f , we let the policy be to treat, œÄf (x) = 1, if
f (x, 1) ‚àí f (x, 0) > Œª, and to not treat, œÄf (x) = 0 otherwise. The policy risk is RPol (œÄf ) = 1 ‚àí (E[Y1 |œÄf (x) =
1] ¬∑ p(œÄf = 1) + E[Y0 |œÄf (x) = 0] ¬∑ p(œÄf = 0)) which
we can estimate for the randomized trial subset of Jobs
by RÃÇPol (œÄf = 1 ‚àí (E[Y1 |œÄf (x) = 1, t = 1] ¬∑ p(œÄf =
1) + E[Y0 |œÄf (x) = 0, t = 0] ¬∑ p(œÄf = 0)). See figure 3
for risk as a function of treatment threshold Œª, aligned by
proportion of treated, and Table 1 for the risk when Œª = 0.
5.3. Results
We note that indeed imbalance confers an advantage to using the IPM regularization term, as our theoretical results
indicate, see e.g. the results for CFR WASS and TARNet
on IHDP in Table 1. We also see in Figure 2 that even
for the harder case of increased imbalance (q > 0) between treated and control, the relative gain from using our
method remains significant. On Jobs, our proposed methods are better than or competitive with state-of-the-art, but
we don‚Äôt see a significant gain from using IPM penalties.
This might be because we evaluate the predictions only on a
randomized subset with treatment groups distributed identically. Non-linear estimators perform significantly better
than linear ones in terms of individual effect (PEHE ). On
the Jobs dataset, straightforward logistic regression does

‚Ä†

Not applicable.

J OBS
RP OL
ATT
.23 ¬± .02 .08 ¬± .04
.24 ¬± .01 .08 ¬± .03
.25 ¬± .02 .08 ¬± .03
.26 ¬± .02 .13 ¬± .05
‚Ä†
‚Ä†
.25 ¬± .02 .08 ¬± .03
.28 ¬± .02 .09 ¬± .04
.20 ¬± .02 .07 ¬± .03
.24 ¬± .02 .09 ¬± .04
.21 ¬± .01 .11 ¬± .04
.21 ¬± .01 .08 ¬± .03
.21 ¬± .01 .09 ¬± .03

remarkably well in estimating the ATT. However, being a
linear model, LR can only ascribe a uniform policy - in
this case, ‚Äútreat everyone‚Äù. The more nuanced policies
offered by non-linear methods achieve lower policy risk
in the case of Causal Forests and CFR. This emphasizes
the fact that estimating average effect and individual effect
can require different models. Specifically, while smoothing
over many units may yield a good ATE estimate, this might
significantly hurt ITE estimation. k-nearest neighbors has
very good within-sample results on Jobs, because evaluation is performed over the randomized component, but suffers heavily in generalizing out of sample, as expected.

6. Conclusion
In this paper we give a meaningful and intuitive errorbound for estimating individual treatment effect. Our
bound relates ITE estimation to the classic machine learning problem of learning from samples, along with methods
for measuring distributional distances from samples. The
bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations
and hypotheses. We apply our theory-guided approach to
both synthetic and real-world tasks, showing that in every
case our method matches or outperforms the state-of-theart. Important open questions are theoretical considerations
in choosing the IPM weight Œ±, how to best derive confidence intervals for our model‚Äôs predictions, and integrating our work with more complicated causal models such as
those with hidden confounding or instrumental variables.
ACKNOWLEDGMENTS
We thank Aahlad Puli for his assistance with the experiments; Sanjog Misra and GuÃànter J. Hitsch for suggesting
the policy risk evaluation; Jennifer Hill, Marco Cuturi and
Esteban Tabak for fruitful conversations; and Stefan Wager
for his help with the code for Causal Forests. DS and US
were supported by NSF CAREER award #1350965.

Estimating individual treatment effect: generalization bounds and algorithms

References
Athey, Susan and Imbens, Guido. Recursive partitioning for
heterogeneous causal effects. Proceedings of the National
Academy of Sciences, 113(27):7353‚Äì7360, 2016.
Athey, Susan, Imbens, Guido W, and Wager, Stefan. Efficient inference of average treatment effects in high dimensions via approximate residual balancing. arXiv preprint
arXiv:1604.07125, 2016.
Austin, Peter C. An introduction to propensity score methods for
reducing the effects of confounding in observational studies.
Multivariate behavioral research, 46(3):399‚Äì424, 2011.
Bareinboim, Elias and Pearl, Judea. Causal inference and the
data-fusion problem. Proceedings of the National Academy
of Sciences, 113(27):7345‚Äì7352, 2016.

immunodeficiency syndrome or death using marginal structural models. American Journal of Epidemiology, 158(7):687‚Äì
694, 2003.
Cortes, Corinna and Mohri, Mehryar. Domain adaptation and
sample bias correction theory and algorithm for regression.
Theoretical Computer Science, 519:103‚Äì126, 2014.
Cuturi, Marco and Doucet, Arnaud. Fast computation of Wasserstein barycenters. In Proceedings of The 31st International
Conference on Machine Learning, pp. 685‚Äì693, 2014.
DaumeÃÅ III, Hal. Frustratingly easy domain adaptation. Conference of the Association for Computational Linguistics (ACL),
2007.
Dehejia, Rajeev H and Wahba, Sadek. Propensity score-matching
methods for nonexperimental causal studies. Review of Economics and statistics, 84(1):151‚Äì161, 2002.

Beck, Nathaniel, King, Gary, and Zeng, Langche. Improving
quantitative studies of international conflict: A conjecture.
American Political Science Review, 94(01):21‚Äì35, 2000.

Dorie, Vincent. NPCI: Non-parametrics for Causal Inference.
https://github.com/vdorie/npci, 2016.

Belloni, Alexandre, Chernozhukov, Victor, and Hansen, Christian. Inference on treatment effects after selection among highdimensional controls. The Review of Economic Studies, 81(2):
608‚Äì650, 2014.

Funk, Michele Jonsson, Westreich, Daniel, Wiesen, Chris,
StuÃàrmer, Til, Brookhart, M Alan, and Davidian, Marie. Doubly robust estimation of causal effects. American journal of
epidemiology, 173(7):761‚Äì767, 2011.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Pereira, Fernando, et al. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19:
137, 2007.

Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain,
Pascal, Larochelle, Hugo, Laviolette, FrancÃßois, Marchand,
Mario, and Lempitsky, Victor. Domain-adversarial training
of neural networks. Journal of Machine Learning Research,
17(59):1‚Äì35, 2016. URL http://jmlr.org/papers/
v17/15-239.html.

Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex,
Pereira, Fernando, and Vaughan, Jennifer Wortman. A theory
of learning from different domains. Machine learning, 79(1-2):
151‚Äì175, 2010.
Bengio, Yoshua, Courville, Aaron, and Vincent, Pierre. Representation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 35
(8):1798‚Äì1828, 2013.
Breiman, Leo. Random forests. Machine learning, 45(1):5‚Äì32,
2001.
Chernozhukov, Victor, Chetverikov, Denis, Demirer, Mert, Duflo, Esther, Hansen, Christian, et al. Double machine learning for treatment and causal parameters. arXiv preprint
arXiv:1608.00060, 2016.

Gretton, Arthur, Smola, Alex, Huang, Jiayuan, Schmittfull, Marcel, Borgwardt, Karsten, and SchoÃàlkopf, Bernhard. Covariate
shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009.
Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
SchoÃàlkopf, Bernhard, and Smola, Alexander. A kernel twosample test. J. Mach. Learn. Res., 13:723‚Äì773, March 2012.
ISSN 1532-4435.
Gruber, Susan and van der Laan, Mark J. tmle: An r package for
targeted maximum likelihood estimation. 2011.
Hartford, Jason, Lewis, Greg, Leyton-Brown, Kevin, and Taddy,
Matt. Counterfactual prediction with deep instrumental variables networks. arXiv preprint arXiv:1612.09596, 2016.

Chipman, Hugh and McCulloch, Robert. BayesTree: Bayesian
Additive Regression Trees. https://cran.r-project.
org/web/packages/BayesTree, 2016.

Hill, Jennifer L. Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics,
20(1), 2011.

Chipman, Hugh A, George, Edward I, and McCulloch, Robert E.
BART: Bayesian additive regression trees. The Annals of Applied Statistics, pp. 266‚Äì298, 2010.

Hoyer, Patrik O, Janzing, Dominik, Mooij, Joris M, Peters, Jonas,
and SchoÃàlkopf, Bernhard. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pp. 689‚Äì696, 2009.

Clevert, Djork-ArneÃÅ, Unterthiner, Thomas, and Hochreiter, Sepp.
Fast and accurate deep network learning by exponential linear
units (elus). International Conference on Learning Representations, 2016.
Cole, Stephen R, HernaÃÅn, Miguel A, Robins, James M, Anastos,
Kathryn, Chmiel, Joan, Detels, Roger, Ervin, Carolyn, Feldman, Joseph, Greenblatt, Ruth, Kingsley, Lawrence, et al. Effect of highly active antiretroviral therapy on time to acquired

Imbens, Guido W and Wooldridge, Jeffrey M. Recent developments in the econometrics of program evaluation. Journal of
economic literature, 47(1):5‚Äì86, 2009.
Johansson, Fredrik D., Shalit, Uri, and Sontag, David. Learning representations for counterfactual inference. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.

Estimating individual treatment effect: generalization bounds and algorithms
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.

Strehl, Alex, Langford, John, Li, Lihong, and Kakade, Sham M.
Learning from logged implicit exploration data. In Advances in
Neural Information Processing Systems, pp. 2217‚Äì2225, 2010.

LaLonde, Robert J. Evaluating the econometric evaluations of
training programs with experimental data. The American economic review, pp. 604‚Äì620, 1986.

Sun, Baochen, Feng, Jiashi, and Saenko, Kate. Return of frustratingly easy domain adaptation. In Thirtieth AAAI Conference
on Artificial Intelligence, 2016.

Maathuis, Marloes H, Colombo, Diego, Kalisch, Markus, and
BuÃàhlmann, Peter. Predicting causal effects in large-scale systems from observational data. Nature Methods, 7(4):247‚Äì248,
2010.

Swaminathan, Adith and Joachims, Thorsten. Batch learning
from logged bandit feedback through counterfactual risk minimization. Journal of Machine Learning Research, 16:1731‚Äì
1755, 2015.

Mansour, Yishay, Mohri, Mehryar, and Rostamizadeh, Afshin.
Domain adaptation: Learning bounds and algorithms. 2009.

Taddy, Matt, Gardner, Matt, Chen, Liyun, and Draper, David. A
nonparametric bayesian analysis of heterogenous treatment effects in digital experimentation. Journal of Business & Economic Statistics, 34(4):661‚Äì672, 2016.

Mooij, Joris M, Peters, Jonas, Janzing, Dominik, Zscheischler,
Jakob, and SchoÃàlkopf, Bernhard. Distinguishing cause from effect using observational data: methods and benchmarks. Journal of Machine Learning Research, 17(32):1‚Äì102, 2016.
MuÃàller, Alfred. Integral probability metrics and their generating
classes of functions. Advances in Applied Probability, pp. 429‚Äì
443, 1997.
Pan, Sinno Jialin, Tsang, Ivor W, Kwok, James T, and Yang,
Qiang. Domain adaptation via transfer component analysis.
Neural Networks, IEEE Transactions on, 22(2):199‚Äì210, 2011.
Pearl, Judea. Causality. Cambridge university press, 2009.
Pearl, Judea. Detecting latent heterogeneity. Sociological Methods & Research, pp. 0049124115600597, 2015.
Peysakhovich, Alexander and Lada, Akos. Combining observational and experimental data to find heterogeneous treatment
effects. arXiv preprint arXiv:1611.02385, 2016.
Robins, James. A new approach to causal inference in mortality
studies with a sustained exposure periodapplication to control
of the healthy worker survivor effect. Mathematical Modelling,
7(9-12):1393‚Äì1512, 1986.
Rolling, Craig Anthony. Estimation of Conditional Average Treatment Effects. PhD thesis, University of Minnesota, 2014.
Rubin, Donald B. Causal inference using potential outcomes.
Journal of the American Statistical Association, 2011.
Shalev-Shwartz, Shai and Ben-David, Shai. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014.
Shpitser, Ilya and Pearl, Judea. Identification of conditional interventional distributions. In Proceedings of the Twenty-second
Conference on Uncertainty in Artificial Intelligence, pp. 437‚Äì
444. UAI Press, 2006.
Smith, Jeffrey A and Todd, Petra E. Does matching overcome
LaLonde‚Äôs critique of nonexperimental estimators? Journal of
econometrics, 125(1):305‚Äì353, 2005.
Sriperumbudur, Bharath K, Fukumizu, Kenji, Gretton, Arthur,
SchoÃàlkopf, Bernhard, Lanckriet, Gert RG, et al. On the empirical estimation of integral probability metrics. Electronic
Journal of Statistics, 6:1550‚Äì1599, 2012.

Triantafillou, Sofia and Tsamardinos, Ioannis. Constraint-based
causal discovery from multiple interventions over overlapping
variable sets. Journal of Machine Learning Research, 16:
2147‚Äì2205, 2015.
Villani, CeÃÅdric. Optimal transport: old and new, volume 338.
Springer Science & Business Media, 2008.
Wager, Stefan and Athey, Susan.
Estimation and inference of heterogeneous treatment effects using random
forests.
arXiv preprint arXiv:1510.04342. https://
github.com/susanathey/causalTree, 2015.

