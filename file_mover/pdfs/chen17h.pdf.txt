Online Partial Least Square Optimization:
Dropping Convexity for Better Efficiency and Scalability
Zhehui Chen 1 Lin F. Yang 2 Chris J. Li 3 Tuo Zhao 1

Abstract
Multiview representation learning is popular for
latent factor analysis. Many existing approaches
formulate the multiview representation learning
as convex optimization problems, where global
optima can be obtained by certain algorithms
in polynomial time. However, many evidences
have corroborated that heuristic nonconvex approaches also have good empirical computational
performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice
motivates us to study a nonconvex formulation
for multiview representation learning, which can
be efficiently solved by a simple stochastic gradient descent method. By analyzing the dynamics of the algorithm based on diffusion processes,
we establish a global rate of convergence to the
global optima. Numerical experiments are provided to support our theory.

1. Introduction
Multiview data have become increasingly available in
many popular real-world data analysis and machine learning problems. These data are collected from diverse domains or different feature extractors, which share latent factors. Existing literature has demonstrated different scenarios. For instance, the pixels and captions of images can
be considered as two-view data, since they are two different features describing the same contents. More motivating examples involving two or more data sets simultaneously can be found in computer vision, natural language processing, and acoustic recognition. See (Hardoon
et al., 2004; Socher and Fei-Fei, 2010; Kidron et al., 2005;
Chaudhuri et al., 2009; Arora and Livescu, 2012; Bharadwaj et al., 2012; Vinokourov et al., 2002; Dhillon et al.,
1

Georgia Institute of Technology; 2 Johns Hopkins University; 3 Princeton University. Correspondence to: Tuo Zhao
<tourzhao@gatech.edu.>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2011). Although these data are usually unlabeled, there exist underlying association and dependency between different views, which allows us to learn useful representations
in a unsupervised manner. Here we are interested in finding a representation that reveals intrinsic low-dimensional
structures and decomposes underlying confounding factors. One ubiquitous approach is partial least square (PLS)
for multiview representation learning. Specifically, given
a data set of n samples of two sets of random variables
(views), X 2 Rm and Y 2 Rd , PLS aims to find an
r-dimensional subspace (r ⌧ min(m, d)) that preserves
most of the covariance between two views. Existing literature has shown that such a subspace is spanned by the
leading r components of the singular
⇥
⇤value decomposition
(SVD) of ⌃XY = E(X,Y )⇠D XY > (Arora et al., 2012),
where we sample (X, Y ) from some unknown distribution
D. Throughout the rest of the paper, if not clear specified,
we denote E(X,Y )⇠D by E for notational simplicity.
A straightforward approach for PLS is “Sample Average
Approximation” (SAA, (Abdi, 2003; Ando and Zhang,
2005)), where we run an offline (batch) SVD algorithm
on the empirical covariance matrix after seeing sufficient
data samples. However, in the “big data” regime, this approach requires unfeasible amount of storage and computation time. Therefore, it is much more practical to consider
the multiview learning problem in a “data laden” setting,
where we draw independent samples from an underlying
distribution D over Rm ⇥ Rd , one at a time. This further
enables us to formulate PLS as a stochastic (online) optimization problem. Here we only consider the rank-1 case
(r = 1) for simplicity, and solve
(b
u, vb) = argmax E v > Y X > u
u2Rm ,v2Rd

subject to u> u = 1, v > v = 1.

(1.1)

Several nonconvex stochastic approximation (SA) algorithms have been proposed in (Arora et al., 2012). These
algorithms work great in practice, but are lack of theoretic
justifications, since the nonconvex nature of (1.1) makes
the theoretical analysis very challenging. To overcome
this obstacle, (Arora et al., 2016) propose a convex relaxation of (1.1) by lifting (Lasserre, 2001). Specifically, by a
reparametrization M = uv > (Recall that we are interested

Online Partial Least Square Optimization

in the rank-1 PLS), they rewrite (1.1) as
c = argmax hM, ⌃XY i
M
M 2Rd⇥m

subject to kM k⇤  1 and kM k2  1.

(1.2)

where ⌃XY = EXY > , and kM k2 and kM k⇤ are the spectral norm(i.e., the largest singular value of M ) and nuclear
norm(i.e., the sum of all singular values of M ) of M respectively. By examining the KKT conditions of (1.2), one
c = u
can verify that M
bvb> is the optimal solution, where
u
b, vb are the leading left and right singular vectors of ⌃XY ,
i.e., a pair of global optimal solutions to (1.1) for r = 1.
Accordingly, they propose a projected stochastic gradienttype algorithm to solve (1.2), which is often referred to the
Matrix Stochastic Gradient (MSG) algorithm. Particularly,
at the (k + 1)-th iteration, MSG takes
Mk+1 = ⇧Fantope (Mk + ⌘Xk Yk> ),
where Xk and Yk are independently sampled from D, and
⇧Fantope (·) is a projection operator to the feasible set of
(1.2). They further prove that given a pre-specified accuracy ✏, MSG requires
N = O(✏

2

log(1/✏))

c, Exy > i
iterations such that hM
high probability.

hMN , Exy > i  ✏ with

Despite of the attractive theoretic guarantee, MSG does not
present superior performance to other heuristic nonconvex
stochastic optimization algorithms for solving (1.1). Although there is a lack of theoretical justification, many
evidences have corroborated that heuristic nonconvex approaches not only converge to the global optima in practice, but also enjoy better empirical computational performance than the convex approaches (Zhao et al., 2015; Candes et al., 2015; Ge et al., 2015; Cai et al., 2016). Another drawback of MSG is the complicated projection step
at each iteration. Although (Arora et al., 2016) further propose an algorithm to compute the projection with a computational cost cubically depending on the rank of the iterates
(the worst case: O(d3 )), such a sophisticated implementation significantly decreases the practicability of MSG. Furthermore, MSG is also unfavored in a memory-restricted
scenario, since storing the update M(k) requires O(md)
real number storage. In contrast, the heuristic algorithm
analyzed in this paper requires only O(m + d) real number
storage.
We aim to bridge the gap between theory and practice
for solving multiview representation learning problems by
nonconvex approaches. Specifically, we analyze the convergence properties of one heuristic stochastic optimization
algorithm for solving (1.1) based on diffusion processes.

Our analysis takes advantage of the strong Markov properties of the stochastic optimization algorithm updates and
casts the trajectories of the algorithm as a diffusion process
(Ethier and Kurtz, 2009; Li et al., 2016b). By leveraging
the weak convergence from discrete Markov chains to their
continuous time limits, we demonstrate that the trajectories
are essentially the solutions to stochastic differential equations (SDE). Such SDE-type analysis automatically incorporates the geometry of the objective and the randomness
of the algorithm, and eventually demonstrates three phases
of convergence.
1. Starting from an unstable equilibrium with negative
curvature, the dynamics of the algorithm can be described by an Ornstein-Uhlenbeck process with a
steady driven force pointing away from the initial.
2. When the algorithm is sufficiently distant from initial
unstable equilibrium, the dynamics can be characterized by an ordinary differential equation (ODE). The
trajectory of this phase is evolving directly toward the
desired global maximum until it reaches a small basin
around the optimal.
3. In this phase, the trajectory can be also described by
an Ornstein-Uhlenbeck process oscillating around the
global maximum. The process has a drifting term that
gradually dies out and eventually becomes a nearly
unbiased random walk centered at the maximum.
The sharp characterization in these three phases eventually allows us to establish strong convergence guarantees. Specifically, we show the nonconvex stochastic gradient algorithm guarantees an ✏-optimal solution in
O(✏

1

log(1/✏))

iterations with high probability, which is a significant
improvement over convex MSG by a factor of ✏ 1 .
Our theoretical analysis reveals the power of the nonconvex optimization in PLS. The simple heuristic algorithm drops the convexity, but achieves much better
efficiency.
Notations: Given a vector v = (v (1)P
, . . . , v (d) )> 2 Rd ,
(j)
we define vector norms: kvk1 =
|, kvk22 =
j |v
P (j) 2
) , and kvk1 = maxj |v (j) |. Given a matrix A 2
j (v
d⇥d
R
, we use Aj = (A1j , ..., Adj )> to denote the
Pj-th column of A and define the matrix norms kAk2F = j kAj k22
and kAk2 as the largest singular value of A.

Online Partial Least Square Optimization

2. Nonconvex Stochastic Optimization

(i)

Recall that we solve (1.1)
(b
u, vb) = argmax u> EXY > v
u,v

subject to kuk22 = 1, kvk22 = 1,

(2.1)

where (X, Y ) follows some unknown distribution D. Due
to symmetrical structure of (2.1), ( u
b, vb) is also a pair
of global optimum. All our analysis holds for both optima.
Throughout the rest of paper, if it is not clearly specified,
we consider (b
u, vb) as the global optimum for simplicity.
We apply a straightforward projected stochastic gradient algorithm (PSG). Specifically, at the k-th iteration, we have
the iterates uk and vk . We then independently sample Xk
and Yk from D, and take
uk+1 = ⇧(uk + ⌘Xk Yk> vk ),

(2.2)

⌘Yk Xk> uk ),

(2.3)

vk+1 = ⇧(vk +

where ⌘ > 0 is the step size parameter, and ⇧(·) is the projection operator on the unit sphere. As can be seen from
(2.2), we have Xk Yk> vk as a unbiased estimator of the gradient of the objective function. The projected stochastic
gradient algorithm has been studied for convex optimization, and their rates of convergence have been characterized in (Ben-Tal and Nemirovski, 2001; Nemirovski et al.,
2009). The problem (2.1), however, is nonconvex, and existing literature in optimization only shows that the stochastic gradient algorithms converge to a stationary solution.

3. Global Convergence by ODE

Assumption 3.1. Xk , Yk , k = 1, ..., N are data samples
identically independently distributed as X, Y 2 Rd respectively satisfying the following conditions:

...
1 > 2
3
d > 0, where
singular values of ⌃XY = EXY > .

i ’s

, ..., X

(d) >

) , and Y = (Y

are the latent variables satisfying:

(1)

, ..., Y

(i)

Y

)=
(i)

X

(d) >

) 2 Rd

i , Var(Y
(j) (j)

Y

(i)

) = !i ,

= ↵ij

Proposition 3.3. Using (2.2) and (2.3), we get a sequence
of (uk , vk ), k = 1, 2, ..., N . They form a discrete-time
Markov process.
With Proposition 3.3, we can construct a continuous time
process to derive an ordinary differential equation to analyze the algorithmic convergence. Before that, we first
compute uk+1 uk and vk+1 vk to see how much they
change in each iteration. We denote the i-th coordinate of
(i)
(i)
uk and vk by uk and vk .
Proposition 3.4. Suppose Assumption 3.1 holds. Given
Bd⌘ < 14 , the following results hold:
(i)

(1) There exist random variables Rk
(i)
20B 2 d2 ⌘ 2 and Qk
(i)
the increments uk+1

with

(i)
Qk

(i)

(i)

with Rk



 20B 2 d2 ⌘ 2 , such that

(i)

uk and vk+1

⇣
(i)
(i)
uk = ⌘ Yk> vk Xk
⇣
(i)
(i)
vk = ⌘ Xk> uk Yk

(i)

uk+1

(i)

vk are

(i)

>
u>
k Xk Yk v k uk

(i)

vk> Yk Xk> uk vk

⌘

⌘

(i)

+ Rk ,
(i)

+ Qk .

(2) Furthermore, there are two deterministic functions
fk (u, v) and gk (u, v) satisfying
max{|fk (u, v)|, |gk (u, v)|}  20B 2 d2 ⌘ 2 for 8u, v 2 S d
such that conditioning on uk and vk , the expectation of the
increments in (1) can be represented as
E[uk+1
= ⌃XY vk

Assumption 3.2. Given the observed random vectors X
and Y , there exist two orthogonal matrices OX , OY 2
Rd⇥d such that X = OX X, Y = OY Y , where
(1)

(i)

The next proposition characterizes the strong Markov property of our algorithm.

are the

Note that here we assume X and Y are of the same dimensions (i.e., m = d) and ⌃XY is full rank for convenience
of analysis. The extension to m 6= d and rank deficient
settings is straightforward.

X = (X

E X

(i)

Before we proceed with our analysis, we first impose some
mild assumptions on the problem.

2.

2. Var(X

vk+1

1. kXk22  Bd, kY k22  Bd for a constant B;

(j)

1. X and Y
are uncorrelated if i 6= j, so that OX
and OY are the left and right singular matrices of ⌃XY
respectively;

E[vk+1
=

⌃>
XY

uk

uk | uk , vk ]

u>
k ⌃XY vk uk · ⌘ + fk (uk , vk ),

v k | u k , vk ]

vk> ⌃>
XY uk vk · ⌘ + gk (uk , vk ).

Proposition 3.4 is obtained by Taylor expansion. Its proof
is presented in Appendix A.1. Result (2) enables us to compute the infinitesimal conditional mean and variance for
the projected stochastic gradient algorithm. Specifically,
as the fixed step size ⌘ ! 0+ , two processes U⌘ (t) =
ub⌘ 1 tc , V⌘ (t) = vb⌘ 1 tc based on the sequence generated
by (2.2) and (2.3), converge to the solution of the following

1

,

Online Partial Least Square Optimization

ODE system in probability (See more details in (Ethier and
Kurtz, 2009)),
dU
= ⌃XY V
dt
dV
= ⌃>
XY U
dt

U > ⌃XY V U ,

(3.1)

V > ⌃>
XY U V ,

(3.2)

where U (0) = u0 and V (0) = v0 . To highlight the sequence generated by (2.2) and (2.3) depending on ⌘, we
redefine u⌘,k = uk , v⌘,k = vk .
Theorem 3.5. As ⌘ ! 0+ , the processes u⌘,k , v⌘,k
weakly converge to the solution of the ODE system in (3.1)
and (3.2) with initial U (0) = u0 , V (0) = v0 .
The proof of Theorem 3.5 is presented in Appendix A.2.
Under Assumption 3.1, the above ODE system admits a
closed form solution. Specifically, we solve U and V simultaneously, since they are coupled together in (3.1) and
(3.2) . To simplify them , we define
1
W = p U> V >
2

>

1
>
and wk = p u>
k vk
2

>

.

We then rewrite (3.1) and (3.2) as
dW
= QW
dt

(3.3)

◆
⌃XY
. By Assumption 3.2, OX
⌃>
0
XY
and OY are left and right singular matrices of ⌃XY respectively, i.e.,
where Q =

✓

W > QW W,

0

>

⌃XY = EXY > = OX EXY OY> ,
>

where EXY is diagonal. For notational simplicity, we
define D = diag( 1 , 2 , ..., d ) such that

One can verify Q = P ⇤P > , where
✓
◆
✓
1
OX OX
D
P =p
,⇤ =
OY
OY
0
2

0
D

◆

. (3.4)

H > ⇤HH,

(3.5)

which is a coordinate separable ODE system. Accordingly,
(i)
we define hk ’s as:
hk = P > w k

(i)

j ) (H

i

(j) 2

) ,

(3.7)

where i =
i d when i > d. This ODE System has a
closed form solution as follows:
H (i) (t) = C(t)

1
2

H (i) (0) exp ( i t),

(3.8)

for i = 1, 2, ..., 2d, where
C(t) =

2d ⇣
X
⇥
j=1

H (j) (0)

⇤2

exp (2

j t)

⌘

is a normalization function such that kH(t)k2 = 1.
The proof of Theorem 3.6 is presented in Appendix A.3.
Without loss of generalization, we assume H (1) (0) > 0.
We can get H (1) (t) ! 1, as t ! 1. We have successfully
characterized the global convergence performance of our
algorithm with an approximate error o(1). The solution to
the ODE system in (3.8), however, does not fully reveal the
algorithmic behavior (more precisely, the rate of convergence) near the equilibria of the ODE system. This further
motivates us to exploit the stochastic differential equation
approach to characterize the dynamics of the algorithm.

4. Global Dynamics by SDE
We analyze the dynamics of the algorithm near the equilibria based on stochastic differential equation by rescaling
analysis. Specifically, we characterize three stages for the
trajectories of solutions:

2. Neighborhood around stable equilibria — maximizers
of (2.1), and

By left multiplying P > both sides of (3.3), we obtain
dH
= ⇤H
dt

2d
X
d (i)
H = H (i)
(
dt
j=1

1. Neighborhood around unstable equilibria — minimizers and saddle points of (2.1),

⌃XY = OX DOY> .

H(t) = P > W (t) with

Theorem 3.6. Given (3.5), we write the ODE in each component H (i) ,

and hk = Pi> wk .

(3.6)

Thus, we can obtain a closed form solution to (3.5) based
on the following theorem.

3. deterministic traverses between equilibria. Moreover,
we provide the approximate the number of iterations
in each phase until convergence.
4.1. Phase I: Escaping from unstable equilibria
Suppose that the algorithm starts to iterate around a unstable equilibrium, (e.g. saddle point). Different from our previous analysis, we rescale two aforementioned processes
U⌘ (t) and V⌘ (t) by a factor of ⌘ 1/2 . This eventually allows us to capture the uncertainty of the algorithm updates
by stochastic differential equations. Roughly speaking, the
ODE approximation is essentially a variant of law of large

Online Partial Least Square Optimization

number for Markov process, while the SDE approximation
serves as a variant of central limit theorem accordingly.
Recall that P is an orthonormal matrix for diagonalizing Q,
(i)
(i)
and H is defined in (3.5). Let Z⌘ and z⌘,k denote the i-th
coordinates of
Z⌘ = ⌘

1/2

H⌘ and z⌘,k = ⌘

1/2

h⌘,k

respectively. The following theorem characterizes the dynamics of the algorithm around the unstable equilibrium.
Theorem 4.1. Suppose z⌘,0 is initialized around some saddle point or minimizer (e.g. j-th column of P with j 6= 1),
i.e., Z (j) (0) ⇡ ⌘ 1 and Z (i) (0) ⇡ 0 for i 6= j. Then as
(i)
⌘ ! 0+ , for all i 6= j, z⌘,k weakly converges to a diffusion
process Z (i) (t) satisfying the following SDE,
dZ (i) (t) =

(

i )Z

j

(i)

(t)dt +

where B(t) is a brownian motion,
8 1p
>
>
i !j + j !i + 2↵i,j
< 2
ij =
>
>
: 1p ! + !
2↵i,j
i j
j i
2

ij

where i = i d for i > d, !j = !j
definition of ↵ij for i > d or j > d.

ij dB(t),

(4.1)

is defined as:
if 1  i, j  d

or d + 1  i, j  2d,

otherwise,
d

for j > d, similar

T2

}

<

j,

we have

E[Z (i) (t)] = Z (i) (0) exp [ (
Var[Z (i) (t)] =

2
ij

2(

j

i)

⇥

1

i )t] ,

j

exp [ 2(

j

i )t]

⇤

As has been shown in [a] that t does not need to be
large for Z (i) (t) to get away from 0. Here we only
consider relatively small t. Since the initial drift for
Z (i) (0) ⇡ 0 is very small, Z (i) tends to stay at 0. As t
increases, the exponential decay term makes the drift
quickly become negligible. Moreover, by mean value
theorem, we know that the variance is bounded, and
increases far slower than the variance in [a]. Thus,
roughly speaking, Z (i) (t) oscillates near 0.
[c] For j = i , we have E[Z (i) (t)] = Z (i) (0) and
2
Var[Z (i) (t)] = ij
. This implies that Z (i) (t) also
tends to oscillate around 0, as t increases.

Overall speaking, [a] is dominative so that it is the major
driving force for the algorithm to escape from this unstable equilibrium. More precisely, let us consider one special
case for Phase I, that is we start from the second maxi(2)
mum singular value, with h⌘,k (0) = 1. We then approximately calculate the number of iterations to escape Phase
(1)
(1)
I using the algorithmic behavior of h⌘,k = ⌘ 1/2 z⌘,k ⇡
(1)

T1

j )t]

i

⌘ 1/2 Z⌘ (t) with t = k⌘ by the following proposition.

The proof of Theorem 4.1 is provided in Appendix B.1.
Note that (4.1) is a Fokker-Planck equation, which admits
a closed form solution as follows,
"
#
Z t
(i)
(i)
Z (t) = Z (0) + i,j
exp [( j
i )s] dB(s)
0
|
{z
}
· exp [( i
|
{z

[b] For

for i 6= j.

(4.2)

Such a solution is well known as the Ornstein-Uhlenbeck
process (Øksendal, 2003), and also implies that the distri(i)
bution of z⌘,k can be well approximated by the normal distribution of Z (i) (t) for a sufficiently small step size. This
continuous approximation further has the following implications:
⇥
⇤
Rt
[a] For i > j , T1 = ij 0 exp ( j
i )s dB(s) +
Z (i) (0) is essentially a random variable with mean
2

Z (i) (0) and variance smaller than 2( i ij j ) . The
larger t is, the closer its variance
gets to
⇥
⇤ this upper
bound. While T2 = exp ( i
)t
essentially
j
amplifies T1 by a factor exponentially increasing in
t. This tremendous amplification forces Z (i) (t) to
quickly get away from 0, as t increases.

Proposition 4.2. Given pre-specified ⌫ > 0 and sufficiently small ⌘, there exists some ⇣ ⌘ µ , where µ 2
(0, 0.5) is a generic constant, such that the following result
holds: We need at most
!
⌘ 1
2⌘ 1 2 ( 1
2)
N1 =
log
+1
1 1+⌫ 2 2
2( 1
2)
2

iterations such that
least 1 ⌫, where
distribution.

12

(2)
(h⌘,N1 )2

2
1
with probability at
(x) is the CDF of standard normal

The proof of Proposition 4.2 is provided in Appendix B.2.
Proposition 4.2 suggests that SGD can escape from unstable equilibria in a few iterations. After escaping from the
saddle, SGD gets into the next phase, which is a deterministic traverse between equilibria.
4.2. Phase II: Traverse between equilibria
When the algorithm is close to neither the saddle points
nor the optima, the algorithm’s performance is nearly deterministic. Since Z(t) is a rescaled version of H(t), their
trajectories are similar. Like before, we have the following proposition to calculate the approximate iterations, N2 ,
following our results in Section 3. We restart the counter of
iteration by Proposition 3.3.

.

Online Partial Least Square Optimization

Proposition 4.3. After restarting counter of iteration,
given sufficiently small ⌘ and defined in Proposition 4.2,
we need at most
N2 =

⌘
2(

1

1

log

2)

⇣
⌘2
(1)
iterations such that h⌘,N2

2

1
2

2

1

.

The proof of Proposition 4.3 is provided in Appendix B.3.
Combining Propositions 4.2 and 4.3, we know that after
N1 + N2 iteration numbers, SGD is close to the optimum
with high probability, and gets into its third phase, i.e., convergence to stable equilibria.

(i)

Recall that the distribution of z⌘,k can be well approximated by the normal distribution of Z (i) (t) for a sufficiently small step size. This further implies that after suf(i)
ficiently many iterations, SGD enforces z⌘,k ! 0 except
i = 1. Meanwhile, SGD behaves like a biased random
walk towards the optimum, when it iterates within a small
neighborhood the optimum. But unlike Phase I, the variance gradually becomes a constant.
Based on theorem 4.4, we establish an iteration complexity
bound for SGD in following proposition.
Proposition 4.5. Given a pre-specified ✏ > 0, a sufficiently small ⌘, and defined in Proposition 4.2, after
restarting counter of iteration, we need at most

4.3. Phase III: Convergence to stable equilibria
Again, we restart the counter of iteration by the strong
Markov property. The trajectory and analysis are similar
to Phase I, since we also characterize the convergence using an Ornstein-Uhlenbeck process. The following theorem characterizes the dynamics of the algorithm around the
stable equilibrium.
Theorem 4.4. Suppose z⌘,0 is initialized around some
1
maximizer (the first column of P ), i.e., Z (1) (0) ⇡ ⌘ 2 and
(i)
Z (i) (0) ⇡ 0 for i 6= 1. Then as ⌘ ! 0+ , for all i 6= 1, z⌘,k
weakly converges to a diffusion process Z (i) (t) satisfying
the following SDE for i 6= 1,
dZ (i) (t) =

(

i )Z

1

(i)

(t)dt +

where B(t) is a brownian motion, and
8
p
>
< 1
i !1 + 1 !i + 2↵i1
2p
i1 =
1
>
:
2↵i1
i !1 + 1 !i
2

(4.3)

i1 dB(t),

if 1  i  d,

i )t]

t)] dB(s).

0

By the property of the O-U process, we characterize the
expectation and variance of Z (i) (t) for i 6= 1.
EZ (i) (t) = Z (i) (0) exp [ ( 1
i )t] ,
⇣
⌘2
2
i1
E Z (i) (t) =
+ exp [ 2( 1
2( 1
i)
"
⇣
⌘2
2
i1
· Z (i) (0)
2( 1

i )t]

i)

#

1

2)

0

log @

iterations such that
at least 3/4.

(

1

P2d ⇣
i=2

4( 1
2 )✏⌘
(i)

h⌘,N3

⌘2

2)
1

2

4d max

1id

2
i1

1

A,

 ✏ with probability

The proof of Proposition 4.5 is provided in Appendix B.5.
Combining Propositions 4.2, 4.3, and 4.5, we obtain a more
refined result in the following corollary.
Corollary 4.6. Given a sufficiently small pre-specified ✏ >
0, we choose
✏( 1
2)
2 .
d max1id i1

We need at most

otherwise.

i )(s

2(

1

⌘⇣

The proof of Theorem 4.4 is provided in Appendix B.4.
Similar to (4.2), the closed form solution to (4.3) for i 6= 1
is as follow:
Z (i) (t) = Z (i) (0) exp [ ( 1
Z t
+ i1
exp [( 1

N3 =

⌘

.

N =O



d
✏(

1

✓ ◆
d
log
2
)
✏
2

iterations such that we have ku⌘,n u
bk22 +kv⌘,n vbk22  3✏
with probability at least 2/3.
The proof of Corollary 4.6 is provided in Appendix B.6.
We can further improve the probability to 1 ⌫ for some
⌫ > 0 by repeating O(log 1/⌫) replicates of SGD. We then
compute the geometric median of all outputs. See more
details in (Cohen et al., 2016).

5. Numerical Experiments
We first provide a simple example to illustrate our theoretical analysis. Specifically, we choose m = d = 3. We first
generate the joint covariance matrix for the latent factors X

Online Partial Least Square Optimization

and Y as

2

3

6 2 1
Cov(X) = ⌃XX = 4 2 6 2 5 ,
1 2 6
2
3
4 0 0
Cov(X, Y ) = ⌃XY = 4 0 2 0 5 ,
0 0 0.5

e and
and ⌃Y Y = ⌃XX . We then generate two matrices U
Ve with each entry independently sampled from N (0, 1).
e and Ve to orthonormal matrices U and
Then we convert U
V by Grand-Schmidt transformation. At last, we generate
the joint covariance matrix for the observational random
vectors X and Y using the following covariance matrix
Cov(X) = U > ⌃XX U,
and

Cov(X, Y ) = U > ⌃XY V,

Cov(Y ) = V > ⌃Y Y V.

We consider the total sample size as n = 2 ⇥ 105 and
choose ⌘ = 5⇥10 5 . The initialization solution (u0 , v0 ) is
a pair of singular vectors associated with the second largest
singular value of ⌃XY , i.e., saddle point. We repeat the
simulation with update (2.2) and (2.3) for 100 times, and
plot the obtained results.
Figure 1(a) illustrates the three phases of the SGD algorithm. Specifically, the horizontal axis is the number of
(1)
iterations, and the vertical axis is hk defined in (3.6). As
(1)
hk ! ±1, we have uk ! ±b
u and vk ! ±b
v , e.g., global
optima. This is due to the symmetric structure of the problem as mentioned in Section 1. Figure 1(a) is consistent
with our theory: In Phase I, the algorithm gradually escapes
from the saddle point; In Phase II, the algorithm quickly
moves towards the optimum; In Phase III, the algorithm
gradually converges to the optimum.
Figure 1(b) further zooms in Phase I of Figure 1(a). We
see that the trajectories of all 100 simulations behave very
similar to an O-U process. Figure 1(c) illustrates the three
(2)
(1)
phases by hk . As our analysis suggests, when hk ! ±1,
(2)
we have hk ! 0. We see that the trajectories of all 100
simulations also behave very similar to an O-U process in
Phase III. These experimental results are consistent with
our theory.
Also, we illustrate h(1) in Phase I and h(2) in Phase III
are O-U process by showing that 100 simulations of h(1)
follow a gaussian distribution in 10, 100 and 1000 iteration and those of h(1) follow a gaussian distribution in
105 , 1.5 ⇥ 105 and 2 ⇥ 105 iteration. This is consistent
with the Theorems 4.1 and 4.4 in Section 4. Also as we can
see that in the Phase I, the variance of h(1) becomes larger
and larger when iteration number increases. Similarly, in
the Phase III, the variance of h(2) becomes closer to a fixed
number.

We then provide a real data experiment for comparing the
computational performance our nonconvex stochastic gradient algorithm for solving (2.1) with the convex stochastic
gradient algorithm for solving (1.2). We choose a subset
of the MNIST dataset, whose labels are 3, 4, 5, or 9. The
total sample size is n = 23343, and m = d = 392. p
As
(Arora et al., 2016) suggest, we choose ⌘k = 0.05/ k
or 2.15 ⇥ 10 5 , for the convex stochastic gradient algorithm. For our nonconvex stochastic gradient algorithm, we
choose either ⌘k = 0.05/k or 10 4 , 2 ⇥ 10 5 , 3 ⇥ 10 5 .
Figure 3 illustrates the computational performance in terms
of iterations and wall clock time. As can be seen, our nonconvex stochastic gradient algorithm outperforms the convex counterpart in iteration complexity, and significantly
outperforms in wall clock time, since the nonconvex algorithm does not need the computationally expensive projection in each iteration. This suggests that dropping convexity for PLS can boost both computational scalability and
efficiency.

6. Discussions
We establish the convergence rate of stochastic gradient
descent (SGD) algorithms for solving online partial least
square (PLS) problems based on diffusion process approximation. Our analysis indicates that for PLS, dropping convexity actually improves efficiency and scalability. Our
convergence results are tighter than existing convex relaxation based method by a factor of O(1/✏), where ✏ is a prespecified error. We believe the following directions should
be of wide interests:
1. Our current results hold only for the top pair of left
and right singular vectors, i.e., r = 1. For r > 1, we
need to solve
b , Vb ) =
(U

argmax
U 2Rm⇥r ,V

2Rd⇥r

subject to U > U = Ir ,

E tr(V > Y X > U )

V > V = Ir .

(6.1)

Our approximations using ODE and SDE, however,
do not admit an unique solution due to rotation. Thus,
extension to r > 1 is a challenging, but also an important future direction.
2. Our current results are only applicable to a fixed step
1
size ⌘ ⇣ ✏( 1
. Our experiments suggest that
2 )d
the diminishing step size
⌘k ⇣ k

1

(

1

2)

1

log d,

k from 1 to N , where N is the sample complexity
from theory, achieves a better empirical performance.
One possible probability tool is Stein’s method (Ross
et al., 2011).

Online Partial Least Square Optimization
Phase III

[uk , vk ]P2

[uk , vk ]P1

2

1
(2)

Phase II

Phase II

hk =

(1)

(1)

hk =

1

2

Phase I

2

[uk , vk ]P1

Phase II

hk =

1

Phase I

Phase III

Phase III

Number of Iteration

Number of Iteration

Number of Iteration
(1)

(1)

(a) All Three Phases of hk .

(2)

(b) Phase I of hk .

(c) All Three Phases of hk .

Figure 1. An illustrative examples of the stochastic gradient algorithm. The three phases of the algorithm are consistent with our theory:
In Phase I, the algorithm gradually escapes from the saddle point; In Phase II, the algorithm quickly iterates towards the optimum; In
Phase III, the algorithm gradually converges to the optimum.

105 iteration

10 iteration

1.5 105 iteration

density value

density value

100 iteration
1000 iteration

(1)

hk =

1
2

2 105 iteration

(2)

hk =

[uk , vk ]P1

(a) The estimated density of h

(1)

1
2

[uk , vk ]P2

(b) The estimated density of h(2) in Phase III.

in Phase I.

5

SGD

3 10

SGD

2 10

5

SGD

1 10

4

SGD

0.05/k

log scale of 1

log scale of 1

û + vk v̂)

3 10 5
2 10 5
1 10 4
0.05/k
0.05/ k
2.15 10

1
(u
2 k

û + vk v̂)

SGD
SGD
SGD
SGD
MSG
MSG

1
(u
2 k

Figure 2. The estimated density based on 100 simulations (obtained by kernel density estimation using 10-fold cross validation) at
(1)
(2)
different iterations in Phase I and Phase III shows that hk ’s in Phase I and hk ’s in Phase III behave very similar to O-U processes,
which is consistent our theory.

Number of Iteration

(a) Comparison by Iteration

5

MSG

0.05/ k

MSG

2.15 10

5

Computational time (seconds)

(b) Comparison by Time

Figure 3. Comparison between nonconvex SGD and convex MSG with different step sizes. We see that SGD not only has a better
iteration complexity, but also is more computationally efficient in wall clock time than convex MSG.

3. Our current results rely on the classical central limit
theorem-type analysis by taking ⌘ ! 0+ . Connecting our analysis to discrete algorithmic proofs, such
as (Jain et al., 2016; Shamir, 2015; Li et al., 2016a),

is an important direction (Barbour and Chen, 2005).
One possible tool for addressing this is Stein’s method
(Ross et al., 2011).

Online Partial Least Square Optimization

References
A BDI , H. (2003). Partial least square regression (pls regression). Encyclopedia for research methods for the
social sciences 792–795.
A NDO , R. K. and Z HANG , T. (2005). A framework for
learning predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Research
6 1817–1853.
A RORA , R., C OTTER , A., L IVESCU , K. and S REBRO , N.
(2012). Stochastic optimization for pca and pls. In Communication, Control, and Computing (Allerton), 2012
50th Annual Allerton Conference on. IEEE.
A RORA , R. and L IVESCU , K. (2012). Kernel cca for
multi-view learning of acoustic features using articulatory measurements. In MLSLP. Citeseer.
A RORA , R., M IANJY, P. and M ARINOV, T. (2016).
Stochastic optimization for multiview representation
learning using partial least squares. In Proceedings of
The 33rd International Conference on Machine Learning.
BARBOUR , A. D. and C HEN , L. H. Y. (2005). An introduction to Stein’s method, vol. 4. World Scientific.
B EN -TAL , A. and N EMIROVSKI , A. (2001). Lectures on
modern convex optimization: analysis, algorithms, and
engineering applications. SIAM.
B HARADWAJ , S., A RORA , R., L IVESCU , K. and
H ASEGAWA -J OHNSON , M. (2012). Multiview acoustic
feature learning using articulatory measurements. In Intl.
Workshop on Stat. Machine Learning for Speech Recognition. Citeseer.

D HILLON , P., F OSTER , D. P. and U NGAR , L. H. (2011).
Multi-view learning of word embeddings via cca. In
Advances in Neural Information Processing Systems 24
(J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira
and K. Q. Weinberger, eds.). Curran Associates, Inc.,
199–207.
E THIER , S. N. and K URTZ , T. G. (2009). Markov processes: characterization and convergence, vol. 282.
John Wiley & Sons.
E VANS , W. (1988). Partial differential equations.
G E , R., H UANG , F., J IN , C. and Y UAN , Y. (2015). Escaping from saddle points-online stochastic gradient for
tensor decomposition. In COLT.
H ARDOON , D. R., S ZEDMAK , S. and S HAWE -TAYLOR ,
J. (2004). Canonical correlation analysis: An overview
with application to learning methods. Neural computation 16 2639–2664.
JAIN , P., J IN , C., K AKADE , S. M., N ETRAPALLI , P. and
S IDFORD , A. (2016). Streaming pca: Matching matrix
bernstein and near-optimal finite sample guarantees for
oja’s algorithm. In 29th Annual Conference on Learning
Theory.
K IDRON , E., S CHECHNER , Y. Y. and E LAD , M. (2005).
Pixels that sound. In Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 1. IEEE.
L ASSERRE , J. B. (2001). Global optimization with polynomials and the problem of moments. SIAM Journal on
Optimization 11 796–817.

C AI , T. T., L I , X., M A , Z. ET AL . (2016). Optimal rates of
convergence for noisy sparse phase retrieval via thresholded wirtinger flow. The Annals of Statistics 44 2221–
2251.

L I , C. J., WANG , M., L IU , H. and Z HANG , T.
(2016a). Near-optimal stochastic approximation for online principal component estimation. arXiv preprint
arXiv:1603.05305 .

C ANDES , E. J., L I , X. and S OLTANOLKOTABI , M. (2015).
Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory 61
1985–2007.

L I , C. J., WANG , Z. and L IU , H. (2016b). Online ica: Understanding global dynamics of nonconvex optimization
via diffusion processes. In Advances in Neural Information Processing Systems.

C HAUDHURI , K., K AKADE , S. M., L IVESCU , K. and
S RIDHARAN , K. (2009). Multi-view clustering via
canonical correlation analysis. In Proceedings of the
26th annual international conference on machine learning. ACM.

N EMIROVSKI , A., J UDITSKY, A., L AN , G. and S HAPIRO ,
A. (2009). Robust stochastic approximation approach to
stochastic programming. SIAM Journal on optimization
19 1574–1609.

C OHEN , M. B., L EE , Y. T., M ILLER , G., PACHOCKI , J.
and S IDFORD , A. (2016). Geometric median in nearly
linear time. In Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory of Computing. ACM.

Ø KSENDAL , B. (2003). Stochastic differential equations.
In Stochastic differential equations. Springer, 65–84.
ROSS , N. ET AL . (2011). Fundamentals of stein’s method.
Probab. Surv 8 210–293.

Online Partial Least Square Optimization

S HAMIR , O. (2015). Fast stochastic algorithms for svd
and pca: Convergence properties and convexity. arXiv
preprint arXiv:1507.08788 .
S OCHER , R. and F EI -F EI , L. (2010). Connecting modalities: Semi-supervised segmentation and annotation of
images using unaligned text corpora. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE.
V INOKOUROV, A., S HAWE -TAYLOR , J. and C RISTIAN INI , N. (2002). Inferring a semantic representation of
text via cross-language correlation analysis. In NIPS,
vol. 1.
Z HAO , T., WANG , Z. and L IU , H. (2015). A nonconvex
optimization framework for low rank matrix estimation.
In Advances in Neural Information Processing Systems.

