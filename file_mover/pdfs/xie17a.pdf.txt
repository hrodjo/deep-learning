Learning Latent Space Models with Angular Constraints

Pengtao Xie 1 2 Yuntian Deng 3 Yi Zhou 4 Abhimanu Kumar 5 Yaoliang Yu 6 James Zou 7 Eric P. Xing 2

Abstract
The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile
renders LSMs to be prone to overfitting. Several
recent studies investigate a new type of regularization approach, which encourages components
in LSMs to be diverse, for the sake of alleviating
overfitting. While they have shown promising
empirical effectiveness, in theory why larger ‚Äúdiversity‚Äù results in less overfitting is still unclear.
To bridge this gap, we propose a new diversitypromoting approach that is both theoretically analyzable and empirically effective. Specifically,
we use near-orthogonality to characterize ‚Äúdiversity‚Äù and impose angular constraints (ACs)
on the components of LSMs to promote diversity. A generalization error analysis shows that
larger diversity results in smaller estimation error and larger approximation error. An efficient
ADMM algorithm is developed to solve the constrained LSM problems. Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversitypromoting approaches.

1. Introduction
Latent space models (LSMs), such as sparse coding (Olshausen & Field, 1997), topic models (Blei et al., 2003)
and neural networks, are widely used in machine learning
to extract hidden patterns and learn latent representations
of data. An LSM consists of a set of components. Each
component aims at capturing one latent pattern and is pa1

Machine Learning Department, Carnegie Mellon University
Petuum Inc. 3 School of Engineering and Applied Sciences, Harvard University 4 College of Engineering and Computer Science,
Syracuse University 5 Groupon Inc. 6 School of Computer Science,
University of Waterloo 7 Department of Biomedical Data Science,
Stanford University. Correspondence to: Pengtao Xie <pengtaox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

rameterized by a weight vector. For instance, in a topic
model (Blei et al., 2003), the components are referred to
as topics, aiming at discovering the semantics underlying
documents. Each topic is associated with a weight vector.
The modeling power of LSMs can be very large when the
number of components is large and the dimension of weight
vectors is high. For instance, in the LightLDA (Yuan et al.,
2015) topic model, the number of topics is 1 million and
the dimension of topic vector is 50000, resulting in a topic
matrix with 50 billion parameters. The vast model capacity of LSMs enables them to flexibly adapt to the complex
patterns underlying data and achieve great predictive performance therefrom.
While highly expressive, LSMs are prone to overfitting,
because of their large amount of model parameters. A
key ingredient to successfully train LSMs is regularization,
which imposes certain control over the model parameters
to reduce model complexity and improve the generalization performance on unseen data. Many regularizers have
been proposed, including `2 regularization, `1 regularization (Tibshirani, 1996), nuclear norm (Recht et al., 2010),
Dropout (Srivastava et al., 2014) and so on.
Recently, a new type of regularization approaches (Yu
et al., 2011; Zou & Adams, 2012a; Xie et al., 2015; 2016a;
Rodrƒ±ÃÅguez et al., 2016), which aim at encouraging the
weight vectors of components in LSMs to be ‚Äúdiverse‚Äù,
are emerging. Zou & Adams (2012b) apply Determinantal
Point Process (Kulesza & Taskar, 2012) to encourage the
topic vectors in LDA to be ‚Äúdiverse‚Äù. Bao et al. (2013) develop a softmax regularizer to promote incoherence among
hidden units in neural network. Xie et al. (2015) propose
an angle-based regularizer to ‚Äúdiversify‚Äù the weight vectors in Restricted Boltzmann Machine (RBM). While these
approaches have demonstrated promising effectiveness on
a wide range of empirical studies, in theory how they reduce overfitting is still unclear. One intuitive explanation
could be: promoting diversity imposes a structural constraint on model parameters, which reduces the model capacity of LSMs and therefore alleviates overfitting. However, how to make this formal is challenging. In this paper, we aim to bridge this gap, by proposing a diversitypromoting approach that is both empirically effective and
theoretically analyzable. We use near-orthogonality to represent ‚Äúdiversity‚Äù and propose to learn LSMs with angular

Learning Latent Space Models with Angular Constraints

constraints (ACs) where the angle between components is
constrained to be close to œÄ2 , which hence encourages the
components to be close to orthogonal (therefore ‚Äúdiverse‚Äù).
Using sparse coding and neural network as study cases, we
analyze how ACs affect the generalization performance of
these two LSMs. The analysis shows that the more close
to œÄ2 the angles are, the smaller the estimation error is and
the larger the approximation error is. The best tradeoffs
of these two errors can be explored by properly tuning the
angles. We develop an alternating direction method of multipliers (ADMM) (Boyd et al., 2011) algorithm to solve
the angle-constrained LSM (AC-LSM) problems. In various experiments, we demonstrate that ACs improve the
generalization performance of LSMs and outperform other
diversity-promoting regularization approaches.

vectors is measured and the regularizer is defined as the
mean of these angles minus their variance. A larger mean
encourages the vectors to have larger angles overall and a
smaller variance encourages the vectors to be evenly different from each other. Xie (2015) apply this regularizer to
encourage the projection vectors in distance metric learning
to be diverse and show that promoting diversity can reduce
model size without sacrificing modeling power. Besides
frequentist-style regularization, diversity-promoting learning is also investigated in Bayesian learning where the components are random variables. Affandi et al. (2013) apply
DPP as a repulsive prior to encourage the location vectors
in GMM to be far apart. Xie et al. (2016a) propose a mutual angular prior that has an inductive bias towards vectors
having larger angles.

The major contributions of this work are:

In the literature of neural networks, many works have studied the ‚Äúdiversification‚Äù of hidden units. Le et al. (2010)
apply a strict-orthogonality constraint over the weight parameters to make the hidden units uncorrelated (therefore
‚Äúdiverse‚Äù). In practice, this hard constraint might be too
restrictive and hurts performance, as we will confirm in experiments. Bao et al. (2013) propose a softmax regularizer to encourage the weight vectors of hidden units to have
small cosine similarity. Cogswell et al. (2015) propose to
decorrelate hidden activations by minimizing their covariance. In convolutional neural networks (CNNs) where the
number of activations is much larger than that of weight
parameters, this regularizer is computationally prohibitive
since it is defined over activations rather than weights.
Henaff et al. (2016) perform a study to show that random
orthogonal initialization of the weight matrices in recurrent neural networks improves its ability to perform longmemory tasks. Xiong et al. (2016) propose a structured
decorrelation constraint which groups hidden units and encourages units within the same group to have strong connections during the training procedure and forces units in
different groups to learn nonredundant representations by
minimizing the cross-covariance between them. Rodrƒ±ÃÅguez
et al. (2016) show that regularizing negatively correlated
features inhibits effective diversity and propose a solution
which locally enforces feature orthogonality. Chen et al.
(2017) propose a group orthogonal CNN which leverages
side information to learn diverse feature representations.
Mao et al. (2017) impose a stochastic decorrelation constraint based on covariance to reduce the co-adaptation of
hidden units. Xie et al. (2017) define a kernel-based regularizer to promote diversity and analyze how it affects the
generalize performance of neural networks (NNs). It is unclear how to generalize the analysis to other LSMs.

‚Ä¢ We propose a new approach to promote diversity
in LSMs, by imposing angular constraints (ACs) on
components, for the sake of alleviating overfitting.
‚Ä¢ We perform theoretical analysis on how ACs affect the
generalization error of two exemplar LSMs: sparse
coding and neural networks.
‚Ä¢ We develop an efficient ADMM algorithm to solve the
AC-LSM problems.
‚Ä¢ Empirical evaluation demonstrates that ACs are very
effective in reducing overfitting and outperforms other
diversity-promoting approaches.
The rest of the paper is organized as follows. Section
2 reviews related works. Second 3 introduces the angleconstrained LSMs and Section 4 gives the theoretical analysis. Section 5 presents experimental results and Section 6
concludes the paper.

2. Related Works
Diversity-promoting learning of latent space models has
been widely studied recently. Ramirez et al. (2010) define a regularizer based on squared Frobenius norm to encourage the dictionary in sparse coding to be incoherent.
Zou & Adams (2012a) use the determinantal point process
(DPP) (Kulesza & Taskar, 2012) to encourage the location
vectors in Gaussian mixture model (GMM) and topics in latent Dirichlet allocation (Blei et al., 2003) to be ‚Äúdiverse‚Äù.
Given m vectors {wj }m
j=1 , DPP is defined as log det(G).
G is a kernel matrix where Gij = k(wi , wj ) and k(¬∑, ¬∑) is a
kernel function. det(G) is the volume of the parallelepiped
formed by {œÜ(wj )}m
j=1 , where œÜ(¬∑) denotes the reproducing kernel feature map associated with kernel k. Vectors
with larger volume are considered to be more diverse since
they are more spread out. Xie (2015) develop an anglebased regularizer to encourage the weight vectors of hidden
units in restricted Boltzmann machine to be close to orthogonal. The non-obtuse angle between each pair of weight

Diversity-promoting learning has been investigated in nonLSM models as well. In multi-class classification, Malkin
& Bilmes (2008) encourage the coefficient vectors of different classes to be diverse by maximizing the determi-

Learning Latent Space Models with Angular Constraints

nant of the covariance matrix of the coefficient vectors. In
classifiers ensemble, Yu et al. (2011) develop a regularizer
to encourage the coefficient vectors of support vector machines (SVMs) to have small cosine similarity and analyze
how this regularizer affects the generalization performance.
The analysis is specific to SVM ensemble. It is unclear how
to generalize it to latent space models.

3. Methods
In this section, we propose Angle-Constrained Latent
Space Models (AC-LSMs) and present an ADMM algorithm to solve them.
3.1. Latent Space Models with Angular Constraints
An LSM consists of m components and these components
are parameterized by vectors W = {wj }m
j=1 . Let L(W)
denote the objective function of this LSM. Similar to (Bao
et al., 2013; Xie et al., 2015; Rodrƒ±ÃÅguez et al., 2016), we use
angle to characterize diversity: the components are considered to be more diverse if they are close to being orthogonal, i.e., their angles are close to œÄ2 . To encourage this, we
require the absolute value of cosine similarity between each
pair of components to be less than a small value œÑ , which
leads to the following angle-constrained LSM (AC-LSM)
problem
min

L(W)

s.t.

1 ‚â§ i < j ‚â§ m, kwi ki2 kwjj k2 ‚â§ œÑ

W

|w ¬∑w |

(1)

The parameter œÑ controls the level of near-orthogonality
(or diversity). A smaller œÑ indicates that the vectors are
more close to being orthogonality, and hence are more diverse. As will be shown later, representing diversity using
the angular constraints facilitates theoretical analysis and is
empirically effective as well.

(denoted by A) by minimizing
the following
objective
Pn
Pm
function: L(W, A) = 12 i=1 (kxi ‚àí j=1 Œ±ij wj k22 +
Pm
Pm
Œª1 j=1 |Œ±ij |1 ) + Œª2 j=1 kwj k22 . Applying ACs to the
basis vectors, we obtain the following AC-SC problem:
L(W, A)
|w ¬∑w |
1 ‚â§ i < j ‚â§ m, kwi ki2 kwjj k2 ‚â§ œÑ

minW,A
s.t.

Neural Networks In a neural network (NN) with L hidden layers, each hidden layer l is equipped with m(l) units
and each unit i is connected with all units in layer l ‚àí 1.
Hidden unit i at layer l is parameterized by a weight vector
(l)
wi . These hidden units aim at capturing latent features
underlying data. Applying ACs to the weight vectors of
hidden units, we obtain the following AC-NN problem
min L(W)
W

s.t.

(l)

‚àÄ 1 ‚â§ l ‚â§ L, 1 ‚â§ i < j ‚â§ m(l) ,

Sparse Coding Given a set of data samples {xi }ni=1 ,
where x ‚àà Rd , sparse coding (SC) (Olshausen & Field,
1997) aims to use a set of ‚Äúbasis‚Äù vectors (referred to as
dictionary) W = {wj }m
j=1 to reconstruct the data samples. Each data sample x is reconstructed by taking
Pm a sparse
linear combination of the basis vectors x ‚âà
j=1 Œ±j wj
where {Œ±j }m
are
the
linear
coefficients
(referred
to as
j=1
sparse codes) and most of them are zero. The reconstruction
Pm error is measured using the squared `2 norm
kx ‚àí j=1 Œ±j wj k22 . To achieve sparsity among the coeffiPm
cients, `1 -regularization is utilized: j=1 |Œ±j |1 . To avoid
the degenerated case where most coefficients are zero and
the basis vectors are of large magnitude, `2 -regularization
is applied to the basis vectors: kwj k22 . Putting these
pieces together, we learn the basis vectors and sparse codes

(l)

|wi ¬∑wj |
(l)

(l)

kwi k2 kwj k2

‚â§œÑ

where W denotes weight vectors in all layers and L(W) is
the objective function of this NN.
3.3. Algorithm
In this section, we develop an ADMM-based algorithm to
solve the AC-LSM problem. To make it amenable for optimization, we first factorize each weight vector w into its
w
e = kwk
`2 norm g = kwk2 and direction w
. Under such a
2
e where
factorization, w can be reparameterized as w = g w,
e 2 = 1. Then the problem defined in Eq.(1)
g > 0 and kwk
can be transformed into
min
f
W,G

s.t.

3.2. Case Studies
In this section, we apply the ACs to two LSMs.

(2)

f G)
L(W,
e j k2 = 1
‚àÄj, gj ‚â• 0, kw
ei ¬∑ w
e j| ‚â§ œÑ
‚àÄi 6= j, |w

(3)

m
f = {w
e j }m
where W
j=1 and G = {gj }j=1 . We solve this
f and G. Fixing W,
f
new problem by alternating between W
the problem defined over G is: minG L(G) s.t. ‚àÄj, gj ‚â•
0, which can be solved using projected gradient descent.
f is
Fixing G, the sub-problem defined over W

min
f
W

s.t.

f
L(W)
e j k2 = 1
‚àÄj, kw
ei ¬∑ w
e j| ‚â§ œÑ
‚àÄi 6= j, |w

(4)

which we solve using an ADMM algorithm. There are
ei ¬∑ w
e j | ‚â§ œÑ . For
R = m(m ‚àí 1) pairwise constraints |w
the r-th constraint, let p(r) and q(r) be the index of the
first and second vector respectively, i.e., the r-th constraint
e p(r) ¬∑ w
e q(r) | ‚â§ œÑ . First, we introduce auxiliary variis |w
(r)

(r)

R
ables {v1 }R
r=1 and {v2 }r=1 , to rewrite the problem in

Learning Latent Space Models with Angular Constraints

Eq.(4) into an equivalent form. For each pairwise cone p(r) ¬∑ w
e q(r) | ‚â§ œÑ , we introduce two auxiliary
straint: |w
(r)
(r)
(r)
(r)
e p(r) = v1 , w
e q(r) = v2 ,
vectors v1 and v2 , and let w

Let Œ≥1 , Œ≥2 , Œª1 ‚â• 0, Œª2 ‚â• 0 be the KKT multipliers associated with the four constraints in this sub-problem. According to the KKT conditions, we have

kv1 k2 = 1, kv2 k2 = 1, |v1 ¬∑ v2 | ‚â§ œÑ . To this end,
we obtain the following problem

e p(r) )+2Œ≥1 v1 +(Œª1 ‚àíŒª2 )v2 = 0 (6)
‚àíy1 +œÅ(v1 ‚àíw

(r)

(r)

min
f
W,V

s.t.

(r)

(r)

(r)

(r)

(r)

(r)

(r)

(r)

(r)

(r)

e q(r) )+2Œ≥2 v2 +(Œª1 ‚àíŒª2 )v1 = 0 (7)
‚àíy2 +œÅ(v2 ‚àíw

f
L(W)
e j k2 = 1
‚àÄj, kw
(r)
(r)
e p(r) = v1 , w
e q(r) = v2
‚àÄr, w
(r)
(r)
(r)
(r)
‚àÄr, kv1 k2 = 1, kv2 k2 = 1, |v1 ¬∑ v2 | ‚â§ œÑ
(r)

(r)

Then we define the
where V = {(v1 , v2 )}R
r=1 .
augmented Lagrangian, with Lagrange multipliers Y =
(r)
(r)
{(y1 , y2 )}R
r=1 and parameter œÅ

We solve these two equations by examining four cases.
Case 1 First, we assume Œª1 = 0, Œª2 = 0, then (œÅ +
(r)
(r)
(r)
(r)
e p(r) and (œÅ + 2Œ≥2 )v2 = y2 +
2Œ≥1 )v1 = y1 + œÅw
(r)
e q(r) . According to the primal feasibility kv1 k2 = 1
œÅw
(r)

and kv2 k2 = 1, we know

f +
L(W)

min
f
W,V,Y

(r)

v1 =

(r)

e p(r) ‚àí v1 )
(y1 ¬∑ (w

r=1
(r)

s.t.

(r)

(r)

(r)

R
P

(r)

(r)

(r)

(5)

e j k2 = 1
‚àÄj, kw

(r)

(r)

(r)

(r)

(r)

(r)

‚àíy1 ¬∑ v1 ‚àí y2 ¬∑ v2

v1 ,v2

(r)

(r)

e p(r) ‚àí v1 k22 + œÅ2 kw
e q(r) ‚àí v2 k22
+ œÅ2 kw
(r)

(r)

(r)

(8)

(r)

(r)

(r)

(9)

(r)

e p(r) k22 (10)
(œÅ+2Œ≥1 )2 +Œª21 +2(œÅ+2Œ≥1 )Œª1 œÑ = ky1 +œÅw
Similarly, from Eq.(9), we get

The corresponding sub-problem is
(r)

(r)

According to the complementary slackness condition, we
(r)
(r)
know v1 ¬∑ v2 = œÑ . For the vectors on both sides of
Eq.(8), taking the square of their `2 norm, we get

e p(r) ‚àí v1 k22 + œÅ2 kw
e q(r) ‚àí v2 k22 )
+ œÅ2 kw

Solve v1 , v2

We assume Œª1 > 0 and Œª2 = 0, then

e q(r)
(œÅ + 2Œ≥2 )v2 + Œª1 v1 = y2 + œÅw

e p(r) + y2 ¬∑ w
e q(r)
(y1 ¬∑ w
(r)

(r)

e p(r)
(œÅ + 2Œ≥1 )v1 + Œª1 v2 = y1 + œÅw

For sparse coding, we solve this sub-problem using coore j by fixing
dinate descent. At each iteration, we update w
the other variables. Please refer to the supplements for details. For neural network, this sub-problem can be solved
using projected gradient descent which iteratively performs
ej
the following three steps: (1) compute the gradient of w
using backpropagation; (2) perform a gradient descent upe j ; (3) project each vector onto the unit sphere:
date of w
ej ‚Üê w
e j /kw
e j k2 .
w

s.t.

(r)

e q(r) k2
ky2 + œÅw

Case 2

r=1

min

e q(r)
y2 + œÅw

e j k2 = 1
‚àÄj, kw
(r)
(r)
(r)
(r)
‚àÄr, kv1 k2 = 1, kv2 k2 = 1, |v1 ¬∑ v2 | ‚â§ œÑ

f +
min L(W)

(r)

(r)

e p(r) k2
ky1 + œÅw

(r)

, v2 =

Then we check whether the constraint |v1 ¬∑ v2 | ‚â§ œÑ is
(r)
(r)
satisfied. If so, then v1 and v2 are the optimal solution.

f The sub-problem defined over W
f is
Solve W

s.t.

e p(r)
y 1 + œÅw

e q(r) ‚àí v2 ) + œÅ2 kw
e p(r) ‚àí v1 k22
+y2 ¬∑ (w
(r) 2
œÅ
e q(r) ‚àí v2 k2 )
+ 2 kw

f V, Y.
which can be solved by alternating between W,

f
W

(r)

(r)

(r)

R
P

(r)

kv1 k2 = 1, kv2 k2 = 1,
(r)
(r)
(r)
(r)
v1 ¬∑ v2 ‚â§ œÑ, ‚àív1 ¬∑ v2 ‚â§ œÑ

(r)

e q(r) k22 (11)
(œÅ+2Œ≥2 )2 +Œª21 +2(œÅ+2Œ≥2 )Œª1 œÑ = ky2 +œÅw
Taking the inner product of the two vectors on the left hand
sides of Eq.(8,9), and that on the right hand sides, we get
(2œÅ + 2Œ≥1 + 2Œ≥2 )Œª1 + ((œÅ + 2Œ≥1 )(œÅ + 2Œ≥2 ) + Œª21 )œÑ
(r)
(r)
e p(r) )> (y2 + œÅw
e q(r) )
= (y1 + œÅw
(12)
Solving the system of equations consisting of Eq.(10-12),
we obtain the optimal values of Œ≥1 , Œ≥2 and Œª1 . Plugging
(r)
them into Eq.(8) and Eq.(9), we obtain a solution of v1
(r)
and v2 . Then we check whether this solution satisfies
(r)
(r)
‚àív1 ¬∑ v2 ‚â§ œÑ . If so, this is an optimal solution.
In Case 3, we discuss Œª1 = 0, Œª2 > 0. In Case 4, we
discuss Œª1 > 0, Œª2 > 0. The corresponding problems can
be solved in a similar way as Case 2. Please refer to the
supplements for details.

Learning Latent Space Models with Angular Constraints
(r)

(r)

Solve y1 , y2

We simply perform the following:

(r)

(r)

(r)

(13)

(r)

(r)

(r)

(14)

e p(r) ‚àí v1 )
y1 = y1 + œÅ(w
e q(r) ‚àí v2 )
y2 = y2 + œÅ(w

empirical risk minimizer. We aim to analyze the generalization error L(fÀÜ) of the empirical risk minimizer fÀÜ. L(fÀÜ)
can be decomposed into L(fÀÜ) = L(fÀÜ) ‚àí L(f ‚àó ) + L(f ‚àó ),
where L(fÀÜ) ‚àí L(f ‚àó ) is the estimation error and L(f ‚àó ) is
the approximation error.

Compared with a vanilla backpropagation algorithm, the
major extra cost in this ADMM algorithm comes from solv(r)
(r)
ing the R = m(m ‚àí 1) pairs of vectors {v1 , v2 }R
r=1 .
Solving each pair incurs O(m) cost. The R pairs bring in
a total cost of O(m3 ). Such a cubic cost is also incurred in
other decorrelation methods such as (Le et al., 2010; Bao
et al., 2013). In practice, m is typically less than 1000. This
O(m3 ) cost does not substantially bottleneck computation,
as we will validate in experiments.

For simplicity, we start with a ‚Äúsimple‚Äù fully connected network with one hidden layer of m units, used for univariate
regression (one output unit) with squared loss. Analysis
for more complicated fully connected NNs with multiple
hidden layers can be achieved in a straightforward way by
cascading our analysis for this ‚Äúsimple‚Äù fully connected
NN. Let x ‚àà Rd be the input vector and y be the response
value. For simplicity, we assume max{kxk2 , |y|} ‚â§ 1. Let
wj ‚àà Rd be the weights connecting the j-th hidden unit
with input units, with kwj k2 ‚â§ C.

4. Analysis

Let Œ± be a vector where Œ±j is the weight connecting hidden unit j to the output unit, with kŒ±k2 ‚â§ B. We assume the activation function h(t) applied on the hidden
units is Lipschitz continuous with constant L. Commonly
used activation functions such as rectified linear h(t) =
max(0, t), tanh h(t) = (et ‚àí e‚àít )/(et + e‚àít ), and sigmoid h(t) = 1/(1 + e‚àít ) are all Lipschitz continuous with
L = 1, 1, 0.25, respectively. Consider the hypothesis set

In this section, we discuss how the parameter œÑ which controls the level of diversity affects the generalization performance of sparse coding and neural network.
4.1. Sparse Coding
Following (Vainsencher et al., 2011), we assume the data
example x ‚àà Rd and basis vector w ‚àà Rd are both of unit
length, and the linear coefficient vector a ‚àà Rm is at most
k sparse, i.e., kak0 ‚â§ k. The estimation error of dictionary
W is defined as
Xm
L(W) = Ex‚àºp‚àó [minkak0 ‚â§k kx ‚àí
aj wj k2 ]. (15)
j=1

Pn
Pm
e
Let L(W)
= n1 i=1 minkak0 ‚â§k kxi ‚àí j=1 aj wj k2 be
the empirical reconstruction error on n samples. We have
the following theorem.
Theorem 1 Assume œÑ < k1 , with probability at least 1 ‚àí Œ¥:
s
‚àö
r
r
nk
dm ln 41‚àíkœÑ
ln 1/Œ¥
4
e
+
+
. (16)
L(W) ‚â§ L(W)+
2n
2n
n

F = {x 7‚Üí

m
X

Œ±j h(wj> x) | kŒ±k2 ‚â§ B, kwj k2 ‚â§ C,

j=1

‚àÄi 6= j, |wi ¬∑ wj | ‚â§ œÑ kwi k2 kwj k2 }.
The estimation error given in Theorem 2 below indicates
how well the algorithm is able to learn from the samples.
Theorem 2 Let the activation function h be L-Lipschitz
continuous and the loss `(yÃÇ, y) = 21 (yÃÇ ‚àí y)2 . Then, with
probability at least 1 ‚àí Œ¥:
p
‚àö
2
2 ln(4/Œ¥) + 4Œ≥B(2CL + |h(0)|) m
Œ≥
‚àó
‚àö
L(fÀÜ)‚àíL(f ) ‚â§
n
(17)
p
‚àö
where Œ≥ = 1 + BCL (m ‚àí 1)œÑ + 1 + mB|h(0)|.

4.2. Neural Network

Note that Œ≥, hence the above bound on estimation error, decreases as œÑ becomes smaller. The bound goes to zero as n
(sample size) goes to infinite. The inverse square root dependence on n matches existing results (Bartlett & Mendelson, 2003). We note that it is straightforward to extend our
bound to any bounded Lipschitz continuous loss `.

The generalization error of a hypothesis f represented with a neural network is defined as L(f ) =
E(x,y)‚àºp‚àó [`(f (x), y)], where p‚àó is the distribution of inputoutput pair (x, y) and `(¬∑,
P¬∑)n is the loss function. The training error is LÃÇ(f ) = n1 i=1 `(f (x(i) ), y (i) ), where n is
the number of training samples. Let f ‚àó ‚àà argminf ‚ààF L(f )
be the true risk minimizer and fÀÜ ‚àà argminf ‚ààF LÃÇ(f ) be the

The approximation error indicates how capable the hypothesis set F is to approximate a target function g = E[y|x],
where the errorR is measured by minf ‚ààF kf ‚àí gkL2 and
kf ‚àí gk2L2 = (f (x) ‚àí g(x))2 P (dx). Following (Barron, 1993), we assume the target function g satisfies certain
smoothness condition that is expressed
in the first moment
R
of its Fourier representation:
kœâk2 |gÃÉ(œâ)|dœâ ‚â§ B/2

Note that the right hand side is an increasing function w.r.t
œÑ . As expected, a smaller œÑ (implying more diversity)
would induce a lower estimation error bound.

Learning Latent Space Models with Angular Constraints

where gÃÉ(œâ) is the Fourier representation of g(x). For such
function the following theorem states its approximation error. (In order to derive explicit constants we restrict h to be
the sigmoid function, but other Lipschitz continuous activation function can be similarly handled.)
œÄ

SC
DCM-SC
CS-SC
DPP-SC
IC-SC
MA-SC
AC-SC

‚àíŒ∏

Theorem 3 Let C > 1, m ‚â§ 2(b 2 Œ∏ c + 1), where
Œ∏ = arccos(œÑ ), and h(t) = 1/(1 + e‚àít ). Then, there is
a function f ‚àà F such that
kf ‚àí gkL2 ‚â§ B( ‚àö1m + 1+2Cln C )+
‚àö
+ 2 mBC sin( min(3mŒ∏,œÄ)
).
2

(18)

This theorem implies that whether to use the angular constraint (AC) or not has a significant influence on the approximate error bound: without using AC (œÑ = 1), the
bound is a decreasing function of m (the number of hidden units); using AC (œÑ < 1), the bound increases with
m. This striking phrase-change indicates the impact of AC.
Given a fixed m, the bound decreases with œÑ , implying that
a stronger regularization (smaller œÑ ) incurs larger approximation error. When œÑ = 1, the second term in the bound
vanishes and the bound is reduced to the one in (Barron,
1993), which is a decreasing function of m (and C, the upper bound on the weights). When œÑ < 1, the second term
œÄ
‚àíŒ∏
increases with m up to the upper bound 2(b 2 Œ∏ c+1). This
is because a larger number of hidden units bear a larger
difficulty in satisfying the pairwise ACs, which causes the
function space F to shrink rapidly; accordingly, the approximation power of F decreases quickly.
The analysis in the two theorems shows that œÑ incurs a
tradeoff between the estimation error and the approximation error: decreasing œÑ reduces the estimation error and
enlarges the approximation error. Since the generalization
error is the sum of the estimation error and the approximation error, œÑ has an optimal value to yield the minimal
generalization error.

5. Experiments
In this section, we present experimental results. Due to
space limit, we put some results into supplements.
5.1. Sparse Coding
Following (Yang et al., 2009), we applied sparse coding
for image feature learning. We used three datasets in the
experiments: Scenes-15 (Lazebnik et al., 2006), Caltech256 (Griffin et al., 2007) and UIUC-Sport (Li & Fei-Fei,
2007). For each dataset, five random train/test splits are
performed and the results are averaged over the five runs.
We extract pixel-level dense SIFT (Lowe, 2004) features
where the step size and patch size are 8 and 16 respectively. On top of the SIFT features, we use sparse coding methods to learn a dictionary and represent each SIFT

Scene
83.6 ¬± 0.2
85.4 ¬± 0.5
84.8 ¬± 0.6
84.6 ¬± 0.3
85.5 ¬± 0.1
86.1 ¬± 0.5
86.5 ¬± 0.7

Caltech
42.3 ¬± 0.4
44.7 ¬± 0.8
45.4 ¬± 0.5
43.5 ¬± 0.3
43.9 ¬± 0.7
45.6 ¬± 0.4
46.1 ¬± 0.3

Sports
87.4 ¬± 0.5
89.6 ¬± 0.1
88.3 ¬± 0.3
88.1 ¬± 0.2
90.2 ¬± 0.7
89.7 ¬± 0.4
90.9 ¬± 0.3

Table 1. Classification accuracy (%) on three datasets.

feature into a sparse code. To obtain image-level features,
we apply max-pooling (Yang et al., 2009) and spatial pyramid matching (Lazebnik et al., 2006; Yang et al., 2009)
over the pixel-level sparse codes. Then a linear SVM is
applied to classify the images. We compare with other
diversity-promoting regularizers including determinant of
covariance matrix (DCM) (Malkin & Bilmes, 2008), cosine similarity (CS) (Yu et al., 2011), determinantal point
process (DPP) (Kulesza & Taskar, 2012; Zou & Adams,
2012b), InCoherence (IC) (Bao et al., 2013) and mutual
angles (MA) (Xie et al., 2015). We use 5-fold cross validation to tune œÑ in {0.3, 0.4, ¬∑ ¬∑ ¬∑ , 1} and the number of
basis vectors in {50, 100, 200, ¬∑ ¬∑ ¬∑ , 500}. The parameter œÅ
in ADMM is set to 1.
Table 1 shows the classification accuracy on three datasets,
from which we can see that compared with unregularized
SC, AC-SC greatly improves performance. For example,
on the Sports dataset, AC improves the accuracy from
87.4% to 90.9%. This suggests that AC is effective in
reducing overfitting and improving generalization performance. Compared with other diversity-promoting regularizers, AC achieves better performance, demonstrating its
better efficacy in promoting diversity.
5.2. Neural Networks
We evaluate AC on three types of neural networks: fullyconnected NN (FNN) for phone recognition (Hinton et al.,
2012), CNN for image classification (Krizhevsky et al.,
2012), and RNN for question answering (Seo et al., 2017).
In the main paper, we report results on four datasets:
TIMIT1 , CIFAR-102 , CNN (Hermann et al., 2015), Daily
Mail (Hermann et al., 2015). Please refer to the supplements for results on other datasets.
FNN for Phone Recognition The TIMIT dataset contains a total of 6300 sentences (5.4 hours), divided into
a training set (462 speakers), a validation set (50 speakers) and a core test set (24 speakers). We used the Kaldi
(Povey et al., 2011) toolkit to train the monophone system
which was utilized to do forced alignment and to get labels for speech frames. The toolkit was also utilized to
preprocess the data into log-filter banks. Among methods based on FNN, Karel‚Äôs recipe in Kaldi achieves state
1
2

https://catalog.ldc.upenn.edu/LDC93S1
https://www.cs.toronto.edu/ kriz/cifar.html

Learning Latent Space Models with Angular Constraints

Network
Segmental NN (Abdel-Hamid et al., 2013)
MCRBM (Dahl et al., 2010)
DSR (Tang et al., 2015)
Rectifier NN (ToÃÅth, 2013)
DBN (Srivastava et al., 2014)
Shallow CNN (Ba & Caruana, 2014)
Structured DNN (Yang et al., 2016)
Posterior Modeling (Prabhavalkar et al., 2013)
Kaldi
CS-Kaldi
IC-Kaldi
MA-Kaldi
DC-Kaldi
AC-Kaldi
CTC (Graves et al., 2013)
RNN Transducer (Graves et al., 2013)
Attention RNN (Chorowski et al., 2015)
CTC+SCRF (Lu et al., 2017)
Segmental RNN (Lu et al., 2016)
RNNDrop (Moon et al., 2015)
CNN (ToÃÅth, 2014)

Error
21.9
20.5
19.9
19.8
19.7
19.5
18.8
18.5
18.53
18.48
18.46
18.51
18.50
18.41
18.4
17.7
17.6
17.4
17.3
16.9
16.7

Network
Maxout (Goodfellow et al., 2013)
NiN (Lin et al., 2013)
DSN (Lee et al., 2015)
Highway Network (Srivastava et al., 2015)
All-CNN (Springenberg et al., 2014)
ResNet (He et al., 2016)
ELU-Network (Clevert et al., 2015)
LSUV (Mishkin & Matas, 2015)
Fract. Max-Pooling (Graham, 2014)
WideResNet (Huang et al., 2016)
CS-WideResNet
IC-WideResNet
MA-WideResNet
DC-WideResNet
OP-WideResNet
AC-WideResNet
ResNeXt (Xie et al., 2016b)
PyramidNet (Huang et al., 2016)
DenseNet (Huang et al., 2016)
PyramidSepDrop (Yamada et al., 2016)

Error
9.38
8.81
7.97
7.60
7.25
6.61
6.55
5.84
4.50
3.89
3.81
3.85
3.68
3.77
3.69
3.63
3.58
3.48
3.46
3.31

Table 3. Classification error (%) on CIFAR-10 test set
Table 2. Phone error rate (%) on the TIMIT test set.

of the art performance. We apply AC to the FNN in this
recipe. The inputs of the FNN are the FMLLR (Gales,
1998) features of the neighboring 21 frames, which are
mean centered and normalized to have unit variance. The
number of hidden layers is 4. Each layer has 1024 hidden units. Stochastic gradient descent (SGD) is used to
train the network. The learning rate is set to 0.008. We
compare with four diversity-promoting regularizers: CS,
IC, MA and DeCorrelation (DC) (Cogswell et al., 2015).
The regularization parameter in these methods are tuned in
{10‚àí6 , 10‚àí5 , ¬∑ ¬∑ ¬∑ , 105 }. The Œ≤ parameter in IC is set to 1.
Table 2 shows state of the art phone error rate (PER) on the
TIMIT core test set. Methods in the first panel are mostly
based on FNN, which perform less well than Kaldi. Methods in the third panel are all based on RNNs which in general perform better than FNN since they are able to capture
the temporal structure in speech data. In the second panel,
we compare AC with other diversity-promoting regularizers. Without regularization, the error is 18.53%. With AC,
the error is reduced to 18.41%, which is very close to a
strong RNN-based baseline Connectionist Temporal Classification (CTC) (Graves et al., 2013). Besides, AC outperforms other regularizers.
CNN for Image Classification The CIFAR-10 dataset
contains 32x32 color images from 10 categories, with
50,000 images for training and 10,000 for testing. We used
5000 training images as the validation set to tune hyperparameters. The data is augmented by first zero-padding the
images with 4 pixels on each side, then randomly cropping

the padded images to reproduce 32x32 images. We apply
AC to wide residual network (WideResNet) (Zagoruyko
& Komodakis, 2016) where the depth is set to 28 and
the width is set to 10. SGD is used for training, with
epoch number 200, initial learning rate 0.1, minibatch size
128, Nesterov momentum 0.9, dropout probability 0.3 and
weight decay 0.0005. The learning rate is dropped by 0.2
at 60, 120 and 160 epochs. The performance is the median of 5 runs. We compare with CS, IC, MA, DC and
an Orthogonality-Promoting (OP) regularizer (Rodrƒ±ÃÅguez
et al., 2016).
Table 3 shows state of the art classification error on the test
set. Compared with the unregularized WideResNet which
achieves an error of 3.89%, applying AC reduces the error
to 3.63%. AC achieves lower error than other regularizers.
LSTM for Question Answering We apply AC to long
short-term memory (LSTM) (Hochreiter & Schmidhuber,
1997) network, which is a type of RNN. Given the input xt
at timestamp t, LSTM produces a hidden state ht based on
the following transition equations:
it = œÉ(W(i) xt + U(i) ht‚àí1 + b(i) )
ft = œÉ(W(f ) xt + U(f ) ht‚àí1 + b(f ) )
ot = œÉ(W(o) xt + U(o) ht‚àí1 + b(o) )
ct = it  tanh(W(c) xt + U(c) ht‚àí1 + b(c) ) + ft  ct‚àí1
ht = ot  tanh(ct )
where Ws are Us are gate-specific weight matrices. On
the row vectors of each weight matrix, the AC is applied.
The LSTM is used for a question answering (QA) task on
two datasets: CNN and DailyMail (Hermann et al., 2015),

Hermann et al. (2015)
Hill et al. (2015)
Kadlec et al. (2016)
Kobayashi et al. (2016)
Sordoni et al. (2016)
Trischler et al. (2016)
Chen et al. (2016)
Cui et al. (2016)
Shen et al. (2016)
BIDAF
CS-BIDAF
IC-BIDAF
MA-BIDAF
DC-BIDAF
AC-BIDAF
Dhingra et al. (2016)
Dhingra et al. (2017)

CNN
Dev
Test
61.6
63.0
63.4
6.8
68.6
69.5
71.3
72.9
72.6
73.3
73.4
74.0
73.8
73.6
73.1
74.4
72.9
74.7
76.31 76.94
76.43 77.10
76.41 77.21
76.49 77.09
76.35 77.15
76.62 77.23
77.9
77.9
79.2
78.6

DailyMail
Dev
Test
70.5
69.0
75.0
73.9
77.6
76.6
77.6
76.6
80.33 79.63
80.37 79.71
80.49 79.83
80.42 79.74
80.38 79.67
80.65 79.88
81.5
80.9
‚Äì
‚Äì

Table 4. Accuracy (%) on the two QA datasets

each containing a training, development and test set with
300k/4k/3k and 879k/65k/53k examples respectively. Each
example consists of a passage, a question and an answer.
The question is a cloze-style task where an entity is replaced by a placeholder and the goal is to infer this missing entity (answer) from all the possible entities appearing in the passage. The LSTM architecture and experimental settings follow the Bidirectional Attention Flow
(BIDAF) (Seo et al., 2017) model, which consists of the
following layers: character embedding, word embedding,
contextual embedding, attention flow, modeling and output. LSTM is applied in the contextual embedding and
modeling layer. Character embedding is based on onedimensional convolutional neural network, where the number of filters is set to 100 and the width of receptive field
is set to 5. In LSTM, the size of hidden state is set to 100.
Optimization is based on AdaDelta (Zeiler, 2012), where
the minibatch size and initial learning rate are set to 48 and
0.5. The model is trained for 8 epochs. Dropout (Srivastava et al., 2014) with probability 0.2 is applied. We
compare with four diversity promoting regularizers: CS,
IC, MA and DC.
Table 4 shows state of the art accuracy on the two datasets.
As can be seen, after applying AC to BIDAF, the accuracy is improved from 76.94% to 77.23% on the CNN test
set and from 79.63% to 79.88% on the DailyMail test set.
Among the diversity-promoting regularizers, AC achieves
the highest accuracy.
5.3. Sensitivity to Parameter œÑ
In the theoretical analysis presented in Section 4, we have
shown that the parameter œÑ which controls the level of near-

Phone error rate

Learning Latent Space Models with Angular Constraints
18.8
18.75
18.7
18.65
18.6
18.55
18.5
18.45
18.4
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1

ùúè

Figure 1. Phone error rate on TIMIT, under varying œÑ

No regularization
CS
IC
MA
DC
OP
AC

TIMIT
1.1
1.2
1.2
1.3
1.5
‚Äì
1.3

CIFAR-10
6.3
6.8
6.7
7.0
7.6
6.8
7.1

CNN
69.4
74.8
76.1
78.6
82.9
‚Äì
79.7

Table 5. Average runtime (hours)

orthogonality (or diversity) incurs a tradeoff between estimation error and approximation error. In this section, we
provide an empirical verification, using FNN on TIMIT as
a study case. Figure 1 shows how the phone error rates vary
on the TIMIT core test set. As can be seen, the lowest test
error is achieved under a moderate œÑ (= 0.75). Either a
smaller or a larger œÑ degrades the performance. This empirical observation is aligned with the theoretical analysis
that the best generalization performance is achieved under
a properly chosen œÑ . When œÑ is close to 0, the hidden units
are close to orthogonality, which yields much poorer performance. This confirms that the strict-orthogonality constraint proposed by (Le et al., 2010) is too restrictive and is
less favorable than a ‚Äúsoft‚Äù regularization approach.
5.4. Computational Time
We compare the computational time of neural networks under different regularizers. Table 5 shows the total runtime
time of FNNs on TIMIT and CNNs on CIFAR-10 with a
single GTX TITAN X GPU, and the runtime of LSTM networks on the CNN dataset with 2 TITAN X GPUs. Compared with no regularization, AC incurs a 18.2% extra time
on TIMIT, 12.7% on CIFAR-10 and 14.8% on CNN. The
runtime of AC is comparable to that under other diversitypromoting regularizers.

6. Conclusions
In this paper, we propose Angled-Constrained Latent Space
Models (AC-LSMs) that aim at promoting diversity among
components in LSMs for the sake of alleviating overfitting. Compared with previous diversity-promoting methods, AC has two benefits. First, it is theoretically analyzable: the generalization error analysis shows that larger diversity leads to smaller estimation error and larger approximation error. Second, it is empirically effective, as validated in various experiments.

Learning Latent Space Models with Angular Constraints

Acknowledgements
We would like to thank the anonymous reviewers for
the suggestions and comments that help to improve
this work a lot, and thank Yajie Miao for helping
with some of the experiments. P.X and E.X are supported by National Institutes of Health P30DA035778,
R01GM114311, National Science Foundation IIS1617583,
DARPA FA872105C0003 and Pennsylvania Department of
Health BD4BH4100070287.

References
Abdel-Hamid, Ossama, Deng, Li, Yu, Dong, and Jiang,
Hui. Deep segmental neural networks for speech recognition. INTERSPEECH, 2013.
Affandi, Raja Hafiz, Fox, Emily, and Taskar, Ben. Approximate inference in continuous determinantal processes.
In Advances in Neural Information Processing Systems,
pp. 1430‚Äì1438, 2013.
Ba, Jimmy and Caruana, Rich. Do deep nets really need to
be deep? In Advances in neural information processing
systems, pp. 2654‚Äì2662, 2014.
Bao, Yebo, Jiang, Hui, Dai, Lirong, and Liu, Cong. Incoherent training of deep neural networks to de-correlate
bottleneck features for speech recognition. In Acoustics,
Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6980‚Äì6984. IEEE, 2013.
Barron, Andrew R. Universal approximation bounds for
superpositions of a sigmoidal function. Information Theory, IEEE Transactions on, 1993.
Bartlett, Peter L and Mendelson, Shahar. Rademacher and
gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463‚Äì
482, 2003.
Blei, David M, Ng, Andrew Y, and Jordan, Michael I. Latent dirichlet allocation. the Journal of machine Learning research, 2003.
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein,
J. Distributed optimization and statistical learning via
alternating direction method of multipliers. Foundations
R in Machine Learning, 2011.
and Trends
Chen, Danqi, Bolton, Jason, and Manning, Christopher D.
A thorough examination of the cnn/daily mail reading
comprehension task. arXiv preprint arXiv:1606.02858,
2016.
Chen, Yunpeng, Jin, Xiaojie, Feng, Jiashi, and Yan,
Shuicheng. Training group orthogonal neural networks with privileged information. arXiv preprint
arXiv:1701.06772, 2017.

Chorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy,
Cho, Kyunghyun, and Bengio, Yoshua. Attention-based
models for speech recognition. In Advances in Neural
Information Processing Systems, pp. 577‚Äì585, 2015.
Clevert, Djork-ArneÃÅ, Unterthiner, Thomas, and Hochreiter, Sepp. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint
arXiv:1511.07289, 2015.
Cogswell, M., Ahmed, F., Girshick, R., Zitnick, L., and Batra, D. Reducing overfitting in deep networks by decorrelating representations. ICLR, 2015.
Cui, Yiming, Chen, Zhipeng, Wei, Si, Wang, Shijin, Liu,
Ting, and Hu, Guoping. Attention-over-attention neural networks for reading comprehension. arXiv preprint
arXiv:1607.04423, 2016.
Dahl, George, Mohamed, Abdel-rahman, Hinton, Geoffrey E, et al. Phone recognition with the meancovariance restricted boltzmann machine. In Advances
in neural information processing systems, pp. 469‚Äì477,
2010.
Dhingra, Bhuwan, Liu, Hanxiao, Cohen, William W, and
Salakhutdinov, Ruslan. Gated-attention readers for text
comprehension. arXiv preprint arXiv:1606.01549, 2016.
Dhingra, Bhuwan, Yang, Zhilin, Cohen, William W, and
Salakhutdinov, Ruslan. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint
arXiv:1703.02620, 2017.
Gales, Mark JF. Maximum likelihood linear transformations for hmm-based speech recognition. Computer
speech & language, 12(2):75‚Äì98, 1998.
Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi,
Courville, Aaron, and Bengio, Yoshua. Maxout networks. ICML, 2013.
Graham, Benjamin. Fractional max-pooling.
preprint arXiv:1412.6071, 2014.

arXiv

Graves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pp.
6645‚Äì6649. IEEE, 2013.
Griffin, Gregory, Holub, Alex, and Perona, Pietro. Caltech256 object category dataset. 2007.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770‚Äì778, 2016.

Learning Latent Space Models with Angular Constraints

Henaff, Mikael, Szlam, Arthur, and LeCun, Yann. Orthogonal rnns and long-memory tasks. arXiv preprint
arXiv:1602.06662, 2016.
Hermann, Karl Moritz, Kocisky, Tomas, Grefenstette, Edward, Espeholt, Lasse, Kay, Will, Suleyman, Mustafa,
and Blunsom, Phil. Teaching machines to read and comprehend. In Advances in Neural Information Processing
Systems, pp. 1693‚Äì1701, 2015.
Hill, Felix, Bordes, Antoine, Chopra, Sumit, and Weston,
Jason. The goldilocks principle: Reading children‚Äôs
books with explicit memory representations. ICLR,
2015.
Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four
research groups. Signal Processing Magazine, IEEE,
2012.
Hochreiter, Sepp and Schmidhuber, JuÃàrgen. Long shortterm memory. Neural computation, 9(8):1735‚Äì1780,
1997.
Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and
van der Maaten, Laurens. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Kadlec, Rudolf, Schmid, Martin, Bajgar, Ondrej, and
Kleindienst, Jan. Text understanding with the attention
sum reader network. ACL, 2016.
Kobayashi, Sosuke, Tian, Ran, Okazaki, Naoaki, and
Inui, Kentaro. Dynamic entity representation with maxpooling improves machine reading. In Proceedings of
NAACL-HLT, pp. 850‚Äì855, 2016.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing
systems, 2012.
Kulesza, Alex and Taskar, Ben. Determinantal point processes for machine learning. Foundations and Trends in
Machine Learning, 2012.
Lazebnik, Svetlana, Schmid, Cordelia, and Ponce, Jean.
Beyond bags of features: Spatial pyramid matching for
recognizing scene categories. In CVPR, 2006.
Le, Quoc V, Ngiam, Jiquan, Chen, Zhenghao, Chia, Daniel,
Koh, Pang Wei, and Ng, Andrew Y. Tiled convolutional neural networks. In Proceedings of the 23rd International Conference on Neural Information Processing
Systems, pp. 1279‚Äì1287. Curran Associates Inc., 2010.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
In Artificial Intelligence and Statistics, pp. 562‚Äì570,
2015.
Li, Li-Jia and Fei-Fei, Li. What, where and who? classifying events by scene recognition. In ICCV, 2007.
Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in
network. arXiv preprint arXiv:1312.4400, 2013.
Lowe, David G. Distinctive image features from scaleinvariant keypoints. IJCV, 2004.
Lu, Liang, Kong, Lingpeng, Dyer, Chris, Smith, Noah A,
and Renals, Steve. Segmental recurrent neural networks for end-to-end speech recognition. arXiv preprint
arXiv:1603.00223, 2016.
Lu, Liang, Kong, Lingpeng, Dyer, Chris, and Smith,
Noah A.
Multi-task learning with ctc and segmental crf for speech recognition. arXiv preprint
arXiv:1702.06378, 2017.
Malkin, Jonathan and Bilmes, Jeff. Ratio semi-definite
classifiers. In Acoustics, Speech and Signal Processing,
2008. ICASSP 2008. IEEE International Conference on,
pp. 4113‚Äì4116. IEEE, 2008.
Mao, Fengling, Xiong, Wei, Du, Bo, and Zhang, Lefei.
Stochastic decorrelation constraint regularized autoencoder for visual recognition. In International Conference on Multimedia Modeling, pp. 368‚Äì380. Springer,
2017.
Mishkin, Dmytro and Matas, Jiri. All you need is a good
init. arXiv preprint arXiv:1511.06422, 2015.
Moon, Taesup, Choi, Heeyoul, Lee, Hoshik, and Song,
Inchul. Rnndrop: A novel dropout for rnns in asr. In Automatic Speech Recognition and Understanding (ASRU),
2015 IEEE Workshop on, pp. 65‚Äì70. IEEE, 2015.
Olshausen, Bruno A and Field, David J. Sparse coding with
an overcomplete basis set: A strategy employed by v1?
Vision research, 37(23):3311‚Äì3325, 1997.
Povey, Daniel, Ghoshal, Arnab, Boulianne, Gilles, Burget, Lukas, Glembek, Ondrej, Goel, Nagendra, Hannemann, Mirko, Motlicek, Petr, Qian, Yanmin, Schwarz,
Petr, et al. The kaldi speech recognition toolkit. In IEEE
2011 workshop on automatic speech recognition and understanding, number EPFL-CONF-192584. IEEE Signal
Processing Society, 2011.
Prabhavalkar, Rohit, Sainath, Tara N, Nahamoo, David,
Ramabhadran, Bhuvana, and Kanevsky, Dimitri. An
evaluation of posterior modeling techniques for phonetic

Learning Latent Space Models with Angular Constraints

recognition. In Acoustics, Speech and Signal Processing
(ICASSP), 2013 IEEE International Conference on, pp.
7165‚Äì7169. IEEE, 2013.
Ramirez, Ignacio, Sprechmann, Pablo, and Sapiro,
Guillermo. Classification and clustering via dictionary
learning with structured incoherence and shared features. In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pp. 3501‚Äì3508.
IEEE, 2010.
Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review,
52(3):471‚Äì501, 2010.
Rodrƒ±ÃÅguez, Pau, GonzaÃÄlez, Jordi, Cucurull, Guillem, Gonfaus, Josep M, and Roca, Xavier. Regularizing cnns
with locally constrained decorrelations. arXiv preprint
arXiv:1611.01967, 2016.
Seo, Minjoon, Kembhavi, Aniruddha, Farhadi, Ali, and
Hajishirzi, Hannaneh. Bidirectional attention flow for
machine comprehension. ICLR, 2017.
Shen, Yelong, Huang, Po-Sen, Gao, Jianfeng, and Chen,
Weizhu. Reasonet: Learning to stop reading in machine
comprehension. arXiv preprint arXiv:1609.05284, 2016.
Sordoni, Alessandro, Bachman, Philip, Trischler, Adam,
and Bengio, Yoshua.
Iterative alternating neural attention for machine reading.
arXiv preprint
arXiv:1606.02245, 2016.
Springenberg, Jost Tobias, Dosovitskiy, Alexey, Brox,
Thomas, and Riedmiller, Martin. Striving for simplicity: The all convolutional net. arXiv preprint
arXiv:1412.6806, 2014.
Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):
1929‚Äì1958, 2014.
Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, JuÃàrgen. Highway networks. arXiv preprint
arXiv:1505.00387, 2015.
Tang, Hao, Wang, Weiran, Gimpel, Kevin, and Livescu,
Karen. Discriminative segmental cascades for featurerich phone recognition. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop
on, pp. 561‚Äì568. IEEE, 2015.
Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267‚Äì288, 1996.

ToÃÅth, LaÃÅszloÃÅ. Phone recognition with deep sparse rectifier
neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference
on, pp. 6985‚Äì6989. IEEE, 2013.
ToÃÅth, LaÃÅszloÃÅ. Combining time-and frequency-domain convolution in convolutional neural network-based phone
recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,
pp. 190‚Äì194. IEEE, 2014.
Trischler, Adam, Ye, Zheng, Yuan, Xingdi, and Suleman, Kaheer. Natural language comprehension with the
epireader. arXiv preprint arXiv:1606.02270, 2016.
Vainsencher, Daniel, Mannor, Shie, and Bruckstein, Alfred M. The sample complexity of dictionary learning.
Journal of Machine Learning Research, 12(Nov):3259‚Äì
3281, 2011.
Xie, Bo, Liang, Yingyu, and Song, Le. Diversity leads to
generalization in neural networks. AISTATS, 2017.
Xie, Pengtao. Learning compact and effective distance
metrics with diversity regularization. In ECML, 2015.
Xie, Pengtao, Deng, Yuntian, and Xing, Eric P. Diversifying restricted boltzmann machine for document modeling. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2015.
Xie, Pengtao, Zhu, Jun, and Xing, Eric. Diversitypromoting bayesian learning of latent variable models.
In Proceedings of The 33rd International Conference on
Machine Learning, pp. 59‚Äì68, 2016a.
Xie, Saining, Girshick, Ross, DollaÃÅr, Piotr, Tu, Zhuowen,
and He, Kaiming.
Aggregated residual transformations for deep neural networks. arXiv preprint
arXiv:1611.05431, 2016b.
Xiong, Wei, Du, Bo, Zhang, Lefei, Hu, Ruimin, and Tao,
Dacheng. Regularizing deep convolutional neural networks with a structured decorrelation constraint. In Data
Mining (ICDM), 2016 IEEE 16th International Conference on, pp. 519‚Äì528. IEEE, 2016.
Yamada, Yoshihiro, Iwamura, Masakazu, and Kise,
Koichi. Deep pyramidal residual networks with separated stochastic depth. arXiv preprint arXiv:1612.01230,
2016.
Yang, J, Ragni, Anton, Gales, Mark JF, and Knill, Kate M.
Log-linear system combination using structured support
vector machines. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, volume 8, pp. 1898‚Äì1902, 2016.

Learning Latent Space Models with Angular Constraints

Yang, Jianchao, Yu, Kai, Gong, Yihong, and Huang,
Thomas. Linear spatial pyramid matching using sparse
coding for image classification. In CVPR, 2009.
Yu, Yang, Li, Yu-Feng, and Zhou, Zhi-Hua. Diversity regularized machine. In IJCAI, 2011.
Yuan, Jinhui, Gao, Fei, Ho, Qirong, Dai, Wei, Wei, Jinliang, Zheng, Xun, Xing, Eric Po, Liu, Tie-Yan, and Ma,
Wei-Ying. Lightlda: Big topic models on modest computer clusters. In Proceedings of the 24th International
Conference on World Wide Web, pp. 1351‚Äì1361. ACM,
2015.
Zagoruyko, Sergey and Komodakis, Nikos. Wide residual
networks. arXiv preprint arXiv:1605.07146, 2016.
Zeiler, Matthew D. Adadelta: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701, 2012.
Zou, James Y. and Adams, Ryan P. Priors for diversity in
generative latent variable models. In NIPS, 2012a.
Zou, James Y and Adams, Ryan P. Priors for diversity in
generative latent variable models. In Advances in Neural
Information Processing Systems, pp. 2996‚Äì3004, 2012b.

