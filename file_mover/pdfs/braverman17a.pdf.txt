Clustering High Dimensional Dynamic Data Streams

Vladimir Braverman 1 Gereon Frahling 2 Harry Lang 1 Christian Sohler 3 Lin F. Yang 1

Abstract
We present data streaming algorithms for the kmedian problem in high-dimensional dynamic
geometric data streams, i.e. streams allowing
both insertions and deletions of points from a
discrete Euclidean space {1, 2, . . . ∆}d . Our algorithms use k−2 poly(d log ∆) space/time and
maintain with high probability a small weighted
set of points (a coreset) such that for every set
of k centers the cost of the coreset (1 + )approximates the cost of the streamed point set.
We also provide algorithms that guarantee only
positive weights in the coreset with additional
logarithmic factors in the space and time complexities. We can use this positively-weighted
coreset to compute a (1 + )-approximation for
the k-median problem by any efficient offline kmedian algorithm. All previous algorithms for
computing a (1 + )-approximation for the kmedian problem over dynamic data streams required space and time exponential in d. Our algorithms can be generalized to metric spaces of
bounded doubling dimension.

1

Introduction

The analysis of very large data sets is still a big challenge. Particularly, when we would like to obtain information from data sets that occur in the form of a data stream
like, for example, streams of updates to a data base system,
internet traffic and measurements of scientific experiments
in astro- or particle physics (e.g. (Liu et al., 2015)). In
such scenarios it is difficult and sometimes even impossible
to store the data. Therefore, we need algorithms that process the data sequentially and maintain a summary of the
data using space much smaller than the size of the stream.
Such algorithms are often called streaming algorithms (for
more introduction on streaming algorithms, please refer to
(Muthukrishnan, 2005)).
One fundamental technique in data analysis is clustering.
The idea is to group data into clusters such that data inside
1

Johns Hopkins University, USA 2 Linguee GmbH 3 TU Dortmund. Correspondence to: Lin F. Yang <lyang@jhu.edu>,
Christian Sohler <christian.sohler@tu-dortmund.de>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

the same cluster is similar and data in different clusters is
different. Center based clustering algorithms also provide
for each cluster a cluster center, which may act as a representative of the cluster. Often data is represented as vectors
in Rd and similarity between data points is often measured
by the Euclidean distance. Clustering has many applications ranging from data compression to unsupervised learning.
In this paper we are interested in clustering problems over
dynamic data streams, i.e. data streams that consist of updates, for example, to a database. Our stream consists of
insert and delete operations of points from {1, . . . , ∆}d .
We assume that the stream is consistent, i.e. there are no
deletions of points that are not in the point set and no insertions of points that are already in the point set. We consider
the k-median clustering problem, which for a given a set of
points P ⊆ Rd asks to compute a set C of k points that
minimizes the sum of distances of the input points to their
nearest points in C.
1.1 Our Results
We develop the first (1 + )-approximation algorithm for
the k-median clustering problem in dynamic data streams
that uses space polynomial in the dimension of the data. To
our best knowledge, all previous algorithms required space
exponentially in the dimension. Formally, our main theorem states,
Theorem 1.1 (Main Theorem). Fix  ∈ (0, 1/2), positive
integers k and ∆, Algorithm 1 makes a single pass over the
dynamic streaming point set P ⊂ [∆]d , outputs a weighted
set S, such that with probability at least 0.99,
 S is an coreset for k-median of size O kd4 L4 /2 , where L =

log ∆. The algorithm uses Õ kd7 L7 /2 bits in the worst
case, processes each update in time Õ(dL2 ) and outputs
the coreset in time poly(d, k, L, 1/) after one pass of the
stream.
The theorem is restated in Theorem 3.6 and the proof is
presented in Section 3.3. The coreset we constructed may
contain negatively weighted points. Thus naı̈ve offline algorithms do not apply directly to finding k-clustering solutions on the coreset. We also provide an alternative approach that output only non-negatively weighted coreset.
The new algorithm is slightly more complicated. The space
complexity and coreset size is slightly worse than the one
with negative weights but still polynomial in d, 1/ and
log ∆ and optimal in k up to polylogk factor.
Theorem 1.2 (Alternative Results). Fix  ∈ (0, 1/2), posi-

Clustering High Dimensional Dynamic Data Streams

tive integers k and ∆, Algorithm 6 makes a single pass over
the streaming point set P ⊂ [∆]d , outputs a weighted set
S with non-negative weights for each point, such that with
probability at least 0.99, S is an -coreset for k-median
 of
size Õ kd4 L4 /2 . The algorithm uses Õ kd8 L8 /2 bits
in the worst case. For each update of the input, the algorithm needs poly(d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/) after one pass of
the stream.
The theorem is restated in Theorem 4.3 in Section 4 and the
proof is presented therein. Both approaches can be easily
extended to maintain a coreset for a general metric space.
1.2 Our Techniques
From a high level, both algorithms can be viewed as
a combination of the ideas introduced by Frahling and
Sohler (Frahling & Sohler, 2005) with the coreset construction by Chen (Chen, 2009).
To explain our high-level idea, we first summarize the idea
of Chen (Chen, 2009). In their construction, they first obtain a (α, β)-bi-criterion solution. Namely find a set of
at most αk centers such that the k-median cost to these
αk centers is at most β OPT, where OPT is the optimal
cost for a k-median solution. Around each of the αk
points, they build logarithmically many concentric ring regions and sample points from these rings. Inside each ring,
the distance from a point to its center is upper and lower
bounded. Thus the contribution to the optimal cost from
the points of this ring is lower bounded by the number of
points times the inner diameter of the ring. To be more precise, their construction requires a partition of Õ(αk) sets
of the original data points satisfying P
the following property: for the partition P1 , P2 , . . . , Pk0 , i |Pi |diam(Pi ) .
β OPT. They then sample a set of points from each part
to estimate the cost of an arbitrary k-set from [∆]d up to
O(|Pi |diam(Pi )/β) additive error. Combining the samples of the k 0 parts, this gives an additive error of at most
OPT and therefore an -coreset.
The first difficulty in generalizing the construction of Chen
to dynamic streams is that it depends on first computing
approximate centers, which seems at first glance to require
two passes. Surprisingly (since we would like to be polynomial in d), we can resolve this difficulty using a gridbased construction. The grid structure can be viewed as a
(2d )-ary tree of cells. The root level of the tree is a single
cell containing the entire set of points. Going down a level
through the tree, each parent cell is split evenly into 2d subcells. Thus in total there are log2 ∆ grid levels. Each cell
of the finest level contains at most a single point.
Without using any information of a pre-computed (α, β)bi-criterion solution to the k-median problem, as it does
in (Chen, 2009), our first idea (similar to the idea used in
(Indyk, 2004)) is to use a randomly shifted grid (i.e. shift
each coordinate of the cell of the root level by a random
value r, where r is uniformly chosen from {1, 2, . . . ∆},
and redefine the tree by splitting cells into 2d subcells recursively). We show that with high probability, in each

level, at most Õ(k) cells are close to (or containing) a center of an optimal solution to the k-median. For the remaining cells, we show that each of them cannot contain
too many points, since otherwise they would contribute too
much to the cost of the optimal solution (since each point
in these cells is far away from each of the optimal centers). We call the cells containing too many points in a
level heavy cells. The immediate non-heavy children of
the heavy cells form a partition of the entire point sets
(i.e. the cells that are not heavy, but have heavy parents). Let C1 , C2 , . . .P
, Ck0 be these cells, and we can immediately show that i |Ci |diam(Ci ) ≤ β OPT for some
β = O(d3/2 ). If we can identify the heavy cells (e.g. use
heavy hitter algorithms), and sample points from their immediate non-heavy children in a dynamic stream, we will
obtain a construction similar to Chen (Chen, 2009).
Our second idea allows us to significantly reduce space requirements and also allows us to do the sampling more easily. For each point p, the cells containing it form a path on
the grid tree. We write each point as a telescope sum as
the cell centers on the path of the point ( recall that the
grids of each level are nested and the c0 is the root of the
tree). For example, let c0 , c1 , . . . , cL be the cell centers of
the path, where cL = p, and define 0 to be the zero vector.
Then p = cL − cL−1 + cL−1 − cL−2 . . . + c0 − 0 + 0.
In this way, we can represent the distance from a point to
a set of points as the distance of cell centers to that set of
points. For example, let Z ⊂ [∆]d be a set of points, and
d(p, Z) be the distance from p to the closest point in Z.
Then d(p, Z) = d(cL , Z) − d(cL−1 , Z) + d(cL−1 , Z) −
d(cL−2 , Z) + . . . + d(0, Z). Thus we can decompose the
costPa set Z into L + 2 levels: the cost in level l ∈ [0, L]
p
l−1
l
is
p d(cp , Z) − d(cp , Z), where cl is the center of
the cell containing p in level l and the cost in the (−1)-st
level is |P |d(0, Z), where P is the entire points set. Since
|d(clp , Z) − d(cl−1
p , Z)| is bounded by the cell diameter of
the level, we can sample points from the non-heavy cells
of the entire level, and guarantee that the cost of that level
is well-approximated. Notice that (a) we do not need to
sample Õ(k) points from every part of the partition, thus
we save a k factor on the space and (b) we do not need
to sample the actual points, but only an estimation of the
number of points in each cell, thus the sampling becomes
much easier (there is no need to store the sampled points).
In the above construction, we are able to obtain a coreset,
but the weights can be negative due the the telescope sum.
It is not easy find an offline k-median algorithm to output
the solutions from a negatively-weighted coreset. To remove the negative weights, we need to adjust the weights
of cells. But the cells with a small number of points (compared to the heavy cells) are problematic – the samplingbased empirical estimations of the number of points in them
has too much error to be adjusted.
In our second construction, we are able to remove all the
negative weights. The major difference is that we introduce
a cut-off on the telescope sum. For example, d(p, Z) =

Clustering High Dimensional Dynamic Data Streams
l(p)

l(p)

l(p)−1

d(cL
)+. . .+d(0, Z)
p , Z)−d(cp , Z)+d(cp )−d(cp
where l(p) is a cutoff level of point p such that the cell containing p in level l(p) is heavy but no longer heavy in level
l(p) + 1. We then sample point p with some probability
defined according to l(p). In other words, we only sample points from heavy cells and not from non-heavy ones.
Since a heavy cell contains enough points, the samplingbased estimation of the number of points is accurate enough
and thus allows us to adjust them to be all positive.
Finally, to handle the insertion and deletions, we use a F2 heavy hitter algorithm to identify the heavy cells. We use
pseudo-random hash functions (e.g. Nisan’s construction
(Nisan, 1992; Indyk, 2000b) or k-wise independent hash
functions) to do the sampling and use a K-Set data structure (Ganguly, 2005) to store the sampled points in the dynamic stream.
1.3 Related Work
There is a rich history in studies of geometric problems
in streaming model. Among these problems some excellent examples are: approximating the diameter of a
point set (Feigenbaum et al., 2005; Indyk, 2003), approximately maitain the convex hull (Cormode & Muthukrishnan, 2003; Hershberger & Suri, 2004), the min-volume
bounding box (Chan, 2004; Chan & Sadjad, 2006), maintain -nets and -approximations of a data stream (Bagchi
et al., 2007). Clustering problem is another interesting and
popular geometric problem studied in streaming model.
There has been a lot of works on clustering data streams
for the k-median and k-means problem based on coresets (Har-Peled & Mazumdar, 2004; Har-Peled & Kushal,
2005; Chen, 2009; Feldman et al., 2007; 2013; Feldman
& Langberg, 2011). Additionally (Charikar et al., 1997;
Guha et al., 2000; Meyerson, 2001) studied the problem in
the more general metric space. The currently best known
algorithm for k-median problem in this setting is an O(1)approximation using O(kpolylogn) space (Charikar et al.,
2003). However, all of the above methods do not work for
dynamic streams.
The most relevant works to ours are those by Indyk (Indyk,
2004), Indyk & Price (Indyk & Price, 2011) and Frahling
& Sohler (Frahling & Sohler, 2005). Indyk (Indyk, 2004)
introduced the model for dynamic geometric data streamings. He studied algorithms for (the weight of) minimum
weighted matching, minimum bichromatic matching and
minimum spanning tree and k-median clustering. He gave
a exhaustive search (1 + ) approximation algorithm for kmedian and a (α, β)-bi-criterion approximation algorithm.
Indyk & Price (Indyk & Price, 2011) studied the problem of sparse recovery under Earth Mover Distance. They
show a novel connection between EMD/EMD sparse recovery problem to k-median clustering problem on a two
dimensional grid. The most related work to current one
is Frahling & Sohler (Frahling & Sohler, 2005), who develop a streaming (1 + )-approximation algorithms for kmedian as well as other problems over dynamic geometric
data streams. All previous constructions for higher dimen-

sional grid require space exponential in the dimension d.

2

Preliminaries

For integer a ≤ b, we denote [a] := {1, 2, . . . , a} and
[a, b] := {a, a + 1, . . . , b} for integer intervals.
We
will consider a point set P from the Euclidean space
{1, . . . , ∆}d . Without loss of generality, we always assume
∆ is of the form 2L for some integer L, since otherwise we
can always pad ∆ without loss of a factor more than 2. Our
streaming algorithm will process insertions and deletions
of points from this space. We study the
P k-median problem,
which is to minimize cost(P, Z) = p∈P d(p, Z) among
all sets Z of k centers from Rd and where d(p, q) denotes
the Euclidean distance between p and q and d(p, Z) for
a set of points Z denotes the distance of p to the closest
point in Z. The following definition is from (Har-Peled &
Mazumdar, 2004).
Definition 2.1. Let P ⊆ [∆]d be a point set. A small
weighted set S is called an -coreset for the k-median problem, if for every set of k centers Z ⊂ [∆]d we have 1
(1 − ) · cost(P, Z) ≤ cost(S, Z) ≤ (1 + ) · cost(P, Z),
P
where cost(S, Z) := s∈S wt(s)d(s, Z) and wt(s) is the
weight of point s ∈ S.
Through out the paper, we assume parameters , ρ, δ, ∈
(0, 21 ) unless otherwise specified. For our algorithms and
constructions we define a nested grid with L levels, in the
following manner.
Definition of grids Let v = (v1 , . . . , vd ) be a vector chosen uniformly at random from [0, ∆ − 1]d . Partition the
space {1, . . . , ∆}d into a regular Cartesian grid G0 with
side-length ∆ and translated so that a vertex of this grid
falls on v. Each cell of this grid can be expressed as [v1 +
n1 ∆, v1 + (n1 + 1)∆) × . . . × [vd + nd ∆, vd + (nd + 1)∆)
for some (n1 , . . . , nd ) ∈ Zd . For i ≥ 1, define the regular
grid Gi as the grid with side-length ∆/2i aligned such that
each cell of Gi−1 contains 2d cells of Gi . The finest grid
is GL where L = dlog2 ∆e; the cells of this grid therefore
have side-length at most 1 and thus contain at most a single
input point. Each grid forms a partition of the point-set S.
There is a d-ary tree such that each vertex at depth i corresponds to a cell in Gi , and this vertex has 2d children which
are the cells of Gi+1 that it contains. For convenience, we
define G−1 as the entire dataset and it contains a single cell
C−1 . For each cell C, we also treat it as a subset of the input
points (i.e. C ∩ P ) if there is no confusion.
We denote Z ∗ ⊂ [∆]d as the optimal solution for k-median
and OPT as the optimal cost for Z ∗ . The proof of the following lemma is delayed to Section A.
Lemma 2.2. Fix a set Z ⊂ [∆]d , then with probability at
least 1 − ρ, for every level i ∈ [0, L], the number of cells
that satisfy d(C, Z) ≤ ∆/(2i+1 d) is at most e|Z|(L+1)/ρ.
1
For simplicity of the presentation, we define the coreset for
all sets of k centers Z ⊂ [∆]d , but it can be generalized to all sets
of k centers Z ⊂ Rd with an additional polylog(1/) factor in
the space. We discuss this point further in Section 6.

Clustering High Dimensional Dynamic Data Streams

2.1 Outline
In Section 3, we introduce the coreset with negative
weights. In Section 4, we introduce a modified construction with all positive weights. Section 6 comes with the
final remarks.

3

Thus we can rewrite the cost term as follows,
X
cost(Gi , Z) :=
d(cip , Z) − d(cpi−1 , Z)
p∈P

=

In this section, we present our generally weighted coreset
construction. In Section 3.1, we introduce the telescope
sum representation of a point p and the coreset framework.
In Section 3.2, we illustrate our coreset framework with an
offline construction. In Section 3.3 we present an one pass
streaming algorithm that implements our coreset framework.
3.1 The Telescope Sum and Coreset Framework
Our first technical idea is to write each point as a telescope
sum. We may interpret this sum as replacing a single point
by a set of points in the following way. Each term (p −
q) of the sum can be viewed as a pair of points p and q,
where p has weight 1 and q has weight −1. The purpose
of this construction is that the contribution of each term
(p − q) (or the corresponding two points) is bounded. This
can be later exploited when we introduce and analyze our
sampling procedure.
We now start to define the telescope sum, which will relate
to our nested grids. For each C ∈ Gi , denote c(C) (or simply c) as its center. For each point p ∈ P , define C(p, i) as
the cell that contains p in Gi , and cip is the center of C(p, i).
Then we can write
L
X
cip − ci−1
p = c−1
p .
p +
i=0

where we set c−1
p = 0 (we also call this the cell center of
the (−1)-st level for convenience). The purpose of this can
be seen when we consider the distance of p to an arbitrary
k-centers Z ⊂ [∆]d , we can write the cost of a single point
p, as
L
X

d(cip , Z) − d(ci−1
p , Z).

i=0

Note that cL
p = p since the cells of GL contain a single
point. Thus the cost of the entire set cost(P, Z) can be
written as,
L X
X
i=0 p∈P

d(cip , Z) − d(ci−1
p , Z) +

X

d(c(C), Z) − d(c(C P ), Z)

C∈Gi p∈C

Generally Weighted Coreset

d(p, Z) = d(c−1
p , Z) +

XX

d(c−1
p , Z).

(1)

p∈P

As one can see, we transform the cost defined using the
original set of points to the “cost” defined using cell centers. To estimate
the cost, it remains to estimate each of
P
the terms, p∈P d(cip , Z) − d(ci−1
p , Z) for i ∈ [0, L] and
P
−1
p∈P d(cp , Z). In other words, assign weights to each
of the centers of the grid cells. For i ∈ [0, L], and a cell
C ∈ Gi , denote C P as the parent cell of C in grid Gi−1 .

=

X



|C| d(c(C), Z) − d(c(C P ), Z)

C∈Gi

=

X

|C|d(c(C), Z)

C∈Gi

−

X

X

|C|d(c(C 0 ), Z).

(2)

C 0 ∈Gi−1 C∈Gi :C⊂C 0

For i = −1, we denote cost(G−1 , Z) = |P |d(c−1
p , Z).
Then this leads to our following coreset construction
framework.
Generally Weighted Construction The coreset S in the
construction is composed by a weighted subset of centers
of grid cells. The procedure of the construction is to assign some (integer) value to each cell center. For instance,
maintain a integer valued function |c
· | on cells (using small
c
amount of space). |C| is called the value of the cell C. Let
c be the center of C, then the weight for c is
X
c−
c0 |.
wt(c) = |C|
|C
(3)
C 0 :C 0 ∈Gi+1 ,C 0 ⊂C

And for the L-th grid GL , the weight for each cell C is just
c Note that there might be negative weights for some
|C|.
cells.
c := |C| as the exact numAs a naı̈ve example, we set |C|
ber of points of a cell C. Then we would expect the cells
in every level except those in GL have weight 0. In other
words, we stored the entire point set as the coreset. As we
c as an approximation of |C| up
will show, if we allow |C|
to additive error, we can compress the number of non-zero
weighted centers to be a smaller number.
Definition 3.1. Given a grid structure, and a real valued
d :
function |c
· | on the set of cells. We define a function cost
d
[∆] × G → R as follows, for i ∈ [0, L] and Z ⊂ [∆]d ,
X
c
d i , Z) :=
cost(G
|C|d(c(C),
Z)
C∈Gi

−

X

X

c · d(c(C 0 ), Z),
|C|

(4)

C 0 ∈Gi−1 C∈Gi :C⊂C 0

[
d −1 , Z) = |C
and cost(G
−1 |d(0, Z), where C−1 is the cell in
G−1 containing the entire set of points.
Lemma 3.2. Fix an integer valued function |c
· | on the set
of cells and parameter 0 <  < 21 . Let S be the set of
all cell centers with weights assigned by Equation (3). If
[
|C
−1 | = |P | (recall that C−1 is the first cell containing the
entire dataset) and for any Z ⊂ [∆]d with |Z| ≤ k and

Clustering High Dimensional Dynamic Data Streams

i ∈ [0, L] and every k-set Z ⊂ [∆]d ,

i ∈ [0, L]



d

cost(Gi , Z) − cost(Gi , Z) ≤ OPT/(L + 1).




d i , Z) ≤ OPT ,
cost(Gi , Z) − cost(G
L+1

then S is an -coreset for k-median.
Proof. Given an arbitrary set of centers Z ⊂ [∆]d ,
X

cost(S, Z) =

wt(s)d(s, Z)

s∈S

=

X X

c−
d(c(C), Z) |C|

c0 |
|C

C 0 :C 0 ∈Gi+1
C 0 ⊂C

i∈[0,L] C∈Gi

+ |P |d(0, Z)
X X
c
=
|C|d(c(C),
Z) −
i∈[0,L]

X

X


0
c
|C|d(c(C
), Z)

C 0 ∈Gi−1
C∈Gi :C⊂C 0

C∈Gi

X

+ |P |d(0, Z) =

d i , Z).
cost(G

i∈[0,L]

It follows that |cost(S, Z) − cost(P, Z)| ≤ OPT.
3.2 An Offline Construction
In this section, we assume we have (10, 10)-bi-criterion ap0
proximation to k-median. Let Z 0 = {z10 , z20 , . . . , z10k
} be
the centers and o is the cost satisfying OPT ≤ o ≤ 10OPT.
This can be done using (Indyk, 2000a). We will show how
we construct the coreset base on the framework described
in the last section.
An Offline Construction For each point in level G−1 , we
sample it with probability π−1 = 1 (i.e. count the number
[
of points exactly) and set |C
−1 | := |P |. For each level i ∈
[0, L], we pick the set of all cells C satisfying d(C, Z 0 ) ≤
W/(2d), where W is the side length of C. Denote the set of
these cells as CZ 0 . We count the number of points in each
c := |C|. For the points in
of these cells exactly, and set |C|
the rest of cells, for each i ∈ [0, L], we sample the points
with probability

πi = min

2(L + 1)∆kd
200(L + 1)2 ∆d2
ln
,1
i
2
2 o
ρ


(5)

uniformly and independently. Denote Si as the set of sampled points at level i. For each C 6∈ CZ 0 , set
c := |Si ∩ C|/πi .
|C|
Then, from the bottom level to the top level, we assign the
weight to the cell centers of each of the cells and their
c using (3). Denote S as the
parent cells with non-zero |C|
coreset, which contains the set of cell centers of non-zero
weight.
Theorem 3.3. Fix , ρ ∈ (0, 1/2), then with probability at
least 1 − 8ρ, the offline construction S is an -coreset for
k-median and that

|S| = O

d4 kL4
1
kL2
log +
2
ρ
ρ


.

[
Proof of Theorem 3.3. By definition |C
−1 | = |P |, it is suffice to show that with probability at least 1 − 4ρ, for every

It follows from Lemma 3.2 that, S is an -coreset.
Let Si be the sampled points of level i. Fix a k-set Z ⊂
[∆]d , for each i ∈ [0, L], by equation (2), we have that,
P
c d(c(C), Z) − d(c(C P ), Z) +
d i , Z) = C∈C |C|
cost(G
0
Z
P
i
i−1
Note that
p∈Si (d(cp , Z) − d(cp , Z)))/πi .
d
E(cost(Gi , Z)) = cost(Gi , Z). The first term cond i , Z) − cost(Gi , Z)
tributes 0 to the difference cost(G
c
since each |C| is exact.
It remains to bound the
error contribution
from
the
second part.
Denote
P
i
i−1
A2 =
(d(c
,
Z)
−
d(c
,
Z)))/π
.
Recall
i
p
p
p∈Si
that Z 0 is the centers of the bi-criterion solution and
CZ 0 is the set of cells with distance less than W/(2d)
to Z 0 , where W is the side-length of a cell. Let A be
event that |CZ 0 | ≤ e|Z 0 |(L + 1)2 /ρ = O(kL2 /ρ). By
Lemma 2.2, A happens with probability at least 1 − ρ.
Conditioning on A happening, for each point p ∈ C ∈
/ CZ 0 ,
0
3/2
we
have
that
d(p,
Z
)
≥
diam(C)/(2d
).
Therefore,
P
P
0
≤ (2d3/2 ) p∈C ∈C
≤
p∈C ∈C
/ Z diam(C)
/ Z d(p, Z )
3/2
20d OPT. By Lemma 3.4, with probability at least
ρ
OPT
1 − (L+1)∆
kd , |A2 − E(A2 )| ≤
L+1 . Since there are at
kd
most ∆ many different k-sets from [∆]d , thus, for a fixed
ρ
i ∈ [0, L] with probability at least 1 − L+1
, for all k-sets



d d
Z ⊂ [∆] , cost(Gi , Z) − cost(Gi , Z) ≤ OPT/(L + 1).
By the union bound, with probability at least 1 − 4ρ, S is
the desired coreset.
It remains to bound the size of S. Conditioning on A
happening, then |CZ 0 | = O(kL2 /ρ). For each level i,
since each point from cells C 6∈ CZ 0 contributes at least
∆/(2i+1 d) to the bi-criterion solution, there are at most
O(2i OPTd/∆) points in cells not in CZ 0 . By a Chernoff
bound, with probability at least 1 − ρ/(L + 1), the number of points sampled from cells C 6∈ CZ 0 of level i is
upper bounded by O(d4 kL3 log ρ1 /2 ). Thus for all levels, with probability at least 1 − ρ, the number of points
sampled is upper bounded by O(d4 kL4 log ρ1 /2 ), which
is also an upper bound of the number of cells occupied
by sampled points. Now we bound the number of nonzero weighted centers. In the coreset construction, if a
cell center has non-zero weight, then either itself or one
c Thus
of its children cells has non-zero assigned value |C|.
the number of non-zero weigted centers is upper bound
by 2 times the number of non-zero valued cells. Thus
2
|S| = O(d4 kL4 log ρ1 /2 + kL
ρ ).
Lemma 3.4. FixP, ρ ∈ (0, 1/2), if a set of cells C from
grid Gi satisfies C∈C |C|diam(C) ≤ β OPT for some β ≥
2/(3(L + 1)), let S be a set of independent samples from
the point set ∪{C ∈ C} with probability


√
2
∆ dβ
2∆kd (L+1)
πi ≥ min 3a(L+1)
ln
,
1
i
2
2  o
ρ

Clustering High Dimensional Dynamic Data Streams

where 0 < o ≤ aOPT for some a > 0, then for a fixed set
Z ⊂ [∆]d , with probability at least 1 − ρ/((L + 1)∆kd ),
X

(d(cip , Z) − d(ci−1
p , Z))/πi −
p∈S

X
p∈∪{C∈C}


OPT

(d(cip , Z) − d(ci−1
.
p , Z)) ≤
L+1

The proof is a straightforward application of Bernstein inequality. It is presented in Section A.
3.3 The Streaming Algorithm
For the streaming algorithm, the first challenge is that we
do not know the actual value of OPT, neither do we have an
(α, β)-bi-criterion solution. To handle this, we will show
that we do not need an actual set of centers of an approximate solution, and that a conceptual optimal solution suffices. We will guess logarithmically many values for OPT
to do the sampling. We re-run the algorithm in parallel for
each guess of OPT.
The
P second challenge is that we cannot guarantee the sum
C∈Gi diam(C) to be upper bounded by β OPT as required
in Lemma 3.4. We will show that we can split the set of
cellsP
into two parts. The first part satisfies the property
that C |C|diam(C) ≤ β OPT for some parameter β. The
second part satisfies that |C|diam(C) ≥ aOPT/k for some
constant a.
For the first part, we use a similar sampling procedure
as we did in the offline case. The challenge here is that
there might be too many points sampled when the algorithm is midway through the stream, and these points may
be deleted later in the stream. To handle this case, we
use a data structure called K-Set structure with parameter
k (Ganguly, 2005). We will insert (with deletions) a multiset of points M ⊂ [N ] into the K-Set. The data structure processes each stream operation in O(log(k/δ)) time.
At each point of time, it supports an operation RETRSET,
that with probability at least 1 − δ either returns the set
of items of M or returns Fail. Further, if the number
of distinct items |M | is at most k, then RETRSET returns
M with probability at least 1 − δ. The space used by the
K-Set data structure is O(k(log |M | + log N ) log(k/δ).
The K-Set construction also returns the frequency of each
stored points upon the RETRSET operation.
For the second part, we call these cells heavy. We first upper bound the number of heavy cells by αk for some α > 1.
We use a heavy hitter algorithm HEAVY-HITTER to retrieve an approximation to the number of points in these
cells. The guarantee is given in the following theorem.
In an insertion-deletion stream, it may that although the
stream has arbitrary large length, at any moment a much
smaller number of elements are active (that is, inserted and
not yet deleted). We define the size of a stream to be the
maximum number of active elements at any point of the
stream.
Theorem 3.5 ((Larsen et al., 2016) Theorem 2). Fix , δ ∈
(0, 1/2). Given a stream (of insertions and deletions) of

size m consisting of items from universe [n], there exists an
algorithm HEAVY-HITTER(n, k, , δ) that makes a single
pass over the stream and outputs a set of pairs H. With
probability at least 1 − δ, the following holds,
Pn
2
(1) for each (i, fˆi ) ∈ H, fi2 ≥
j=1 fj /k −
P
n
2
2

j=k+1 fj ;
Pn
Pn
(2) if for any i ∈ [n] and fi2 ≥ j=1 fj2 /k+2 j=k+1 fj2 ,
then (i, fˆi ) ∈ H;
qP
n
2
(3) for each (i, fˆi ) ∈ H, |fˆi − fi | ≤ 
j=k+1 fj .

The algorithm uses O (k + 12 ) log nδ log m bits of space,
O(log n) update time and O(k + 1/2 )polylog(n) query
time.
Thus, using HEAVY-HITTER, we are guaranteed that the
error of the number of points in heavy cells is upper
bounded by  times the number of points in the non-heavy
cells. The first heavy hitter algorithm that achieves an l2
guarantee is by (Charikar et al., 2002), who has the same
space and update time as that of the above algorithm. However the update time is slow, i.e. O(n log n) time to output
the set of heavy hitters.
Lastly, we will use fully independent random hash function to sample the points. We will use Nissan’s pseudorandom generator to de-randomize the hash functions by
the method of (Indyk, 2000b). Our main theorem for this
section is as follows. The formal proof of this theorem is
postponed to Section B.
Theorem 3.6 (Main Theorem). Fix , ρ ∈ (0, 1/2), positive integers k and ∆, Algorithm 1 makes a single pass over
the streaming point set P ⊂ [∆]d , outputs a weighted set
S, such that with probability
 S is an -coreset
 4 4at least21−ρ,
L k
d L k
+ ρ , where L = log ∆.
for k-median of size O
2
The algorithm uses
  7 7


d L
d3 L5
dkL d5 L6
O k
+
+
log
2
ρ
ρ
2 ρ
bits in  the worst case, processes each update in
and outputs the coreset in time
time O dL2 log dkL
ρ
poly(d, k, L, 1/) after one pass of the stream.

4

Positively Weighted Coreset

In this section, we will introduce a modification to our previous coreset construction, which leads to a coreset with all
positively weighted points. The full algorithm and proofs
are postponed to Section C. We present the main steps in
this section.
The high level idea is as follows. When considering the estimate of the number of points in a cell, the estimate is only
accurate when it truly contains a large number of points.
However, in the construction of the previous section, we
sample from each cell of each level, even though some of
the cells contain a single point. For those cells, we cannot
adjust their weights from negative to positive, since doing
so would introduce large error. In this section, we introduce
an ending level to each point. In other words, the number of

Clustering High Dimensional Dynamic Data Streams

Algorithm 1 CoreSet(S, k, ρ, ): construct a -coreset
for dynamic stream S.
Initization:
Initialize a grid structure;
√
O ← {1, 2, 4, . . . , d∆d+1 };
L ← dlog ∆e;

2
∆d2
2∆kd (L+1)
πi (o) ← min 3(L+1)
ln
,
1
;
i
2
2  o
ρ
4

3

(2+e)(L+1)k
k
+ 24d (L+1)
ln ρ1 ,
2
 q ρ
ρ
0 ←  8(2+e)2 kd
; m ← 0;
3 (L+1)3

K←

For each o ∈ O and i ∈ [0, L], construct fully independent hash function ho,i : [∆]d → {0, 1} with
P rho,i (ho,i [q] = 1) = πi (o);
Initialize K-Set instances KSo,i with error probability
ρ/(L + 1), size parameter K;
Initialize HEAVY-HITTER(∆d , (e + 2)(L + 1)k/ρ,
0 , ρ/(L + 1)) instances, HH0 , HH1 , . . . , HHL , one for
a level;
Update (S):
for each update (op, q) ∈ S:
/*op ∈ {Insert, Delete}*/
m ← m ± 1; /*Insert: +1, Delete:−1*/
for each i ∈ [0, L]:
ciq ← the center of the cell contains q at level i;
HHi .update(op, ciq );
for each o ∈ [O]:
if ho,i (q) == 1:
KSo,i .update(op, ciq );
Query:
Let o∗ be the smallest o such that no instance of
KSo,0 , KSo,1 , . . . , KSo,L returns Fail;
R ← {};
for i = −1 to L:
for each cell center c in level i:
Let C be the cell containing c;
if i = -1:
f ← m;
else:
f ← GetFreq(c, HHi , KSo∗ ,i , πi (o∗ ));
if i < L:P
g ← C 0 ⊂C:C 0 ∈G i+1
GetFreq (c(C 0 ), HHi+1 , KSo∗ i+1 , πi (o∗ ));
Assign weight f − g to c;
if f − g 6= 0:
R ← R ∪ {c};

else:
Assign weight f to c;
if f 6= 0:
R ← R ∪ {c};
return R.

points of a cell is estimated by sampling only if it contains
many points. Thus, the estimates will be accurate enough

Algorithm 2 GetFreq(e, HH, KS, πi ): retrieve the correct freuquency of cell center e, given the instance of
HEAVY-HITTER and K-set.
fS (e) ← the frequency of e returned by HH;
fK (e) ← the frequency of e returned by KS;
k 0 ← (e + 2)(L + 1)k/ρ;
F ← the set of top-k 0 heavy hitters returned by
HEAVY-HITTER;
if e ∈ F :
return fS (e);
else:
return fK (e)/πi .

and allow us to rectify the weights to be all positive.
4.1 Reformulation of the Telescope Sum
Definition 4.1. A heavy cell identification scheme H
is a map H : G → {heavy, non-heavy} such that,
h(C−1 ) =heavy and for cell C ∈ Gi for i ∈ [0, L]
i
ρdOPT
then H(C) = heavy;
1. if |C| ≥ 2k(L+1)∆
2. If H(C) = non-heavy, then H(C 0 ) = non-heavy for every subsell C 0 of C.
3. For every cell C in level L, H(C) = non-heavy.
4. For each i ∈ [0, L], |{C ∈ Gi : H(C) = heavy}| ≤
λ1 kL
ρ , where λ1 ≤ 10 is a positive universal constant.
The output for a cell not specified by the above conditions
can be arbitrary. We call a cell heavy if it is identified heavy
by H. Note that a heavy cell does not necessarily contain a
large number of points, but the total number of these cells
is always bounded.
In the sequel, heavy cells are defined by an arbitrary fixed
identification scheme unless otherwise specified.
Definition 4.2. Fix a heavy cell identification scheme H.
For level i ∈ [−1, L], let C(p, i) ∈ Gi be the cell in Gi
containing p. The ending level l(p) of a point p ∈ P is the
largest level i such that H(C(p, i)) =heavy, and H(C(p, i+
1)) =non-heavy.
Note that the ending level is uniquely defined if a heavy
cell identification scheme is fixed. We now rewrite the telescope sum for p as follows,
p=

l(p) 
X


l(p)
cip − ci−1
+ cL
p
p − cp ,

i=0

c−1
p

where
= 0 and cL
p = p. For arbitrary k-centers Z ⊂

P
i
i−1
d
[∆] , we write, d(p, Z) = l(p)
i=0 d(cp , Z) − d(cp , Z) +
l(p)

d(cL
p , Z) − d(cp , Z) + d(0, Z) .

4.2 The New Construction (with arbitrary weights)
For these heavy cells, we use HEAVY-HITTER algorithms
to obtain accurate estimates of the number of points in these
cells, thus providing a heavy cell identification scheme. For
the non-heavy cells, we only need to sample points from the
bottom level, GL , but with a different probability for points
with different ending levels.
We now describe the new construction. This essentially

Clustering High Dimensional Dynamic Data Streams

has the same gaurantee as the simpler construction from
the previous section, however the benefit here is that (as
shown in the next subsection) it can be modified to output
only positive weights. In the following paragraph, the estic are given as a blackbox. In proposition C.9 we
mations |C|
specify the conditions these estimations must satisfy.
Non-Negatively Weighted Construction Fix an arbitrary
heavy cell identification scheme H. Let Pl be all the points
c
with ending level l(p) = l. For each heavy cell C, let |C|
c
be an estimation of number of points of |C|, we also call |C|
0
c0 | = 0.
the value of cell C. For each non-heavy cell C , let |C
Let S be a set samples of P constructed as follows: S =
S−1 ∪ S0 ∪ S1 , ∪ . . . ∪ SL , where Sl is a set of i.i.d samples
from Pl with probability πl . Here πl for l ∈ [−1, L] is
redefined as πl =
min



λ3 d2 ∆L2
2l 2 o

log



2L∆dk
ρ



+

λ4 d2 kL3 ∆
2i 2 ρo

log



30kL2
,1
ρ2

where λ3 > 0 and λ4 > 0 are universal constants. Our
coreset S is composed by all the sampled points in S and
the cell centers of heavy cells, with each point p assigned
a weight 1/πl(p) and for each cell center c of a heavy cell
C ∈ Gi , the weight is,
c−
wt(c) = |C|

X
C 0 :C 0 ∈Gi+1 ,C 0 ⊂C,
C 0 is heavy

c0 | − |Si ∩ C| .
|C
πi

at least 0.99, S is an -coreset for k-median of size
O

h

d 3 L4 k
2



d+

1
ρ

log

kL
ρ

i

where L = log ∆. The algorithm uses
O

h

d 7 L7 k
2



ρdL +

L
log2 dkL
ρ
ρ



log2

dkL
ρ

i

bits in the worst case. For each update of the input, the
algorithm needs poly (d, 1/, L, log k) time to process and
outputs the coreset in time poly(d, k, L, 1/, 1/ρ, log k) after one pass of the stream.

5

Experiments

We illustrate our construction using an offline construction
on Gaussian mixture data in R2 . As shown in Figure 2 in
Section D, we randomly generated 65536 points from R2 ,
then rounded the points to a grid of size ∆ = 512. Our
coreset uses log2 ∆+2 = 11 levels of grids. The storage in
each level is very sparse. As shown in Figure 1(a), only 90
points are stored in total. We compared the 1-median costs
estimated using the coreset and the dataset, the resulting
difference is very small, as illustrated in Figure 1(b).

(6)

For each non-heavy cell C except for those in the bottom
level, wt(c(C)) = 0. The weight of each point from S is
the value of the corresponding cell in the bottom level.
4.3 Ensuring Non-Negative Weights
We now provide a procedure to rectify all the weights for
the coreset constructed in the last sub-section. The idea is
similar to the method used in (Indyk & Price, 2011). The
procedure is shown in Algorithm 4.3. After this procedure,
there will be no negative weights in the coreset outputs.


c1 |, |C
d
d
Algorithm 3 RectifyWeights |C
2 | . . . , |Ck0 |, S :
input the estimates of number of points in each cell and the
weighted sampled points, output a weighted coreset with
non-negative weights.
for i = −1 to L:
for each heavy cell C center in Gi :
if wt(C) < 0:
Decrease the value of the children heavy cells
in level Gi+1 and sampled points Si arbitrarily by total |wt(C)| amount, such that for each
c0 | is non-negative,
children cell C 0 ∈ Gi+1 , |C
and for each sampled point p ∈ Si , the weight
is non-negative.
return Rectified Coreset
Theorem 4.3. Fix , ρ ∈ (0, 1/2), positive integers k and
∆, Algorithm 6 makes a single pass over the streaming
point set P ⊂ [∆]d , outputs a weighted set S with nonnegative weights for each point, such that with probability

(a)
(b)
Figure 1. (a) The layer structure of the coreset. Cells with more
weight are shaded darker. (b) The relative error of a 1-median
cost function. Using only 90 points, the global maximum error
was under 10%.

6

Concluding Remark

We develop algorithms that make a single pass over the dynamic stream and output, with high probability, a coreset
for the original k-median problem. Both the space complexity and the size of the coreset are polynomially dependent on d, whereas the only previous known bounds are
exponential in d. We constructed our coreset for the possible solutions in discrete space [∆]d , but it is easy to modify
the coreset to be a coreset in continuous space [0, ∆]d (note
that we still require the input dataset to be from a discrete
space). The way to do this is by modifying the sampling
probability πi in the algorithm, i.e. replacing the factor of
ln(Ω(∆kd L/ρ)) to ln(Ω((∆/)kd L/ρ)). Then any k-set
from [0, ∆]d can be rounded to the closest k-set in [∆/]d
and the cost only differs by a (1 ± ) factor while the space
bound changes only by a polylog(1/) factor. Lastly, we
remark that the coreset scheme can be easily modified to
other metric spaces, e.g. the lp metric. The space bound
depends on the doubling dimension of the metric.
As shown in our experiments, a 2D implementation using
our framework is very efficient. We believe that a highdimensional implementation will be efficient as well. We
leave the full implementation as a future project.

Clustering High Dimensional Dynamic Data Streams

Acknowledgment
V. Braverman is supported by the NSF Grants IIS-1447639,
EAGER CCF- 1650041, and CAREER CCF-1652257. H.
Lang is supported by the Franco-American Fulbright Commission. H. Lang thanks INRIA (l’Institut national de
recherche en informatique et en automatique) for hosting
him during the writing of this paper. C. Sohler is Supported by DFG within the Collaborative Research Center
SFB 876 “Providing Information by Resource-Constrained
Analysis”, project A2. L. Yang is supported by the NSF
Grant IIS-1447639.

References
Bagchi, Amitabha, Chaudhary, Amitabh, Eppstein, David,
and Goodrich, Michael T. Deterministic sampling and
range counting in geometric data streams. ACM Transactions on Algorithms (TALG), 3(2):16, 2007.
Chan, Timothy M. Faster core-set constructions and data
stream algorithms in fixed dimensions. In Proceedings
of the twentieth annual symposium on Computational geometry, pp. 152–159. ACM, 2004.
Chan, Timothy M and Sadjad, Bashir S. Geometric optimization problems over sliding windows. International
Journal of Computational Geometry & Applications, 16
(02n03):145–157, 2006.
Charikar, Moses, Chekuri, Chandra, Feder, Tomás, and
Motwani, Rajeev. Incremental clustering and dynamic
information retrieval. In Proceedings of the twenty-ninth
annual ACM symposium on Theory of computing, pp.
626–635. ACM, 1997.
Charikar, Moses, Chen, Kevin, and Farach-Colton, Martin. Finding frequent items in data streams. In International Colloquium on Automata, Languages, and Programming, pp. 693–703. Springer, 2002.
Charikar, Moses, O’Callaghan, Liadan, and Panigrahy,
Rina. Better streaming algorithms for clustering problems. In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pp. 30–39. ACM, 2003.
Chen, Ke. On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications. SIAM J. Comput., 39(3):923–947, 2009. doi:
10.1137/070699007. URL http://dx.doi.org/
10.1137/070699007.
Cormode, Graham and Muthukrishnan, S. Radial histograms for spatial streams. DIM ACS Technical Report,
11, 2003.
Feigenbaum, Joan, Kannan, Sampath, and Zhang, Jian.
Computing diameter in the streaming and slidingwindow models. Algorithmica, 41(1):25–41, 2005.
Feldman, Dan and Langberg, Michael. A unified framework for approximating and clustering data. In Proceedings of the 43rd ACM Symposium on Theory of

Computing, STOC 2011, San Jose, CA, USA, 6-8 June
2011, pp. 569–578, 2011. doi: 10.1145/1993636.
1993712. URL http://doi.acm.org/10.1145/
1993636.1993712.
Feldman, Dan, Monemizadeh, Morteza, and Sohler, Christian. A PTAS for k-means clustering based on weak
coresets. In Proceedings of the 23rd ACM Symposium
on Computational Geometry, Gyeongju, South Korea,
June 6-8, 2007, pp. 11–18, 2007. doi: 10.1145/1247069.
1247072. URL http://doi.acm.org/10.1145/
1247069.1247072.
Feldman, Dan, Schmidt, Melanie, and Sohler, Christian. Turning big data into tiny data: Constant-size
coresets for k-means, PCA and projective clustering.
In Proceedings of the Twenty-Fourth Annual ACMSIAM Symposium on Discrete Algorithms, SODA 2013,
New Orleans, Louisiana, USA, January 6-8, 2013, pp.
1434–1453, 2013. doi: 10.1137/1.9781611973105.
103. URL http://dx.doi.org/10.1137/1.
9781611973105.103.
Frahling, Gereon and Sohler, Christian. Coresets in dynamic geometric data streams. In Proceedings of the
thirty-seventh annual ACM symposium on Theory of
computing, pp. 209–217. ACM, 2005.
Ganguly, Sumit. Counting distinct items over update
streams. In International Symposium on Algorithms and
Computation, pp. 505–514. Springer, 2005.
Guha, Sudipto, Mishra, Nina, Motwani, Rajeev, and
O’Callaghan, Liadan. Clustering data streams. In Foundations of computer science, 2000. proceedings. 41st annual symposium on, pp. 359–366. IEEE, 2000.
Har-Peled, Sariel and Kushal, Akash. Smaller coresets
for k-median and k-means clustering. In Proceedings
of the 21st ACM Symposium on Computational Geometry, Pisa, Italy, June 6-8, 2005, pp. 126–134, 2005. doi:
10.1145/1064092.1064114. URL http://doi.acm.
org/10.1145/1064092.1064114.
Har-Peled, Sariel and Mazumdar, Soham. On coresets for
k-means and k-median clustering. In Proceedings of the
36th Annual ACM Symposium on Theory of Computing,
Chicago, IL, USA, June 13-16, 2004, pp. 291–300, 2004.
doi: 10.1145/1007352.1007400. URL http://doi.
acm.org/10.1145/1007352.1007400.
Hershberger, John and Suri, Subhash. Adaptive sampling
for geometric problems over data streams. In Proceedings of the twenty-third ACM SIGMOD-SIGACTSIGART symposium on Principles of database systems,
pp. 252–262. ACM, 2004.
Indyk, Piotr. High-dimensional computational geometry.
PhD thesis, Citeseer, 2000a.

Clustering High Dimensional Dynamic Data Streams

Indyk, Piotr. Stable distributions, pseudorandom generators, embeddings and data stream computation. In Foundations of Computer Science, 2000. Proceedings. 41st
Annual Symposium on, pp. 189–197. IEEE, 2000b.
Indyk, Piotr. Better algorithms for high-dimensional proximity problems via asymmetric embeddings. In Proceedings of the fourteenth annual ACM-SIAM symposium on
Discrete algorithms, pp. 539–545. Society for Industrial
and Applied Mathematics, 2003.
Indyk, Piotr. Algorithms for dynamic geometric problems
over data streams. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 373–
380. ACM, 2004.
Indyk, Piotr and Price, Eric. K-median clustering, modelbased compressive sensing, and sparse recovery for earth
mover distance. In Proceedings of the forty-third annual
ACM symposium on Theory of computing, pp. 627–636.
ACM, 2011.
Larsen, Kasper Green, Nelson, Jelani, Nguyên, Huy L, and
Thorup, Mikkel. Heavy hitters via cluster-preserving
clustering. arXiv preprint arXiv:1604.01357, 2016.
Liu, Zaoxing, Ivkin, Nikita, Yang, Lin, Neyrinck,
Mark, Lemson, Gerard, Szalay, Alexander, Braverman,
Vladimir, Budavari, Tamas, Burns, Randal, and Wang,
Xin. Streaming algorithms for halo finders. In e-Science
(e-Science), 2015 IEEE 11th International Conference
on, pp. 342–351. IEEE, 2015.
Meyerson, Adam. Online facility location. In Foundations
of Computer Science, 2001. Proceedings. 42nd IEEE
Symposium on, pp. 426–431. IEEE, 2001.
Muthukrishnan, Shanmugavelayutham. Data streams: Algorithms and applications. Now Publishers Inc, 2005.
Nisan, Noam.
Pseudorandom generators for spacebounded computation. Combinatorica, 12(4):449–461,
1992.

