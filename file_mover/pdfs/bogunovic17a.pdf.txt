Robust Submodular Maximization:
A Non-Uniform Partitioning Approach
Ilija Bogunovic 1 Slobodan Mitrović 2 Jonathan Scarlett 1 Volkan Cevher 1

Abstract
We study the problem of maximizing a monotone
submodular function subject to a cardinality constraint k, with the added twist that a number of
items ⌧ from the returned set may be removed.
We focus on the worst-case setting considered in
(Orlin et al., 2016), in which a constant-factor
papproximation guarantee was given for ⌧ = o( k).
In this paper, we solve a key open problem
raised therein, presenting a new Partitioned Robust (PRO) submodular maximization algorithm
that achieves the same guarantee for more general ⌧ = o(k). Our algorithm constructs partitions consisting of buckets with exponentially
increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRO in
data summarization and influence maximization,
demonstrating gains over both the greedy algorithm and the algorithm of (Orlin et al., 2016).

1. Introduction
Discrete optimization problems arise frequently in machine
learning, and are often NP-hard even to approximate. In the
case of a set function exhibiting submodularity, one can efficiently perform maximization subject to cardinality constraints with a 1 1e -factor approximation guarantee. Applications include influence maximization (Kempe et al.,
2003), document summarization (Lin & Bilmes, 2011),
sensor placement (Krause & Guestrin, 2007), and active
learning (Krause & Golovin, 2012), just to name a few.
1

LIONS, EPFL, Switzerland 2 LTHC, EPFL, Switzerland.
Correspondence to: Ilija Bogunovic <ilija.bogunovic@epfl.ch>,
Slobodan Mitrović <slobodan.mitrovic@epfl.ch>, Jonathan
Scarlett
<jonathan.scarlett@epfl.ch>,
Volkan
Cevher
<volkan.cevher@epfl.ch>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

In many applications of interest, one requires robustness in
the solution set returned by the algorithm, in the sense that
the objective value degrades as little as possible when some
elements of the set are removed. For instance, (i) in influence maximization problems, a subset of the chosen users
may decide not to spread the word about a product; (ii)
in summarization problems, a user may choose to remove
some items from the summary due to their personal preferences; (iii) in the problem of sensor placement for outbreak
detection, some of the sensors might fail.
In situations where one does not have a reasonable prior
distribution on the elements removed, or where one requires robustness guarantees with a high level of certainty,
protecting against worst-case removals becomes important.
This setting results in the robust submodular function maximization problem, in which we seek to return a set of cardinality k that is robust with respect to the worst-case removal of ⌧ elements.
The robust problem formulation was first introduced in
(Krause et al., 2008), and was further studied in (Orlin
et al., 2016). In fact, (Krause et al., 2008) considers a more
general formulation where a constant-factor approximation
guarantee is impossible in general, but shows that one can
match the optimal (robust) objective value for a given set
size at the cost of returning a set whose size is larger by a
logarithmic factor. In contrast, (Orlin et al., 2016) designs
an algorithm that obtains the first constant-factor approxip
mation guarantee to the above problem when ⌧ = o( k).
A key difference between the two frameworks is that the
algorithm complexity is exponential in ⌧ in (Krause et al.,
2008), whereas the algorithm of (Orlin et al., 2016) runs in
polynomial time.
Contributions. In this paper, we solve a key open problem
posed in (Orlin et al., 2016), namely, whether a constantfactor approximation guarantee p
is possible for general ⌧ =
o(k), as opposed to only ⌧ = o( k). We answer this question in the affirmative, providing a new Partitioned Robust
(PRO) submodular maximization algorithm that attains a
constant-factor approximation guarantee; see Table 1 for
comparison of different algorithms for robust monotone
submodular optimization with a cardinality constraint.

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

Algorithm

Max. Robustness

Cardinality

Oracle Evals.

Approx.

S ATURATE (K RAUSE ET AL ., 2008)
OSU (O RLIN ET AL ., 2016)

Arbitrary
p
o( k)

k(1 + ⇥(log(⌧ k log n)))
k

1.0
0.387

PRO -G REEDY (O URS )

o(k)

k

exponential in ⌧
O(nk)
O(nk)

0.387

Table 1. Algorithms for robust monotone submodular optimization with a cardinality constraint. The proposed algorithm is efficient and
allows for greater robustness.

Achieving this result requires novelty both in the algorithm
and its mathematical analysis: While our algorithm bears
some similarity to that of (Orlin et al., 2016), it uses a novel
structure in which the constructed set is arranged into partitions consisting of buckets whose sizes increase exponentially with the partition index. A key step in our analysis
provides a recursive relationship between the objective values attained by buckets appearing in adjacent partitions.
In addition to the above contributions, we provide the first
empirical study beyond what is demonstrated for ⌧ = 1
in (Krause et al., 2008). We demonstrate several scenarios
in which our algorithm outperforms both the greedy algorithm and the algorithm of (Orlin et al., 2016).

2. Problem Statement
Let V be a ground set with cardinality |V | = n, and let f :
2V ! R 0 be a set function defined on V . The function f
is said to be submodular if for any sets X ✓ Y ✓ V and
any element e 2 V \ Y , it holds that
f (X [ {e})

f (X)

f (Y [ {e})

f (Y ).

We use the following notation to denote the marginal gain
in the function value due to adding the elements of a set Y
to the set X:

S

f (S)

;
{s1 }
{s2 }
{s3 }
{s1 , s2 }
{s1 , s3 }
{s2 , s3 }

0
n
✏

mins2S f (S \ s)

n 1
n+✏
n
n

0
0
0
0
✏

n

1
✏

Table 2. Function f used to demonstrate that G REEDY can perform arbitrarily badly.

In this paper, we consider the following robust version of
(1), introduced in (Krause et al., 2008):
max

min

S✓V,|S|k Z✓S,|Z|⌧

f (S \ Z)

(2)

We refer to ⌧ as the robustness parameter, representing the
size of the subset Z that is removed from the selected set
S. Our goal is to find a set S such that it is robust upon
the worst possible removal of ⌧ elements, i.e., after the removal, the objective value should remain as large as possible. For ⌧ = 0, our problem reduces to Problem (1).

(1)

The greedy algorithm, which is near-optimal for Problem (1) can perform arbitrarily badly for Problem (2). As
an elementary example, let us fix ✏ 2 [0, n 1) and n 0,
and consider the non-negative monotone submodular function given in Table 2. For k = 2, the greedy algorithm selects {s1 , s2 }. The set that maximizes mins2S f (S\s) (i.e.,
⌧ = 1) is {s1 , s3 }. For this set, mins2{s1 ,s2 } f ({s1 , s2 } \
s) = n 1, while for the greedy set the robust objective
value is ✏. As a result, the greedy algorithm can perform
arbitrarily worse.

has been studied extensively.
A celebrated result
of (Nemhauser et al., 1978) shows that a simple greedy
algorithm that starts with an empty set and then iteratively adds elements with highest marginal gain provides
a (1 1/e)-approximation.

In our experiments on real-world data sets (see Section 5),
we further explore the empirical behavior of the greedy solution in the robust setting. Among other things, we observe that the greedy solution tends to be less robust when
the objective value largely depends on the first few elements selected by the greedy rule.

f (Y |X) := f (X [ Y )

f (X).

In the case that Y is a singleton of the form {e}, we adopt
the shorthand f (e|X). We say that f is monotone if for any
sets X ✓ Y ✓ V we have f (X)  f (Y ), and normalized
if f (;) = 0.
The problem of maximizing a normalized monotone submodular function subject to a cardinality constraint, i.e.,
max

S✓V,|S|k

f (S),

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

Related work. (Krause et al., 2008) introduces the following generalization of (2):

targeted set might make the decision not to spread the word
about the product.

(3)

For many of the existing diffusion models used in the literature (e.g., see (Kempe et al., 2003)), given the targeted set
S, the expected number of influenced nodes at the end of
the diffusion process is a monotone and submodular function of S (He & Kempe, 2016). For simplicity, we consider
a basic model in which all of the neighbors of the users in
S become influenced, as well as those in S itself.

max

min

S✓V,|S|k i2{1,··· ,n}

fi (S),

where fi are normalized monotone submodular functions.
The authors show that this problem is inapproximable in
general, but propose an algorithm S ATURATE which, when
applied to (2), returns a set of size k(1 + ⇥(log(⌧ k log n)))
whose robust objective is at least as good as the optimal
size-k set. S ATURATE requires a number of function evaluations that is exponential in ⌧ , making it very expensive to
run even for small values. The work of (Powers et al., 2016)
considers the same problem for different types of submodular constraints.
Recently, robust versions of submodular maximization
have been applied to influence maximization. In (He &
Kempe, 2016), the formulation (3) is used to optimize a
worst-case approximation ratio. The confidence interval
setting is considered in (Chen et al., 2016), where two runs
of the G REEDY algorithm (one pessimistic and one optimistic) are used to optimize the same ratio. By leveraging
connections to continuous submodular optimization, (Staib
& Jegelka, 2017) studies a related continuous robust budget
allocation problem.
(Orlin et al., 2016) considers the formulation in (2), and
provides the first constant
0.387-factor approximation rep
sult, valid for ⌧ = o( k). The algorithm proposed therein,
which we refer to via the authors surnames as OSU, uses
the greedy algorithm (henceforth referred to as G REEDY)
as a sub-routine ⌧ + 1 times. On each iteration, G REEDY is
applied on the elements that are not yet selected on previous
iterations, with these previously-selected elements ignored
in the objective function. In the first ⌧ runs, each solution is
of size ⌧ log k, while in the last run, the solution is of size
k ⌧ 2 log k. The union of all the obtained disjoint solutions
leads to the final solution set.

3. Applications
In this section, we provide several examples of applications where the robustness of the solution is favorable. The
objective functions in these applications are non-negative,
monotone and submodular, and are used in our numerical
experiments in Section 5.
Robust influence maximization. The goal in the influence
maximization problem is to find a set of k nodes (i.e., a
targeted set) in a network that maximizes some measure
of influence. For example, this problem appears in viral
marketing, where companies wish to spread the word of a
new product by targeting the most influential individuals in
a social network. Due to poor incentives or dissatisfaction
with the product, for instance, some of the users from the

More formally, we are given a graph G = (V, E), where V
stands for nodes and E are the edges. For a set S, let N (S)
denote all of its neighboring nodes. The goal is to solve the
robust dominating set problem, i.e., to find a set of nodes S
of size k that maximizes
min

|RS |⌧,RS ✓S

|(S \ RS ) [ N (S \ RS )|,

(4)

where RS ✓ S represents the users that decide not to
spread the word. The non-robust version of this objective
function has previously been considered in several different
works, such as (Mirzasoleiman et al., 2015b) and (NorouziFard et al., 2016).
Robust personalized image summarization. In the personalized image summarization problem, a user has a collection of images, and the goal is to find k images that are
representative of the collection.
After being presented with a solution, the user might decide
to remove a certain number of images from the representative set due to various reasons (e.g., bad lighting, motion
blur, etc.). Hence, our goal is to find a set of images that
remain good representatives of the collection even after the
removal of some number of them.
One popular way of finding a representative set in a massive
dataset is via exemplar based clustering, i.e., by minimizing
the sum of pairwise dissimilarities between the exemplars
S and the elements of the data set V . This problem can be
posed as a submodular maximization problem subject to a
cardinality constraint; cf., (Lucic et al., 2016).
Here, we are interested in solving the robust summarization
problem, i.e., we want to find a set of images S of size k
that maximizes
min

|RS |⌧,RS ✓S

f ({e0 })

f ((S \ RS ) [ {e0 }),

(5)

where
P e0 is a reference element and f (S) =
1
v2V mins2S d(s, v) is the k-medoid loss func|V |
tion, and where d(s, v) measures the dissimilarity between
images s and v.
Further potential applications not covered here include robust sensor placement (Krause et al., 2008), robust protection of networks (Bogunovic & Krause, 2012), and robust
feature selection (Globerson & Roweis, 2006).

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

4. Algorithm and its Guarantees
4.1. The algorithm
Our algorithm, which we call the Partitioned Robust (PRO)
submodular maximization algorithm, is presented in Algorithm 1. As the input, we require a non-negative monotone
submodular function f : 2V ! R 0 , the ground set of elements V , and an optimization subroutine A. The subroutine A(k 0 , V 0 ) takes a cardinality constraint k 0 and a ground
set of elements V 0 . Below, we describe the properties of A
that are used to obtain approximation guarantees.
The output of the algorithm is a set S ✓ V of size k that is
robust against the worst-case removal of ⌧ elements. The
returned set consists of two sets S0 and S1 , illustrated in
Figure 1. S1 is obtained by running the subroutine A on
V \ S0 (i.e., ignoring the elements already placed into S0 ),
and is of size k |S0 |.
We refer to the set S0 as the robust part of the solution set S.
It consists of dlog ⌧ e + 1 partitions, where every partition
i 2 {0, · · · , dlog ⌧ e} consists of d⌧ /2i e buckets Bj , j 2
{1, · · · , d⌧ /2i e}. In partition i, every bucket contains 2i ⌘
elements, where ⌘ 2 N+ is a parameter that is arbitrary
for now; we use ⌘ = log2 k in our asymptotic theory, but
our numerical studies indicate that even ⌘ = 1 works well
in practice. Each bucket Bj is created afresh by using the
subroutine A on V \ S0,prev , where S0,prev contains all
elements belonging to the previous buckets.
The following proposition bounds the cardinality of S0 , and
is proved in the supplementary material.
Proposition 4.1 Fix k
⌧ and ⌘ 2 N+ . The size of the
robust part S0 constructed in Algorithm 1 is
|S0 | =

dlog ⌧ e

X
i=0

d⌧ /2i e2i ⌘  3⌘⌧ (log k + 2).

This proposition reveals that the feasible values of ⌧ (i.e.,
k
. We will
those with |S0 |  k) can be as high as O ⌘⌧
2
later set ⌘ = O(log k), thus permitting all ⌧ = o(k) up
to a few logarithmic factors. In contrast, we recall that the
algorithm OSU proposed in (Orlin et al., 2016) adopts a
simpler approach where a robust set is used consisting of
⌧ buckets of equal
p size ⌧ log k, thereby only permitting the
scaling ⌧ = o( k).
We provide the following intuition as to why PRO succeeds
despite having a smaller size for S0 compared to the algorithm given in (Orlin et al., 2016). First, by the design of
the partitions, there always exists a bucket in partition i that
at most 2i items are removed from. The bulk of our analysis is devoted to showing that the union of these buckets
yields a sufficiently high objective value. While the earlier

Algorithm 1 Partitioned Robust Submodular optimization
algorithm (PRO)
Require: Set V , k, ⌧ , ⌘ 2 N+ , algorithm A
Ensure: Set S ✓ V such that |S|  k
1: S0 , S1
;
2: for i
0 to dlog ⌧ e do
3:
for j
1 to d⌧ /2i e do
4:
Bj
A (2i ⌘, (V \ S0 ))
5:
S0
S0 [ B j
6: S1
A (k |S0 |, (V \ S0 ))
7: S
S0 [ S1
8: return S

buckets have a smaller size, they also have a higher objective value per item due to diminishing returns, and our
analysis quantifies and balances this trade-off. Similarly,
our analysis quantifies the trade-off between how much the
adversary can remove from the (typically large) set S1 and
the robust part S0 .
4.2. Subroutine and assumptions
PRO accepts a subroutine A as the input. We consider a
class of algorithms that satisfy the -iterative property, defined below. We assume that the algorithm outputs the final
set in some specific order (v1 , . . . , vk ), and we refer to vi
as the i-th output element.
Definition 4.2 Consider a normalized monotone submodular set function f on a ground set V , and an algorithm A.
Given any set T ✓ V and size k, suppose that A outputs
an ordered set (v1 , . . . , vk ) when applied to T , and define
Ai (T ) = {v1 , . . . , vi } for i  k. We say that A satisfies
the -iterative property if
f (Ai+1 (T ))

f (Ai (T ))

1

max f (v|Ai (T )).
v2T

(6)

Intuitively, (6) states that in every iteration, the algorithm
adds an element whose marginal gain is at least a 1/ fraction of the maximum marginal. This necessarily requires
that
1.
Examples. Besides the classic greedy algorithm, which
satisfies (6) with
= 1, a good candidate for our
subroutine is T HRESHOLDING -G REEDY (Badanidiyuru &
Vondrák, 2014), which satisfies the -iterative property
with = 1/(1 ✏). This decreases the number of function
evaluations to O(n/✏ log n/✏).
S TOCHASTIC -G REEDY (Mirzasoleiman et al., 2015a) is
another potential subroutine candidate. While it is unclear whether this algorithm satisfies the -iterative property, it requires an even smaller number of function eval-

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

uations, namely, O(n log 1/✏). We will see in Section 5 that PRO performs well empirically when used
with this subroutine. We henceforth refer to PRO used
along with its appropriate subroutine as PRO-G REEDY,
PRO-T HRESHOLDING -G REEDY, and so on.

1η

partitions

2η

Properties. The following lemma generalizes a classical
property of the greedy algorithm (Nemhauser et al., 1978;
Krause & Golovin, 2012) to the class of algorithms satisfying the -iterative property. Here and throughout the paper,
we use OPT(k, V ) to denote the following optimal set for
non-robust maximization:

buckets

/2

( / 2)η
2

OPT(k, V ) 2 argmax f (S),

η

S✓V,|S|=k

1

S0

Lemma 4.3 Consider a normalized monotone submodular
function f : 2V ! R 0 and an algorithm A(T ), T ✓ V ,
that satisfies the -iterative property in (6). Let Al (T ) denote the set returned by the algorithm A(T ) after l iterations. Then for all k, l 2 N+
⇣
⌘
l
f (Al (T ))
1 e k f (OPT(k, T )).
(7)

S1
1

k - |S0|

Figure 1. Illustration of the set S = S0 [ S1 returned by PRO.
The size of |S1 | is k |S0 |, and the size of |S0 | is given in Proposition 4.1. Every partition in S0 contains the same number of
elements (up to rounding).

We will also make use of the following property, which is
implied by the -iterative property.
Proposition 4.4 Consider a submodular set function f :
2V ! R 0 and an algorithm A that satisfies the iterative property for some
1. Then, for any T ✓ V
and element e 2 V \ A(T ), we have
f (e|A(T )) 

f (A(T ))
.
k

(8)

Intuitively, (8) states that the marginal gain of any nonselected element cannot be more than times the average
objective value of the selected elements. This is one of the
rules used to define the -nice class of algorithms in (Mirrokni & Zadimoghaddam, 2015); however, we note that in
general, neither the -nice nor -iterative classes are a subset of one another.
4.3. Main result: Approximation guarantee
For the robust maximization problem, we let OPT(k, V, ⌧ )
denote the optimal set:
OPT(k, V, ⌧ ) 2 argmax

min

S✓V,|S|=k E✓S,|E|⌧

f (S \ E).

Moreover, for a set S, we let ES⇤ denote the minimizer
ES⇤ 2 argmin f (S \ E).

Theorem 4.5 Let f be a normalized monotone submodular function, and let A be a subroutine satisfying the iterative property. For a given budget k and parameters
2  ⌧  3⌘(logk k+2) and ⌘ 4(log k + 1), PRO returns a
set S of size k such that

(9)

E✓S,|E|⌧

With these definitions, the main theoretical result of this
paper is as follows.

⌘

f (S \ ES⇤ )

5

1+

5

⇣

k |S0 |
(k ⌧ )

1 e
⇣
⌘
1 e
3 dlog ⌧ e+⌘

3 dlog ⌧ e+⌘

⌘

k |S0 |
(k ⌧ )

⌘

⇤
⇥ f (OPT(k, V, ⌧ ) \ EOPT(k,V,⌧
) ),

(10)

⇤
where ES⇤ and EOPT(k,V,⌧
) are defined as in (9).
⇣
⌘
k
In addition, if ⌧ = o ⌘ log
and ⌘
log2 k, then we
k
have the following as k ! 1:

f (S \

ES⇤ )

✓

◆
1 e 1/
+ o(1)
2 e 1/
⇤
⇥ f (OPT(k, V, ⌧ ) \ EOPT(k,V,⌧
) ).

(11)

In particular, PRO-G REEDY achieves an asymptotic
approximation factor of at least 0.387, and PROT HRESHOLDING -G REEDY with parameter ✏ achieves an
asymptotic approximation factor of at least 0.387(1 ✏).
This result solves an open problem raised in (Orlin et al.,
2016), namely, whether a constant-factor approximation
guarantee can be obtained for ⌧ = o(k) as opposed to

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

p
only ⌧ = o k . In the asymptotic limit, our constant
factor of 0.387 for the greedy subroutine matches that of
(Orlin et al., 2016), but our algorithm permits significantly
“higher robustness” in the sense of allowing larger ⌧ values. To achieve this, we require novel proof techniques,
which we now outline.
4.4. High-level overview of the analysis
The proof of Theorem 4.5 is provided in the supplementary material. Here we provide a high-level overview of the
main challenges.
Let E denote a cardinality-⌧ subset of the returned set S
that is removed. By the construction of the partitions, it is
easy to verify that each partition i contains a bucket from
which at most 2i items are removed. We denote these by
B0 , . . . , Bdlog ⌧ e , and write EBi := E \ Bi . Moreover, we
define E0 := E \ S0 and E1 := E \ S1 .

We establish the following lower bound on the final objective function value:
⇢
f (S \ E) max f (S0 \ E0 ), f (S1 ) f (E1 |(S \ E)),
f

✓ dlog
[⌧ e
i=0

(Bi \ EBi )

◆

.

(12)

The arguments to the first and third terms are trivially seen
to be subsets of S \ E, and the second term represents the
utility of the set S1 subsided by the utility of the elements
removed from S1 .
The first two terms above are easily lower bounded by
convenient expressions via submodular and the -iterative
property. The bulk of the proof is dedicated to bounding the
third term. To do this, we establish the following recursive
relations with suitably-defined “small” values of ↵j :
!
!
j
[
1
f
(Bi \ EBi )
1
f (Bj )
1 + ↵1j
i=0
!
!
j[1
j[1
f E Bj
(Bi \ EBi )  ↵j f
(Bi \ EBi ) .
i=0

i=0

Intuitively, the first equation shows that the objective value
from buckets i = 0, . . . , j with removals cannot be too
much smaller than the objective value in bucket j without
removals, and the second equation shows that the loss in
bucket j due to the removals is at most a small fraction of
the objective value from buckets 0, . . . , j 1. The proofs of
both the base case of the induction and the inductive step
make use of submodularity properties and the -iterative
property (cf., Definition 4.2).
Once the suitable lower bounds are obtained for the terms
in (12), the analysis proceeds similarly to (Orlin et al.,

2016). Specifically, we can show that as the second term
increases, the third term decreases, and accordingly lower
bound their maximum by the value obtained when the two
are equal. A similar balancing argument is then applied to
the resulting term and the first term in (12).
The condition ⌧  3⌘(logk k+2) follows directly from Proposition 4.1; namely, it is a sufficient condition for |S0 |  k,
as is required by PRO.

5. Experiments
In this section, we numerically validate the performance of
PRO and the claims given in the preceding sections. In particular, we compare our algorithm against the OSU algorithm proposed in (Orlin et al., 2016) on different datasets
and corresponding objective functions (see Table 3). We
demonstrate matching or improved performance in a broad
range of settings, as well as observing that PRO can be
implemented with larger values of ⌧ , corresponding to a
greater robustness. Moreover, we show that for certain realworld data sets, the classic G REEDY algorithm can perform
badly for the robust problem. We do not compare against
S ATURATE (Krause et al., 2008), due to its high computational cost for even a small ⌧ .
Setup. Given a solution set S of size k, we measure the performance in terms of the minimum objective value upon the
worst-case removal of ⌧ elements, i.e. minZ✓S,|Z|⌧ f (S\
Z). Unfortunately, for a given solution set S, finding such a
set Z is an instance of the submodular minimization problem with a cardinality constraint,1 which is known to be
NP-hard with polynomial approximation factors (Svitkina
& Fleischer, 2011). Hence, in our experiments, we only
implement the optimal “adversary” (i.e., removal of items)
for small to moderate values of ⌧ and k, for which we use
a fast C++ implementation of branch-and-bound.
Despite the difficulty in implementing the optimal adversary, we observed in our experiments that the greedy adversary, which iteratively removes elements to reduce the
objective value as much as possible, has a similar impact
on the objective compared to the optimal adversary for the
data sets considered. Hence, we also provide a larger-scale
experiment in the presence of a greedy adversary. Throughout, we write OA and GA to abbreviate the optimal adversary and greedy adversary, respectively.
In our experiments, the size of the robust part of the solution set (i.e., |S0 |) is set to ⌧ 2 and ⌧ log ⌧ for OSU and
PRO, respectively. That is, we set ⌘ = 1 in PRO, and
similarly ignore constant and logarithmic factors in OSU,
since both appear to be unnecessary in practice. We show
1

This can be seen by noting that for submodular f and any
Z ✓ X ✓ V , f 0 (Z) = f (X \ Z) remains submodular.

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

both the “raw” objective values of the solutions, as well
as the objective values after the removal of ⌧ elements. In
all experiments, we implement G REEDY using the L AZYG REEDY implementation given in (Minoux, 1978).
The objective functions shown in Table 3 are given in
Section 3. For the exemplar objective function, we use
d(s, v) = ks vk2 , and let the reference element e0 be the
zero vector. Instead of using the whole set V , we approximate the objective by considering a smaller random subset
of V for improved computational efficiency. Since the objective is additively decomposable and bounded, standard
concentration bounds (e.g., the Chernoff bound) ensure that
the empirical mean over a random subsample can be made
arbitrarily accurate.
Data sets. We consider the following datasets, along with
the objective functions given in Section 3:
• EGO -FACEBOOK: This network data consists of social circles (or friends lists) from Facebook forming an
undirected graph with 4039 nodes and 88234 edges.
• EGO -T WITTER: This dataset consists of 973 social circles from Twitter, forming a directed graph
with 81306 nodes and 1768149 edges. Both EGO FACEBOOK and EGO -T WITTER were used previously
in (Mcauley & Leskovec, 2014).
• T INY 10 K and T INY 50 K: We used two Tiny Images
data sets of size 10k and 50k consisting of images
each represented as a 3072-dimensional vector (Torralba et al., 2008). Besides the number of images,
these two datasets also differ in the number of classes
that the images are grouped into. We shift each vectors to have zero mean.
• CM-M OLECULES: This dataset consists of 7211
small organic molecules, each represented as a 276
dimensional vector. Each vector is obtained by processing the molecule’s Coulomb matrix representation
(Rupp, 2015). We shift and normalize each vector to
zero mean and unit norm.

Dataset

n

dimension

f

Tiny-10k
Tiny-50k
CM-Molecules

10 000
50 000
7211

3074
3074
276

Exemplar
Exemplar
Exemplar

Network

# nodes

# edges

f

ego-Facebook
ego-Twitter

4039
81 306

88 234
1 768 149

DomSet
DomSet

Table 3. Datasets and corresponding objective functions.

Results. In the first set of experiments, we compare PROG REEDY (written using the shorthand PRO -G R in the legend) against G REEDY and OSU on the EGO -FACEBOOK
and EGO -T WITTER datasets. In this experiment, the dominating set selection objective in (4) is considered. Figure 2
(a) and (c) show the results before and after the worst-case
removal of ⌧ = 7 elements for different values of k. In
Figure 2 (b) and (d), we show the objective value for fixed
k = 50 and k = 100, respectively, while the robustness
parameter ⌧ is varied.
G REEDY achieves the highest raw objective value, followed by PRO-G REEDY and OSU. However, after the
worst-case removal, PRO-G REEDY-OA outperforms both
OSU-OA and G REEDY -OA. In Figure 2 (a) and (b),
G REEDY -OA performs poorly due to a high concentration
of the objective value on the first few elements selected by
G REEDY. While OSU requires k ⌧ 2 , PRO only requires
k ⌧ log ⌧ , and hence it can be run for larger values of ⌧
(e.g., see Figure 2 (b) and (c)). Moreover, in Figure 2 (a)
and (b), we can observe that although PRO uses a smaller
number of elements to build the robust part of the solution
set, it has better robustness in comparison with OSU.
In the second set of experiments, we perform the same
type of comparisons on the T INY 10 and CM-M OLECULES
datasets. The exemplar based clustering in (5) is used as the
objective function. In Figure 2 (e) and (h), the robustness
parameter is fixed to ⌧ = 7 and ⌧ = 6, respectively, while
the cardinality k is varied. In Figure 2 (f) and (h), the cardinality is fixed to k = 100 and k = 50, respectively, while
the robustness parameter ⌧ is varied.
Again, G REEDY achieves the highest objective value. On
the T INY 10 dataset, G REEDY -OA (Figure 2 (e) and (f))
has a large gap between the raw and final objective, but it
still slightly outperforms PRO-G REEDY-OA. This demonstrates that G REEDY can work well in some cases, despite failing in others. We observed that it succeeds here
because the objective value is relatively more uniformly
spread across the selected elements. On the same dataset,
PRO-G REEDY-OA outperforms OSU-OA. On our second
dataset CM-M OLECULES (Figure 2 (g) and (h)), PROG REEDY-OA achieves the highest robust objective value,
followed by OSU-OA and G REEDY -OA.
In our final experiment (see Figure 2 (i)), we compare
the performance of PRO-G REEDY against two instances of
PRO-S TOCHASTIC -G REEDY with ✏ = 0.01 and ✏ = 0.08
(shortened to PRO -S T in the legend), seeking to understand
to what extent using the more efficient stochastic subroutine impacts the performance. We also show the performance of OSU. In this experiment, we fix k = 100 and
vary ⌧ . We use the greedy adversary instead of the optimal
one, since the latter becomes computationally challenging
for larger values of ⌧ .

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

(b) ego-Facebook, k = 50
4000

3500

3500

3000
2500

PRo-Gr
OSU
Greedy
PRo-Gr - OA
OSU - OA
Greedy - OA

2000
1500
1000

3000
2500
2000
1500

3.5

3

2.5

2

500

30

40

50

60

70

80

90

2

100

3

4

5

Cardinality k
×104

(c) ego-Twitter, τ = 7

1000

500

6

7

8

9

10

30

40

50

(d) ego-Twitter, k = 100
3.4

(e) Tiny10, τ = 7

×106

3.4

80

90

100

3.2

3

Obj. value

Obj. value

3.6

70

(f) Tiny10, k = 100

×106

3.2

3.8

60

Cardinality k

Robustness τ

4

Obj. value

×104
4

Obj. value

Obj. value

Obj. value

(a) ego-Facebook, τ = 7
4000

2.8
2.6
2.4

3.1

3

2.2
2

2.9

3.2
2

3

4

5

6

7

1.8
30

8

40

50

60

70

80

90

100

2

3

4

Cardinality k

Robustness τ

(g) CM-Molecules, k = 50

(h) CM-Molecules, τ = 6

0.98

5

6

7

Robustness τ

(i) Tiny50, k = 100

×106

0.985
3.6

0.975

0.98

0.965
0.96
0.955

Obj. value

Obj. value

Obj. value

3.4
0.97

0.975
0.97
0.965
0.96

0.95
3

4

5

6

7

Robustness τ

8

9

0.955
40

3.2
PRo-Gr
OSU
PRo-St ϵ = 0.01
PRo-St ϵ = 0.08
PRo-Gr - GA
OSU - GA
PRo-St - GA ϵ = 0.01
PRo-St - GA ϵ = 0.08

3
2.8
2.6
2.4

50

60

70

80

Cardinality k

90

100

4

6

8

10

12

14

16

18

20

22

Robustness τ

Figure 2. Numerical comparisons of the algorithms PRO-G REEDY, G REEDY and OSU, and their objective values PRO -OA, OSU-OA
and G REEDY -OA once ⌧ elements are removed. Figure (i) shows the performance on the larger scale experiment where both G REEDY
and S TOCHASTIC -G REEDY are used as subroutines in PRO.

In Figure 2 (i), we observe a slight decrease in the objective value of PRO-S TOCHASTIC -G REEDY due to the
stochastic optimization. On the other hand, the gaps between the robust and non-robust solutions remain similar,
or even shrink. Overall, we observe that at least in this example, the stochastic subroutine does not compromise the
quality of the solution too significantly, despite having a
lower computational complexity.

6. Conclusion
We have provided a new Partitioned Robust (PRO) submodular maximization algorithm attaining a constantfactor approximation guarantee for general ⌧ = o(k), thus

resolving an open problem posed in (Orlin et al., 2016).
Our algorithm uses a novel partitioning structure with partitions consisting of buckets with exponentially decreasing
size, thus providing a “robust part” of size O(⌧ poly log ⌧ ).
We have presented a variety of numerical experiments
where PRO outperforms both G REEDY and OSU. A potentially interesting direction for further research is to understand the linear regime, in which ⌧ = ↵k for some constant ↵ 2 (0, 1), and in particular, to seek a constant-factor
guarantee for this regime.
Acknowledgment. This work was supported in part by
the European Commission under Grant ERC Future Proof,
SNF 200021-146750 and SNF CRSII2-147633, and ‘EPFL
Fellows’ (Horizon2020 665667).

Robust Submodular Maximization: A Non-Uniform Partitioning Approach

References
Badanidiyuru, Ashwinkumar and Vondrák, Jan. Fast algorithms for maximizing submodular functions. In ACMSIAM Symp. Disc. Alg. (SODA), pp. 1497–1514, 2014.
Bogunovic, Ilija and Krause, Andreas. Robust protection
of networks against cascading phenomena. Tech. Report
ETH Zürich, 2012.
Chen, Wei, Lin, Tian, Tan, Zihan, Zhao, Mingfei, and
Zhou, Xuren. Robust influence maximization. arXiv
preprint arXiv:1601.06551, 2016.
Globerson, Amir and Roweis, Sam. Nightmare at test time:
robust learning by feature deletion. In Int. Conf. Mach.
Learn. (ICML), 2006.
He, Xinran and Kempe, David. Robust influence maximization. In Int. Conf. Knowledge Discovery and Data
Mining (KDD), pp. 885–894, 2016.
Kempe, David, Kleinberg, Jon, and Tardos, Éva. Maximizing the spread of influence through a social network.
In Int. Conf. on Knowledge Discovery and Data Mining
(SIGKDD), 2003.
Krause, Andreas and Golovin, Daniel. Submodular function maximization. Tractability: Practical Approaches
to Hard Problems, 3(19):8, 2012.
Krause, Andreas and Guestrin, Carlos. Near-optimal observation selection using submodular functions. In Conf.
Art. Intell. (AAAI), 2007.
Krause, Andreas, McMahan, H Brendan, Guestrin, Carlos,
and Gupta, Anupam. Robust submodular observation selection. Journal of Machine Learning Research, 9(Dec):
2761–2801, 2008.
Lin, Hui and Bilmes, Jeff. A class of submodular functions
for document summarization. In Assoc. for Comp. Ling.:
Human Language Technologies-Volume 1, 2011.
Lucic, Mario, Bachem, Olivier, Zadimoghaddam, Morteza,
and Krause, Andreas. Horizontally scalable submodular
maximization. In Proc. Int. Conf. Mach. Learn. (ICML),
2016.
Mcauley, Julian and Leskovec, Jure. Discovering social circles in ego networks. ACM Trans. Knowl. Discov. Data,
2014.
Minoux, Michel. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques, pp. 234–243. Springer, 1978.

Mirrokni, Vahab and Zadimoghaddam, Morteza. Randomized composable core-sets for distributed submodular
maximization. In ACM Symposium on Theory of Computing (STOC), 2015.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrák, Jan, and Krause, Andreas.
Lazier than lazy greedy. In Proc. Conf. Art. Intell.
(AAAI), 2015a.
Mirzasoleiman, Baharan, Karbasi, Amin, Badanidiyuru,
Ashwinkumar, and Krause, Andreas. Distributed submodular cover: Succinctly summarizing massive data. In
Adv. Neur. Inf. Proc. Sys. (NIPS), pp. 2881–2889, 2015b.
Nemhauser, George L, Wolsey, Laurence A, and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265–294, 1978.
Norouzi-Fard, Ashkan, Bazzi, Abbas, Bogunovic, Ilija,
El Halabi, Marwa, Hsieh, Ya-Ping, and Cevher, Volkan.
An efficient streaming algorithm for the submodular
cover problem. In Adv. Neur. Inf. Proc. Sys. (NIPS),
2016.
Orlin, James B, Schulz, Andreas S, and Udwani, Rajan.
Robust monotone submodular function maximization. In
Int. Conf. on Integer Programming and Combinatorial
Opt. (IPCO). Springer, 2016.
Powers, Thomas, Bilmes, Jeff, Wisdom, Scott, Krout,
David W, and Atlas, Les. Constrained robust submodular optimization. NIPS OPT2016 workshop, 2016.
Rupp, Matthias. Machine learning for quantum mechanics
in a nutshell. Int. Journal of Quantum Chemistry, 115
(16):1058–1073, 2015.
Staib, Matthew and Jegelka, Stefanie.
Robust budget allocation via continuous submodular functions.
http://people.csail.mit.edu/stefje/
papers/robust_budget.pdf, 2017.
Svitkina, Zoya and Fleischer, Lisa. Submodular approximation: Sampling-based algorithms and lower bounds.
SIAM Journal on Computing, 40(6):1715–1737, 2011.
Torralba, Antonio, Fergus, Rob, and Freeman, William T.
80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Trans. Patt. Ana.
Mach. Intel., 30(11):1958–1970, 2008.

