Partitioned Tensor Factorizations for Learning Mixed Membership Models

Zilong Tan 1 Sayan Mukherjee 1

Abstract
We present an efficient algorithm for learning
mixed membership models when the number of
variables p is much larger than the number of
hidden components k. This algorithm reduces
the computational complexity of state-of-the-art
tensormethods, which require decomposing an
O p3 tensor, to factorizing
O (p/k) sub-tensors

each of size O k 3 . In addition, we address the
issue of negative entries in the empirical method
of moments based estimators. We provide sufficient conditions under which our approach has
provable guarantees. Our approach obtains competitive empirical results on both simulated and
real data.

1. Introduction
Mixed membership models (Woodbury et al., 1978;
Pritchard et al., 2000a;b; Blei et al., 2003; Erosheva, 2005)
have been used extensively across applications ranging
from modeling population structure in genetics (Pritchard
et al., 2000a;b) to topic modeling of documents (Woodbury et al., 1978; Blei et al., 2003; Erosheva, 2005). Mixed
membership models use Dirichlet latent variables to define
cluster membership where samples can partially belong to
each of k latent components. Parameter estimation for such
latent variables models (LVMs) using maximum likelihood
methods such as expectation maximization is computationally intensive for large data, for example, if number of samples n is large.
Parameter estimation using the method of moments for
LVMs is an attractive scalable alternative that has been
shown to have certain theoretical and computational advantages over maximum likelihood methods in the setting when n is large. For LVMs, method of moments approaches reduce to tensor methods—the moments of the
model parameters are expressed as a function of statistics
1
Duke University, Durham, NC. Correspondence to: Sayan
Mukherjee <sayan@stat.duke.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

of the observations in a tensor form. Inference in this setting becomes a problem of tensor factorization. Computational advantages of using tensor methods have been observed for many popular models, including latent Dirichlet
allocation (Anandkumar et al., 2012), spherical Gaussian
mixture models (Hsu & Kakade, 2013), hidden Markov
models (Anandkumar et al., 2012), independent component
analysis (Comon & Jutten, 2010), and multi-view models
(Anandkumar et al., 2014). An appealing property of tensor
methods is the guarantee of a unique decomposition under
mild conditions (cf. Kruskal, 1977; Leurgans et al., 1993).
There are two complications to using standard tensor decomposition methods (Anandkumar et al., 2016; 2014; Gu
et al., 2014; Kuleshov et al., 2015; Nicolò Colombo and
Nikos Vlassis; Kim et al., 2014; Chi & Kolda, 2012) for
LVMs. The first problem is computation and space complexity. Given p variables in the LVM, parameter inference
requires factorizing typically a non-orthogonal estimator
tensor of size O p3 (Anandkumar et al., 2014; Kuleshov
et al., 2015; Nicolò Colombo and Nikos Vlassis), which
is prohibitive for large p. When the estimator is orthogo
nal and symmetric, this can be done in O p2 log p (Wang
et al., 2015). Online tensor decomposition (Huang et al.,
2013) uses dimension reduction to instead factorize a reduced k-by-k-by-k tensor. However, the dimension reduction can be slower than decomposing the estimator directly
for large sample sizes, as well as suffer from high variance
(Wang et al., 2015). We introduce a simple factorization
with improved complexity for the general case where the
parameters are not required to be orthogonal.
The second problem arises from negative entries in the
empirical moments tensor. LVMs for count data are constrained to have nonnegative parameters. However, the empirical moments tensor computed from the data may contain negative elements due to sampling variation and noise.
Indeed, for small sample sizes or data with many small
or zero counts, there will be many negative entries in the
empirical moments tensor. General tensor decomposition
algorithms (Kuleshov et al., 2015; Nicolò Colombo and
Nikos Vlassis), including the tensor power method (TPM)
(Anandkumar et al., 2014), do not guarantee the nonnegativity of model parameters. Approaches such as positive/nonnegative tensor factorization (Chi & Kolda, 2012;
Shashua & Hazan, 2005; Welling & Weber, 2001) also do

Partitioned Tensor Factorizations for Learning Mixed Membership Models

not address this situation as they require all the elements of
the tensor to be factorized to be nonnegative. With robust
tensor methods (Anandkumar et al., 2016; Gu et al., 2014),
sparse negative entries may potentially be treated as corrupted elements; however, these methods are not applicable
in this setting since there can be many negative elements.
In this paper, we introduce a novel parameter inference
algorithm called partitioned tensor parallel quadratic programming (PTPQP) that is efficient in the setting where
the number of variables p is much larger than the number
of latent components k. The algorithm is also robust to
negative entries in the empirical moments tensor. There
are two key innovations in the PTPQP algorithm. The first
innovation is a partitioning technique which recovers the
parameters through factorizing
O (p/k) much smaller sub
tensors each of size O k 3 . The second innovation is a parallel quadratic programming (Brand & Chen, 2011) based
algorithm to factor tensors with negative entries under the
constraint that the factors are all nonnegative. To the best
of our knowledge, this is the first algorithm designed to address the problem of negative entries in empirical estimator tensors. We show that the proposed factorization algorithm converges linearly with respect to each factor matrix.
We also provide sufficient conditions under which the partitioned factorization scheme is consistent, the parameter
estimates converge to the true parameters.

2. Preliminaries

>

T(1) = A (C  B) ,

Element-wise matrix operators include  and , e.g., A 
0 means that A has nonnegative entries. (·)+ refers to
element-wise max (·, 0). ∗ and  respectively represent
element-wise multiplication and division. Moreover, ×
refers to the outer product and  denotes the Khatri-Rao
product. k·kF and k·k2 represent the Frobenius norm and
spectral norm, respectively.
Tensor basics. This paper uses similar tensor notations
as (Kolda & Bader, 2009). In particular, we are primarily
concerned with Kruskal tensors in Rd1 ×d2 ×d3 , which can
be expressed in the form of
r
X

>

T(2) = B (C  A)

>

T(3) = C (B  A) .

(2)

3. Learning through Method of Moments
3.1. Generalized Dirichlet latent variable models
A generalized Dirichlet latent variable model (GDLM) was
proposed in (Zhao et al., 2016) for the joint distribution of
n observations y1 , y2 , · · · , yn . Each observation yi con>
sists of p variables yi = (yi1 , yi2 , · · · , yip ) . GDLM
assumes a generative process involving k hidden components. For each observation, sample a random Dirichlet
>
vector xi = (xi1 , xi2 , · · · , xik ) ∈ ∆k−1 with concen>
tration parameter α = (α1 , α2 , · · · , αk ) . The elements
of xi are the membership probabilities for yi to belong to
each of the k components. Specifically,
yij ∼

Notations. We use bold lowercase letters to represent vectors and bold capital letters for matrices. Tensors are denoted by calligraphic capital letters. The subscript notation
Aj refers to j-th column of matrix A. We denote the jth column of the identity matrix as ej and 1 is a vector
of ones. We further write diag (x) for a diagonal matrix
whose diagonal entries are x, and diag (A) to mean a vector of the diagonal entries of A.

T =

d3 -by-r factor matrices. The rank of T is defined as the
smallest r that admits such a decomposition. The decomposition is known as the CP (CANDECOMP/PARAFAC)
decomposition. The j-mode unfolding
ofT , denoted by
Q
T(j) , for j = 1, 2, 3 is a dj -byd
t6=j t matrix whose
rows are serializations of the tensor fixing the index of the
j-th dimension. The unfoldings have the following wellknown compact expressions:

k
X

xih gj (θjh ) ,

h=1

where gj (θjh ) is the density of the j-th variable specific
to component h with parameter θj = (θj1 , θj2 , · · · , θjk ).
One advantage of GLDM is that yij can take categorical
values. Let dj denote the number of categories for the j-th
variable (set dj = 1 for scalar variables), θj becomes a dj by-k probability matrix where the c-th row corresponds to
category c. We aim to accurately recover θj from independent copies of yi involving variables of mixed data types,
either categorical or non-categorical.
3.2. Moment-based estimators
The moment estimators of latent variable models typically
take the form of a tensor (Anandkumar et al., 2014). Consider the estimators of GDLM (Zhao et al., 2016) for example. Let bij = eyij if variable j is categorical; bij = yij
otherwise. The second- and third- order parameter estimators for variable j, s, and t are written
>

Mjs = E [bij × bis ] − c1 E [bij ] E [bis ]
Aj × Bj × C j ,

(1)

j=1

where A, B, and C are respectively d1 -by-r, d2 -by-r, and

Mjst = E [bij × bis × bit ] + c2 E [bij ] × E [bis ] × E [bit ]
− c3 (E [E [bij ] × bis × bit ] + E [bij × E [bis ] × bit ]
+E [bij × bis × E [bit ]]) .

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Alternatively, Mjs and Mjst have the following CP decomposition into parameters θj :
Mjs =

X

ηh θjh × θsh ,

θuv ∈ Rdi

(3)

h≥1

Mjst =

X

λh θjh × θsh × θth ,

θuv ∈ Rdi .

(4)

h≥1

P
Here, c1 , c2 , c3 , ηh , and λh depend on only α0 = j≥1 αj .
For the special case of latent Dirichlet allocation, Mjs and
Mjst are scalar joint probabilities.
The parameters θj are typically obtained by factorizing the
block tensor M2 whose (j, s)-th element is the empirical
cjs and/or M3 whose (j, s, t)-th element is the empirical
M
c
Mjst (Anandkumar et al., 2016; 2014; Zhao et al., 2016).
Note that θj are generally non-orthogonal, and thus preprocessing steps are needed for orthogonal decomposition
methods (Wang et al., 2015; Song et al., 2016; Anandkumar et al., 2014). The preprocessing can be expensive and
often leads to suboptimal performance (Souloumiac, 2009;
Nicolò Colombo and Nikos Vlassis). Here, we highlight a
few relevant observations:
• Mjs alone does not yield unique parameters θj due
to the well-known rotation problem. This is true even
when enforcing nonnegativity constraints on parameters (Donoho & Stodden, 2004).
• Mjst is sufficient to uniquely recover the parameters
under certain mild conditions (Kruskal, 1977); for example, when any two of θj , θs , and θt have linearly
independent columns and the columns of the third are
pair-wise linearly independent (Leurgans et al., 1993).
cjst generally contains neg• The empirical estimator M
ative entries due to variance and noise. The fraction of
negative entries can approach 50%, as we shall see in
experiments. We address this issue in § 4.4.
• While the decomposition (4) can be unique up to permutation and rescaling, the correspondence between
each column of the factor matrix and each hidden
component may not be consistent across multiple decompositions. Techniques for achieving consistency
are developed in § 4.2.
3.3. Computational complexity
Tensor methods
such as TPM typically decompose the

O p3 d3max full estimator tensor that includes all variables. More efficient algorithms have been developed for
the case that parameters are orthogonal (Wang et al., 2015;
Song et al., 2016), and when the sample size is small
(Huang et al., 2013). However, these methods do not

apply in the general case where the parameters are nonorthogonal and the sample size can be potentially large. A
key insight underlying our approach is that it is sufficient to
recover the parameters by factorizing only
 O (p/k) much
smaller sub-tensors each of size O k 3 . This technique
can also be combined with the aforementioned methods to
further improve the complexity in certain cases.

4. An efficient algorithm
In this section, we develop partitioned tensor parallel
quadratic programming (PTPQP) an efficient approximate
algorithm for learning mixed membership models. We first
introduce a novel partitioning-and-matching scheme that
reduces parameter estimation to factorizing a sequence of
sub-tensors. Then, we develop a nonnegative factorization algorithm that can handle negative entries in the subtensors.
4.1. Partitioned factorization
Factorizing the full tensor formed by all Mjst is expensive
while a three-variable tensor Mjst in (4) alone may not be
sufficient to determine θj when k is large. In this section,
we consider factorizing the sub-tensors corresponding to a
cover of the set of variables [p] such that each sub-tensor
admits an identifiable CP decomposition (1), i.e. unique
up to permutation and rescaling of columns. This gives
the parameters for all variables. Suppose that p > k and
the maximum number of categories dmax is a constant, the
aggregated
can be much smaller, i.e.,
 size of the sub-tensors

O pk 2 , than the size O p3 of the full estimator.
Let π j , π s , and π t denote ordered subsets ⊆ [p], with cardinality |πj | = pj , |πs | = ps , and |πt | = pt , respectively.
j s t
Consider the pj -by-ps -by-pt block tensor Mπ π π whose
j s t
j s t
π π
(u, v, w)-th element is the tensor Mπuvw
= Mπu πv πw .
j s t
From (4) the tensor Mπ π π is
 

 

θπ j h
θπ1t h
θπ1s h
1
θ j  θ s  θ t 
k
X
 π2 h   π2 h   π2 h 

λh 
(5)
..  ×  .  .
 ..  × 
 .   .   .. 
h=1
θπpss h
θπpt t h
θπpj h
j

Clearly, the block tensor is identifiable if it has an identiu v w
fiable sub-tensor. Suppose that a sub-tensor Mπ π π is
identifiable, then one can construct an identifiable tensor
j0 s0 t0
j s t
Mπ π π from Mπ π π by setting
π j0 = π j ∪ π u ,

π s0 = π s ∪ π v ,

π t0 = π t ∪ π w . (6)

We further remark that a sub-tensor can be identifiable under mild conditions, for example, if the sum of the Kruskal
rank of the three factor matrices is at least than 2k + 2
(Kruskal, 1977).

Partitioned Tensor Factorizations for Learning Mixed Membership Models
u

v

w

Given an identifiable sub-tensor Mπ π π of anchor variables indexed by π u , π v , and π w , the partitioning produces
a set of sub-tensors (partitions) constructed through (6),
u v w
that includes all variables. Thus, Mπ π π is a common
sub-tensor shared across all partitions. We choose anchor
variables whose parameter matrices are of full column rank
u v w
to obtain an identifiable Mπ π π . Finally, one can divide
the rest of variables evenly and randomly into the partitions.
4.2. Matching parameters with hidden components
Since the factorization of a partition (5) can only be identifiable up to permutation and rescaling of the columns of
constituent θj , the correspondence between the columns of
θj and hidden components can differ across partitions. To
enforce consistency, we associate a permutation
operator

ψ j for each variable j such that ψ j θj h are the parameters specific to hidden component h across all variables j.
Consider the following vector representation of ψ:
ψ = (ψ1 , ψ2 , · · · , ψk ) ,

ψi ∈ [k]

ψA = [Aψ1 , Aψ1 , · · · , Aψk ] .
Observe that ψ j = ψ s = ψ t within a factorization of
Mjst , and this also holds for the partitioned factorizaj s t
tion (5) of Mπ π π as well, i.e., ψ x = ψ y , ∀x, y ∈
πj ∪ πs ∪ πt .
j

s

t

u

v

computed by:
ψsu = arg max θ̄j0> ψ j θ̄j
t


ts

.

(7)

Here, θ̄j and θ̄j0 represent respectively the normalized θj
and θj0 with each column having unit Euclidean norm.
There are cases that ψ u computed via Equation (7) is not
consistent: 1) ψ u contains duplicate entries and hence is
ineligible; and 2) since θj and θj0 are the factorized parameter matrices which are generally perturbed from the
ground-truth, the resulting ψ u may differ from the consistent permutation. To cope with these cases, we establish in
§ 5 the sufficient conditions for ψ u to be consistent.
Orthogonal Procrustes matching One issue with the
smallest angle matching is that each column is paired independently. It is easy for multiple columns to be paired with
a common nearest neighbor. We describe a more robust algorithm based on the orthogonal Procrustes problem, and
show improved guarantees. Since a consistent permutation
is orthogonal, a natural relaxation is to only require the operator to be orthogonal. This is an orthogonal Procrustes
problem, formulated in the same settings as § 4.2

2
min θ̄j0 Ψ − ψ j θ̄j F , s.t. Ψ> Ψ = I.
(8)
Ψ

Let θ̄j0> ψ j θ̄j = U ΣV > be the singular value decomposition (SVD), the solution Ψ∗ is given by the polar factor
(Schönemann, 1966)

w

Consider the factorizations of Mπ π π and Mπ π π and
suppose that ∃x ∈ π j ∪ π s ∪ π t ∩ (π u ∪ π v ∪ π w ). The
permutation operator for one factorization is determined
given the other by column matching the parameters of variable x in both factorizations. Thus, an inductive way to
achieve a consistent factorization is to start with one factorization, and let its permutation be the identity (1, 2, · · · , k),
then perform the factorization over new sets of variables
with at least one variable in common with the initial factorization. Permutations for the sequential factorizations are
determined via column matching parameter matrices of the
common variables.

Ψ∗ = U V > .

(9)

Here, Ψ∗ is orthogonal and does not immediately imply
the desired permutation ψ u . To compute ψ u , one can additionally restrict Ψ to be a permutation matrix, and solve
for ψ u using linear programming (Gower & Dijksterhuis).
Aside from efficiency, one fundamental question is that under what assumptions the objective (8) yields the consistent
permutation.
Given the solution Ψ∗ to the Procrustes problem, we propose the following simple algorithm for computing ψ u :
ψsu = arg max Ψ∗ts .

Given two factorized parameter matrices θj and θj0 of variable j, our goal is to find a consistent permutation ψ (of θj
0
with respect to θj0 ) such that (ψθj )h and θjh
correspond
to the same hidden component for all h ∈ [k]. We now
present an algorithm with provable guarantees to compute
a consistent permutation.

We first establish through Theorem 1 that if ψ u obtained
using (10) is a valid permutation, i.e., no duplicate entries,
then it is optimal in terms of the objective (8).

Smallest angle matching A simple matching algorithm is
to match the two columns of the two parameter matrices
that have the smallest angle between them. Consider the
factorizations of Mjst and Mjuv which yield respectively
parameters θj and θj0 for the common variable j. Given the
permutation ψ j for Mjst , the permutation ψ u for Mjuv is

for all permutations ψ.

t

(10)

Theorem 1. The ψ u obtained using (10) satisfies
 u 0



ψ θ̄j − ψ j θ̄j 2 ≤ ψ θ̄j0 − ψ j θ̄j 2
F
F

In section § 5 we state sufficient conditions under which the
objective (8) yields a consistent permutation.

Partitioned Tensor Factorizations for Learning Mixed Membership Models

4.3. Approximate nonnegative factorization
In previous sections, we reduced the inference problem to
factorizing partitioned sub-tensors. We now present a factorization algorithm for the sub-tensors that contain negative entries. Our goal is to approximate a sub-tensor M by
f = P Aj × Bj × Cj where the factors A,
a sub-tensor M
j
B, and C are nonnegative. The Frobenius norm is used to
quantify the approximation



f
(11)
min M − M
 .
A,B,C0

F

Note that we do not assume that M  0 in (11) which
distinguishes our optimization problem from other approximate factorization algorithms (Welling & Weber, 2001; Chi
& Kolda, 2012; Kim et al., 2014; Shashua & Hazan, 2005).
In § 4.4, we provide some details as to why negative entries
are problematic for standard approximate factorization algorithms. We can rewrite (11) using the 1-mode unfolding
as



>
min M(1) − A (C  B)  .
(12)
A,B,C0

F

Equivalent formulations with respect to the 2-mode and 3mode unfoldings can be readily obtained from (2).
We point out that another widely-used error measure — the
I-divergence (Finesso & Spreij, 2006; Chi & Kolda, 2012)
— may not be suitable for our learning problem. The optimization using I-divergence is given by

X 
Muvw
fuvw .
− Muvw + M
min
Muvw log
fuvw
A,B,C0
M
u,v,w
This optimization is useful for nonnegative M when each
entry follows a Poisson distribution. In this case, the objective is equivalent to the sum of Kullback-Leibler divergence
across all entries of M:



X

fuvw .
DKL Pois (x; Muvw )  Pois x; M
u,v,w

However, the Poisson assumption does not generally hold
for the estimator tensor (4).
4.4. Handling negative entries in empirical estimators
We first illustrate that factorizing a tensor with negative entries using either positive tensor factorization (Welling &
Weber, 2001) or nonnegative tensor factorization (Chi &
Kolda, 2012; Shashua & Hazan, 2005) will either result
in factors that violate the the nonnegativity constraint or
the result of the algorithm diverges. In addition, we show
that general tensor decompositions cannot enforce the factor nonnegativity even after rounding the negative entries
to zero.

We then present a simple method based on weighted nonnegative matrix factorization (WNMF) (Zhang et al., 1996)
that enforce the factor nonnegativity constraint. We further
generalize this method using parallel quadratic programming (PQP) (Brand & Chen, 2011) to obtain a method with
a provable convergence rate.
Issue of negative entries If the tensor is strictly nonnegative, the optimization specified in (11) can be reduced to
nonnegative matrix factorization (NMF). Solvers abound
for NMF including the celebrated Lee-Seung’s multiplicative updates (Lee & Seung, 2001). The reduction is done by
2
viewing (12) as kY − W HkF with Y = Mjst
(1) , W = A,
>

and H = (C  B) , and alternating

Y H > st
,
Wst ← Wst
(W HH > )st

(13)

over each unfolding and factor matrix W . Obviously, the
updates may yield negative entries in W when the unfolding contains negative entries. In addition, convergence relies on the nonnegativity of the unfolding (cf. Lee & Seung, 2001). This issue extends to their tensor factorization
variants (Welling & Weber, 2001; Chi & Kolda, 2012; Kim
et al., 2014) known as the positive tensor factorization and
nonnegative tensor factorization. For these approaches, a
cjst to 0,
naive resolution is to round negative entries of M
this however lacks theoretical guarantees.
It is important to note that the rounding does not help general tensor decompositions like TPM. The following example illustrates that the unique decomposition (up to permutation and rescaling) of a positive tensor can contain negative entries. Consider a 2-by-2-by-2 positive tensor, whose
1-mode unfolding is given by


1 3 2 2
,
2 2 2 2
where the vertical bar separates two frontal slices. It has
the following decomposition, written in the form of (1):




1 1
2 −1
A=C=
, B=
.
1 0
2 1
Since all factors are of full-rank, the decomposition
is unique up to permutation and rescaling of columns
(Kruskal, 1977). Thus, a general tensor decomposition
yields a B with negative entries regardless of rescaling.
4.5. Factorization via WNMF
Since the ground-truth Mjst are nonnegative, we may “igcjst by treating them as
nore” the negative entries of M
missing values. This idea leads to the following modified
objective:
2

min kΩ ∗ (Y − W H)kF

W ,H0

(14)

Partitioned Tensor Factorizations for Learning Mixed Membership Models

where Y , W , H are chosen identically as (13), and define
the boolean Ωuv := Yuv ≥ 1. The optimization can be
carried out using WNMF. Here, we modify the original updates by introducing a positive constant  to ensure that the
updates are well-defined:
Wuv



(Ω ∗ Y ) H > uv + 
.
← Wuv
[((W H) ∗ Ω) H > ]uv + 

(15)

It can be easily shown that the presence of  does not affect the solution accuracy. In addition, having  in both
the numerator and denominator of (15) guarantees that the
objective (14) is non-increasing under the updates.
4.6. Parallel quadratic programming
We now generalize the WNMF approach using parallel
quadratic programming to obtain a convergence rate. Let
S++ denote the set of symmetric positive definite matrices,
we consider the following optimization problem

Algorithm 1 Factorize (M, k, d)
M ← M/ maxuvw |Muvw | ,  ← 10−10
% Initialize with random nonnegative matrices:
A ← rand (dj , k), B ← rand (ds , k), C ← rand (dt , k)
% Create
variable
tuples:
 a set >of alternating


	
>
F ← A,
C
C
∗
B
B
,
−M
(C  B) 	
(1)



F ← F ∪ B, C > C  ∗ A> A , −M(2) (C  A)	
F ← F ∪ C, B > B ∗ A> A , −M(3) (B  A)
repeat
for each [X, Q, Z] in F do
p
−1/2
>
Φ ← λmin (Q) diag (ZQ−1 Z > )diag (Q)
Φ ← (Φ − |Z|)+ /2 + 11>

 

X ← X ∗ (−Z)+ + Φ  XQ + (Z)+ + Φ
end for
until X ceased to change, or reached max #iterations
Normalize the columns of A, B, C to sum to 1.
return A, B, C

with
1
min x> Qx + z > x
x 2

s.t.

x ≥ 0, Q ∈ S++ ,

(16)

which can be solved by iterating multiplicative updates
(Brand & Chen, 2011; Sha et al., 2003). We use the parallel
quadratic programming (PQP) algorithm (Brand & Chen,
2011; Brand et al., 2011) to solve (16), partly because it has
a provable linear convergence rate. The PQP multiplicative
update for (16) takes the following simple form:


x ← x ∗ Q− x + z −  Q+ x + z + ,

+

where λmin (·) is the smallest eigenvalue. Similar updates
for B and C are obtained using (2).
4.7. Proposed approach

(17)

with
Q+ = (Q)+ + diag (γ) ,

Q− = (−Q)+ + diag (γ)

z + = (z)+ + φ,

z − = (−z)+ + φ.

Here γ and φ are arguments to PQP, we will discuss these
arguments in section § 5.2. The update maintains nonnegativity since all items are nonnegative. We make the following observation.
Theorem 2. The multiplicative updates for Lee-Seung and
WNMF are special cases of PQP.
We can now solve the approximate nonnegative factorization problem stated in (11) using (17). Theorem 3 states the
multiplicative updates. A more detailed discussion of Φ is
included in § 5.2. We present pseudo-code in Algorithm 1.
Theorem 3. For optimization (11), the following update
converges linearly to a local optimum

 

A ← A ∗ (−Z)+ + Φ  AQ + (Z)+ + Φ



Q = C >C ∗ B>B ,
Z = −M(1) (C  B)
s
!
1
diag (ZQ−1 Z > )
>
Φ
diag (Q) − |Z|
,
2
λmin (Q)

(18)

To summarize, the proposed approach, referred to as PTPQP, consists of three steps. Given the indexes of anchor
variables π u ∪ π v ∪ π w , the variables [p] \ (π u ∪ π v ∪ π w )
are first evenly divided into r partitions, and the anchor
variables are added to each partition. The second step
consists of forming and factorizing the sub-tensor of each
partition using Algorithm 1, this step can be parallelized.
 u
v
w >
Third, normalize the anchor matrix θ π > , θ π > , θ π >
formed by the anchor variable parameters to have unit column Euclidean norm, and then use either (7) or (10) to
match over the anchor matrix.
Efficiency. Most of the computational cost is in the
factorization. Consider one partition, and let Mπj πs πt
be the
sub-tensor, the sub-tensor size is
Q corresponding
P
d
.
The maximum number of cateh∈π h
π∈{π j ,π s ,π t }

gories for a variable is generally a constant for the GDLM.
Under smallest partitioning, this size is determined
by the

sub-tensor of anchor variables, i.e., O k 3 , which corresponds to (p/k) partitions. One benefit of PTPQP is that
the number of sub-tensor factorizations is linear in p due to
the partitioned factorization, this results in significant efficiency gains when p  k. Furthermore, PTPQP is easy to

Partitioned Tensor Factorizations for Learning Mixed Membership Models

be parallelized across multiple CPUs and machines, since
the computation as well as data are not distributed across
partitions.

5. Provable Guarantees
In this section, we state the main theoretical results of the
proposed partitioned factorization and tensor PQP factorization. The proofs can be found in the longer version of
this paper (Tan & Mukherjee, 2017).

θ and θ 0 in Theorem 5 to first obtain the consistent permutation of θ 0 with respect to θ, ψ then follows immediately.
Theorem 5 states that solving (10) recovers a consistent
permutation whenever the error spectral norm is small as
compared to the smallest singular value of θ > θ. This is
especially useful for θ ∈ Rd×k with the number of rows
d much larger than the number of columns k. In particular, for θ with independentand identically distributed
subgaussian entries, σk θ > θ is at least of the order
2
√
√
d − k − 1 (Rudelson & Vershynin, 2009).

5.1. Sufficient conditions for guaranteed matching
Theorem 4 and Theorem 5 state that when the anchor parameter matrices from two factorizations are “close”, the
proposed matching algorithms obtain a consistent permutation.
Theorem 4. Suppose that θj is the ground-truth matrix
for variable j. Solving Equation (7) results in a consistent
permutation if for all factors θbj of variable j


v
s 
u



u1
θjh − θbjh 

1
t
2
>
<1−
+
1 + max θ̄j θ̄j uv
u<v
kθjh k2
2
8

The following theorem states a sufficient condition for PQP
to achieve linear convergence rate. The theorem statement
and proof is an adaptation of results stated in (Brand &
Chen, 2011)—the proof in (Brand & Chen, 2011) overlooks a required condition on φ and the condition γ 
diag (Qjj ) in the original proof is unnecessary.
Theorem 6. The PQP algorithm given by (17) monotonically decreases the objective (16) and has linear convergence, if γ  (−Q)+ 1 and
1
φ
2

for all h ∈ [k], where θ̄jh = θjh / kθjh k2 .
Theorem 4 states that one obtains a consistent permutation
by solving Equation (7) in the columns of the ground-truth
parameter matrix are distinct from each other in angles and
the factorized parameter matrix is near the ground-truth in
Frobenius norm. Thus, a good anchor variable for the partitioned factorization (5) is one whose parameter matrix has
distant columns in angles.
The bound in Theorem 4 can be made sharp for certain θj ,
and thus the smallest angle matching algorithm has general
guarantees only when the perturbation
is small, i.e., the relp
√
ative error ratio is less than 1 − 2 + 2/2 ≈ 1/13.
Theorem 5. Suppose that θ and θ 0 are two factorized parameter matrices for a variable. Solving (10)results in a
consistent permutation ψ, if kEk2 < σk θ > θ and
√

kEk2
ρ 2 − 2
log 1 −
<
−
ρ
ν
4
with
ρ = σ1 (E) + σ2 (E) ,

5.2. Convergence



ν = σk θ > θ + σk−1 θ > θ
>

where the error matrix is define as E = (ψθ) (θ 0 − ψθ),
and σj (·) denotes the j-th largest singular value.
The first condition in Theorem 5 requires that at least one
of θ and θ 0 must have full column rank. We may exchange

s

!
z > Q−1 z
diag (Q) − |z|
,
λmin (Q)

(19)

+

where λmin (·) is the smallest eigenvalue.

6. Results on real and simulated data
We compare the proposed algorithm ptpqp with state-ofthe-art approaches including: 1) the tensor power method
tpm (Anandkumar et al., 2014) and matrix simultaneous diagonalization, nojd0 and nojd1 (Kuleshov et al.,
2015)—two general tensor decomposition methods; 2)
nonnegative tensor factorization hals (Kim et al., 2014);
and 3) generalized method of moments meld (Zhao et al.,
2016). We use the online code provided by the corresponding authors. The code to reproduce the experiments is
available at: https://goo.gl/3DBXIo.
6.1. Learning GDLMs on simulated data
We adapt a simulation study from (Zhao et al., 2016) to
compare runtime and accuracy of parameter estimation.
We consider a GDLM where each variable takes categorical values {0, 1, 2, 3} and the parameters of the Dirichlet mixing distribution are {αj = 0.1}kj=1 . We initially
consider 25 variables. The true parameters for each hidden component h are drawn from the Dirichlet distribution Dir (0.5, 0.5, 0.5, 0.5). The resulting moment estimator is a 100-by-100-by-100 tensor. We vary the number
of components k and add noise by replacing a fraction δ

δ = 0%

Partitioned Tensor Factorizations for Learning Mixed Membership Models

0.2

RMSE

δ = 5%

0

5

10

15

20

5

10

15

20

5

10

15

20

0.4
0.2
0

δ = 0.1%

n = 100

0.4

0.4
0.2
0

0.4
0.3
0.2
0.1
0

n = 500

0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0

k

5

10

15

20

5

10

15

20

5

10

15

20

0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0

k

n = 1000

5

10

15

20

5

10

15

20

5

10

15

20

0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0
0.4
0.3
0.2
0.1
0

n = 5000

5

10

15

20

5

10

15

20

5

10

15

20

k

ptpqp
hals
meld
tpm
nojd0
nojd1

k

Figure 1. RMSE between inferred and true parameters.

of the observations with draws from a discrete uniform
distribution. We also vary the number of samples n =
100, 500, 1000, 5000, number of clusters k = 3, 5, 10, 20,
and contamination δ = 0, 0.05, 0.1. Across these settings
we found that the empirical third-order estimator typically
exhibits between 20% and 50% negative entries.
Accuracy of inference: Accuracy is measured by rootmean-square error (RMSE) which we compare across algorithms as a function of the number of components for various sample sizes and levels of contamination, see Figure 1.
Both hals and ptpqp are consistently among the top estimators, and ptpqp outperforms hals as n grows. For small
sample sizes and many hidden components meld achieves
the smallest RMSE. The RMSE of tpm is relatively large,
probably due to the whitening technique used to approximately transform the nonorthogonal factorization into an
orthogonal one, see (Souloumiac, 2009; Nicolò Colombo
and Nikos Vlassis). The most relevant observation is that
ptpqp outperforms other methods for large, noisy data.
Computational cost: We examined how runtime scales as
a function of the number of partitions. For the same model
we set p = 1000 variables and n = 1000 samples. The
tensor is now 4000-by-4000-by-4000. We evaluated the
runtime of ptpqp (without parallelization) with the number of partitions set to {30, 40, 50, 100, 200}. On a laptop
with Intel i7-4702HQ@2.20GHz CPU and 8GB memory,
ptpqp with 100 partitions completes within 3.5 min, 4 min,
and 5 min for k = 4, 8, 12, respectively. In addition, the
runtime monotonically decreases with the number of partitions. Further speedups can be obtained by parallelizing
the factorization of partitions across multiple CPUs or machines.
6.2. Predicting crowdsourced labels
In (Zhang et al., 2014), a combination of EM and tensor
decompositions was used to predict crowdsourcing annotations. The task is to predict the true label given incom-

plete and noisy observations from a set of workers, this is
a mixed membership problem (Dawid & Skene, 1979). In
(Zhang et al., 2014) a third-order tensor estimator was proposed to obtain an initial estimate for the EM algorithm.
We compare the predictive performance on five data sets of
several tensor decomposition methods as well as the EM
algorithm initialized with majority voting by the workers
(MV+EM). The fraction of incorrect predictions and the
size of each dataset are in the table below. Note that ptpqp matches or outperforms the other tensor methods on
all but one dataset, and even outperforms MV+EM on two
datasets.

Table 1. Incorrectly predicted labels (%)
DATASET
PTPQP
HALS
TPM
NOJD 0
NOJD 1

MV+EM
S IZE

B IRDS
11.11
12.96
11.11
12.04
12.04
11.11
108

RTE
7.75
7.75
7.62
8.00
8.00
7.12
800

TREC
30.81
31.47
31.87
32.97
35.91
30.20
19033

D OGS
15.37
20.57
15.49
15.49
15.86
15.86
807

W EB
14.44
26.84
14.70
18.39
25.97
15.91
2665

7. Conclusions
We proposed an efficient algorithm for learning mixed mixture models based on the idea of partitioned factorizations.
The key challenge is to consistently match the partitioned
parameters with the hidden components. We provided sufficient conditions to ensure consistency. In addition, we
have also developed a nonnegative approximation to handle
the negative entries in the empirical method of moments estimators, a problem not addressed by several recent tensor
methods. Results on synthetic and real data corroborate
that the proposed approach achieves improved inference
accuracy as well as computational efficiency than state-ofthe-art methods.

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Acknowledgements
Z.T. would like to thank Rong Ge for sharing helpful insights. S.M. would like to thank Lek-Heng Lim for insights. Z.T. would like to acknowledge the support of
grants NSF CNS-1423128, NSF IIS-1423124, and NSF
CNS-1218981. S.M. would like to acknowledge the support of grants NSF IIS-1546331, NSF DMS-1418261,
NSF IIS-1320357, NSF DMS-1045153, and NSF DMS1613261.

References
Anandkumar, Anima, Foster, Dean P, Hsu, Daniel J,
Kakade, Sham M, and kai Liu, Yi. A spectral algorithm for latent dirichlet allocation. In NIPS, pp. 917–
925. 2012.
Anandkumar, Anima, Jain, Prateek, Shi, Yang, and Niranjan, U. N. Tensor vs. matrix methods: Robust tensor
decomposition under block sparse perturbations. In AISTATS, pp. 268–276, 2016.
Anandkumar, Animashree, Ge, Rong, Hsu, Daniel,
Kakade, Sham M., and Telgarsky, Matus. Tensor decompositions for learning latent variable models. JMLR,
15(1):2773–2832, January 2014.
Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent Dirichlet allocation. JMLR, 3:993–1022, 2003.
Brand, M. and Chen, D. Parallel quadratic programming
for image processing. In IEEE International Conference
on Image Processing (ICIP), pp. 2261–2264, September
2011.
Brand, M., Shilpiekandula, V., and Bortoff, S.A. A parallel
quadratic programming algorithm for model predictive
control. In World Congress of the International Federation of Automatic Control (IFAC), volume 18, August
2011.
Chi, Eric C. and Kolda, Tamara G. On tensors, sparsity,
and nonnegative factorizations. SIAM Journal on Matrix
Analysis and Applications, 33(4):1272–1299, December
2012.
Comon, Pierre and Jutten, Christian (eds.). Handbook of
blind source separation : independent component analysis and applications. Communications engineering. Elsevier, 2010.
Dawid, A. P. and Skene, A. M. Maximum likelihood estimation of observer error-rates using the em algorithm.
Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28, 1979.

Donoho, David and Stodden, Victoria. When does nonnegative matrix factorization give a correct decomposition into parts? In NIPS, pp. 1141–1148. 2004.
Erosheva, Elena A. Comparing latent structures of the
grade of membership, rasch, and latent class models.
Psychometrika, 70(4):619–628, 2005.
Finesso, Lorenzo and Spreij, Peter. Nonnegative matrix
factorization and I-divergence alternating minimization.
Linear Algebra and its Applications, 416(2):270–287,
July 2006.
Gower, J.C. and Dijksterhuis, G.B. Procrustes Problems.
Oxford Statistical Science Series. OUP Oxford. ISBN
9780198510581.
Gu, Quanquan, Gui, Huan, and Han, Jiawei. Robust tensor decomposition with gross corruption. In NIPS, pp.
1422–1430. 2014.
Hsu, Daniel and Kakade, Sham M. Learning mixtures of
spherical gaussians: Moment methods and spectral decompositions. In Innovations in Theoretical Computer
Science (ITCS), pp. 11–20, 2013.
Huang, Furong, Niranjan, U. N., Hakeem, Mohammad Umar, Verma, Prateek, and Anandkumar, Animashree. Fast detection of overlapping communities via online tensor methods on gpus.
CoRR,
abs/1309.0787, 2013. URL http://arxiv.org/
abs/1309.0787.
Kim, Jingu, He, Yunlong, and Park, Haesun. Algorithms
for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework.
Journal of Global Optimization, 58(2):285–319, 2014.
Kolda, Tamara G. and Bader, Brett W. Tensor decompositions and applications. SIAM Rev., 51(3):455–500, August 2009.
Kruskal, Joseph B. Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to
arithmetic complexity and statistics. Linear Algebra and
Its Applications, 18(2):95–138, 1977.
Kuleshov, V., Chaganty, A., and Liang, P. Tensor factorization via matrix factorization. In AISTATS, 2015.
Lee, Daniel D. and Seung, H. Sebastian. Algorithms for
non-negative matrix factorization. In NIPS, pp. 556–562,
2001.
Leurgans, S. E., Ross, R. T., and Abel, R. B. A decomposition for three-way arrays. SIAM Journal on Matrix
Analysis and Applications, 14:1064–1083, 1993.

Partitioned Tensor Factorizations for Learning Mixed Membership Models

Nicolò Colombo and Nikos Vlassis, title = Tensor Decomposition via Joint Matrix Schur Decomposition, booktitle = ICML pages = 2820-2828 year = 2016.
Pritchard, Jonathan K., Stephens, Matthew, and Donnelly,
Peter. Inference of population structure using multilocus
genotype data. Genetics, 155(2):945–959, 2000a.
Pritchard, Jonathan K., Stephens, Matthew, Rosenberg,
Noah A., and Donnelly, Peter. Association mapping in
structured populations. The American Journal of Human
Genetics, 67(1):170–181, 2000b.
Rudelson, Mark and Vershynin, Roman. Smallest singular
value of a random rectangular matrix. Comm. Pure Appl.
Math, pp. 1707–1739, 2009.
Schönemann, Peter. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1–10,
1966.
Sha, Fei, Saul, Lawrence K., and Lee, Daniel D. Multiplicative updates for nonnegative quadratic programming in support vector machines. In NIPS, pp. 1065–
1072. 2003.
Shashua, Amnon and Hazan, Tamir. Non-negative tensor
factorization with applications to statistics and computer
vision. In ICML, pp. 792–799, 2005.
Song, Zhao, Woodruff, David P., and Zhang, Huan. Sublinear time orthogonal tensor decomposition. In NIPS,
pp. 793–801, 2016.
Souloumiac, A. Joint diagonalization: is non-orthogonal
always preferable to orthogonal? In 3rd International
Workshop on Computational Advances in Multi-Sensor
Adaptive Processing, pp. 305–308, 2009.
Tan, Zilong and Mukherjee, Sayan. Efficient learning of
graded membership models. CoRR, abs/1702.07933,
2017.
URL http://arxiv.org/abs/1702.
07933.
Wang, Yining, Tung, Hsiao-Yu, Smola, Alexander J, and
Anandkumar, Anima. Fast and guaranteed tensor decomposition via sketching. In NIPS, pp. 991–999. 2015.
Welling, Max and Weber, Markus. Positive tensor factorization. Pattern Recognition Letters, 22:1255–1261,
2001.
Woodbury, Max A., Clive, Jonathan, and Garson, Arthur,
Jr. Mathematical typology: A grade of membership technique for obtaining disease definition. Computers and
Biomedical Research, 11:277–298, 1978.

Zhang, Sheng, Wang, Weihong, Ford, James, and Makedon, Fillia. Learning from incomplete ratings using nonnegative matrix factorization. In In Proceedings of the
6th SIAM Conference on Data Mining (SDM), pp. 549–
553, 1996.
Zhang, Yuchen, Chen, Xi, Zhou, Dengyong, and Jordan,
Michael I. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In NIPS, pp. 1260–
1268, 2014.
Zhao, Shiwen, Engelhardt, Barbara E., Mukherjee, Sayan,
and Dunson, David B. Fast moment estimation for generalized latent dirichlet models. CoRR, abs/1603.05324,
2016.
URL http://arxiv.org/abs/1603.
05324.

