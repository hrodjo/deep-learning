Automatic Discovery of the Statistical Types of Variables in a Dataset
Isabel Valera 1 Zoubin Ghahramani 1 2

Abstract
A common practice in statistics and machine
learning is to assume that the statistical data types
(e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is
known. However, as the availability of realworld data increases, this assumption becomes
too restrictive. Data are often heterogeneous,
complex, and improperly or incompletely documented. Surprisingly, despite their practical
importance, there is still a lack of tools to automatically discover the statistical types of, as
well as appropriate likelihood (noise) models for,
the variables in a dataset. In this paper, we fill
this gap by proposing a Bayesian method, which
accurately discovers the statistical data types in
both synthetic and real data.

1. Introduction
Data analysis problems often involve pre-processing raw
data, which is a tedious and time-demanding task due to
several reasons: i) raw data is often unstructured and largescale; ii) it contains errors and missing values; and iii)
documentation may be incomplete or not available. As a
consequence, as the availability of data increases, so does
the interest of the data science community to automate this
process. In particular, there are a growing body of work
which focuses on automating the different stages of data
pre-processing, including data cleaning (Hellerstein, 2008),
data wrangling (Kandel et al., 2011) and data integration
and fusion (Dong & Srivastava, 2013).
The outcome of data pre-processing is commonly a structured dataset, in which the objects are described by a set of
attributes. However, before being able to proceed with the
predictive analytics step of the data analysis process, the
data scientist often needs to identify which kind of variables (i.e., real-values, categorical, ordinal, etc.) these attributes represent. This labeling of the data is necessary
to select the appropriate machine learning approach to ex1

University of Cambridge, Cambridge, United Kingdom;
Uber AI Labs, San Francisco, California, USA. Correspondence
to: Isabel Valera <miv24@cam.ac.uk>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

plore, find patterns or make predictions on the data. As an
example, a prediction task is solved differently depending
on the kind of data to be predicted—e.g., while prediction
on categorical variables is usually formulated as a classification task, in the case of ordinal variables it is formulated
as an ordinal regression problem (Agresti, 2010). Moreover, different data types should be pre-processed and input
differently in the predictive tool—e.g., categorical inputs
are often transformed into as many binary inputs (which
state whether the object belongs to a category or not) as
number of categories; positive real inputs might be logtransformed, etc.
Information on the statistical data types in a dataset becomes particularly important in the context of statistical
machine learning (Breiman, 2001), where the choice of a
likelihood model appears as a main assumption. Although
extensive work has focused on model selection (Ando,
2010; Burnham & Anderson, 2003), the likelihood model
is usually assumed to be known and fixed. As an example,
a common approach is to model continuous data as Gaussian variables, and discrete data as categorical variables.
However, while extensive work has shown the advantages
of capturing the statistical properties of the observed data in
the likelihood model (Chu & Ghahramani, 2005a; Schmidt
et al., 2009; Hilbe, 2011; Valera & Ghahramani, 2014),
there still exists a lack of tools to automatically perform
likelihood model selection, or equivalently to discover the
most plausible statistical type of the variables in the data,
directly from the data.
In this work, we aim to fill this gap by proposing a general and scalable Bayesian method to solve this task. The
proposed method exploits the latent structure in the data to
automatically distinguish among real-valued, positive realvalued and interval data as types of continuous variables,
and among categorical, ordinal and count data as types of
discrete variables. The proposed method is based on probabilistic modeling and exploits the following key ideas:
i) There exists a latent structure in the data that capture
the statistical dependencies among the different objects and attributes in the dataset. Here, as in standard
latent feature modeling, we assume that we can capture this structure by a low-rank representation, such
that conditioning on it, the likelihood model factorizes
for both number of objects and attributes.
ii) The observation model for each attribute can be ex-

Automatic Discovery of the Statistical Types of Variables in a Dataset

pressed as a mixture of likelihood models, one per
each considered data type, where the inferred weight
associated to a likelihood model captures the probability of the attribute belonging to the corresponding
data type.
We derive an efficient MCMC inference algorithm to
jointly infer both the low-rank representation and the
weight of each likelihood model for each attribute in the
observed data. Our experimental results show that the proposed method accurately discovers the true data type of the
variables in a dataset, and by doing so, it fits the data substantially better than modeling continuous data as Gaussian
variables and discrete data as categorical variables.

2. Problem Statement
As stated above, the outcome of the pre-processing step
of data analysis is a structured dataset, in which a set of
objects are defined by a set of attributes, and our objective is to automatically discover which type of variables
these attributes correspond to. In order to distinguish between discrete and continuous variables, we can apply simple logic rules, e.g.. count the number of unique values that
the attribute takes and how many times we observe these attributes. Moreover, binary variables are invariant to the labeling of the categories, and therefore, both categorical and
ordinal models are equivalent in this case. However, distinguishing among different types of discrete and continuous
variables cannot be easily solved using simple heuristics.
In the context of continuous variables, given the finite size
of observed datasets, it is complicated to identify whether
a variable may take values in the entire real line, or only on
an interval of it, e.g., (0, ∞) or (θL , θH ). In other words,
due to the finite observation sample, we cannot distinguish
whether the data distribution has an infinite tail that we
have not observed, or its support is limited to an interval.
As an illustrative example, Figures 2(d)&(f) in Section 4
show two data distributions that, although at a first sight
look similar, correspond respectively to a Beta variable,
which therefore takes values in the interval (0, 1), and a
gamma variable, which takes values in (0, ∞).
In the context of discrete data, it is impossible to tell the difference between categorical and ordinal variables in isolation. The presence of an order in the data only makes sense
given a context. As an example, while colors in M&Ms
usually do not present an order, colors in a traffic light
clearly do. Similarly, we cannot easily distinguish between
ordinal data (which take values in a finite ordered set) and
count data (which take values in an infinite ordered set with
equidistant values) due to two main reasons. First, similarly
to continuous variables, since datasets contain a finite number of examples, it is difficult to tell whether we have observed the finite set of possible values of a variable, or simply a finite subsample of an infinite set. Second, we would

need access to exact information on whether its consecutive values are equidistant or not, however, this information
depends on how the data have been gathered. For example, an attribute that collects information on “frequency of
an action” will correspond to an ordinal variable if its categories belong to, e.g., {“never”, “sometimes”, “usually”,
“often”}, and to a count variable if it takes values in {‘‘0
times per week”, “1 time per week”, . . . }.
Previous work (Hernandez-Lobato et al., 2014) proposed to
distinguish between categorical and ordinal data by comparing the model evidence and the predictive test loglikelihood of ordinal and categorical models. However, this
approach can be only used to distinguish between ordinal
and categorical data, and it does so by assuming that it has
access to a real-valued variable that contains information
about the presence of an ordering in the observed discrete
(ordinal or categorical) variable. As a consequence, it cannot be easily generalizable to label the data type of all the
variables (or attributes) in a dataset. In contrast, in this paper we proposed a general method that allows us to distinguish among real-valued, positive real-valued and interval
data as types of continuous variables, and among categorical, ordinal and count data as types of discrete variables.
Moreover, the general framework we present can be readily
extended to other data types as needed.

3. Methodology
In this section, we introduce a Bayesian method to determine the statistical type of variable that corresponds to each
of the attributes describing the objects in an observation
matrix X. In particular, we propose a probabilistic model,
in which we assume that there exists a low-rank representation of the data that captures its latent structure, and therefore, the statistical dependencies among its objects and attributes. In detail, we consider that each observation xdn
can be explained by a K-length vector of latent variables
zn = [zn1 , . . . , znK ] associated to the n-th object and a
weighting vector bd = [bd1 , . . . , bdK ] (with K being the
number of latent variables), whose elements bdk weight the
contribution of k-th the latent feature to the d-th attribute
in X. Then, given the latent low-rank representation of the
data, the attributes describing the objects in a dataset are
assumed to be independent, i.e.,
p(X|Z, {bd }D
d=1 ) =

D
Y
d=1

p(xd |Z, bd ),

where we gather the latent feature vectors zn in a N × K
matrix Z. For convenience, here zn is a K-length row vector, while bd is a K-length column vector. The above model
resembles standard latent feature models (Salakhutdinov &
Mnih, 2007; Griffiths & Ghahramani, 2011), which assume
known and fixed likelihood models p(xd |Z, bd ). In contrast, in this paper we aim to infer the statistical data type

Automatic Discovery of the Statistical Types of Variables in a Dataset

(or equivalently, the likelihood model) that better captures
the distribution of each attribute in X. To this end, here we
assume that the likelihood model of the d-th attribute in X
is a mixture of likelihood functions such that
X
w`d p` (xd |Z, bd` ),
p(xd |Z, {bd` }`∈Ld ) =
`∈Ld

where Ld is the set of possible types of variables (or equivalently, likelihood models) to be considered for this attribute, and the weight w`d captures the probability of the
likelihood function ` in the d-th attribute of the observation matrix X. Note that, the P
above expression is a valid
d
likelihood model as long as
`∈Ld w` = 1 and each
d
d
d
p` (x |Z, b` , Ψ` ) is a normalized probability density function or probability mass function for, respectively, continuous and discrete variables. Hence, under the proposed
model, which is is illustrated in Figure 1a, the likelihood
factorizes as
D X
Y
w`d p` (xd |Z, bd` ).
p(X|Z, {bd` }) =
(1)
d=1 `∈Ld

We place a Dirichlet prior distribution on the likelihood
weights wd = [w`d ]`∈Ld , and similarly to (Salakhutdinov
& Mnih, 2007), assume that both the latent feature vectors
zn and the weighting vectors bdj are Gaussian distributed
with zero mean and covariance matrices σz2 I and σb2 I, respectively. Here, I denotes the identity matrix of size equal
to the number of latent features K.
Moreover, we consider the following types of data for, respectively, continuous and discrete variables:
• Continuous variables:
1. Real-valued data, which takes values in the real
line, i.e., xdn ∈ <.
2. Positive real-valued data, which takes values in
the positive real line, i.e., xdn ∈ <+ .
3. Interval data, which takes values in an interval of
the real line, i.e., xdn ∈ (θL , θH ), where θL , θH ∈
< and θL ≤ θH .
• Discrete variables:
1. Categorical data, which takes values in a finite
unordered set, e.g., xdn ∈ {‘blue’, ‘red’, ‘black’}.
2. Ordinal data, which takes values in a finite ordered set, e.g., xdn ∈ {‘never’, ‘sometimes’, ‘often’, ‘usually’, ‘always’}.
3. Count data, which takes values in the natural
numbers, i.e., xdn ∈ {0, . . . , ∞}.
We remark that the main goal of this paper is to determine
the types of variables that better capture each attribute in
the observed matrix X, which in our method translates to
inferring the likelihood weights wd . However, solving this
inference problem in an efficient way is a challenging task
for several reasons. First, we need to jointly infer all the latent variables in the model, i.e., the low-rank representation

` 2 Ld
Z

x

d

w

wd

bd`

sdn

d
yn`

xdn

d

zn
Bd
d = 1, . . . , D

d = 1, . . . , D
n = 1, . . . , N

(a) Proposed model
(b) Alternative representation
Figure 1. Model illustration.

of the data (which includes the latent feature matrix Z and
the corresponding weighting vectors {bd` }{`∈Ld |d=1,...,D} )
and the likelihood weights {wd }D
d=1 . Second, we need to
do so given a heterogeneous (and non-conjugate) observation model, which combines D different likelihood models, corresponding each of them to a mixture of likelihood
functions and coupled through the latent feature matrix Z.
Additionally, these likelihood functions do not only correspond to either a probability density function or a probability mass function depending on whether we are dealing with a continuous or a discrete variable, but also each
mixture combines likelihood functions with different supports. For example, while real-valued data lead to a likelihood function with the real line as support, interval data
only accounts for a segment of the real line. Similarly, both
categorical and ordinal data assume a finite support, while
count data requires an infinite-support likelihood function.
In order to allow for efficient inference, we exploit the key
idea in (Valera & Ghahramani, 2014) to propose an alternative and equivalent model representation (shown in Figure 1b), which efficiently deal with heterogeneous likelihood functions. In this alternative model representation,
we include for each observation xdn as many Gaussian varid
ables (or pseudo-observations) yn`
∼ N (zn bd` , σy2 ) as the
number of likelihood functions in Ld , and assume that
d
there exists a transformation function over the variables yn`
which maps the real line < into the support of the likelihood
function `, Ω` , i.e.,
f` : <
y

7→ Ω`
.
→ x

(2)

Note that, if we condition on the pseudo-observations the
latent variable model behaves as a conjugate Gaussian
model, allowing for efficient inference of the latent feature matrix Z and the weighting vectors {bd` }. Additionally, we include a latent multinomial variable sdn ∼
M ultinomial(wd ) which indicates the type of variable
(or likelihood function) that the observation xdn belongs to.
Then, given sdn , we can obtain the observation xdn as
d
d
xdn = fsdn (yns
d + un ),
n

(3)

where udn ∼ N (0, σu2 ) is a noise variable. We gather the
likelihood assignments sdn in a N × D matrix S.

Automatic Discovery of the Statistical Types of Variables in a Dataset

3.1. Likelihood functions
In this section, we provide the set of transformations to map
d
from the Gaussian pseudo-observations yn`
into the types
of data defined above, specifying also the six likelihood
functions that our method will account for.
3.1.1. C ONTINUOUS VARIABLES
In the case of continuous variables, we assume that the
mapping functions f` are continuous invertible and differentiable functions, such that we can obtain corresponding likelihood function (after integrating out the pseudod
observation yn`
) as


 d −1 d 
1


f
(x
)
p` (xdn |zn , bd` , sdn = `) = q
n 
 d `
2π(σy2 + σu2 ) dxn


1
−1 d
d 2
× exp −
(f
(x
)
−
z
b
)
,
n `
n
2(σy2 + σu2 ) `
where f`−1 is the inverse function of the transformation
f` (·), i.e., f`−1 (f` (v)) = v. Next, we provide examples of
mapping functions that allow us to account for real-valued,
positive real-valued, and interval data.
1. Real-valued Data. In order to obtain real-valued observations, i.e., xdn ∈ <, we need a transformation over
ynd that maps from the real numbers to the real numbers,
i.e., f< : < → <. The simplest case is to assume that
x = f< (y + u) = y + u, and therefore, each observation
is distributed as xdn ∼ N (zn bd< , σy2 + σu2 ). Nevertheless,
other mapping functions can be used, e.g., we will use in
our experiments the transformation
x = f< (y + u) = w(y + u) + µ,
where w and µ are parameters allowing attribute rescaling,
and tuneable by the user.
2. Positive Real-valued Data. As an example of a function that maps from the real numbers to the positive real
numbers, i.e., f<+ : < 7→ <+ , we consider
x = f<+ (y + u) = log(1 + exp(w(y + u))).
where w allows attribute rescaling.
3. Interval Data. As an example of a function the maps
from the real numbers into the interval (θL , θH ), i.e., f<+ :
< 7→ (θL , θH ), we consider the transformation
x = fInt (y + u) =

θH − θL
+ θL ,
1 + exp(−w(y + u))

where w, θL and θH are user hyperparameters.1
In our experiments, we assume θL = arg minn (xdn ) −  and
θH = arg maxn (xdn )+, where  → 0 is a user hyper-parameter.
We set the rescaling parameter w = 2/ max(xd ) for the three
continuous data types.
1

3.1.2. D ISCRETE VARIABLES
1. Categorical Data. Now we account for categorical observations, i.e., each observation xdn can take values in the
unordered index set {1, . . . , Rd }. Hence, assuming a multinomial probit model, we can write
x = fcat (y) = arg max y(r),
r∈{1,...,Rd }

where in this case there are as many pseudo-observations as
number of categories and each pseudo-observation can be
d
sampled as yncat
(r) ∼ N (zn bdcat (r), σy2 ) where bdcat (r)
denotes the K-length weighting vector, which weights the
influence the latent features for a categorical observation
xdn taking value r. Note that, under this likelihood model,
d
we need one pseudo-observation yncat
(r) and a weighting
d
vector bcat (r) for each possible value of the observation
r ∈ {1, . . . , Rd }.
Under the multinomial probit model, we can obtain the
probability of xdn taking value r ∈ {1, . . . , Rd } as (Girolami & Rogers, 2005)
pcat (x = r|zn , bdcat , sdn = cat)
" R
#
d


Y
= Ep(u)
Φ u + zn (bdcat (r) − bdcat (r0 )) ,
r 0 =1
r 0 6=r

where Φ(·) denotes the cumulative density function of the
standard normal distribution and Ep(u) [·] denotes expectation with respect to the distribution p(u) = N (0, σy2 ).
2. Ordinal Data. Consider ordinal data, in which each element xdn takes values in the ordered index set {1, . . . , Rd }.
Then, assuming an ordered probit model, we can write

d
1
if ynord
≤ θ1d



d
 2
if θ1d < ynord
≤ θ2d
d
xdn = ford (ynord
)=
..

.



d
d
< ynord
Rd
if θR
d −1
d
where again ynord
is Gaussian distributed with mean
d
zn bord and variance σy2 , and θrd for r ∈ {1, . . . , Rd − 1}
are the thresholds that divide the real line into Rd regions. We assume the thresholds θrd are sequentially
generated from the truncated Gaussian distribution θrd ∼
d
d
T N (0, σθ2 , θr−1
, ∞), where θ0d = −∞ and θR
= +∞.
d
As opposed to the categorical case, now we have a unique
d
weighting vector bdord and a unique Gaussian variable ynord
for each observation xdn , and the value of xdn is determined
d
by the region in which ynord
falls.

Under the ordered probit model (Chu & Ghahramani,
2005b), the probability of each element xdn taking value
r ∈ {1, . . . , Rd } can be written as
pord (xdn = r|zn , bdord , sdn = ord)
!
!
d
θr−1
− zn bdord
θrd − zn bdord
=Φ
−Φ
.
σy
σy

Automatic Discovery of the Statistical Types of Variables in a Dataset

3. Count Data. In count data each observation xdn takes
non-negative integer values, i.e., xdn ∈ {0, . . . , ∞}. Then,
we assume
xdn = fcount (ynd ) = bg(ynd )c,

Algorithm 1 Inference Algorithm.

Input: X
d
Initialize: S, {bd` } and {yn`
}
1: for each iteration do
d
2:
Update Z given {bd` } and {yn`
}.
where bvc returns the floor of v, that is the largest integer
3: for d = 1, . . . , D do
that does not exceed v, and g : < → <+ is a monotonic
4:
for ` ∈ Ld do
5:
for n = 1, . . . , N do
differentiable function, in our experiments g(y) = log(1 +
d
6:
Sample {yn`
} given xdn , Z, {bd` } and sdn .
exp(wy)). We can thus write the likelihood function as
7:
end for
d
pcount (xdn |zn , bdord , sdn = count) =
8:
Sample {bd` } given Z and {yn`
}.
!
! 9:
for
n
=
1,
.
.
.
,
N
do
−1 d
d
−1 d
d
g (xn ) − zn bcount
g (xn + 1) − zn bcount
10:
Sample sdn given xdn , Z and {bd` }.
−Φ
Φ
11:
end
for
σy
σy
12:
end for
13:
Sample wd given S.
where g −1 : <+ → < is the inverse function of the trans14:
end for
formation g(·).
15: end for
Output: Likelihood weights wd .
3.2. Inference Algorithm

Here, we exploit the model representation in Figure 1b to
derive an efficient inference algorithm that allows us to infer all the latent variables in the model, providing as output
the likelihood weights wd , which determine the probability of the d-th attribute in X belonging to each of the above
data types. Algorithm 1 summarizes the inference.
Sampling low-rank decomposition.
In order to
sample the latent feature matrix Z and the associated weighting vectors {bd` }, we condition on the
pseudo-observations such that we can efficiently
 sample the feature vectors as zn ∼ N µdz , Σz , where
−1
P
P
d
d d >
−2
I
and µz =
Σz =
b
(b
)
+
σ
d
z
` `
P P d=1 `∈L 
N
d d
Σz
n
`∈Ld b` yn` . Note that this step involves a
matrix inversion of size K (the number of latent features)
per iteration of the algorithm. Similarly, the weighting vecd
tors can be sampled as bd` ∼ N µ
Σb =
` , Σb , where

PN > d 
−2 −1
d
−2 >
and µ` = Σb
σy Z Z + σb I
n zn yn` . Since
Σb is shared for all {bd` } with ` ∈ Ld and d = 1 . . . , D,
this step also involves one matrix inversions of size K per
iteration of the algorithm.
Sampling pseudo-observations. Given the low-rank decomposition and the likelihood assignments S, we can samd
from its prior distribution
ple each pseudo-observation yn`
d
if sn 6= `, and from its posterior distribution if sdn = `.
In the case of continuous variables, the posterior distribution of the pseudo-observation can be obtained as
 


d
d
d d
d
2
p(yn` |xn , zn , b` , sn = `) = N yn µ̂y , σ̂y ,
where µ̂y =

−1
1
1
+
.
σ2
σ2
y



(zn bd
`)
σy2

+

f`−1 (xd
n)
2
σu



σ̂y2 , and σ̂y2

=

u

In the case of discrete variables, the posterior distribution
of the pseudo-observation can be computed as follows.

1. For categorical observations:
d
p(yncat
(r)|xdn = T, zn , bdcat , sdn = cat)

d
(j)), ∞), r = T
T N (zn bdcat (r), σy2 , maxj6=r (yncat
=
d
2
d
T N (zn bcat (r), σy , −∞, yncat (T )), r 6= T
d
In words, if xdn = T = r we sample ynr
from a truncated Normal distribution with mean
zn bdcat (r), variance σy2 and truncated on the left by
d
(j)). Otherwise, we sample from a
maxj6=r (yncat
truncated Gaussian (with same mean and variance)
d
truncated on the right by yncat
(r) with r = xdn . Note
d
that sampling from the variables yncat
(r) corresponds
to solve a multinomial probit regression problem.
Hence, to achieve identifiability we assume, without
loss of generality, that the regression function fRd (zn )
is identically zero, and thus, we fix bd` (Rd ) = 0.
2. For ordinal observations:
d
p(ynord
|xdn = r, zn , bdord , sdn = ord)
d
d
= T N (ynord
|zn bdord , σy2 , θr−1
, θrd ).

Note that in this case, we also need to sample the values for the thresholds θrd with r = 1, . . . , Rd − 1 as
d
p(θrd |ynord
) = T N (θrd |0, σθ2 , θmin , θmax ),

d
d
where θmin = max(θr−1
, maxn (ynord
|xdn = r)) and
d
d
d
θmax = min(θr , minn (ynord |xn = r + 1)). In words,
d
d
each θrd is constrained to be between θr−1
and θr+1
,
d
as well as to ensure that the pseudo-observations ynord
associated to the observations xdn = r and xdn = r + 1
fall respectively at the left and at the right side of θrd .
Since in this ordinal regression problem the thresholds
d
{θr }R
r=1 are unknown, we set θ1 to a fixed value in
order to achieve identifiability.
3. For count observations:
d
p(yncount
|xdn , zn , bdcount , sdn = count)

d
= T N (yncount
|zn bdcount , σy2 , g −1 (xdn ), g −1 (xdn + 1)),

Weights w

d

Automatic Discovery of the Statistical Types of Variables in a Dataset
1

250

400

0.8

200

300

0.6
0.4

Real
Positive
Interval

200
100

Beta(0.5,0.5) Beta(0.5,1)

Beta(0.5,3)

0

0

d

Weights w

0.6

0.8

1

Real
Positive
Interval

0.4

400

!(3,1)

!(5,1)

Real
Positive
Interval

1
0.8

0

0.2

0.4

0.6

0.8

1

0

400

250

300

200

0

0

2

4

6

8

0

(f) Γ(1, 1)

5

10

15

0

1

250
200
150

0.4

100

100

100

0.2

50

50

50

-20

0

20

40

(j) N (0, 10)

0
-20

0

5

10

15

(h) Γ(5, 1)

250

0
-40

0

(g) Γ(3, 1)

250

150

(i) Real-valued Data

0.8

50
0

200

N(0,100)

0.6

100
100

150

N(10,10)

0.4

150

200

N(0,10)

0.2

(d) Beta(0.5, 3)

0.6

0

0

200
200

0.2
!(1,1)

0

(c) Beta(0.5, 1)

600

(e) Positive Real-valued Data
d

0.4

(b) Beta(0.5, 0.5)

1

Weights w

0.2

0.8
0.6

200

100

50

(a) Interval Data

0

400

150

0.2
0

600

20

(k) N (10, 10)

40

0
-400

-200

0

200

400

(l) N (10, 100)

Figure 2. [Synthetic Continuous Data] The first column shows the distribution of the inferred likelihood weights wd when the ground
truth data is (a) interval, (e) positive real-valued, and (i) real-valued. The remaining columns show example histograms of the datasets.

where g −1 : <+ → < is the inverse function of g, i.e.,
d
g −1 (g(y)) = y. Therefore, yncount
is sampled from a
Gaussian truncated on the left by g −1 (xdn ) and on the
right by g −1 (xdn + 1).
Sampling likelihood assignments. In order to improve
the mixing properties of the sampler, when sampling {sdn }
d
we integrate out the pseudo-observations {yn`
}. Then, the
posterior probability of each observation being assigned to
the likelihood model ` can be obtained as
w`d p` (xdn |zn , bd` )
p(sdn = `|wd , Z, {bd` }) = P
.
d
d
d
0
`0 ∈Ld w`0 p` (xn |zn , b`0 )
Sampling likelihood weights. We assume the prior distribution on the vector wd to be a Dirichlet distribution with
parameters {α` }`∈Ld . Then, by conjugacy, we can sample wd given the likelihood assignments
P S from a Dirichlet
distribution with parameters {α` + n δ(sdn == `)}`∈Ld .
Scalability. The overall complexity of Algorithm 1 is
O(N DLmax + K 3 ) per iteration, where N is the number
of objects, D the number of attributes, Lmax the maximum
number of considered data types (or likelihood models) and
K the size of the low-rank representation. In all of our experiments, we ran the MCMC for 5000 iterations, which
lasted 10-100 minutes depending on the dataset.

4. Evaluation
4.1. Experiments on synthetic data
In this section, we show that the proposed method is able
to accurately discover the true statistical type of variables
in synthetic datasets, where we have perfect knowledge of
the distribution from which the data have been generated.

First, we focus on continuous variables by generating univariate datasets with 1, 000 observations sampled from a
known probability density function, which corresponds to
i) a Gaussian distribution when considering real-valued
data; ii) a Gamma distribution for positive real-valued data;
and iii) a (scaled) Beta distribution for interval data lying
in the interval (0, θL ) where θL takes values 0.1, 1 or 100.
Figure 2 shows the distribution, by means of a boxplot,2 of
the inferred likelihood weights wd for 10 independent simulations of Algorithm 1 with 500 iterations on 10 independent datasets generated with the parameters detailed in the
figure. Reassuringly we observe that the proposed method
identifies interval data as the most likely type of data for the
three considered Beta distributions; moreover, as the tail of
the Beta distribution increases, so does the weight given to
the positive real-valued variables. This effect can be explained by the finite size of the dataset, since it is hard to
determine whether the variable is limited to values smaller
than θL , or we simply have not observed them in the finite
set of observations. A similar effect occurs when applying
our method to data sampled from Gamma (Figure 2(e)-(h))
and Gaussian (Figure 2(i)-(l)) distributions. Here, we observe that in addition to, respectively, positive real-valued
and real-valued data types, our model finds that the variable
may also be of interval data type. This effect is larger for
Gaussian variables, since in this example the Gaussian is a
more heavy-tailed distribution than the Gamma.
2
In a boxplot, the central mark is the median, the edges of the
box are the 25th and 75th percentiles, the whiskers extend to the
10th and 90th percentiles.

Weights wd

Weights wd

1
0.8
0.6
0.4
0.2
0

3 5 7

1
0.8

Weight wd

Weight wd

Automatic Discovery of the Statistical Types of Variables in a Dataset
Table 1. Information on real datasets
1
Categorical
Categorical
Dataset
N
D # of Discrete # of Binary
0.8
Ordinal
Ordinal
Count
Count
0.6
Abalone
4, 177
9
2
0
0.4
Adult
32, 561 15
12
2
0.2
Chess
28, 056
7
7
0
0
9 3 5 7 9 3 5 7 9
3 5 7 9 3 5 7 9 3 5 7 9
Dermatology
366
35
35
0
# of Categories
# of Categories
German
1000
21
20
4
1
Categorical
Student
395
33
33
13
0.8
Ordinal
Count
0.6
Wine
177
14
2
0

0.6
0.4
0.2
0

1

3

5 1

3

5 1

3

0.4
0.2
0

5

1

# of Latent Variables (K)

3

5 1

3

5 1

3

5

# of Latent Variables (K)

(a) Categorical Data

(b) Ordinal Data

Weight wd

1

Categorical
Ordinal
Count

0.8
0.6
0.4
0.2
0

1

3

5 1

3

5 1

3

5

# of Latent Variables (K)

(c) Count Data
Figure 3. [Synthetic Discrete Data] Distribution of the inferred
likelihood weights wd when the ground truth data is (a) categorical, (b) ordinal, and (c) count data. For categorical and ordinal
data, we plot the likelihood weight distribution with respect to
both the number of categories in the data and the model complexity K, and for count data with respect to K.

Next, we study whether the proposed model is able to disambiguate among different discrete types of variables, particularly, among categorical, ordinal and count data. To
this end, we generate three types of datasets of size 1, 000.
In the first type we account for categorical data by sampling a multinomial variable with R categories, where the
probability of the categories is sampled from a Dirichlet
distribution. Then, for each category we sample a multidimensional Gaussian centroid that corresponds to the mean
of the multivariate Gaussian observations that complete the
dataset. To account for ordinal observations, we first sample the first variable in our dataset from a uniform distribution in the interval (0, R), which we randomly divide into
R categories that correspond to the ordinal variable in our
dataset. Finally, to account for count data we first generate a Gamma variable sampled from Γ(α, α/4), and then
generate the counting variable in the dataset by taking the
floor of the Gamma variable. For both categorical and ordinal data, we generate 10 independent datasets for each
value of the number of categories R ∈ {3, . . . , 10}, and for
count data we generate another 10 datasets for each value
of α ∈ {2, . . . , 8}. Figure 3 summarizes the likelihood
weights obtained for each type of datasets (i.e., for each
type of discrete variable) after running on each dataset 10
independent simulations of Algorithm 1 with 500 iterations
for different model complexity values, i.e., for different
numbers of latent feature variables K = 1, . . . , 10. In this
figure we can observe that we can accurately discover the
true type of discrete variable robustly and independently
of the assumed model complexity K. We also observe on

the top row of Figure 3(a)-(b) that i) as the number of categories R in the discrete variable decreases, the harder is
to distinguish between ordinal and categorical data, i.e., to
find out whether the data takes values in a ordered set or
in an unordered set; and ii) as R in ordinal data increases,
the ordinal variable is more likely to be identified as count
data. Both of these effects are intuitively sensible.
4.2. Experiments on real data
In this section, we evaluate the performance of the proposed method on seven real datasets collected from the
UCI machine learning repository (Lichman, 2013). Table 1
summarizes theses datasets by providing the number of objects and attributes in the dataset, as well as how many of
these attributes are discrete.
In order to quantitatively evaluate the performance of the
proposed method, we select at random 10% of the observations in each dataset as a held-out set and compare the predictive performance, in terms of average test log-likelihood
per observation, of our method with a baseline method.
The baseline method corresponds to a latent feature model
in which all the continuous variables are modeled as realvalued data and the discrete variables as categorical data.
Figure 4 shows the obtained results for our method (solid
line) and the baseline (dashed line) for several values of the
model complexity (i.e., the number of latent features K)
averaged over 10 independent runs of the corresponding
inference algorithms. Here, we observe that i) both methods provide robust results with respect to the number of
variables K; and ii) our method clearly outperforms the
baseline in all the datasets, except for the Student dataset
where the baseline performs slightly better. In other words,
this figure shows that by taking into account the uncertainty
in the statistical types of the variables, we provide a better
fitting of the data.
Additionally, Table 2 shows the list of (non-binary) attributes in the Adult and the German datasets together with
the data types with larger inferred likelihood weights,3 i.e.,
the discovered statistical data types. Here, the number in
parenthesis corresponds to the observed number of categories in discrete data. The very heterogeneous nature of
these datasets explains the substantial gain observed in Figure 4. Moreover, Table 2 shows some expected results, e.g.,
3
In cases in which two data types present very similar likelihood weights (< 10% difference), we display both of them.

Automatic Discovery of the Statistical Types of Variables in a Dataset
Test log-likelihood

Abalone

Adult

Chess

Dermatology

0

0

-1

-0.5

German

Student

Wine

200

wRe = 0.99, wRe+ = 0.00, wInt = 0.00

150

150

wRe = 0.07, wRe+ = 0.92, wInt = 0.01

100

100

-2

-1

-3

-1.5

-4

2

4

6

8

Number of Latent Variables (K)

10

-2

50

50
0

2

4

6

8

0

0.2

0.4

0.6

10

Number of Latent Variables (K)

Figure 4. [Real Data] Comparison between our model (solid) and
the baseline (dashed) in terms of average test log-likelihood per
observation evaluated on a held-out set containing 10% of the observations in each dataset.
Table 2. Inferred data types.
Adult
German
Attribute Type
Attribute
age (74) ord.
status account (4)
duration (69)
workclass (8) cat.
final weight positive
credit hist. (5)
purpose (10)
education (16) cat.
education num. (16) cat.
amount
marital status (7) cat.
savings (5)
occupation (14) cat./ord.
installment (5)
personal status (4)
relationship (6) ord.
debtors (4)
race (5) cat.
sex (2) binary
residence (3)
property (4)
capital-gain real
capital-loss real
age (57)
hours per week (99) cat./ord.
plans (3)
housing (3)
native-country (41) ord.
# credits (4)
job (4)

0.8

1

In order to better understand these results, we show the histograms of several variables in these datasets and the associated inferred likelihood weights. Figure 5 shows the
histograms of two continuous variables, length and weight
of the Abalone dataset, which take only positive real values, but are assigned to different data types (respectively, to
real-valued and positive real-valued data). This can be explained by the fact that, while the distribution of the length
presents large tails, the distribution of the weight is clearly
truncated at zero. Additionally, Figure 6(a)-(b) shows two
discrete variables, the duration (in months) and the age in
German data, which based on the documentation are expected to be count data. However, our model assigns the
duration to ordinal data. This result can be explained by the
irregular distribution that this variable has. In count data
the distance between every two consecutive values should
be roughly the same (there is the same distance from “1
pen” to “2 pens” as from “2 pens” to “3 pens”, that is 1
pen), resulting therefore in smooth probability mass functions. We found in Figure 6(c)-(d) that while the number of
credits and the job variables can be a priori thought as re-

1

2

3

Figure 5. [Abalone dataset]
wcat = 0.22, word = 0.56, wcount = 0.22

200
150

wcat = 0.16, word = 0.22, wcount = 0.61

60
40

100

0

marital status and race are identified as categorical, while
the age is of count data type for both datasets. However,
other results might seem surprising. For example, the duration (in months), which one would expect it to be count
data, is identified as ordinal; or the a priori categorical attributes native country and job are inferred to be ordinal.

0

(b) Weight (grams)

20

50

Type
cat.
ord.
cat./ord.
cat./ord.
interval
ord.
cat./ord.
cat.
ord.
cat.
cat./ord.
count
cat.
ord.
ord.
ord.

0

(a) Length (mm)

0

20

40

60

80

0

(a) Duration (months)
wcat = 0.29, word = 0.54, wcount = 0.16

600

600

400

400

200

200

2

3

4

40

60

wcat = 0.31, word = 0.54, wcount = 0.15

800

1

20

(b) Age

800

0

0

0

non-resident resident

skilled

highly qualified

(c) #of Credits
(d) Job
Figure 6. [German dataset]

spectively count and categorical data, they are both inferred
to be ordinal data. In the case of the number of credits,
this can be explained by the small (finite) number of values
that the variable takes, while in the case of the job, this assignment can be explained by the labels of its categories,
i.e., {unskilled non-resident, unskilled resident, skilled employee and highly qualified employee}, which clearly represent an ordered set.
From these results, we can conclude that i) our model accurately discovers the true statistical type of the data, which
might not be easily extracted from its documentation; and
by doing so, ii) it provides a better fit of the data. Moreover,
apparent failures are in fact sensible when data histograms
are carefully examined.

5. Conclusions
In this paper, we presented the first approach to automatically discover the statistical types of the variables in a
dataset. Our experiments showed that the proposed approach accurately infers the data type, or equivalently likelihood model, that best fits the data.
Our work opens many interesting avenues for future work.
For example, it would be interesting to extend the proposed method to account for other data types. We would
like to include directional data, also called circular data,
which arise in a multitude of data-modelling contexts ranging from robotics to the social sciences (Navarro et al.,
2016). Moreover, since the proposed method can be seen
as a likelihood selection method, it would be interesting to
study how to incorporate our framework in any statistical
machine learning tool, where the likelihood model, instead
of being fixed a priori, would be inferred directly from the
data jointly with the rest of the model parameters.

Automatic Discovery of the Statistical Types of Variables in a Dataset

Acknowledgement
Isabel Valera acknowledges her Humboldt Research Fellowship for Postdoctoral Researchers, which funded this
research during her stay at the Max Planck Institute for
Software Systems. Zoubin Ghahramani acknowledges
support from the Alan Turing Institute (EPSRC Grant
EP/N510129/1) and EPSRC Grant EP/N014162/1, and donations from Google and Microsoft Research.
The code implementing the proposed method, as
well as the scripts that reproduce the experiments
presented in the paper, are publicly available at:
https://github.com/ivaleraM/DataTypes

References
Agresti, A. Analysis of ordinal categorical data, volume
656. John Wiley & Sons, 2010.
Ando, T. Bayesian model selection and statistical modeling. CRC Press, 2010.
Breiman, L. Statistical modeling: The two cultures (with
comments and a rejoinder by the author). Statistical science, 16(3):199–231, 2001.
Burnham, K. P and Anderson, D. R. Model selection and
multimodel inference: a practical information-theoretic
approach. Springer Science & Business Media, 2003.
Chu, W. and Ghahramani, A. Gaussian processes for ordinal regression. Journal of Machine Learning Research,
6(Jul):1019–1041, 2005a.
Chu, W. and Ghahramani, Z. Gaussian processes for ordinal regression. J. Mach. Learn. Res., 6:1019–1041,
December 2005b. ISSN 1532-4435.
Dong, X. Luna and Srivastava, D. Big data integration.
In Data Engineering (ICDE), 2013 IEEE 29th International Conference on, pp. 1245–1248. IEEE, 2013.

Girolami, M. and Rogers, S. Variational Bayesian multinomial probit regression with Gaussian process priors.
Neural Computation, 18:2006, 2005.
Griffiths, T. L. and Ghahramani, Z. The Indian buffet process: an introduction and review. Journal of Machine
Learning Research, 12:1185–1224, 2011.
Hellerstein, J. M. Quantitative data cleaning for large
databases, 2008.
Hernandez-Lobato, J. M., Lloyd, J. R., Hernandez-Lobato,
D., and Ghahramani, Z. Learning the semantics of discrete random variables: Ordinal or categorical? In NIPS
Workshop on Learning Semantics, 2014.
Hilbe, J. M. Negative binomial regression. Cambridge University Press, 2011.
Kandel, S., Heer, J., Plaisant, C., Kennedy, J., van Ham,
F., Riche, N. H., Weaver, C., Lee, B., Brodbeck, D., and
Buono, P. Research directions in data wrangling: Visualizations and transformations for usable and credible
data. Information Visualization, 10(4):271–288, 2011.
Lichman, M. UCI machine learning repository, 2013. URL
http://archive.ics.uci.edu/ml.
Navarro, A. KW, Frellsen, J., and Turner, R. E. The multivariate generalised von mises: Inference and applications. arXiv preprint arXiv:1602.05003, 2016.
Salakhutdinov, R. and Mnih, A. Probabilistic matrix factorization. In Advances in Neural Information Processing
Systems, 2007.
Schmidt, M. N, Winther, O., and Hansen, L. K. Bayesian
non-negative matrix factorization. In International Conference on Independent Component Analysis and Signal
Separation, pp. 540–547. Springer, 2009.
Valera, I. and Ghahramani, Z. General table completion
using a Bayesian nonparametric model. In Advances in
Neural Information Processing Systems 27, 2014.

