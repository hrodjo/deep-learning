Learning to Learn without Gradient Descent
by Gradient Descent

Yutian Chen 1 Matthew W. Hoffman 1 Sergio GoÃÅmez Colmenarejo 1 Misha Denil 1 Timothy P. Lillicrap 1
Matt Botvinick 1 Nando de Freitas 1

Abstract
We learn recurrent neural network optimizers
trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that
they can be used to efficiently optimize a broad
range of derivative-free black-box functions, including Gaussian process bandits, simple control
objectives, global optimization benchmarks and
hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to tradeoff exploration and exploitation, and compare
favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.

1. Introduction
Findings in developmental psychology have revealed that
infants are endowed with a small number of separable
systems of core knowledge for reasoning about objects,
actions, number, space, and possibly social interactions
(Spelke and Kinzler, 2007). These systems enable infants
to learn many skills and acquire knowledge rapidly. The
most coherent explanation of this phenomenon is that the
learning (or optimization) process of evolution has led to
the emergence of components that enable fast and varied
forms of learning. In psychology, learning to learn has a
long history (Ward, 1937; Harlow, 1949; Kehoe, 1988).
Inspired by this, many researchers have attempted to build
agents capable of learning to learn (Schmidhuber, 1987;
Naik and Mammone, 1992; Thrun and Pratt, 1998; Hochreiter et al., 2001; Santoro et al., 2016; Duan et al., 2016;
Wang et al., 2016; Ravi and Larochelle, 2017; Li and Malik, 2017). The scope of research under the umbrella of
learning to learn is very broad. The learner can implement
and be trained by many different algorithms, including gra1
DeepMind, London, United Kingdom. Correspondence to:
Yutian Chen <yutianc@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

dient descent, evolutionary strategies, simulated annealing,
and reinforcement learning.
For instance, one can learn to learn by gradient descent by
gradient descent, or learn local Hebbian updates by gradient descent (Andrychowicz et al., 2016; Bengio et al.,
1992). In the former, one uses supervised learning at the
meta-level to learn an algorithm for supervised learning,
while in the latter, one uses supervised learning at the metalevel to learn an algorithm for unsupervised learning.
Learning to learn can be used to learn both models and
algorithms. In Andrychowicz et al. (2016) the output of
meta-learning is a trained recurrent neural network (RNN),
which is subsequently used as an optimization algorithm to
fit other models to data. In contrast, in Zoph and Le (2017)
the output of meta-learning can also be an RNN model, but
this new RNN is subsequently used as a model that is fit to
data using a classical optimizer. In both cases the output
of meta-learning is an RNN, but this RNN is interpreted
and applied as a model or as an algorithm. In this sense,
learning to learn with neural networks blurs the classical
distinction between models and algorithms.
In this work, the goal of meta-learning is to produce an
algorithm for global black-box optimization. Specifically,
we address the problem of finding a global minimizer of an
unknown (black-box) loss function f . That is, we wish to
compute x? = arg minx‚ààX f (x) , where X is some search
space of interest. The black-box function f is not available
to the learner in simple closed form at test time, but can be
evaluated at a query point x in the domain. This evaluation
produces either deterministic or stochastic outputs y ‚àà R
such that f (x) = E[y | f (x)]. In other words, we can only
observe the function f through unbiased noisy point-wise
observations y.
Bayesian optimization is one of the most popular black-box
optimization methods (Brochu et al., 2009; Snoek et al.,
2012; Shahriari et al., 2016). It is a sequential modelbased decision making approach with two components.
The first component is a probabilistic model, consisting of
a prior distribution that captures our beliefs about the behavior of the unknown objective function and an observation model that describes the data generation mechanism.

Learning to Learn without Gradient Descent by Gradient Descent

The model can be a Beta-Bernoulli bandit, a random forest, a Bayesian neural network, or a Gaussian process (GP)
(Shahriari et al., 2016). Bayesian optimization is however
often associated with GPs, to the point of sometimes being
referred to as GP bandits (Srinivas et al., 2010).
The second component is an acquisition function, which
is optimized at each step so as to trade-off exploration
and exploitation. Here again we encounter a huge variety
of strategies, including Thompson sampling, information
gain, probability of improvement, expected improvement,
upper confidence bounds (Shahriari et al., 2016). The requirement for optimizing the acquisition function at each
step can be a significant cost, as shown in the empirical section of this paper. It also raises some theoretical concerns
(Wang et al., 2014).
In this paper, we present a learning to learn approach for
global optimization of black-box functions and contrast it
with Bayesian optimization. In the meta-learning phase,
we use a large number of differentiable functions generated with a GP to train RNN optimizers by gradient descent. We consider two types of RNN: long-short-term
memory networks (LSTMs) by Hochreiter and Schmidhuber (1997) and differentiable neural computers (DNCs) by
Graves et al. (2016).
During meta-learning, we choose the horizon (number of
steps) of the optimization process. We are therefore considering the finite horizon setting that is popular in AB tests
(Kohavi et al., 2009; Scott, 2010) and is often studied under the umbrella of best arm identification in the bandits
literature (Bubeck et al., 2009; Gabillon et al., 2012).
The RNN optimizer learns to use its memory to store information about previous queries and function evaluations,
and learns to access its memory to make decisions about
which parts of the domain to explore or exploit next. That
is, by unrolling the RNN, we generate new candidates for
the search process. The experiments will show that this
process is much faster than applying standard Bayesian optimization, and in particular it does not involve either matrix inversion or optimization of acquisition functions.
In the experiments we also investigate distillation of acquisition functions to guide the process of training the RNN
optimizers, and the use of parallel optimization schemes
for expensive training of deep networks.
The experiments show that the learned optimizers can
transfer to optimize a large and diverse set of blackbox functions arising in global optimization, control, and
hyper-parameter tuning. Moreover, withing the training
horizon, the RNN optimizers are competitive with stateof-the-art heavily engineered packages such as Spearmint,
SMAC and TPE (Snoek et al., 2014; Hutter et al., 2011a;
Bergstra et al., 2011)

2. Learning Black-box Optimization
A black-box optimization algorithm can be summarized by
the following loop:
1. Given the current state of knowledge ht propose a
query point xt
2. Observe the response yt
3. Update any internal statistics to produce ht+1
This easily maps onto the classical frameworks presented in
the previous section where the update step computes statistics and the query step uses these statistics for exploration.
In this work we take this framework as a starting point and
define a combined update and query rule using a recurrent
neural network parameterized by Œ∏ such that
ht , xt = RNNŒ∏ (ht‚àí1 , xt‚àí1 , yt‚àí1 ),
yt ‚àº p(y | xt ) .

(1)
(2)

Intuitively this rule can be seen to update its hidden state
using data from the previous time step and then propose
a new query point. In what follows we will apply this
RNN, with shared parameters, to many steps of a blackbox optimization process. An example of this computation
is shown in Figure 1. Additionally, note that in order to
generate the first query x1 we arbitrarily set the initial ‚Äúobservations‚Äù to dummy values x0 = 0 and y0 = 0; this is a
point we will return to in Section 2.3.
2.1. Loss Function
Given this rule we now need a way to learn the parameters
Œ∏ with stochastic gradient descent for any given distribution of differentiable functions p(f ). Perhaps the simplest
loss function one could use is the loss of the final iteration: Lfinal (Œ∏) = Ef,y1:T ‚àí1 [f (xT )] for some time-horizon
T . This loss was considered by Andrychowicz et al. (2016)
in the context of learning first-order optimizers, but ultimately rejected in favor of the summed loss
" T
#
X
Lsum (Œ∏) = Ef,y1:T ‚àí1
f (xt ) .
(3)
t=1

A key reason to prefer Lsum is that the amount of information conveyed by Lfinal is temporally very sparse. By
instead utilizing a sum of losses to train the optimizer we
are able to provide information from every step along this
trajectory. Although at test time the optimizer typically
only has access to the observation yt , at training time the
true loss can be used. Note that optimizing the summed
loss is equivalent to finding a strategy which minimizes the
expected cumulative regret. Finally, while in many optimization tasks the loss associated with the best observation
mint f (xt ) is often desired, the cumulative regret can be
seen as a proxy for this quantity.

Learning to Learn without Gradient Descent by Gradient Descent

Loss
f

f

yt-1
xt-1

ht-2

RNN

ht-1

yt

f

yt+1

xt
RNN

ht

RNN

ht+1

xt-2 yt-2
Figure 1. Computational graph of the learned black-box optimizer unrolled over multiple steps. The learning process will consist of
differentiating the given loss with respect to the RNN parameters

By using the above objective function we will be encouraged to trade off exploration and exploitation and hence
globally optimize the function f . This is due to the fact that
in expectation, any method that is better able to explore and
find small values of f (x) will be rewarded for these discoveries. However the actual process of optimizing the above
loss can be difficult due to the fact that nothing explicitly
encourages the optimizer itself to explore.
We can encourage exploration in the space of optimizers by
encoding an exploratory force directly into the meta learning loss function. Many examples exist in the bandit and
Bayesian optimization communities, for example
" T
#
X
LEI (Œ∏) = ‚àíEf,y1:T ‚àí1
EI(xt | y1:t‚àí1 )
(4)
t=1

where EI(¬∑) is the expected posterior improvement of
querying xt given observations up to time t. This can
encourage exploration by giving an explicit bonus to the
optimizer rather than just implicitly doing so by means of
function evaluations. Alternatively, it is possible to use the
observed improvement (OI)
" T

#
X
LOI (Œ∏) = Ef,y1:T ‚àí1
min f (xt ) ‚àí min(f (xi )), 0
t=1

i<t

(5)
We also studied a loss based on GP-UCB (Srinivas et al.,
2010) but in preliminary experiments this did not perform
as well as the EI loss and is thus not included in the later
experiments.
The illustration of Figure 1 shows the optimizer unrolled
over many steps, ultimately culminating in the loss function. To train the optimizer we will simply take derivatives

of the loss with respect to the RNN parameters Œ∏ and perform stochastic gradient descent (SGD). In order to evaluate these derivatives we assume that derivatives of f can
be computed with respect to its inputs. This assumption is
made only in order to backpropagate errors from the loss to
the parameters, but crucially is not needed at test time. If
the derivatives of f are also not available at training time
then it would be necessary to approximate these derivatives
via an algorithm such as REINFORCE (Williams, 1992).
2.2. Training Function Distribution
To this point we have made no assumptions about the distribution of training functions p(f ). In this work we are
interested in learning general-purpose black-box optimizers, and we desire our distribution to be quite broad.
As a result we propose the use of GPs as a suitable training distribution. Under the GP prior, the joint distribution of function values at any finite set of query points follows a multivariate Gaussian distribution (Rasmussen and
Williams, 2006), and we generate a realization of the training function incrementally at the query points using the
chain rule with a total time complexity of O(T 3 ) for every function sample.
The use of functions sampled from a GP prior also provides functions whose gradients can be easily evaluated at
training time as noted above. Further, the posterior expected improvement used within LEI can be easily computed (MocÃåkus, 1982) and differentiated as well. Search
strategies based on GP losses, such as LEI , can be thought
of as a distilled strategies. The major downside of search
strategies which are based on GP inference is their cubic
complexity.

Learning to Learn without Gradient Descent by Gradient Descent

algorithm.

~
yt
xt ~

f

‚Ä¶
1

f

f

‚Ä¶
i

‚Ä¶
j

f

A well-trained optimizer must learn to condition on ot‚àí1 in
order to either generate initial queries or generate queries
based on past observations. Another key point to consider
is that the batch nature of the optimizer can result in the
ordering of queries being permuted: i.e. although xt is proposed before xt+1 it is entirely plausible that xt+1 is evaluated first. In order to account for this at training time and
not allow the optimizer to rely on a specific ordering, we
simulate a runtime ‚àÜt ‚àº Uniform(1 ‚àí œÉ, 1 + œÉ) associated
with the t-th query. Observations are then made based on
the order in which they complete.

N

xt
ht-1

RNN

It is worth noting that the sequential setting is a special case
of this parallel policy where N = 1 and every observation
is made with ot‚àí1 = 1. Note also that we have kept the
number of workers fixed for simplicity of explanation only.
The architecture allows for the number of workers to vary.

ht

~
xt-1 ~
yt-1 ot-1
Figure 2. Graph depicting a single iteration of the parallel algorithm with N workers. Here we explicitly illustrate the fact that
the query xt is being assigned to the i-th worker for evaluation,
and that the the next observation pair (xÃÉt , yÃÉt ) is the output of the
j-th worker, which has completed its function evaluation.

While training with a GP prior grants us the convenience
to assess the efficacy of our training algorithm by comparing head-to-head with GP-based methods, it is worth noting that our model can be trained with any distribution that
permits efficient sampling and function differentiation. The
flexibility could become useful when considering problems
with specific prior knowledge and/or side information.

It is instructive to contrast this strategy with what is done
in parallel Bayesian optimization (Desautels et al., 2014;
Snoek et al., 2012). There care must be taken to ensure
that a diverse set of queries are utilized‚Äîin the absence of
additional data the standard sequential strategy would propose N queries at the same point. Exactly computing the
optimal N -step query is typically intractable, and as a result hand-engineered heuristics are employed. Often this
involves synthetically reducing the uncertainty associated
with outstanding queries in order to simulate later observations. In contrast, our RNN optimizer can store in its hidden state any relevant information about outstanding observations. Decisions about what to store are learned during
training and as a result should be more directly related to
later losses.

2.3. Parallel Function Evaluation
The use of parallel function evaluation is a common technique in Bayesian optimization, often used for costly, but
easy to simulate functions. For example, as illustrated in
the experiments, when searching for hyper-parameters of
deep networks, it is convenient to train several deep networks in parallel.
Suppose we have N workers, and that the process of
proposing candidates for function evaluation is much faster
than evaluating the functions. We augment our RNN optimizer‚Äôs input with a binary variable ot as follows:
ht , xt = RNNŒ∏ (ht‚àí1 , ot‚àí1 , xÃÉt‚àí1 , yÃÉt‚àí1 ).

(6)

For the first t ‚â§ N steps, we set ot‚àí1 = 0, arbitrarily
set the inputs to dummy values xÃÉt‚àí1 = 0 and yÃÉt‚àí1 = 0,
and generate N parallel queries x1:N . As soon as a worker
finishes evaluating a query, the query and its evaluation are
fed back to the network by setting ot‚àí1 = 1, resulting in
a new query xt . Figure 2 displays a single iteration of this

3. Experiments
We present several experiments that show the breadth
of generalization that is achieved by our learned algorithms. We train our algorithms to optimize very simple
functions‚Äîsamples from a GP with a fixed length scale‚Äî
and show that the learned algorithms are able to generalize
from these simple objective functions to a wide variety of
other test functions that were not seen during training.
We experimented with two different RNN architectures:
LSTMs and DNCs. However, we found the DNCs to perform slightly (but not significantly) better. For clarity, we
only show plots for DNCs in most of the figures.
We train each RNN optimizer with trajectories of T steps,
and update the RNN parameters using BPTT with Adam.
We use a curriculum to increase the length of trajectories
gradually from T = 10 to 100. We repeat this process for
each of the loss functions discussed in Section 2. Hyper-

Learning to Learn without Gradient Descent by Gradient Descent

‚àí0.8
‚àí1.0
‚àí1.2
‚àí1.4
40

60

80

‚àí2.0
‚àí2.5
‚àí3.0

100

0

GP samples, dim=6

20

‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5
‚àí4.0

40

60

80

100

GP samples, dim=9

0

Min function value

‚àí1.5

Min function value

20

Spearmint
Spearmint Fixed Hyper-parameters
TPE
SMAC
DNC sum
DNC OI
DNC EI

‚àí1.5

‚àí1
‚àí2
‚àí3
‚àí4
‚àí5

0

20

40

60

80

100

Compare DNC vs LSTM, dim=6

0

Min function value

0

GP samples, dim=3

‚àí1.0

Min function value

Min function value

GP samples, dim=1
‚àí0.6

DNC sum
LSTM sum
DNC OI
LSTM OI
DNC EI
LSTM EI

‚àí1
‚àí2
‚àí3
‚àí4
‚àí5

0

20

40

60

80

100

0

20

40

60

80

100

Figure 3. Average minimum observed function value, with 95% confidence intervals, as a function of search steps on functions sampled
from the training GP distribution. Left four figures: Comparing DNC with different reward functions against Spearmint with fixed and
estimated GP hyper-parameters, TPE and SMAC. Right bottom: Comparing different DNCs and LSTMs. As the dimension of the search
space increases, the DNC‚Äôs performance improves relative to the baselines.
DNC sum
1.0

40

0.8
0.6
0.4
0.2

0.2

0.4

0.6

0.8

20

0.4

0
1.0

40

0.8
0.6

0.2

0.2

0.4

0.6

0.8

20

0.4

0
1.0

40

0.8
0.6

0.2

0.2

0.4

0.6

0.8

1.0

50
40

0.8

30

0.6

30

20

0.4

20

10

0.0
0.0

Spearmint Fixed Hypers

50

1.0

30

10

0.0
0.0

DNC EI

50

1.0

30

10

0.0
0.0

DNC OI

50

0
1.0

0.2

10

0.0
0.0

0.2

0.4

0.6

0.8

0
1.0

Figure 4. How different methods trade-off exploration and exploitation in a one-dimensional example. Blue: Unknown function being
optimized. Green crosses: Function values at query points. Red trajectory: Query points over 50 steps.

parameters for the RNN optimization algorithm (such as
learning rate, number of hidden units, and memory size
for the DNC models) are found through grid search during training. When ready to be used as an optimizer, the
RNN requires neither tuning of hyper-parameters nor handengineering. It is fully automatic.

with integer inputs, we treat them as piece-wise constant
functions and round the network output to the closest values. We evaluate the performance at a given search step
t ‚â§ T = 100, according to the minimum observed function value up to step t, mini‚â§t f (xi ).

In the following experiments, DNC sum refers to the DNC
network trained using the summed loss Lsum , DNC OI to
the network trained using the loss LOI , and DNC EI to the
network trained with the loss LEI .

3.1. Performance on Functions Sampled from the
Training Distribution

We compare our learning to learn approach with popular state-of-the-art Bayesian optimization packages, including Spearmint with automatic inference of the GP hyperparameters and input warping to deal with non-stationarity
(Snoek et al., 2014), Hyperopt (TPE) (Bergstra et al.,
2011), and SMAC (Hutter et al., 2011b). For test functions

We first evaluate performance on functions sampled from
the training distribution. Notice, however, that these functions are never observed during training. Figure 3 shows
the best observed function values as a function of search
step t, averaged over 10, 000 sampled functions for RNN
models and 100 sampled functions for other models (we
can afford to do more for RNNs because they are very fast
optimizers). For Spearmint, we consider both the default

Learning to Learn without Gradient Descent by Gradient Descent

Min function value

‚àí0.90
‚àí0.95
‚àí1.00
‚àí1.05
‚àí1.10
0

40

60

80

100

0

Hartmann3 dim=3

‚àí1.0
‚àí1.5
‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5
‚àí4.0

20

40

60

80

100

Hartmann6 dim=6

0.0

Spearmint
TPE
SMAC
DNC sum
DNC OI
DNC EI

‚àí0.5

Min function value

0.0

Min function value

20

‚àí0.5
‚àí1.0
‚àí1.5
‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5

0

20

40

60

80

100

0

20

40

60

80

1.0
0.5
0.0
‚àí0.5
‚àí1.0
‚àí1.5
‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5
10 -3

GoldsteinPrice dim=2

10 -2

100

10 -1

10 0

10 1

10 2

10 3

Hartmann6 dim=6

0.0

Min function value

Min function value

‚àí0.85

GoldsteinPrice dim=2

1.0
0.5
0.0
‚àí0.5
‚àí1.0
‚àí1.5
‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5

Min function value

Branin dim=2

‚àí0.80

‚àí0.5
‚àí1.0
‚àí1.5
‚àí2.0
‚àí2.5
‚àí3.0
‚àí3.5
10 -3

10 -2

10 -1

10 0

10 1

10 2

10 3

Figure 5. Left: Average minimum observed function value, with 95% confidence intervals, as a function of search steps on 4 benchmark
functions: Branin, Goldstein price, 3d-Hartmann and 6d-Hartmann. Again we see that as the dimension of the search space increases,
the learned DNC optimizers are more effective than the Spearmint, TPE and SMAC packages within the training horizon. Right:
Average minimum observed function value in terms of the optimizer‚Äôs run-time (seconds), illustrating the superiority in speed of the
DNC optimizers over existing black-box optimization methods.

setting with a prior distribution that estimates the GP hyperparameters by Monte Carlo and a setting with the same
hyper-parameters as those used in training. For the second
setting, Spearmint knows the ground truth and thus provides a very competitive baseline. As expected Spearmint
with a fixed prior proves to be one of the best models
under most settings. When the input dimension is 6 or
higher, however, neural network models start to outperform Spearmint. We suspect it is because in higher dimensional spaces, the RNN optimizer learns to be more
exploitative given the fixed number of iterations. Among
all RNNs, those trained with expected/observed improvement perform better than those trained with direct function
observations.
Figure 4 shows the query trajectories xt , t = 1, . . . , 100,
for different black-box optimizers in a one-dimensional example. All of the optimizers explore initially, and later settle in one mode and search more locally. The DNCs trained
with EI behave most similarly to Spearmint. DNC with direct function observations (DNC sum) tends to explore less
than the other optimizers and often misses the global optimum, while the DNCs trained with the observed improvement (OI) keep exploring even in later stages.
3.2. Transfer to Global Optimization Benchmarks
We compare the algorithms on four standard benchmark functions for black-box optimization with dimensions

ranging from 2 to 6. To obtain a more robust evaluation of
the performance of each model, we generate multiple instances for each benchmark function by applying a random
translation (‚àí0.1‚Äì0.1), scaling (0.9‚Äì1.1), flipping, and dimension permutation in the input domain.
The lef hand side of Figure 5 shows the minimum observed
function values achieved by the learned DNC optimizers,
and contrasts these against the ones attained by Spearmint,
TPE and SMAC. All methods appear to have similar performance with Spearming doing slightly better in low dimensions. As the dimension increases, we see that the DNC optimizers converge at at a much faster rate within the horizon
of T = 100 steps.
We also observe that DNC OI and DNC EI both outperform
DNC with direct obsevations of the loss (DNC sum). It is
encouraging that the curves for DNC OI and DNC EI are
so close. While DNC EI is distilling a popular acquisition
function from the EI literature, the DNC OI variant is much
easier to train as it never requires the GP computations necessary to construct the EI acquisition function.
The right hand side of Figure 5 shows that the neural network optimizers run about 104 times faster than Spearmint
and 102 times faster than TPE and SMAC with the DNC
architecture. There is an additional 5 times speedup when
using the LSTM architecture, as shown in Table 1. The
negligible runtime of our optimizers suggests new areas
of application for global optimization methods that require

Learning to Learn without Gradient Descent by Gradient Descent
sample
trajectory

both high sample efficiency and real-time performance.
Table 1. Run-time (seconds) for 100 iterations excluding the
black-box function evaluation time.

Branin
Goldstein
Hartmann 3
Hartmann 6

Spearmint
1239
1238
1524
2768

TPE SMAC DNC LSTM
16.3 16.3
0.1
0.02
16.2 16.2
0.1
0.02
19.3 19.3
0.1
0.02
20.8 20.8
0.1
0.02

repeller2

3.3. Transfer to a Simple Control Problem

contours of
immediate reward

Repellers
Min function value

We also consider an application to a simple reinforcement learning task described by (Hoffman et al., 2009).
In this problem we simulate a physical system consisting
of a number of repellers which affect the fall of particles
through a 2D-space. The goal is to direct the path of the
particles through high reward regions of the state space and
maximize the accumulated discounted reward. The fourdimensional state-space in this problem consists of a particle‚Äôs position and velocity. The path of the particles can
be controlled by the placement of repellers which push the
particles directly away with a force inversely proportional
to their distance from the particle. At each time step the
particle‚Äôs position and velocity are updated using simple
deterministic physical forward simulation. The control policy for this problem consists of 3 learned parameters for
each repeller: 2d location and the strength of the repeller.

repeller1

Spearmint
TPE
SMAC
DNC sum
DNC OI
DNC EI

‚àí1
‚àí2
‚àí3
‚àí4
‚àí5
‚àí6
0

20

40

60

80

100

In our experiments we consider a problem with 2 repellers,
i.e. 6 parameters. An example trajectory along with the reward structure (contours) and repeller positions (circles) is
displayed in Figure 6. We apply the same perturbation as in
the previous subsection to study the average performance.
The loss (minimal negative reward) of all models are also
plotted in Figure 6. Neural network models outperform all
the other competitors in this problem.

Figure 6. Top: An example trajectory of a falling particle in red,
where solid circles show the position and strength of the two repellers and contour lines show the reward function. The aim to to
position and choose the strength of the repellers so that the particle spends more time in regions of high reward. Bottom: The
results of each method on optimizing the controller by direct policy search. Here, the learned DNC OI optimizer appears to have
an edge over the other techniques.

3.4. Transfer to ML Hyper-parameter Tuning

2.3, with 5 parallel proposal mechanisms. This approach is
about five times more efficient.

Lastly, we consider hyper-parameter tuning for machine
learning problems. We include the three standard benchmarks in the HPOLib package (Eggensperger et al., 2013):
SVM, online LDA, and logistic regression with 3, 3, and 4
hyper-parameters respectively. We also consider the problem of training a 6-hyper-parameter residual network for
classification on the CIFAR-100 dataset.
For the first three problems, the objective functions have already been pre-computed on a grid of hyper-parameter values, and therefore evaluation with different random seeds
(100 for Spearmint, 1000 for TPE and SMAC) is cheap.
For the last experiment, however, it takes at least 16 GPU
hours to evaluate one hyper-parameter setting. For this reason, we test the parallel proposal idea introduced in Section

For the first three tasks, our model is run once because
the setup is deterministic. For the residual network task,
there is some random variation so we consider three runs
per method.
The results are shown in Figure 7. The plots report the
negative accuracy against number of function evaluations
up to a horizon of T = 100. The neural network models
especially when trained with observed improvement show
competitive performance against the engineered solutions.
In the ResNet experiment, we also compare our sequential
DNC optimizers with the parallel versions with 5 workers. In this experiment we find that the learned and engineered parallel optimizers perform as well if not slightly

Learning to Learn without Gradient Descent by Gradient Descent

SVM, dim=3

0.275

Min function value

Min function value

0.280
0.270
0.265
0.260
0.255
0.250
0.245
0.240
0

40

60

80

0

Logistic Regression, dim=4

0.11
0.10
0.09
0.08
0.07
0

20

40

60

80

20

40

60

80

ResNet on Cifar-100, dim=6

‚àí0.40

Min function value

0.12

Min function value

20

Online LDA, dim=3

1350
1340
1330
1320
1310
1300
1290
1280
1270
1260

Spearmint
Spearmint Parallel
TPE
SMAC
DNC sum
DNC OI
DNC OI Parallel
DNC EI
DNC EI Parallel

‚àí0.45
‚àí0.50
‚àí0.55
‚àí0.60
‚àí0.65
0

20

40

60

80

Figure 7. Average test loss, with 95% confidence intervals, for the SVM, online LDA, and logistic regression hyper-parameter tuning
benchmarks. The bottom-right plot shows the performance of all methods on the problem of tuning a residual network, demonstrating
that the learned DNC optimizers are close in performance to the engineered optimizers, and that the faster parallel versions work
comparably well.

better than the sequential ones. These minor differences
arise from random variation.

optimizers. The parallel version of the algorithm also
performed well when tuning the hyper-parameters of an
expensive-to-train residual network.

4. Conclusions and Future Work

However, the current RNN optimizers also have some
shortcomings. Training for very long horizons is difficult.
This issue was also documented recently in (Duan et al.,
2016). We believe curriculum learning should be investigated as a way of overcoming this difficulty. In addition, a
new model has to be trained for every input dimension with
the current network architecture. While training optimizers
for every dimension is not prohibitive in low dimensions,
future works should extend the RNN structure to allow a
variable input dimension. A promising solution is to serialize the input vectors along the search steps.

The experiments have shown that up to the training horizon the learned RNN optimizers are able to match the performance of heavily engineered Bayesian optimization solutions, including Spearmint, SMAC and TPE. The trained
RNNs rely on neither heuristics nor hyper-parameters when
being deployed as black-box optimizers.
The optimizers trained on synthetic functions were able to
transfer successfully to a very wide class of black-box functions, associated with GP bandits, control, global optimization benchmarks, and hyper-parameter tuning.
The experiments have also shown that the RNNs are massively faster than other Bayesian optimization methods.
Hence, for applications involving a known horizon and
where speed is crucial, we recommend the use of the RNN

References
M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau,
T. Schaul, B. Shillingford, and N. de Freitas. Learning to learn
by gradient descent by gradient descent. In Advances in Neural
Information Processing Systems, 2016.

Learning to Learn without Gradient Descent by Gradient Descent
Y. Bengio, S. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In Conference on Optimality
in Biological and Artificial Networks, 1992.
J. S. Bergstra, R. Bardenet, Y. Bengio, and B. KeÃÅgl. Algorithms
for hyper-parameter optimization. In Advances in Neural Information Processing Systems, pages 2546‚Äì2554, 2011.
E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on Bayesian
optimization of expensive cost functions, with application to
active user modeling and hierarchical reinforcement learning.
Technical Report UBC TR-2009-23 and arXiv:1012.2599v1,
Dept. of Computer Science, University of British Columbia,
2009.
S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multiarmed bandits problems. In International Conference on Algorithmic Learning Theory, 2009.
T. Desautels, A. Krause, and J. Burdick. Parallelizing explorationexploitation tradeoffs with Gaussian process bandit optimization. Journal of Machine Learning Research, 2014.
Y. Duan, J. Schulman, X. Chen, P. Bartlett, I. Sutskever, and
P. Abbeel. Rl2 : Fast reinforcement learning via slow reinforcement learning. Technical report, UC Berkeley and OpenAI,
2016.
K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek,
H. Hoos, and K. Leyton-Brown. Towards an empirical foundation for assessing bayesian optimization of hyperparameters.
In NIPS workshop on Bayesian Optimization in Theory and
Practice, 2013.
V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Processing Systems,
pages 3212‚Äì3220, 2012.
A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka,
A. Grabska-BarwiAÃäska, S. G. Colmenarejo, E. Grefenstette,
T. Ramalho, J. Agapiou, A. A. P. Badia, K. M. Hermann,
Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield,
P. Blunsom, K. Kavukcuoglu, and D. Hassabis. Hybrid computing using a neural network with dynamic external memory.
Nature, 2016.
H. F. Harlow. The formation of learning sets. Psychological review, 56(1):51, 1949.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1780, 1997.
S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to
learn using gradient descent. In International Conference on
Artificial Neural Networks, pages 87‚Äì94. Springer, 2001.
M. W. Hoffman, H. Kueck, N. de Freitas, and A. Doucet. New inference strategies for solving Markov decision processes using
reversible jump MCMC. In Uncertainty in Artificial Intelligence, pages 223‚Äì231, 2009.
F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential modelbased optimization for general algorithm configuration. In
LION, pages 507‚Äì523, 2011a.
F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential modelbased optimization for general algorithm configuration. In International Conference on Learning and Intelligent Optimization, pages 507‚Äì523. Springer, 2011b.
E. J. Kehoe. A layered network model of associative learning:
learning to learn and configuration. Psychological review, 95
(4):411, 1988.
R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne.
Controlled experiments on the web: survey and practical guide.

Data mining and knowledge discovery, 18(1):140‚Äì181, 2009.
S. Li and J. Malik. Learning to optimize. In International Conference on Learning Representations, 2017.
J. MocÃåkus. The Bayesian approach to global optimization. In
Systems Modeling and Optimization, volume 38, pages 473‚Äì
481. Springer, 1982.
D. K. Naik and R. Mammone. Meta-neural networks that learn
by learning. In International Joint Conference on Neural Networks, volume 1, pages 437‚Äì442, 1992.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for
Machine Learning. The MIT Press, 2006.
S. Ravi and H. Larochelle. Optimization as a model for few-shot
learning. In International Conference on Learning Representations, 2017.
A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-augmented neural networks.
In International Conference on Machine Learning, 2016.
J. Schmidhuber. Evolutionary Principles in Self-Referential
Learning. On Learning how to Learn: The Meta-Meta-Meta...Hook. PhD thesis, Institut f. Informatik, Tech. Univ. Munich,
1987.
S. L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26
(6):639‚Äì658, 2010.
B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the human out of the loop: A review of Bayesian
optimization. Proceedings of the IEEE, 104(1):148‚Äì175, 2016.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pages 2951‚Äì2959, 2012.
J. Snoek, K. Swersky, R. S. Zemel, and R. P. Adams. Input warping for Bayesian optimization of non-stationary functions. In
International Conference on Machine Learning, 2014.
E. S. Spelke and K. D. Kinzler. Core knowledge. Developmental
science, 10(1):89‚Äì96, 2007.
N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian
process optimization in the bandit setting: No regret and experimental design. In International Conference on Machine
Learning, pages 1015‚Äì1022, 2010.
S. Thrun and L. Pratt. Learning to learn. Springer Science &
Business Media, 1998.
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo,
R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. arXiv Report 1611.05763, 2016.
Z. Wang, B. Shakibi, L. Jin, and N. de Freitas. Bayesian multiscale optimistic optimization. In AI and Statistics, pages 1005‚Äì
1014, 2014.
L. B. Ward. Reminiscence and rote learning. Psychological
Monographs, 49(4), 1937.
R. J. Williams. Simple statistical gradient-following algorithms
for connectionist reinforcement learning. Machine learning, 8
(3-4):229‚Äì256, 1992.
B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations, 2017.

