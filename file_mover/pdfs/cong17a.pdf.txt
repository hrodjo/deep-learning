Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive
Stochastic Gradient Riemannian MCMC
Yulai Cong 1 Bo Chen 1 Hongwei Liu 1 Mingyuan Zhou 2

Abstract
It is challenging to develop stochastic gradient
based scalable inference for deep discrete latent
variable models (LVMs), due to the difficulties
in not only computing the gradients, but also
adapting the step sizes to different latent factors
and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep
discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet
allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive
a block-diagonal Fisher information matrix and
its inverse for the simplex-constrained global
model parameters of DLDA. Exploiting that
Fisher information matrix with stochastic gradient
MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that
jointly learns simplex-constrained global parameters across all layers and topics, with topic and
layer specific learning rates. State-of-the-art results are demonstrated on big data sets.

1. Introduction
The increasing amount and complexity of data call for largecapacity models, such as deep discrete latent variable models (LVMs) for unsupervised data analysis (Hinton et al.,
2006; Bengio et al., 2007; Srivastava et al., 2013; Ranganath
et al., 2015; Zhou et al., 2016a), and scalable inference methods, such as stochastic gradient Markov chain Monte Carlo
(SG-MCMC) that provides posterior samples in a non-batch
learning setting (Welling & Teh, 2011; Patterson & Teh,
2013; Ma et al., 2015). Unfortunately, most deep LVMs,
1
National Laboratory of Radar Signal Processing, Collaborative Innovation Center of Information Sensing and Understanding,
Xidian University, Xiâ€™an, China. 2 McCombs School of Business,
The University of Texas at Austin, Austin, TX 78712, USA. Correspondence to: Bo Chen <bchen@mail.xidian.edu.cn>, Mingyuan
Zhou <mingyuan.zhou@mccombs.utexas.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

such as deep belief network (DBN) (Hinton et al., 2006)
and deep Boltzmann machines (DBM) (Salakhutdinov &
Hinton, 2009), use greedy layerwise training, without a principled way to jointly learn multilayers in an unsupervised
manner (Bengio et al., 2007). While SG-MCMC has recently been successfully applied to several â€œshallowâ€ LVMs,
such as mixture models (Welling & Teh, 2011) and mixedmembership models (Patterson & Teh, 2013), it has been
rarely applied to â€œdeepâ€ ones, probably due to the lack of
understanding on how to jointly learn the latent variables
of different layers and adjust the layer and topic specific
learning rates in a non-batch learning setting.
To investigate scalable SG-MCMC inference for deep
LVMs, we focus our study on the recently proposed Poisson gamma belief network (PGBN), whose hidden layers
are parameterized with gamma distributed hidden units and
connected with Dirichlet distributed basis vectors (Zhou
et al., 2016a). The PGBN is capable of extracting topics
from a text corpus at multiple layers and outperforms a large
number of topic modeling algorithms. However, the PGBN
is currently trained with a batch Gibbs sampler that is not
scalable to big data. In this paper, we focus on developing
scalable multilayer joint inference for the PGBN.
We will show that scalable multilayer joint inference of the
PGBN could be facilitated by its Fisher information matrix (FIM) (Amari, 1998; Girolami & Calderhead, 2011;
Pascanu & Bengio, 2013), which, although seemingly impossible to derive and challenging to work with due to the
need to compute the expectations over trigamma functions,
is readily available under an alternative representation of
the PGBN, referred to as deep latent Dirichlet allocation
(DLDA). DLDA, derived by exploiting data augmentation
and marginalization techniques on the PGBN, can be considered as a multilayer generalization of latent Dirichlet
allocation (LDA) (Blei et al., 2003). Following a general
framework for SG-MCMC (Ma et al., 2015), the block diagonal structure of the FIM of DLDA makes it be easily
inverted to precondition the mini-batch based noisy gradients to exploit the second-order local curvature information,
leading to topic-layer-adaptive step sizes based on the Riemannian manifold and the same asymptotic performance as
a natural gradient based batch-learning algorithm (Amari,
1998; Pascanu & Bengio, 2013). To the best of our knowl-

Deep Latent Dirichlet Allocation with TLASGR MCMC

edge, this is the first time that the FIM of a deep LVM is
shown to have an analytical and practical form. How we
derive the FIM for the PGBN using data augmentation and
marginalization techniques in this paper may serve as an
example to help derive the FIMs for other deep LVMs.
Besides presenting the analytical FIM of the PGBN, important for the marriage of a deep LVM and SG-MCMC,
we make another contribution in showing how to facilitate
SG-MCMC for an LVM equipped with simplex-constrained
model parameters Ï†k = (Ï†1k , . . . , Ï†V k )T , which means
PV
v=1 Ï†vk = 1 and Ï†vk âˆˆ R+ , where R+ := {x, x â‰¥ 0},
by using a reduced-mean simplex parameterization together
with a fast sampling procedure recently introduced in Cong
et al. (2017). Unlike other simplex parameterizations, the
reduced-mean one does not make heuristic pseudolikelihood assumptions. Though it has previously been deemed
unsound, it is successfully integrated into our SG-MCMC
framework to deliver state-of-the-art results. Exploiting the
analytical FIM of DLDA and novel inference for simplexconstrained parameters under a general SG-MCMC framework (Ma et al., 2015), we present topic-layer-adaptive
stochastic gradient Riemannian (TLASGR) MCMC for
DLDA, which automatically adjusts the learning rates of
global model parameters across all layers and topics, without the need to set the same learning rate for all that is
commonly used in practice due to the difficulty in identifying an appropriate combination of the learning rates for
different layers and topics.

2. PGBN and SG-MCMC
The generative model of the Poisson gamma belief network
(PGBN) (Zhou et al., 2016a) with L hidden layers, from top
to bottom, is expressed as


(L+1)
âˆ¼ Gam r, 1/cj
,
Â·Â·Â·


(l)
(l+1)
(l+1)
Î¸ j âˆ¼ Gam Î¦(l+1) Î¸ j
,
, 1/cj
Â·Â·Â·



(1)
(1)
(2)
âˆ¼ Pois Î¦(1) Î¸ j
, Î¸ j âˆ¼ Gam Î¦(2) Î¸ j ,
(L)

Î¸j

(1)

xj

(1)
(2)
pj
(2)
1âˆ’pj


,

where the j th observed or latent V -dimensional count vec(1)
tors xj âˆˆ ZV , where Z := {0, 1, . . .}, are factorized under
(l)

l
the Poisson (Pois) likelihood; the hidden units Î¸ j âˆˆ RK
+
of layer l are factorized under the gamma (Gam) likelihood into the product of the basis vector matrix Î¦(l) =
K
Ã—K
(l)
(l) 
Ï†1 , . . . , Ï†Kl âˆˆ R+ lâˆ’1 l and the hidden units of the

(l)
next layer, where Ï†k âˆ¼ Dir Î· (l) 1Klâˆ’1 are Dirichlet (Dir)
distributed and 1Klâˆ’1 is a Klâˆ’1 -dimensional vector of all
T
ones; the gamma shape parameters r = (r1 , Â· Â· Â· , rKL )
(l)
at the top layer are shared across all j; {1/cj }3,L+1 are

(l)

gamma scale parameters, where cj âˆ¼ Gam(e0 , 1/f0 ), and
(2)
(2)  (2)
(2)
cj := 1 âˆ’ pj /pj , where pj âˆ¼ Beta(a0 , b0 ) are

(1)

introduced to help reduce the dependencies between Î¸jk
(2)

and cj . The PGBN in (1) can be further extended un
(1)
(1)
der the Bernoulli-Poisson link as bj = 1 xj > 0 ,
(1)

and under the Poisson randomized gamma link as y j

(1)
Gam xj , 1/aj , where aj âˆ¼ Gam(e0 , 1/f0 ).

âˆ¼

The PGBN infers a multilayer deep representation of the
(l)
data, whose inferred basis vectors Ï†k at hidden layer l
 Qlâˆ’1 (t)  (l)
can be directly visualized as
Ï†k , which are
t=1 Î¦
their projections into the V -dimensional probability simplex.
The information of the whole data set is compressed by the
PGBN into the inferred sparse network {Î¦(1) , . . . , Î¦(L) },
(l)
where Ï†k1 k2 indicates the connection strength between node
(basis vector) k1 of layer l âˆ’ 1 and node k2 of layer l.
Moreover, the network structure can be inferred from the
data by combining the gamma-negative binomial process
of Zhou & Carin (2015) with a greedy layer-wise training
strategy. Extensive experiments in Zhou et al. (2016a) show
that the PGBN can extract basis vectors that are very specific/abstract in the bottom layer and become increasingly
more general when moving upwards from the bottom to top
(1)
hidden layers, and the K1 hidden units Î¸ j in the first hidden layer, which are unsupervisedly extracted and regularized with the deep network, are well suited for out-of-sample
prediction and being used as features for classification.
Despite all these attractive model properties, the current
inference of the PGBN relies on an upward-downward
Gibbs sampler that requires processing all data in each
iteration and hence often does not scale well to big data
unless with parallel computing. To make its inference
scalable to allow processing a large amount of data sufficiently fast on a regular personal computer, we resort to
SG-MCMC that subsamples the data and utilizes stochastic gradients in each MCMC iteration to generate posterior
samples for globally shared model parameters. Let us denote the posterior of model parameters z given the data
X = {xj }1,J as p (z |X ) âˆPeâˆ’H(z) , with potential function H (z) = âˆ’ ln p (z) âˆ’ j ln p (xj |z ). As in Theorem 1 of Ma et al. (2015), p (z |X ) is the stationary distribution of the dynamics defined by the
p stochastic differential
equation (SDE) dz = f (z) dt + 2D (z)dW (t), if the
deterministic drift f (z) is restricted to the form
f (z) = âˆ’ [D (z) + Q (z)] âˆ‡H (z) + Î“ (z) ,
P
Î“i (z) = j âˆ‚zâˆ‚ j [Dij (z) + Qij (z)] ,

(2)
(3)

where D (z) is a positive semidefinite diffusion matrix,
W(t) is a Wiener process, Q (z) is a skew-symmetric curl
matrix, and Î“i (z) is the ith element of the compensation
vector Î“(z). Thus one has a mini-batch update rule as
n 
o

z t+1 = z t + Îµt âˆ’ D(z t )+Q(z t ) âˆ‡HÌƒ(z t )+Î“(z t )



+ N 0, Îµt 2D (z t ) âˆ’ Îµt BÌ‚t ,
(4)

Deep Latent Dirichlet Allocation with TLASGR MCMC

where
P Îµt denotes step sizes, HÌƒ (z) = âˆ’ ln p (z) âˆ’
Ï xâˆˆXÌƒ ln p (x |z ), XÌƒ the mini-batch, Ï the ratio of the
dataset size |X| to the mini-batch size |XÌƒ|, and BÌ‚t an estimate of the stochastic gradient noise variance satisfying a
positive definite constraint as 2D (z t ) âˆ’ Îµt BÌ‚t  0.
As shown in Ma et al. (2015), stochastic gradient Riemannian Langevin dynamics (SGRLD) of Patterson & Teh
âˆ’1
(2013) is a special case with D (z) = G(z) , Q (z) =
0, BÌ‚t = 0, where G (z) denotes the Fisher information
matrix (FIM). SGRLD is designed to solve the inference on
the probability simplex, where four different parameterizations of the simplex-constrained basis vectors are discussed,
including reduced-mean, expanded-mean, reduced-natural,
and expanded-natural. Here, we consider both expandedmean, previously shown to provide the best overall results,
and reduced-mean, which, although discarded in Patterson
& Teh (2013) due to its unstable gradients, is used in this
paper to produce state-of-the-art results.
Let us denote Ï†k âˆˆ RV+ as a vector on the probability simplex, Ï†Ì‚k âˆˆ RV+ as a nonnegative vector, and
Ï•k âˆˆ RV+ âˆ’1 as a nonnegative vector constrained with
PV âˆ’1
Ï•Â·k :=
v=1 Ï•vk â‰¤ 1. For convenience, the symbol
â€œÂ·â€ will denote the operation of summing over the corT  P
responding index. We use Ï†Ì‚1k , Â· Â· Â· , Ï†Ì‚V k
v Ï†Ì‚vk
as an expanded-mean parameterization of Ï†k and
T
P
Ï•1k , Â· Â· Â· , Ï•(V âˆ’1)k , 1 âˆ’ v<V Ï•vk as a reduced-mean
parametrization of Ï†k . SGRLD focuses on a single-layer
model with a multinomial likelihood nk âˆ¼ Mult (nÂ·k , Ï†k )
and a Dirichlet distributed prior Ï†k âˆ¼ Dir (Î·1V ). For
inference, it adopts the expanded-mean parameterization
of Ï†k and
 makes a heuristic assumption that nÂ·k âˆ¼
Pois Ï†Ì‚Â·k . While that heuristic pseudolikelihood assumption of SGRLD is neither supported by the original generative model nor rigorously justified in theory, it converts
a Dirichlet-multinomial model into a gamma-Poisson one,
allowing a simple sampling equation for Ï†Ì‚k as

h
i




Ï†Ì‚k t+1 =  Ï†Ì‚k t +Îµt (nk +Î·)âˆ’ nÂ·k + Ï†Ì‚Â·k (Ï†k )t

h
 i
+ N 0, 2Îµt diag Ï†Ì‚k t  ,

(5)

where the absolute operation |Â·| is used to ensure positivevalued Ï†Ì‚k . Below we show how to eliminate that heuristic
assumption by parameterizing Ï†k with reduced-mean, and
develop efficient SG-MCMC for the PGBN, which reduces
to LDA when the number of hidden layers is one.

3. Deep Latent Dirichlet Allocation
While the original construction of PGBN in (1) makes it
seemingly impossible to compute the FIM, as shown in
Appendix A, we find that, by exploiting data augmentation
and marginalization techniques, the PGBN generative model

can be rewritten under an alternative representation that
marginalizes out all the gamma distributed hidden units,
as shown in the following Lemma, where Log(Â·) denotes
the logarithmic distribution (Johnson et al., 1997), m âˆ¼
SumLog(x, p) represents
Px the sum-logarithmic distribution
generated with m = i=1 ui , ui âˆ¼ Log(p) (Zhou et al.,
2016b). The proof is deferred to the Appendix.


(l+1)
(l) (l+1)
Lemma 3.1. Denote qj
= ln 1 + qj /cj
for
(1)

(l+1)

l = 1, . . . , L, where qj := 1, which means qj
=





1
1
1
ln 1 + (l+1)
ln 1 + (l)
ln 1 + Â· Â· Â· ln 1 + (2)
.
cj

cj
(l)

(l)
pj

cj

(L+1)
qÂ·
/(c0

âˆ’qj

(L+1)

With
:= 1 âˆ’ e
and pÌƒ :=
+ qÂ·
),
one may re-express the hierarchical model of the PGBN as
deep latent Dirichlet allocation (DLDA) as
(L+1)

âˆ¼ Log(pÌƒ), KL âˆ¼ Pois[âˆ’Î³0 ln(1 âˆ’ pÌƒ)],
P L (L+1)
X (L+1) = K
Î´Ï†(L) ,
k=1 xkÂ·
k
h
i
(L+1) 
(L+1)
(L+1)   (L+1)
xvj
âˆ¼
Mult
x
,
q
qÂ·
,
vÂ·
j
j
j
xkÂ·

(L)(L+1)

mvj
(l)

xvj =

(l)

PKl

(l)

k=1 xvkj , xvkj
(lâˆ’1)(l)

mvj
(1)

(L+1)

âˆ¼ SumLog(xvj
Â·Â·Â·



xvj =

PK1

k=1

(1)

v

(L+1)

, pj



(l)(l+1)
(l)
âˆ¼ Mult mkj
, Ï†k ,
(l)

(l)

âˆ¼ SumLog(xvj , pj ),
Â·
 Â·Â·

(1)

xvkj , xvkj

),

(1)(2)

v

âˆ¼ Mult mkj

(1)

, Ï†k



. (6)

Note that the equations in the first four lines of (6) precisely
represent a random count matrix generated from a gammanegative binomial process that can also be generated from
(L+1)

xkj



(L+1)
âˆ¼ Pois rk qj
, rk âˆ¼ Gam (Î³0 /K, 1/c0 ) ,


(L)(L+1)
(L+1)
(L+1)
mkj
âˆ¼ SumLog xkj , pj
(7)

by letting K â†’ âˆ (Zhou et al., 2016b). When L = 1, the
PGBN whose (rk , Ï†k ) are the points of a gamma process reduces to the gamma-negative binomial process PFA of Zhou
& Carin (2015), whose alternative representation is provided in Corollary D.1 in the Appendix. Note that how we
re-express the PGBN as DLDA is related to how Schein et al.
(2016) re-express their Poissonâ€“gamma dynamic systems
into an alternative representation that facilitates inference.
DLDA, designed to infer a multilayer representation of observed or latent high-dimensional sparse count vectors, constrains all the basis vectors of different layers to probability
simplices. It is clear from (6) that a data point backpropagates its counts through the network one layer at a time via
a sum-logarithmic distribution to enlarge each element of
a Kl -dimensional count vector, a multinomial distribution
to partition that enlarged count vector into a Klâˆ’1 Ã— Kl
count matrix, and then a row-sum operation to aggregate

Deep Latent Dirichlet Allocation with TLASGR MCMC

that latent count matrix into a Klâˆ’1 -dimensional count vector, where K0 := V is the feature dimension. Below we
show that such an alternative representation that repeats the
enlarge-partition-augment operation brings significant benefits when it comes to deriving SG-MCMC inference with
preconditioned gradients.
3.1. Fisher Information Matrix of DLDA
In deep LVMs, whose parameters of different layers are
often highly correlated to each other, it is often difficult to
tune the step sizes of different layers together and hence
one often chooses to train an unsupervised deep model in
a greedy layer-wise manner (Bengio et al., 2007), which
is a sensible but not optimal training strategy. To address
that issue, we resort to the inverse of the FIM that is widely
used to precondition the gradients to adjust the learning
rates of different model parameters (Amari, 1998; Pascanu
& Bengio, 2013; Ma et al., 2015; Li et al., 2016). However,
it is often difficult to compute the FIMs of deep LVMs as


âˆ‚2
G (z) = Eâ„¦|z âˆ’ 2 ln p (â„¦ |z ) ,
âˆ‚z

(8)

where z denotes the set of all global variables and â„¦ is the
set of all observed and local variables.
Although deriving the FIM for the PGBN generative model
shown in (1) seems impossible, we find it to be straightforward under the alternative DLDA representation in (6).
Since the likelihood of (6) is fully factorized with respect
(l)
to the global parameters z, i.e., Ï†k and r, one may readily
show the FIM G (z) of (6) has a block diagonal form as
h 



i
(1)
(L)
diag I Ï•1 , Â· Â· Â· , I Ï•KL , I (r) ;



(l)





(l)(l+1)

with the likelihood xvkj âˆ¼ Mult mkj
v
the reduced-mean parameterization, we have

(9)
(l)

, Ï†k



and


 
(l)
I Ï•k = âˆ’E

Q
h
i
(l)
(l)(l+1)
(l)
âˆ‚2
ln
Mult
(x
)
;
m
,
Ï†
v
(l)2
vkj
kj
k
j
âˆ‚Ï•k
h


i
(l)
(l)
(l)
= Mk diag 1/Ï•k +11T /(1âˆ’Ï•Â·k ) ,
(10)

h
i
h i
(l)
(l)(l+1)
(l)
where Mk := E mkÂ·
=E xÂ·kÂ· . Similarly, with the
(L+1)

likelihood xkj

(L+1)

âˆ¼ Pois(rk qj

), we have

I (r) = M (L+1) diag (1/r) ,

where M (L+1)

(11)

h
i
(L+1)
:= E qÂ·
.

The block diagonal structure of the FIM of DLDA makes
it computationally appealing to apply its inverse for preconditioning. Under the framework suggested by (4), we
adopt the similar settings used in SGRLD (Patterson & Teh,
âˆ’1
2013) that lets D (z) = G(z) , Q (z) = 0, and BÌ‚t = 0.
While other more sophisticated settings described in Ma
et al. (2015), including as special examples stochastic gradient Hamiltonian Monte Carlo in Chen et al. (2014) and

stochastic gradient thermostats in Ding et al. (2014), may
be used to further improve the performance, we choose this
specific one to make a direct comparison with SGRLD.
By substituting the FIM G (z) and the adopted settings
into (4), it is apparent that we only need to choose a single
step size Îµt , relying on the FIM to automatically adjust the
relatively learning rates for different parameters across all
layers and topics. Moreover, the block-diagonal structure
of G (z) will be carried over to its inverse D (z), making it
simple to perform updating using (4), as described below.
3.2. Inference on the Probability Simplex
As discussed in Section 2, to sample simplex-constrained
model parameters for a Dirichlet-multinomial model, the
SGRLD of Patterson & Teh (2013) adopts the expandedmean parameterization of simplex-constrained vectors and
makes a pseudolikelihood assumption to simplify the derivation of update equations. In this paper, without replying
on that pseudolikelihood assumption, we choose to use the
reduced-mean parameterization of simplex-constrained vectors, despite being considered as an unsound choice in Patterson & Teh (2013). In the following discussion, we omit
the layer-index superscript (l) for simplicity.
With the multinomial likelihood in (6) and the Dirichletmultinomial conjugacy, the conditional posterior of Ï†k can
be expressed as (Ï†k | âˆ’) âˆ¼ Dir(x1kÂ· + Î·, . . . , xV kÂ· + Î·).
Taking the gradient with respect to Ï•k âˆˆ RV+ âˆ’1 on the
summation of the negative log-likelihood of a mini-batch XÌƒ
scaled by Ï = |X|/|XÌƒ| and the negative log-likelihood of
the Dirichlet prior, we have
h
i ÏxÌ„ +Î·âˆ’1 ÏxÌƒ
:kÂ·
V kÂ· +Î·âˆ’1
âˆ‡Ï•k âˆ’HÌƒ(Ï•k ) =
âˆ’
,
Ï•k
1 âˆ’ Ï•Â·k

(12)

P
where xÌƒvkÂ·
=
and xÌ„:kÂ·
:=
j:xj âˆˆXÌƒ xvkj
T
(xÌƒ1kÂ· , . . . , xÌƒ(V âˆ’1)kÂ· ) . Note the gradient in (12) becomes unstable when some components of Ï•k approach
zeros, a key reason that this approach is mentioned but not
further pursued in Patterson & Teh (2013).
However, after preconditioning the noisy gradient with the
inverse of the FIM, it is intriguing to find out that the stability
issue completely disappears. More specifically, by plugging
both the FIM in (10) and noisy gradient in (12) into the SGMCMC update in (4), a noisy estimate of the deterministic
drift defined in (2) obtained using the current mini-batch
can be expressed as
h
i
I (Ï•k )âˆ’1 âˆ‡Ï•k âˆ’HÌƒ (Ï•k ) + Î“ (Ï•k )
= Mkâˆ’1 [(ÏxÌ„:kÂ· +Î·)âˆ’(ÏxÌƒÂ·kÂ· +Î·V ) Ï•k ] ,

(13)

where Î“ (Ï•k ) = Mkâˆ’1 [1 âˆ’ V Ï•k ] according to (3), as derived in detail in Appendix B. Consequently, with [Â·]4 de-

Deep Latent Dirichlet Allocation with TLASGR MCMC

noting the constraint that Ï•vk â‰¥ 0 and
using (4), the sampling of Ï•k becomes
h

PV âˆ’1
v=1

Ï•vk â‰¤ 1,

Îµt
(ÏxÌ„:kÂ· +Î·)âˆ’(ÏxÌƒÂ·kÂ· +Î·V )(Ï•k )t
(Ï•k )t+1 = (Ï•k )t + M
k
h
ii

T
2Îµt
diag
(Ï•
)
âˆ’(Ï•
)
(Ï•
)
.
(14)
+N 0, M
k
k
k
t
t
t
k





4

Even without the [Â·]4 constraint, the multivariate normal
(MVN) simulation in (14), although easy to interpret and
numerically stable, is computationally expensive if the
Cholesky decomposition, with O((V âˆ’ 1)3 ) complexity
(Golub & Van Loan, 2012), is adopted directly. Fortunately,
using Theorem 2 of Cong et al. (2017), the special structure of its covariance matrix allows an equivalent but substantially more efficient simulation of O(V ) complexity
by transforming a random variable drawn from a related
MVN that has a diagonal covariance matrix. More specifically, the sampling of (14) can be efficiently realized in a
V -dimensional space as


Îµt 
(Ï†k )t+1 = (Ï†k )t +
(ÏxÌƒ:kÂ· +Î·)âˆ’(ÏxÌƒÂ·kÂ· +Î·V )(Ï†k )t
Mk


2Îµt
diag (Ï†k )t
,
(15)
+ N 0,
Mk
âˆ 

where [Â·]âˆ  denotes the simplex constraint that Ï†vk â‰¥ 0 and
PV
v=1 Ï†vk = 1. More details on simulating (14) and (15)
can be found in Examples 1-3 of Cong et al. (2017).
Similarly, with the gamma-Poisson construction in (7), we
have Î“k (r) = 1/M (L+1) , as in Appendix B, and
h

âˆ‡rk âˆ’HÌƒ(r)

i

= rkâˆ’1

(L+1)
ÏxÌƒkÂ·
+


Î³0
(L+1) 
âˆ’1 âˆ’ c0 + ÏqÌƒÂ·
,
KL
(16)

which also becomes unstable if rk approaches zero. Substituting (16) and (11) into (4) leads to






Îµt
Î³0
(L+1)
r t+1 = r t + (L+1) ÏxÌƒ:Â·(L+1) +
âˆ’r t c0 +ÏqÌƒÂ·
KL
M



2Îµt
+ N 0, (L+1) diag (r t ) ,
(17)
M

for which there is no stability issue.
3.3. Topic-Layer-Adaptive Stochastic Gradient
Riemannian MCMC
(l)

Note that M (L+1) and Mk for l âˆˆ {1, . . . , L}, appearing
as denominates in (17) and (15), respectively, are expectations that need to be approximately calculated. We update
them using annealed weighting (Polatkan et al., 2015) as
(l)

Mk

M (L+1)



0



0

h
i
(l)
(l)
= 1 âˆ’ Îµt Mk + Îµt ÏE xÌƒÂ·kÂ· ,


h
i
0
0
(L+1)
= 1 âˆ’ Îµt M (L+1) + Îµt ÏE qÌƒÂ·
,

(18)
(19)

where E[Â·] denotes averaging over the collected MCMC
0
samples. For simplicity, we set Îµt = Îµt in this paper, which
is found to work well in practice.

Algorithm 1 TLASGR MCMC for DLDA (PGBN).
Input: Data mini-batches;
Output: Global parameters of DLDA (PGBN).
1: for t = 1, 2, Â· Â· Â· do
2: /* Collect local information
3: Upward-downward Gibbs sampling (Zhou et al., 2016a) on
(L+1)
(L+1)
the tth mini-batch for xÌƒ:kÂ· , xÌƒÂ·kÂ· , xÌƒ:Â·
, and qÌƒÂ·
;
4: /* Update global parameters
5:
for l = 1, Â· Â· Â· , L and k = 1, Â· Â· Â· , Kl do
(l)
(l)
6:
Update Mk with (18); then Ï†k with (15);
7:
end for
8:
Update M (L+1) with (19) and then r with (17).
9: end for

Note that as in (15) and (17), instead of having a single learning rate for all layers and topics, a common practice due to
the difficulty to adapt the step sizes to different layers and/or
topics, the proposed inference employs topic-layer-adaptive
(l)
(L+1)
learning rates as Îµt /Mk , where Mk
:= M (L+1) ,
adapting a single step size Îµt to different topics and layers by
(l)
multiplying it with the weights 1/Mk for l âˆˆ {1, . . . , L}
and k âˆˆ {1, . . . , Kl }. We refer to the proposed inference algorithm with adaptive learning rates as topic-layer-adaptive
stochastic gradient Riemannian (TLASGR) MCMC, as summarized in Algorithm 1 that is simple to implement.

4. Related Work
Both LDA (Blei et al., 2003) and the related Poisson factor
analysis (PFA) (Zhou et al., 2012) are equipped with scalable inference, such as stochastic variational inference (SVI)
(Hoffman et al., 2010; Mimno et al., 2012) and SGRLD
(Patterson & Teh, 2013). However, both are shallow LVMs
whose modeling capacities are often insufficient for big and
complex data. The deep Poisson factor models of Gan et al.
(2015) and Henao et al. (2015) are proposed to generalize
PFA with deep structures, but both of them only explore the
deep information in binary topic usage patterns instead of
the full connection weights that are used in the PGBN. The
proposed DLDA shares some similarities with the pachinko
allocation model of Li & McCallum (2006) in that they both
adopt layered construction and use Dirichlet distributed topics. Ranganath et al. (2015) propose deep exponential family
(DEF), which differs from the PGBN in connecting adjacent
layers via the gamma rate parameters and using black-box
variational inference (BBVI) (Ranganath et al., 2014).
Some commonly used neural networks, such as deep belief
network (DBN) (Hinton et al., 2006) and deep Boltzmann
machines (DBM) (Salakhutdinov & Hinton, 2009), have
also been modified for text analysis (Hinton & Salakhutdinov, 2009; Larochelle & Lauly, 2012; Srivastava et al.,
2013). Although they may work well for certain text analysis tasks, they are not naturally designed for count data and
often yield latent structures that are not readily interpretable.

Deep Latent Dirichlet Allocation with TLASGR MCMC

The neural variational document model (NVDM) of Miao
et al. (2016), even though using deep neural networks in its
variational auto-encoder (VAE) (Kingma & Welling, 2013),
still relies on a single-layer model for data generalization.
Generally speaking, it is challenging to develop an efficient
and principled multilayer joint learning algorithm for deep
LVMs. Scalable variational inference, such as BBVI, often
makes the restrictive mean-field assumption. Neural variational inference and learning (NVIL) relies on variance
reduction techniques that are often difficult to be generalized for discrete LVMs (Mnih & Gregor, 2014; Rezende
et al., 2014). When a SG-MCMC algorithm is used, a
single learning rate is often applied for different variables
across all layers (Welling & Teh, 2011; Neal et al., 2011;
Chen et al., 2014; Ding et al., 2014). It is possible to improve SG-MCMC by adjusting its noisy gradients with some
stochastic optimization technique, such as Adagrad (Duchi
et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba,
2014), and RMSprop (Tieleman & Hinton, 2012). For example, Li et al. (2016) show that preconditioning the gradients
with diagonal approximated FIM improves SG-MCMC in
both training speed and predictive accuracy for supervised
learning where gradients are easy to calculate. Other efforts
exploiting similar preconditioning idea focus on shallow
and/or binary models (Mimno et al., 2012; Patterson & Teh,
2013; Grosse & Salakhutdinov, 2015; Song et al., 2016;
Simsekli et al., 2016), and it is unclear how that idea can be
extended to deep LVMs whose gradients and FIM maybe
difficult to approximate.

5. Experiment results
We present experimental results on three benchmark corpora:
20Newsgroups (20News), Reuters Corpus Volume I (RCV1)
that is moderately large, and Wikipedia (Wiki) that is huge.
20News consists of 18,845 documents with a vocabulary
size of 2,000, partitioned into 11,315 training documents
and 7,531 test ones. RCV1 consists of 804,414 documents
with a vocabulary size of 10,000, where 10,000 documents
are randomly selected for testing. Wiki consists of 10 million documents randomly downloaded from Wikipedia using
the scripts provided in Hoffman et al. (2010); as in Hoffman
et al. (2010), Gan et al. (2015), and Henao et al. (2015),
we use a vocabulary with 7,702 words and randomly select
1,000 documents for testing. To make a fair comparison,
these corpora, including the training/testing partitions, are
set to be the same as those in Gan et al. (2015) and Henao
et al. (2015). To be consistent with the settings of Gan et al.
(2015) and Henao et al. (2015), no precautions are taken in
the scripts for Wikipedia to prevent a testing document from
being downloaded into a mini-batch for training.
We consider two related performance measures. The first
one is the commonly-used per-heldout-word perplexity cal-

culated as follows: for each test document, we randomly
select 80% of the word tokens to sample the local variables
specific to the document, under the global model parameters of each MCMC iteration; after the burn-in period, we
accumulate the first layerâ€™s Poisson rates in each collected
MCMC sample; in the end, we normalize these accumulated
Poisson rates to calculate the perplexity using the remaining
20% word tokens. Similar evaluation methods have been
widely used, e.g., in Wallach et al. (2009), Paisley et al.
(2011), and Zhou & Carin (2015). Although a good measure
for overall performance, the per-heldout-word perplexity,
calculated based on multiple collected MCMC samples of
global parameters, may not be ideal to check the performance in real time to assess how efficient an iterative algorithm improves its performance as time increases. Therefore,
we slightly modify it to provide a point per-heldout-word
perplexity calculated based on only the global parameters
of the most recent MCMC sample. For simplicity, we refer
to (point) per-heldout-word perplexity as (point) perplexity.
For comparison, we consider LDA of Blei et al. (2003), focused topic model (FTM) of Williamson et al. (2010), replicated softmax (RSM) of Hinton & Salakhutdinov (2009),
nested Hierarchical Dirichlet process (nHDP) of Paisley
et al. (2015), DPFA of Gan et al. (2015), and DPFM of
Henao et al. (2015). For these methods, the perplexity
results are taken from Gan et al. (2015) and Henao et al.
(2015). For the proposed algorithms, we set the mini-batch
size as 200, and use as burn-in 2000 mini-batches for both
20News and RCV1 and 3500 mini-batches for Wiki. We
collect 1500 samples to calculate perplexity. For point perplexity, given the global parameters of an MCMC sample,
we sample the local variables with 600 iterations and collect
one sample every two iterations during the last 400 iterations.
The hyperparameters of DLDA are set as: Î· (l) = 1/Kl ,
a0 = b0 = 0.01, and Î³0 = c0 = e0 = f0 = 1. Note Î· (l)
and Kl are set similar to that of DPFM for fair comparisons,
while other hyperparameters follow Zhou et al. (2016a).
To demonstrate the advantages of using the reduced-mean
simplex parameterization and inverting the FIM for preconditioning to obtain topic-layer-adaptive learning rates, we
consider four different inference methods:
1) TLASGR: topic-layer-adaptive stochastic gradient Riemannian MCMC for DLDA, as described in Algorithm 1.
2) TLFSGR: topic-layer-fixed stochastic gradient Riemannian MCMC for DLDA that replaces the adaptive learning
PK1
(l)
(1)
rates Îµt /Mk of TLASGR with Îµt /( k=1
Mk /K1 ).
(l)

3) SGRLD: updating each Ï†k under the expanded-mean
parameterization as in (5), served as a good scalable baseline
for comparison since it was shown in Patterson & Teh (2013)
to perform significantly better than SVI. It differs from
TLFSGR mainly in using a different parameterization for

Deep Latent Dirichlet Allocation with TLASGR MCMC
1050

2500

950
820

810

900

800
800

900

1000 1100

2000

850

800

2500
DLDA-Gibbs
DLDA-SGRLD
DLDA-TLFSGR
DLDA-TLASGR
880

1500

860
840
820
8000

10000

10 3

10 2

10 3

Time (Seconds)

Time (Seconds)

(a) A single-layer DLDA on 20News

880

1500

860
840
820
800
0.8

(b) DLDA of size 128-64 on RCV1

10 4

1

1.2
#10 4

1000

1000

10 2

DLDA-Gibbs
DLDA-SGRLD
DLDA-TLFSGR
DLDA-TLASGR

2000

Point perplexity

Point perplexity

1000

Point perplexity

DLDA-Gibbs
DLDA-SGRLD
DLDA-TLFSGR
DLDA-TLASGR

10 2

10 3

10 4

Time (Seconds)

(c) DLDA of size 128-64 on Wiki

Figure 1. Plot of point perplexity as a function of time. (a) 20News with a single-layer DLDA with 128 topics. (b) RCV1 with a two-layer
DLDA with 128 and 64 topics in the first and second layers, respectively. (c) Wiki with a two-layer DLDA, with 128 and 64 topics in the
first and second layers, respectively. Note a small subset of 106 documents from Wiki is used for demonstration.
Table 1. Per-heldout-word perplexities on 20 News, RCV1 and
Wiki. For models except DLDA, the results are taken from Gan
et al. (2015) and Henao et al. (2015). Note that for Wiki, DPFM
with MCMC infers the global parameters on a subset of the corpus
with 3,000 MCMC iterations.
Model
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DLDA
DPFM
DPFM
DPFA-SBN
DPFA-RBM
nHDP
LDA
FTM
RSM

Method
TLASGR
TLASGR
TLASGR
TLFSGR
TLFSGR
TLFSGR
SGRLD
SGRLD
SGRLD
Gibbs
Gibbs
Gibbs
SVI
MCMC
SGNHT
SGNHT
SVI
Gibbs
Gibbs
CD5

Size
128-64-32
128-64
128
128-64-32
128-64
128
128-64-32
128-64
128
128-64-32
128-64
128
128-64
128-64
128-64-32
128-64-32
(10,10,5)
128
128
128

20 News
757
758
770
760
759
772
775
770
777
752
754
768
818
780
827
896
889
893
887
877

RCV1
815
817
823
817
819
829
827
823
829
802
804
818
961
908
1143
920
1041
1179
1155
1171

Wiki
786
787
802
789
791
804
792
792
803
â€”
â€”
â€”
791
783
876
942
932
1059
991
1001

(l)

Ï†k and adding a pseudolikelihood assumption.
4) Gibbs: the upward-downward Gibbs sampler in Zhou
et al. (2016a).
Both TLASGR and TLFSGR differ from SGRLD mainly
(l)
in how the global parameters Ï†k are updated. While
TLASGR uses topic-layer-adaptive learning rates, both TLFSGR and SGRLD use a single learning rate, a common
practice due to the difficulty of tuning the step sizes across
layers and topics. We keep the same stepsize schedule of
Îµt = a(1 + t/b)âˆ’c as in Patterson & Teh (2013) and Ma
et al. (2015).
Let us first examine how various inference algorithms perform on 20News with a single-layer DLDA of size 128,
which can be considered as a topic model that imposes an

asymmetric prior on a documentâ€™s proportion over these
128 topics. Under this setting, as shown in Fig. 1(a), TLFSGR clearly outperforms SGRLD in providing lower point
perplexities as time progresses, which is not surprising as
under the reduced-mean simplex parameterization, to derive its sampling equations, TLFSGR does not rely on a
pseudolikelihood assumption that is adopted by SGRLD
in its expanded-mean simplex parameterization. Moreover,
TLASGR is found to further improve TLFSGR, suggesting
that even for a single-layer model, replacing a fixed learning
PK1
(1)
rate as Îµt /( k=1
Mk /K1 ) with topic-adaptive learning
(1)
rates as Îµt /Mk could further improve the performance.
Let us then examine how these algorithms perform on two
larger corporaâ€”RCV1 and Wikiâ€”with a two-layer DLDA
of size 128-64, which improves the single-layer one by capturing the co-occurrence patterns between the topics of the
first layer with those of the second layer (Zhou et al., 2016a).
As show in Figs. 1(b) and 1(c), it is clear that the proposed
TLASGR performs the best for the two-layer DLDA and
consistently outperforms TLFSGR as time progresses. In
comparison, SGRLD quickly improves its performance as
a function of time in the beginning but its point perplexity
remains higher even after 10,000 seconds, whereas Gibbs
sampling slowly improves its performance as a function of
time in the beginning but moves its point perplexity closer
and closer to that of TLASGR as time progresses.
Note that for 20News, the point perplexity of the mini-batch
based TLASGR quickly decreases as time increases, while
that of Gibbs sampling decreases relatively slowly. That discrepancy of convergence rate as a function of time becomes
much more evident for both RCV1 and Wiki, as shown in
Figs. 1(b) and 1(c). This is expected as both RCV1 and
Wiki are much larger corpora, for which a mini-batch based
inference algorithm can already make significant progress
in learning the global model parameters, before a batchlearning Gibbs sampler finishes a single iteration that needs
to go through all documents.

#10-5

Layer-adaptive step sizes

Layer-adaptive step sizes

Deep Latent Dirichlet Allocation with TLASGR MCMC

) (1)

4

) (2)
)
r

(3)

2

0
0

1000

2000

3000

1

#10-6
) (1)
) (2)
) (3)
r

0.5

0
0

1000

Iterations

2000

3000

Iterations

(b)
Layer-adaptive step sizes

(a)
5

(a)

(b)

(c)

(d)

(e)

(f)

#10-7
) (1)

4

) (2)

3

) (3)
r

2
1
0
0

1000 2000 3000 4000 5000

Iterations

(c)
Figure 2. Topic-layer-adaptive learning rates inferred with a threelayer DLDA of size 128-64-32. (a) 20News. (b) RCV1. (c) Wiki.
Note the layer-adaptive learning rate for layer l is obtained by
(l)
averaging over the topic-layer-adaptive learning rates of all Ï†k
for k = 1, . . . , Kl .

To illustrate the working mechanism of TLASGR, we show
how its inferred learning rates are adapted to different layers
in Fig. 2. By contrast, TLFSGR admits a fixed learning
rate that leads to worse performance. Several interesting
observations can be made for TLASGR from Figs. 2(a)-2(c):
1) for Î¦(l) , higher layers prefer larger step sizes, which
may be explained by the enlarge-partition-augment data
generating mechanism of DLDA; 2) larger datasets prefer
slower learning rates, reflected by the scales of the vertical
axes; 3) and the relative learning rates between different
layers vary across different datasets.
To further verify the excellent performance of DLDA inferred with TLASGR, we compare a wide variety of models
and inference algorithms in Table 1. For 20News and RCV1,
DLDA with Gibbs sampling performs the best in terms of
perplexity and exhibits a clear trend of improvement as the
number of hidden layers increases. For Wiki, a single iteration of the DLDA Gibbs sampler on the full corpus is so
expensive in both time and memory that its performance is
not reported. For DLDA on 20News and RCV1, TLASGR
only slightly underperforms Gibbs sampling, and the performance degradation from Gibbs sampling to TLASGR is significantly smaller than that from MCMC to SVI for DPFM.
That relative small degradation caused by changing from
Gibbs sampling to the mini-batch based TLASGR could be
attributed to the Fisher efficiency brought by the FIM. Generally speaking, in comparison to SGRLD, TLASGR brings
a clear boost in performance, which is particularly evident
for a deeper DLDA, and TLASGR consistently outperforms
TLFSGR that does not adapt its learning rates to different
topics and layers.

Figure 3. Learned dictionary atoms on MNIST digits with a threelayer GBN of size 128-64-32 after one full epoch. Shown in (a)-(c)
(1)
(2)
(3)
are example atoms for Ï†k , Î¦(1) Ï†k , and Î¦(1) Î¦(2) Ï†k , respectively, learned with TLFSGR, and shown in (d)-(f) are example
ones learned with TLASGR.

MNIST. To further illustrate the advantages of using the
inverse of the FIM for preconditioning in a deep generative
model, and to visualize the benefits of automatically adjusting the relative learning rates of different hidden layers,
we apply a three-layer Poisson randomized gamma gamma
belief network (PRG-GBN) (Zhou et al., 2016a) to 60,000
MNIST digits and present the learned dictionary atoms after
one full epoch, as shown in Fig. 3. It is clear that, with
topic-layer-adaptive learning rates, which are made possible by utilizing the FIM, TLASGR provides more effective
mini-batch based stochastic updates to allow better information propagation between different hidden layers, extracting
more interpretable features at multiple layers.

6. Conclusions
For scalable multilayer joint inference of the Poisson gamma
belief network (PGBN), we introduce an alternative representation of the PGBN, which is referred to as deep latent
Dirichlet allocation (DLDA) that can be considered as a multilayer generalization of latent Dirichlet allocation. We show
how to reparameterize the simplex constrained basis vectors,
derive a block-diagonal Fisher information matrix (FIM),
and efficiently compute the inverse of the FIM, leading to a
stochastic gradient MCMC algorithm referred to as topiclayer-adaptive stochastic gradient Riemannian (TLASGR)
MCMC. The proposed TLASGR-MCMC is able to jointly
learn the parameters of different layers with topic-layeradaptive step sizes, which makes DLDA (PGBN) much
more practical in a big data setting. Compelling experimental results on large text corpora and the MNIST dataset
demonstrated the advantages of TLASGR-MCMC.

Deep Latent Dirichlet Allocation with TLASGR MCMC

Acknowledgements
Bo Chen thanks the support of the Thousand Young Talent Program of China, NSFC (61372132), and NDPR9140A07010115DZ01019. Hongwei Liu thanks the support
of NSFC for Distinguished Young Scholars (61525105).

References
Amari, S. Natural gradient works efficiently in learning.
Neural Computation, 10(2):251â€“276, 1998.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layer-wise training of deep networks. In NIPS,
pp. 153â€“160, 2007.
Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet
allocation. JMLR, 3:993â€“1022, 2003.
Chen, T., Fox, E. B., and Guestrin, C. Stochastic gradient
Hamiltonian Monte Carlo. In ICML, pp. 1683â€“1691,
2014.
Cong, Y., Chen, B., and Zhou, M. Fast simulation of
hyperplane-truncated multivariate normal distributions.
Bayesian Analysis Advance Publication, 2017.

Hoffman, M. D., Bach, F. R., and Blei, D. M. Online
learning for latent Dirichlet allocation. In NIPS, pp. 856â€“
864, 2010.
Johnson, N. L., Kotz, S., and Balakrishnan, N. Discrete
Multivariate Distributions, volume 165. Wiley New York,
1997.
Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. arXiv:1412.6980, 2014.
Kingma, Diederik P and Welling, Max. Auto-encoding
variational Bayes. In ICLR, number 2014, 2013.
Larochelle, H. and Lauly, S. A neural autoregressive topic
model. In NIPS, 2012.
Li, C., Chen, C., Carlson, D., and Carin, L. Preconditioned
stochastic gradient Langevin dynamics for deep neural
networks. AAAI, 2016.
Li, W. and McCallum, A. Pachinko allocation: DAGstructured mixture models of topic correlations. In ICML,
pp. 577â€“584, 2006.
Ma, Y., Chen, T., and Fox, E. A complete recipe for stochastic gradient MCMC. In NIPS, pp. 2899â€“2907, 2015.

Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., and
Neven, H. Bayesian sampling using stochastic gradient
thermostats. In NIPS, pp. 3203â€“3211, 2014.

Miao, Y., Yu, L., and Blunsom, P. Neural variational inference for text processing. In ICML, 2016.

Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient
methods for online learning and stochastic optimization.
JMLR, 12(Jul):2121â€“2159, 2011.

Mimno, D., Hoffman, M. D., and Blei, D. M. Sparse stochastic inference for latent Dirichlet allocation. In ICML, pp.
362 â€“ 365, 2012.

Gan, Z., Chen, C., Henao, R., Carlson, D., and Carin, L.
Scalable deep Poisson factor analysis for topic modeling.
In ICML, pp. 1823â€“1832, 2015.

Mnih, A. and Gregor, K. Neural variational inference and
learning in belief networks. 2014.

Girolami, M. and Calderhead, B. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods. JRSSB, 73(2):123â€“214, 2011.

Neal, R. M. et al. MCMC using Hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, 2:113â€“162,
2011.

Golub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHU Press, 2012.

Paisley, J., Wang, C., and Blei, D. The discrete infinite logistic normal distribution for mixed-membership modeling.
In AISTATS, 2011.

Grosse, R. B. and Salakhutdinov, R. Scaling up natural
gradient by sparsely factorizing the inverse Fisher matrix.
In ICML, pp. 2304â€“2313, 2015.

Paisley, J., Wang, C., Blei, D. M., and Jordan, M. I. Nested
hierarchical dirichlet processes. PAMI, 37(2):256â€“270,
2015.

Henao, R., Gan, Z., Lu, J., and Carin, L. Deep Poisson
factor modeling. In NIPS, pp. 2782â€“2790, 2015.

Pascanu, R. and Bengio, Y. Revisiting natural gradient for
deep networks. arXiv:1301.3584, 2013.

Hinton, G. E. and Salakhutdinov, R. R. Replicated softmax:
an undirected topic model. In NIPS, pp. 1607â€“1614,
2009.

Patterson, S. and Teh, Y. W. Stochastic gradient Riemannian
Langevin dynamics on the probability simplex. In NIPS,
pp. 3102â€“3110, 2013.

Hinton, G. E., Osindero, S., and Teh, Y. W. A fast learning
algorithm for deep belief nets. Neural Computation, 18
(7):1527â€“1554, 2006.

Polatkan, G., Zhou, M., Carin, L., Blei, D., and Daubechies,
I. A Bayesian nonparametric approach to image superresolution. PAMI, 37(2):346â€“358, 2015.

Deep Latent Dirichlet Allocation with TLASGR MCMC

Ranganath, R., Gerrish, S., and Blei, D. M. Black box
variational inference. In AISTATS, 2014.
Ranganath, R., Tang, L., Charlin, L., and Blei, D. M. Deep
exponential families. In AISTATS, 2015.
Rezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.
Stochastic backpropagation and approximate inference in
deep generative models. In ICML, pp. 1278â€“1286, 2014.
Salakhutdinov, R. and Hinton, G. E. Deep Boltzmann machines. In AISTATS, volume 1, pp. 3, 2009.
Schein, A., Zhou, M., and Wallach, H. Poissonâ€“gamma
dynamical systems. In NIPS, pp. 5006â€“5014, 2016.
Simsekli, U., Badeau, R., Cemgil, A. T., and G., Richard.
Stochastic quasi-Newton Langevin Monte Carlo. In
ICML, 2016.
Song, Z., Henao, R., Carlson, D., and Carin, L. Learning
sigmoid belief networks via Monte Carlo expectation
maximization. In AISTATS, pp. 1347â€“1355, 2016.
Srivastava, N., Salakhutdinov, R. R., and Hinton, G. E.
Modeling documents with deep Boltzmann machines. In
UAI, 2013.
Tieleman, T. and Hinton, G. Lecture 6.5-rmsprop: Divide
the gradient by a running average of its recent magnitude.
COURSERA: Neural networks for machine learning, 4
(2), 2012.
Wallach, H. M., Murray, I., Salakhutdinov, R., and Mimno,
D. Evaluation methods for topic models. In ICML, 2009.
Welling, M. and Teh, Y.-W. Bayesian learning via stochastic
gradient Langevin dynamics. In ICML, pp. 681â€“688,
2011.
Williamson, S., Wang, C., Heller, K. A., and Blei, D. M.
The IBP compound Dirichlet process and its application
to focused topic modeling. In ICML, pp. 1151â€“1158,
2010.
Zeiler, M. D. Adadelta: an adaptive learning rate method.
arXiv:1212.5701, 2012.
Zhou, M. and Carin, L. Negative binomial process count
and mixture modeling. PAMI, 37(2):307â€“320, 2015.
Zhou, M., Hannah, L., Dunson, D. B., and Carin, L. Betanegative binomial process and Poisson factor analysis. In
AISTATS, pp. 1462â€“1471, 2012.
Zhou, M., Cong, Y., and Chen, B. Augmentable gamma
belief networks. Journal of Machine Learning Research,
17(163):1â€“44, 2016a.

Zhou, M., Padilla, O., and Scott, J. G. Priors for random
count matrices derived from a family of negative binomial
processes. J. Amer. Statist. Assoc., 111(515):1144â€“1156,
2016b.

