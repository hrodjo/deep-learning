Latent Feature Lasso
Ian E.H. Yen 1 Wei-Cheng Lee 2 Sung-En Chang 2 Arun S. Suggala 1 Shou-De Lin 2 Pradeep Ravikumar 1

Abstract
The latent feature model (LFM), proposed in
(Griffiths & Ghahramani, 2005), but possibly
with earlier origins, is a generalization of a mixture model, where each instance is generated not
from a single latent class but from a combination of latent features. Thus, each instance has
an associated latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference
of LFMs is considerably intractable, and accordingly, most of the attention has focused on nonparametric LFMs, with priors such as the Indian
Buffet Process (IBP) on infinite binary matrices.
Recent efforts to tackle this complexity either
still have computational complexity that is exponential, or sample complexity that is high-order
polynomial w.r.t. the number of latent features.
In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel
atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the
data distribution.

1. Introduction
Latent variable models are widely used in unsupervised
learning, in part because they provide compact and interpretable representations of the distribution over the observed data. The most common and simplest such latent
variable model is a mixture model, which associates each
observed object with a latent class. However, in many
real-world applications, observations are better described
by a combination of latent features than a single latent
class. Accordingly, admixture or mixed membership models have been proposed (Airoldi et al., 2014), that in the
simplest settings, assign each object to a convex combi1

Carnegie Mellon University, U.S.A. 2 National Taiwan
University, Taiwan.
Correspondence to: Ian E.H. Yen
<eyan@cs.cmu.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

nation of latent classes. For instance, a document object
could be modeled as a convex combination of topic objects. There are many settings however where a convex
combination might be too restrictive, and the objects are
better modeled as simply a collection of latent classes. An
example is web image, which can often described by multiple tags rather than a single class, or even by a convex
combination of tag objects. Another example is the model
of user, who might have multiple interests in the context of
a recommendation system, or be involved in multiple communities in a social network. With such settings in mind,
(Griffiths & Ghahramani, 2005) proposed a latent feature
model (LFM), where each observed object can be represented by a binary vector that indicates the presence or absence of each of a collection of latent features. Their proposed model extended earlier models with a similar flavor
for specialized settings, such as (Ueda & Saito, 2003) for
bag of words models for text. The latent feature model
can also be connected to sparse PCA models (d’Aspremont
et al., 2007; Jolliffe et al., 2003) by considering a pointwise product of the binary feature incidence vector with
another real-valued vector. As (Griffiths & Ghahramani,
2005) showed, LFM handily outperforms clustering as an
efficient and interpretable data representation, particularly
in settings where the object can be naturally represented as
a collection of latent features or parts.
However, the estimation (inference) of an LFM from data
is difficult, due to the combinatorial nature of the binary
feature incidence vectors. Indeed, with N samples, and K
latent features, the number of possible binary matrices consisting of the N binary feature incidence vectors is 2N K .
And not in the least, the log-likelihood of LFM is not a
concave function of its parameters.
Given that the finite feature case seems intractable, right
from the outset, attention has focused on the nonparametric infinite feature case, where a prior known as the Indian
Buffet Process (IBP) has been proposed for the infinite binary matrices consisting of the feature incidence vectors
given infinite set of latent features (Griffiths & Ghahramani, 2011). While the IBP prior provides useful structure,
inference remains a difficult problem, and in practice, one
often relies on local search methods (Broderick et al., 2013)
to find an estimate of parameters, or employ Markov Chain
Monte Carlo (MCMC) (Doshi-Velez & Ghahramani, 2009)

Latent Feature Lasso

or variational methods (Doshi-Velez et al., 2009) to obtain
an approximate posterior distribution. However, none of
these approaches can provide guarantees on the quality of
solution in polynomial time.
Note that both in the mixture model, as well as the admixture model cases, the parametric variants have been hugely
popular alongside or perhaps even more so than the nonparametric variants e.g. clustering procedures based on finite number of clusters, or topic models with a finite number of topics. This is in part because the parametric variants
have a lower model complexity, which might be desired
under certain settings, and also have simpler inference procedures. However, in the LFM case, the parametric variant has received very little attention, which might suggest
the relatively lesser popularity for LFMs when compared
to mixture or admixture/topic models.
Accordingly, in this paper, we consider the question of
computationally tractable estimation of parametric LFMs.
In the nonparametric setting with an IBP prior, (Tung &
Smola, 2014) have proposed the use of spectral methods,
which bypasses the problem of non-concave log-likelihood
by estimating the moments derived from the model, and
then recovers parameters by solving a system of equations.
Their spectral methods based procedure produces consistent estimates of LFMs in polynomial time, however with
a sample complexity that has a high-order (more than sixorder) polynomial dependency on the number of latent features and the occurrence probability of each feature. Moreover, the application of spectral methods requires knowledge of the distribution, which results in non-robustness
to model mis-specification in practice. Under a noiseless
setting, (Slawski et al., 2013) leveraged identifiability conditions under which the solution is unique, to propose an
algorithm for a parametric LFM. Their algorithm is guaranteed to recover the parameters in the noiseless setting,
but with the caveat that it has a computational complexity
that is exponential in the number of latent features.
We note that even under the assumption of an nonparametric LFM, specifically an Indian Buffet Process with Linear
Gaussian Observations, deriving its MAP point estimate
under low-variance asymptotics following the approach of
MAD-Bayes Asymptotics (Broderick et al., 2013) yields an
objective similar to that of a parametric LFM with an additional term that is linear in the number of latent features.
Thus, developing computationally tractable approaches for
parametric LFMs would be broadly useful.
In this work, we propose the Latent Feature Lasso, a novel
convex estimation procedure for the estimation of a Latent
Feature Model using atomic-norm regularization. We construct a greedy algorithm with strong optimization guarantees for the estimator by relating each greedy step to a
MAX-CUT like problem. We also provide a risk bound

for the estimator under general data distribution settings,
which trades off between risk and sparsity, and has a sample complexity linear in the number of components and dimension. Under the noiseless setting, we also show that
Latent Feature Lasso estimator recovers the parameters of
LFM under an identifiability condition similar to (Slawski
et al., 2013).

2. Problem Setup
A Latent Feature Model represents data as a combination
of latent features. Let x ∈ RD be an observed random
vector that is generated as:
x = W T z + e,
where z ∈ {0, 1}K is a latent binary feature incidence
vector that denotes the presence or absence of K features,
W ∈ RK×D is an unknown matrix of K latent features
of dimension D, and e ∈ RD is an unknown noise vector. We say that the model is biased when E[e|z] =
E[x|z] − W T z 6= 0, and which we allow in our analysis.
Suppose we observe N samples of the random vector x. It
will be useful in the sequel to collate the various vectors
corresponding to the N samples into matrices. We collate
the observations into a matrix X ∈ RN ×D , the N latent
incidence vectors into a matrix Z ∈ {0, 1}N ×K , and the
noise vectors into an N × D matrix . We thus obtained the
vectorized form of the model as X = ZW + .
Most existing works on LFM make two strong assumptions. The first is that the model has zero bias E[e|z] =
0 (Tung & Smola, 2014; Broderick et al., 2013; Griffiths
& Ghahramani, 2011; Slawski et al., 2013; Zoubin, 2013;
Doshi-Velez & Ghahramani, 2009; Doshi-Velez et al.,
2009; Hayashi & Fujimaki, 2013). The second common
but strong class of assumptions is distributional (Hayashi
& Fujimaki, 2013; Tung & Smola, 2014):
p(x|z) = N (W T z, σ 2 I) , p(z) = Bern(π),
where Bern(π) denotes the distribution of K independent
Bernoulli with zk ∼ Bern(πk ). In the Nonparametric
Bayesian setting (Griffiths & Ghahramani, 2011; Zoubin,
2013; Doshi-Velez et al., 2009; Doshi-Velez & Ghahramani, 2009; Broderick et al., 2013), one replaces Bern(π)
with an Indian Buffet Process IBP(α) over the N × K + bi+
nary incidence matrix Z ∈ {0, 1}N ×K where K + can be
inferred from data instead of being specified a-priori. We
note that both classes of assumptions need not hold in practice: the zero bias assumption E[x|z] = W T z is stringent
given the linearity of the model, while the Bernoulli and
IBP distributional assumptions are also restrictive, in part
since they assume independence between the presence of
two features zik and zik0 . Our method and analyses do not
impose either of these assumptions.

Latent Feature Lasso

It is useful to contrast the different estimation goals ranging over the LFM literature. In the Bayesian approach line
of work (Griffiths & Ghahramani, 2011; Broderick et al.,
2013; Zoubin, 2013; Doshi-Velez & Ghahramani, 2009;
Hayashi & Fujimaki, 2013), the goal is to infer the posterior
distribution P (Z, W |X) given X. The line of work using
Spectral Methods (Tung & Smola, 2014) on the other hand
aim to estimate p(z), p(x|z) in turn by estimating parameters (π, W ). In some other work (Slawski et al., 2013), they
aim to estimate W , leaving the distribution of z unmodeled.
In this paper, we focus on the more realistic setting where
we make no assumption on p(x) except that of boundedness, and aim to find an LFM W ∗ that minimizes the risk
r(W ) := E[ min

z∈{0,1}K

1
kx − W T zk2 ].
2

(1)

where the expectation is over the random observation x.

3. Latent Feature Lasso

K∈N,Z∈{0,1}N ×K ,W ∈RK×D

1
kX − ZW k2F + λK. (2)
2N

The estimation problem in (Slawski et al., 2013) could also
be cast in the above form with λ = 0 and K treated as a
fixed hyper-parameter, while (Broderick et al., 2013) treats
K as a variable and controls it through λ. (2) is a combinatorial optimization of N × K + 1 integer variables. In
the following we develop a tight convex approximation to
(2) with `2 regularization on W , by introducing a type of
atomic norm (Chandrasekaran et al., 2012).
For a fixed K, Z, consider the minimization over W of the
`2 regularized version of (2)
min

W ∈RK×D

1
τ
kX − ZW k2F + kW k2F ,
2N
2

(3)

which is a convex minimization problem. Applying Lagrangian duality to (3) results in the following dual form

max

A∈RN ×D

0: A =, c = 0.
for t = 1...T do
1: Find a greedy atom zz T by solving (8).
2: Add zz T to an active set A.
3: Minimize (7) w.r.t. coordinates in A via updates (9).
4: Eliminate {zj zjT |cj = 0} from A.
end for.
of the objective when optimized over A. The objective in
(2) for a fixed K could thus be simply reformulated as a
minimization of this dual-derived objective g(M ). It can
be seen that g(M ) is a convex function w.r.t. M since it
is the maximum of linear functions of M . The key caveat
however is the combinatorial structure on M since it has
the form M = ZZ T , Z ∈ {0, 1}N ×K . We address this
caveat by introducing the following atomic norm
kM kS := min

We first consider the non-convex formulation that was also
previously studied in (Broderick et al., 2013) as asymptotics of the MAP estimator of IBP Linear-Gaussian model:

min

Algorithm 1 A Greedy Algorithm for Latent Feature Lasso


1 ∗
−1
T
tr(AA M ) − L (xi , −Ai,: ) . (4)
2N 2 τ
N

where M := ZZ T , A ∈ RN ×D are dual variables that
satisfy W ∗ = N1 Z ∗ A∗ at the optimum of (3) and (4), and
L∗ (x, α) = hx, αi + 21 kαk2 is the convex conjugate of
square loss L(x, ξ) = 12 kx−ξk2 w.r.t. its second argument.
Let G(M, A) denote the objective in (4) for any fixed M ,
and let g(M ) = maxA G(M, A) denote the optimal value

c≥0

K
X

ca s.t. M =

a∈S

X

ca a.

(5)

a∈S

P
with S := {zz T |z ∈ {0, 1}N }. Note kM kS = a∈S ca =
K when ca in (5) are constrained at integer value {0, 1},
and it serves a convex approximation to K similar to the
`1 -norm used in Lasso for the approximation of cardinality.
This results in the following Latent Feature Lasso estimator
min {g(M ) + λkM kS } .

(6)

M

4. Algorithm
The estimator (6) seems intractable at first sight in part
since the atomic norm involves a set S of 2N atoms. In
this section, we study a variant of approximate greedy coordinate descent method for tractably solving problem (6).
We begin by rewriting the optimization problem (6) as an
`1 -regularized problem with K̄ = 2N − 1 coordinates, by
expanding the matrix M in terms of the K̄ atoms underlying the atomic norm k.kS :














K̄


X
T
min g 
(7)
cj zj zj  +λkck1


c∈RK̄

+ 
j=1






{z
}
|

f (c)

{z

|

F (c)

}

N
where {zj }K̄
patterns
j=1 enumerates all possible {0, 1}
except the 0 vector. Our overall algorithm is depicted in
Algorithm 1. In each iteration, it finds

j ∗ := arg max −∇j f (x)
j

= arg max h−∇g(M ), zj zjT i
j

(8)

Latent Feature Lasso

approximately with a constant approximation ratio via a
reduction to a MAX-CUT-like problem (see Section 4.1).
An active set A is maintained to contain all atoms zj zjT
with non-zero coefficients cj and the atom returned by the
greedy search (8). Then we minimize (7) over coordinates
in A by a sequence of proximal updates:


∇f (cr ) + λ
, r = 1...T2
(9)
cr+1 ← cr −
γ|A|
+
where γ is the Lipschitz-continuous constant of the
coordinate-wise gradient ∇cj f (c).
Computing cooordinate-wise gradients. By Danskin’s
Theorem, the gradient of function f (c) takes the form
∗

∇cj f (c) = zj A A

∗T

2

zj /(2N τ ),

(10)

4.1. Greedy Atom Generation
A key step to the greedy algorithm (Algorithm 1) is to find
the direction (8) of steepest descent, which however is a
convex maximization problem with binary constraints that
in general cannot be exactly solved in polynomial time.
Fortunately in this section, we show that (8) is equivalent to a MAX-CUT-like Boolean Quadratic Maximization problem that has efficient Semidefinite relaxation with
constant approximation guarantee. Furthermore, the resulting Semidefinite Programming (SDP) problem is of special
structure that allows iterative method of complexity linear
to the matrix size (Boumal et al., 2016; Wang & Kolter,
2016).
In particular, let C=∇g(M )=A∗ A∗T /(2τ N ) the maximization problem

which in turn requires finding the maximizer A∗ of (4).

max hC, zz T i

(13)

z∈{0,1}N

Computing A∗ . By taking advantage of the strong duality between (4) and (3), the maximizer A∗ can be found by
finding the minimizer W ∗ of
X τ
1
kX − ZA W k2F +
kWk,: k2
(11)
min
W
2N
2ck
k∈A

max

y∈{−1,1}N

and computing A∗ = (X − ZA W ∗ ), where ZA denotes
N × |A| matrix of columns taking from the active atom
basis {zk }k∈A .
Computing W ∗ . There is a closed-form solution W ∗ to
(11) of the form
T
T
W ∗ = (ZA
ZA + N τ diag−1 (cA ))−1 ZA
X.

can be reduced to an optimization problem over variables
taking values in {−1, 1} via the transformation y = 2z − 1,
which results in the problem

(12)

T
An efficient way of computing (12) is to maintain ZA
ZA
T
and ZA X whenever the active set of atoms A changes.
This has a cost of O(N DKA ) for a bound KA on the active
size, which however is almost neglectable compared to the
other costs when amortized over iterations. Then the eval3
2
uation of (12) would cost only O(KA
+ KA
D) for each
evaluation of different c. Similarly the matrix computation
of (10) can be made more efficient as ∇c f (c) ∝
T
T
T
T
diag((ZA
X − ZA
ZA W ∗ )(ZA
X − ZA
ZA W ∗ )T )

can be computed in O(K 2 D + K 3 ) via the maintenance of
T
T
ZA
ZA , ZA
X.
The output of Algorithm 1 is the coefficient vector c, and
with the resulting latent feature matrix W (c) given by
(12). Since the solution could contain many atoms of small
weight ck . In practice, we perform a rounding procedure
that ranks atoms according to the score {ck kWk,: k2 }k∈A
and then pick top K atoms as the output Z ∗ , and solve a
simple least-squares problem to obtain the corresponding
W ∗.


1
hC, yy T i + 2hC, 1y T i + hC, 11T i . (14)
4

where 1 denotes N -dimensional vector of all 1s. By introducing a dummy variable y0 , (14) can be expressed as

max

(y0 ;y)∈{−1,1}N +1

1
4



y0
y

T 

1T C1 1T C
C1
C



y0
y


.

(15)
Note that one can ensure finding a solution with y0 = 1 by
flipping signs of the solution vector to (15), since this does
not change the quadratic form objective value. Denote the
quadratic form matrix in (15) be Ĉ. Problem of form (15)
is a MAXCUT-like Boolean Quadratic problem, for which
there is SDP relaxation of the form
max

Y ∈SN

s.t.

hĈ, Y i
(16)
Y  0, diag(Y ) = 1

rounding from which guarantees a solution ŷ to (15) satisfying
h − h(ŷ) ≤ ρ(h − h)
(17)
for ρ = 2/5 (Nesterov et al., 1997), where h(y) denotes the
objective function of (15) and h, h denote the maximum,
minimum value achievable by some y ∈ {−1, 1}N +1 respectively. Note this result holds for any symmetric matrix
Ĉ. Since our problem has a positive-semidefinite matrix Ĉ,
h = 0 and thus
−∇ĵ f (c) = h(ŷ) ≥ µh = µ(−∇j ∗ f (c))

(18)

Latent Feature Lasso

for µ = 1 − ρ = 3/5, where ĵ is coordinate selected by
rounding from a solution of (16) and j ∗ is the exact maximizer of (8).
Finally, it is noteworthy that, although solving a general
SDP is computationally expensive, SDP of the form (16)
has been shown to allow much faster solver that has linear
cost w.r.t. the matrix size nnz(Ĉ) (Boumal et al., 2016;
Wang & Kolter, 2016). In our implementation we adopt
the method of (Wang & Kolter, 2016) due to its strong empirical performance.

5. Analysis

Then since kc∗ k1 = kc∗ k0 = K ∗ , Theorem 2 shows that
our algorithm has an iteration complexity of O(K/) to
achieve
√  error, with an additional error term proportional
to λ K due to the approximation made in (18).
5.2. Risk Analysis
In this section, we investigate the performance of the output from Algorithm 1 in terms of the population risk r(·)
defined in (1). Given coefficients c with support A obtained
from algorithm (1) for T iterations, we construct the weight
√
T ∗
A ,
matrix by Ŵ = diag( cA )W ∗ with W ∗ (cA ) = N1 ZA
∗
where A is the maximizer of (4) as a function of c. It can
be seen that Ŵ satisfies

5.1. Convergence Analysis
The aim of this section is to show the convergence of Algorithm 1 under the approximation of greedy atom generation. In particular, we show the multiplicative approximation error incurred in the step (8) only contributes an
additive approximation error proportional to λ, as stated in
the following theorem.
Theorem 1. The greedy algorithm proposed (Algorithm 1)
satisfies
F (ct ) − F (c∗ ) ≤

2γkc∗ k21 1 2(1 − µ)
+
λkc∗ k1 ,
µ2
t
µ
|
{z
}

F (c) =

1
τ
kX − ZA Ŵ k2 + kŴ k2F + λkcA k1 . (19)
2N
2

The following theorem gives a risk bound for Ŵ . Without
loss of generality, we assume x is bounded and scaled such
that kxk ≤ 1.
√
Theorem 3. Let Ŵ = diag( cA )W ∗ (cA ) be the weight
matrix obtained from T iterations of Algorithm 1, and W̄
be the minimizer of the population risk (1) with K components and kW̄ kF ≤ R. We then have the following bound
on population risk: r(Ŵ ) ≤ r(W̄ ) +  with probability
1 − ρ for

∆(λ)

T ≥
where c∗ is any reference solution, µ = 3/5 is the approximation ratio given by (18) and γ is the Lipschitz-continuous
constant of coordinate-wise gradient ∇j f (c), ∀j ∈ [K].
The theorem thus shows that the iterates converge sublinearly to within statistical precision λ of any reference
solution c∗ scaled in main by its `1 norm kc∗ k1 . In the following theorem, we show that, with the additional assumption that F (c) is strongly convex over a restricted support
set A∗ , one can get a bound in terms of the `0 -norm of a
reference solution c∗ with support A∗ .
Theorem 2. Let A∗ ∈ [K̄] be a support set and c∗ :=
arg minc:supp(c)=A∗ F (c∗ ). Suppose F (c) is strongly convex on A∗ with parameter β. The solution given by Algorithm 1 satisfies
s
 
∗
2(1
−
µ)λ
2kc∗ k0
1
4γkc
k
0
F (cT ) − F (c∗ ) ≤
+
.
2
βµ
T
µ
β
Let K̄ = 2N be the size of the atomic set. Any target latent
structure Z ∗ W ∗ can be expressed as ZD(c∗ )W̃ ∗ where Z
is an N × K̄ dictionary matrix, D(c∗ )p
is a K̄ × K̄ diagonal
matrix of diagonal elements Dkk = c∗k with c∗k = 1 for
columns corresponding to Z ∗ and c∗k = 0 for the others,
and W̃ ∗ is W ∗ padded with 0 on rows in {k | ck = 0}.

4γ K
DK
RK
( ) and N = Ω( 3 log(
)),
µ2 β 

ρ

with λ, τ chosen appropriately as functions of N .
Note the output of Algorithm 1 has number of components
K̂ bounded by number of iterations T . Therefore, Theorem (3) gives us a trade-off between risk and sparsity—
one can guarantee to achieve -suboptimal risk compared
to the optimal solution of size K, via O(K/) components and Õ(DK/3 ) samples. Notice the result (3) is obtained without any distributional assumption on p(x) and
p(z) except that of boundedness. Comparatively, the theoretical result obtained from Spectral Method (Tung &
Smola, 2014) requires the knowledge/assumption of the
distribution p(x|z), p(z), which is sensitive to model misspecification in practice.
5.3. Identifiability
It is noteworthy that the true parameters (Z ∗ , W ∗ ) might
not be identifiable. In particular, it is possible to have
(Z, W ) 6= (Z ∗ , W ∗ ) with ZW = Z ∗ W ∗ , in which case
it is impossible to recover the true parameters (Z ∗ , W ∗ )
from Θ∗ = Z ∗ W ∗ . The following theorem introduces conditions that ensure uniqueness of the factorization Θ∗ =
Z ∗W ∗.
Theorem 4. Let Θ∗ = Z ∗ W ∗ be of rank K. If

Latent Feature Lasso
Fail-Prob

λ ≤ λ̄ and solve (20) to obtain a solution (c, W ) of (21),
which satisfies the following theorem.
Theorem 5. Let (c, W ) be a solution to (21), and
(ZS , WS ) be columns of Z and rows of W corresponding
to the set of non-zero indexes S of c respectively. Suppose
the identifiability condition in Theorem 4 holds and WS has
full row-rank. Then

100
K=10
K=15
K=20
K=5

90
80

number-of-failures

70
60
50
40
30
20

∗ K
∗ K
{Z:,j }j∈S = {Z:,j
}j=1 , {Wj,: }j∈S = {Wj,:
}j=1

10
0
10 -1

10 0

p

Figure 1: The frequency of failures of condition in Theorem 4 out of 100 trials, for a spectrum of i.i.d. Bernoulli
parameter p and different K. We use algorithm proposed
in (Slawski et al., 2013) to check the condition efficiently.

Note since we can choose an arbitrarily small λ ≤ λ̄ to
find a solution of (21). The approximation error due to
approximate atom generation can be reduced to arbitrarily
small.
5.5. Parameter Recovery under Noise

1. Z ∗ :N × K and W ∗ :K × D are both of rank K.
∗

N

2. span(Z ) ∩ {0, 1} \ {0} =

∗ K
{Z:,j
}j=1 .

Then for any rank-K matrices Z:N × K and W :K ×
∗ K
D, ZW = Θ∗ implies {Z:,j }K
j=1 = {Z:,j }j=1 and
K
∗ K
{Wj,: }j=1 = {Wj,: }j=1 .
The conditions in Theorem 4 are similar to that discussed
in (Slawski et al., 2013), where an additional affine constraint on W is considered. For random binary matrix of
binary value {−1, +1} instead of {0, 1}, the conditions
are known to hold with high probability when entries are
i.i.d. Bernoulli(0.5) (Tao & Vu, 2007; Kahn et al., 1995).
Here we also conduct numerical experiments for matrices
of i.i.d. Bernoulli(p) with a wide range of p. Results in
Figure 1 shows that the probability with which such condition fails is almost 0 when p ≥ 0.1, while it increases when
p becomes smaller than 0.1.
5.4. Parameter Recovery without Noise
Let the true parameters be (Z ∗ , W ∗ ) with kW ∗ k2F = R.
We can find some τ (R) such that the estimator (6) is equivalent to solving the following problem:
1
kX − Zdiag(c)W k2 + λkck1
2N
(20)
2
s.t.
kW kF ≤ R.
√
where diag(c) is a diagonal matrix with diagkk (c) = ck .
In the noiseless setting ( = 0), one can find a feasible
solution to the following problem
min

K̄×D
c∈RK̄
+ ,W ∈R

min
K̄×D
c∈RK̄
+ ,W ∈R

s.t.

In the noisy setting, parameter recovery is more tricky.
When the model is unbiased (i.e. E[x|z] = (W ∗ )T z) , by
appealing to well-known results in high-dimensional estimation (Negahban et al., 2009), we can achieve a bound on
the `2 norm of the error ĉ−c∗ , where c∗ is coefficient vector
corresponding to the ground-truth parameter (W ∗ , Z ∗ ).
We defer the resulting Theorem 8 to Section 7.8 in the Appendix. √The theorem bounds the `2 error kĉ − c∗ k2 as
(1/κn ) K ∗ ρn , where ρn is a term capturing the noiselevel, κn is a term capturing the restricted strong convexity
of the objective, and defined in detail in Section 7.8.
However, extending this bound on kc−c∗ k to derive bounds
on kZ − Z ∗ k and kW − W ∗ k is a delicate matter that we
defer to future work, in part due to the exponential size
K̄ = 2N of the atomic set, and since the size of Z grows
with N . In particular, in the following theorem, we show
that it is in general not possible to estimate Z ∗ accurately
even with a large number of samples.
Theorem 6. Let K = D = 1. Consider the following
noise model: X = Z ∗ W ∗ + E, where Z ∗ ∈ {0, 1}N ,
W ∗ ∈ R and ∀i ∈ [N ], Ei are i.i.d random variables with
Ei ∼ N (0, 1). Moreover, suppose we know the true parameter W ∗ = 1. Then the Latent Feature Model estimator
for Z ∗ given by:
Ẑ = argmin
Z∈{0,1}N

1
kX − ZW ∗ k2
2N

(22)

satisfies the following:
E(kẐ − Z ∗ k22 ) ≥ cN,
for some positive constant c.

kck1
(21)
ZD(c)W = X, kW k2F ≤ R,

which is equivalent to problem (20) with any λ ≤ λ̄ for
some λ̄ > 0. One can thus choose an arbitrarily small

6. Experiments
In this section, we compare our proposed method with
other state-of-the-art approaches on both synthetic and real

Latent Feature Lasso
Syn0-Hamming

0.4

Syn2-Hamming

Syn1-Hamming

0.45
0.4

0.4

0.35

0.35

0.2
0.15

0.3
0.25
0.2

Hamming-Err

0.4

Hamming-Err

Hamming-Err

0.25

0.45

0.35

0.3

Hamming-Err

Syn3-Hamming

0.5
0.45

0.35

0.3
0.25
0.2
0.15

0.15

0.3
0.25
0.2
0.15

0.1
0.05

0

2

4

6

8

0.1

0.1

0.05

0.05

0

10

0

5

10

K
0.25

15

0

20

0

10

20

K

Syn0-RMSE

0.25

0.2

30

40

0

50

0.35

RMSE

RMSE

RMSE

0.15

0.1

10

3

0.25

2.5

0.2

6

8

10

5

10

K

15

20

50

BP-Means
LatentLasso
MCMC
MF-Binary
Spectral
Variational

0.5

0

40

2

0.05
4

50

1.5
1

0.1
0.05

2

40

3.5

0.3

0.15

0.1

30

K
Syn1-RMSE

0.05

0

20

Syn3-RMSE

0.2

0.15

0

K

Syn2-RMSE

RMSE

0

0.1
0.05

0

10

20

K

30

40

0

50

0

10

20

K

30

K

Figure 2: From left to right, each column are results for Syn0 (K=4), Syn2 (K=14), Syn3 (K=35) and Syn1 (K=35)
respectively. The first row shows the Hamming loss between the ground-truth binary assignment matrix Z ∗ and the
recovered ones Ẑ. The second row shows RMSE between Θ∗ = Z ∗ W ∗ and the estimated Θ̂ = Ẑ Ŵ .
Tabletop-RMSEnoisy

Mnist1k-RMSEnoisy

0.065

0.26

0.055

0.24

YaleFace-RMSEnoisy

0.18

0.28

0.06

Yeast-RMSEnoisy
0.095

0.17

0.09

0.16

0.085

0.15

0.04

0.2

0.14

RMSE

0.045

RMSE

0.22

RMSE

RMSE

0.05

0.13

0.18

0.12

0.07

0.16

0.11

0.065

0.1

0.06

0.035
0.03
0.14
0.025

0.09
2

4

6

8

10

0

10

K

20

30

40

50

BP-Means
LatentLasso
MCMC
MF-Binary
Spectral
Variational

0.055

0.12
0

0.08
0.075

0

10

K

20

30

K

40

50

0

10

20

30

40

50

K

Figure 3: From left to right are results for Tabletop, Mnist1k, YaleFace and Yeast, where Spectral Method does not appear
in the plots for YaleFace and Yeast due to a much higher RMSE, and Variational method reports a runtime error when
running on the YaleFace data set.
Table 1: Data statistics.
Dataset
Syn0
Syn1
Syn2
Syn3
Tabletop
Mnist1k
YaleFace
Yeast

N
100
1000
1000
1000
100
1000
165
1500

D
196
1000
900
900
8560
777
2842
104

K
4
35
14
35
4
n/a
n/a
n/a

σ
0
0.01
0.1
0.1
n/a
n/a
n/a
n/a

∗
nnz(Wk,:
)
≤8
1000
49
36
n/a
n/a
n/a
n/a

data sets. The dataset statistics are listed in Table 1. For
the synthetic data experiments, we used a benchmark simulated dataset Syn0 that was also used in (Broderick et al.,
2013; Tung & Smola, 2014). But since this has only a
small number of latent features (K = 4), to make the task
more challenging, we created additional synthetic datasets
(which we denote Syn1, Syn2, Syn3) with more latent fea-

tures. Figure 4 shows example of our synthetic data, where
we reshape dimension D into an image and pick a contiguous region. Each pixel W (k, j) in the region is set as
N (0, σ 2 ), while pixels not in the region are set to 0. In
the examples of Figure 4, the region has size nnz(W (k, :
))=36. Note the problem becomes harder when the region
size nnz(W (k, :)), number of features K, or noise level σ
becomes larger. For real data, we use a benchmark Tabletop data set constructed by (Griffiths & Ghahramani, 2005),
where there is a ground-truth number of features K = 4 for
the 4 objects on the table. We also take two standard multilabel (multiclass) classification data sets Yeast and Mnist1k
from the LIBSVM repository 1 , and one Face data set YaleFace from the Yale Face database 2 .
Given the estimated factorization (Z, W ), we use the following 3 evaluation metrics to compare different algorithms:
1
2

https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
http://vision.ucsd.edu/content/yale-face-database

Latent Feature Lasso

Figure 4: Sample images from the synthetic data we created (i.e. Syn1, Syn2, Syn3). The first row shows observations Xi,: , and the second row shows latent features Wk,: .
• Hamming-Error: minS:|S|=K
• RMSE:

kZ ∗ W ∗ −ZW kF
√
ND

• RMSEnoisy:

kZ:,S −Z ∗ k2F
NK

.

.

kX−ZW kF
√
ND

.

where the first two can only be applied when the ground
truth Z ∗ are W ∗ are given. For real data, we can only evaluate the noisy version of RMSE, which can be interpreted
as trying to find a best approximation to the observation X
via a factorization with binary components.
The methods in comparison are listed as follows: (a)
MCMC: An accelerated version of the Collapsed Gibbs
sampler for the Indian Buffet Process (IBP) model (DoshiVelez & Ghahramani, 2009). We adopted the implementation published by 3 . We ran it with 25 random restarts
and recorded the best results for each K. (b) Variational:
A Variational approximate inference method for IBP proposed in (Doshi-Velez et al., 2009). We used implementation published by the author 4 . (c) MF-Binary: A Matrix
Factorization with the Binary Components model (Slawski
et al., 2013), which has recovery guarantees in the noiseless case but has a O(K2K ) complexity and thus cannot
scale to K > 30 on our machine. We use the implementation published by the author 5 . (d) BP-Means: A local
search method that optimizes a MAD-Bayes Latent Feature objective function (Broderick et al., 2013). We used
code provided by the author 6 . We ran it with 100 random
restarts and recorded the best result. (e) Spectral: Spectral
Method for IBP Linear Gaussian model proposed in (Tung
& Smola, 2014). We used code from the author. The implementation has a memory requirement that restricts its
use to K < 14. (f) LatentLasso: The proposed Latent
Feature Lasso method (Algorithm 1).
The results are shown in Figure 2 and 3. On synthetic data,
we observe that, when the number of features K is small
3

https://github.com/davidandrzej/PyIBP
http://mloss.org/software/view/185/
5
https://sites.google.com/site/
slawskimartin/code
6
https://github.com/tbroderick/bp-means
4

(e.g. Syn0), most of methods perform reasonably well.
However, when the number of features becomes slightly
larger (i.e. K = 35 in Syn1, Syn3), most of algorithms
lose their ability of recovering the hidden structure, and
when they fail to do so, they can hardly find a good approximation to Θ∗ = Z ∗ W ∗ even using a much larger
number of components up to 50. We found the proposed
LatentLasso method turns out to be the only method that
can still recover the desired hidden structure on the Syn1
and Syn3 data sets, which gives 0 RMSE and Hamming
Error. On Syn2 (K = 14) data set, MF-Binary and LatentLasso are the only two methods that achieve 0 RMSE
and Hamming-Error. However, MF-Binary has a complexity growing exponential with K, which results in its failure
on Syn1 and Syn3 due to a running time more than one
day when K > 30. The proposed LatentLasso algorithm
actually runs significantly faster than other methods in our
experiments. For example, on the Syn1 dataset (N=1000,
D=1000, K=35), the runtime of LatentLasso is 398s, while
MCMC, Variational, MF-Binary and BP-Means all take
more than 10000s to obtain their best results reported in
the Figures. We provide a comparison of the time complexities of all compared methods in Section 7.1 in the Appendix. Our overall lower time complexity is also corroborated empirically by our experiments. We also observe that
LatentLasso is the only method that has RMSE and Hamming error monotonically decreasing with K. On Syn0 and
Tabletop which have ground-truth K = 4, we found most
of algorithms could become unstable when trying to use
a number of components K larger than the ground truth.
Among all algorithms, Spectral, Variational methods are
the most unstable, while BP-Means and MCMC are more
stable possibly due to the large number of random re-trials
employed in their procedures.
On real data sets, the LFM model assumption might not
hold, and might serve at best as an approximation to the
ground-truth. Even in such cases, we found that our LatentLasso method finds a better approximation than other
existing approaches, especially when using a larger number of components K. We conjecture that for local search
methods, the performance breakdown for larger K is possibly due to an exponentially increased number of local
optimums, which makes strategies such as random restarts
less effective for methods such as BP-Means and MCMC.
On the other hand, the Spectral Method simply has a sample complexity bound with a high-order polynomial dependency on K, which makes the estimation error increase dramatically as K becomes larger.

Latent Feature Lasso

Acknowledgements P.R. acknowledges the support of
ARO via W911NF-12-1-0390 and NSF via IIS-1149803,
IIS-1447574, and DMS-1264033, and NIH via R01
GM117594-01. S.D. Lin acknowledges the support of
AOARD and MOST via 104-2628-E-002-015-MY3.

References
Airoldi, Edoardo M, Blei, David, Erosheva, Elena A, and
Fienberg, Stephen E. Handbook of Mixed Membership Models and Their Applications. Chapman and
Hall/CRC, 2014.
Boumal, Nicolas, Voroninski, Vlad, and Bandeira, Afonso.
The non-convex burer-monteiro approach works on
smooth semidefinite programs. In Advances in Neural
Information Processing Systems, pp. 2757–2765, 2016.
Broderick, Tamara, Kulis, Brian, and Jordan, Michael I.
Mad-bayes: Map-based asymptotic derivations from
bayes. In ICML (3), pp. 226–234, 2013.
Chandrasekaran, Venkat, Recht, Benjamin, Parrilo,
Pablo A, and Willsky, Alan S. The convex geometry of
linear inverse problems. Foundations of Computational
mathematics, 12(6):805–849, 2012.
d’Aspremont, Alexandre, El Ghaoui, Laurent, Jordan,
Michael I, and Lanckriet, Gert RG. A direct formulation
for sparse pca using semidefinite programming. SIAM
review, 49(3):434–448, 2007.
Doshi-Velez, Finale and Ghahramani, Zoubin. Accelerated
sampling for the indian buffet process. In Proceedings
of the 26th annual international conference on machine
learning, pp. 273–280. ACM, 2009.
Doshi-Velez, Finale, Miller, Kurt T, Van Gael, Jurgen, Teh,
Yee Whye, and Unit, Gatsby. Variational inference for
the indian buffet process. In Proceedings of the Intl.
Conf. on Artificial Intelligence and Statistics, volume 12,
pp. 137–144, 2009.
Griffiths, Thomas L and Ghahramani, Zoubin. Infinite latent feature models and the indian buffet process. In
NIPS, volume 18, pp. 475–482, 2005.
Griffiths, Thomas L and Ghahramani, Zoubin. The indian
buffet process: An introduction and review. Journal of
Machine Learning Research, 12(Apr):1185–1224, 2011.
Hayashi, Kohei and Fujimaki, Ryohei. Factorized asymptotic bayesian inference for latent feature models. In Advances in Neural Information Processing Systems, pp.
1214–1222, 2013.
Jolliffe, Ian T, Trendafilov, Nickolay T, and Uddin, Mudassir. A modified principal component technique based

on the lasso. Journal of computational and Graphical
Statistics, 12(3):531–547, 2003.
Kahn, Jeff, Komlós, János, and Szemerédi, Endre. On the
probability that a random±1-matrix is singular. Journal
of the American Mathematical Society, 8(1):223–240,
1995.
Negahban, Sahand, Yu, Bin, Wainwright, Martin J, and
Ravikumar, Pradeep K. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information
Processing Systems, pp. 1348–1356, 2009.
Nesterov, Yurii et al. Quality of semidefinite relaxation for nonconvex quadratic optimization. Université
Catholique de Louvain. Center for Operations Research
and Econometrics [CORE], 1997.
Shaban, Amirreza, Farajtabar, Mehrdad, Xie, Bo, Song, Le,
and Boots, Byron. Learning latent variable models by
improving spectral solutions with exterior point method.
In UAI, pp. 792–801, 2015.
Slawski, Martin, Hein, Matthias, and Lutsik, Pavlo. Matrix
factorization with binary components. In Advances in
Neural Information Processing Systems, pp. 3210–3218,
2013.
Tao, Terence and Vu, Van. On the singularity probability
of random bernoulli matrices. Journal of the American
Mathematical Society, 20(3):603–628, 2007.
Tung, Hsiao-Yu and Smola, Alexander J. Spectral methods for indian buffet process inference. In Advances in
Neural Information Processing Systems, pp. 1484–1492,
2014.
Ueda, Naonori and Saito, Kazumi. Parametric mixture
models for multi-labeled text. Advances in neural information processing systems, pp. 737–744, 2003.
Wang, Po-Wei and Kolter, J Zico. The mixing method for
maxcut-sdp problem. NIPS LHDS Workshop., 2016.
Wu, Lingfei, Yen, Ian EH, Chen, Jie, and Yan, Rui. Revisiting random binning features: Fast convergence and
strong parallelizability. In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1265–1274. ACM, 2016.
Zhao, Han and Poupart, Pascal. A sober look at spectral
learning. arXiv preprint arXiv:1406.4631, 2014.
Zoubin, Ghahramani. Scaling the indian buffet process via
submodular maximization. In Proceedings of the 30th
International Conference on Machine Learning (ICML13), pp. 1013–1021, 2013.

