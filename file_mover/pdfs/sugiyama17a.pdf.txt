Tensor Balancing on Statistical Manifold

Mahito Sugiyama 1 2 Hiroyuki Nakahara 3 Koji Tsuda 4 5 6

Abstract
We solve tensor balancing, rescaling an N th order nonnegative tensor by multiplying N tensors of order N ‚àí 1 so that every fiber sums to
one. This generalizes a fundamental process of
matrix balancing used to compare matrices in a
wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton‚Äôs method and show in numerical experiments
that the proposed algorithm is several orders of
magnitude faster than existing ones. To theoretically prove the correctness of the algorithm,
we model tensors as probability distributions in
a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to
our algorithm is that the gradient of the manifold,
used as a Jacobian matrix in Newton‚Äôs method,
can be analytically obtained using the MoÃàbius inversion formula, the essential of combinatorial
mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it
includes various statistical and machine learning
models such as weighted DAGs and Boltzmann
machines.

1. Introduction
Matrix balancing is the problem of rescaling a given square
nonnegative matrix A ‚àà Rn√ón
‚â•0 to a doubly stochastic matrix RAS, where every row and column sums to one, by
multiplying two diagonal matrices R and S. This is a
fundamental process for analyzing and comparing matrices in a wide range of applications, including input-output
analysis in economics, called the RAS approach (Parikh,
1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004),
seat assignments in elections (Balinski, 2008; Akartunalƒ± &
1
National Institute of Informatics 2 JST PRESTO 3 RIKEN
Brain Science Institute 4 Graduate School of Frontier Sciences,
The University of Tokyo 5 RIKEN AIP 6 NIMS. Correspondence
to: Mahito Sugiyama <mahito@nii.ac.jp>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Statistical manifold ùì¢
(dually flat Riemannian manifold)

Given tensor A

Probability
distribution P
Tensor balancing

Projection

Multistochastic tensor A‚Äô

Submanifold ùì¢(Œ≤)

Every fiber
sums to 1

Projected
distribution PŒ≤

Figure 1. Overview of our approach.

Knight, 2016), Hi-C data analysis (Rao et al., 2014; Wu &
Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and
the optimal transportation problem (Cuturi, 2013; Frogner
et al., 2015; Solomon et al., 2015). An excellent review of
this theory and its applications is given by Idel (2016).
The standard matrix balancing algorithm is the SinkhornKnopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp,
1967; Marshall & Olkin, 1968; Knight, 2008), a special
case of Bregman‚Äôs balancing method (Lamond & Stewart,
1981) that iterates rescaling of each row and column until
convergence. The algorithm is widely used in the above
applications due to its simple implementation and theoretically guaranteed convergence. However, the algorithm
converges linearly (Soules, 1991), which is prohibitively
slow for recently emerging large and sparse matrices. Although Livne & Golub (2004) and Knight & Ruiz (2013)
tried to achieve faster convergence by approximating each
step of Newton‚Äôs method, the exact Newton‚Äôs method with
quadratic convergence has not been intensively studied yet.
Another open problem is tensor balancing, which is a generalization of balancing from matrices to higher-order multidimentional arrays, or tensors. The task is to rescale an
N th order nonnegative tensor to a multistochastic tensor,
in which every fiber sums to one, by multiplying (N ‚àí 1)th
order N tensors. There are some results about mathematical properties of multistochastic tensors (Cui et al., 2014;
Chang et al., 2016; Ahmed et al., 2003). However, there
is no result for tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now.

Tensor Balancing on Statistical Manifold

Here we show that Newton‚Äôs method with quadratic convergence can be applied to tensor balancing while avoiding solving a linear system on the full tensor. Our strategy is to realize matrix and tensor balancing as projection onto a dually flat Riemmanian submanifold (Figure 1),
which is a statistical manifold and known to be the essential structure for probability distributions in information
geometry (Amari, 2016). Using a partially ordered outcome space, we generalize the log-linear model (Agresti,
2012) used to model the higher-order combinations of binary variables (Amari, 2001; Ganmor et al., 2011; Nakahara & Amari, 2002; Nakahara et al., 2003), which allows
us to model tensors as probability distributions in the statistical manifold. The remarkable property of our model is
that the gradient of the manifold can be analytically computed using the MoÃàbius inversion formula (Rota, 1964), the
heart of combinatorial mathematics (Ito, 1993), which enables us to directly obtain the Jacobian matrix in Newton‚Äôs
method. Moreover, we show that (n ‚àí 1)N entries for the
size nN of a tensor are invariant with respect to one of the
two coordinate systems of the statistical manifold. Thus
the number of equations in Newton‚Äôs method is O(nN ‚àí1 ).
The remainder of this paper is organized as follows: We
begin with a low-level description of our matrix balancing
algorithm in Section 2 and demonstrate its efficiency in numerical experiments in Section 3. To guarantee the correctness of the algorithm and extend it to tensor balancing, we
provide theoretical analysis in Section 4. In Section 4.1, we
introduce a generalized log-linear model associated with a
partial order structured outcome space, followed by introducing the dually flat Riemannian structure in Section 4.2.
In Section 4.3, we show how to use Newton‚Äôs method to
compute projection of a probability distribution onto a submanifold. Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6.

2. The Matrix Balancing Algorithm
Given a nonnegative square matrix A = (aij ) ‚àà Rn√ón
‚â•0 , the
task of matrix balancing is to find r, s ‚àà Rn that satisfy
(RAS)1 = 1,

(RAS)T 1 = 1,

(1)

where R = diag(r) and S = diag(s). The balanced matrix
A‚Ä≤ = RAS is called doubly stochastic, in which each entry
a‚Ä≤ij = aij ri sj and all the rows and columns sum to one.
The most popular algorithm is the Sinkhorn-Knopp algorithm, which repeats updating r and s as r = 1/(As) and
s = 1/(AT r). We denote by [n] = {1, 2, . . . , n} hereafter.
In our algorithm, instead of directly updating r and s, we
update two parameters Œ∏ and Œ∑ defined as
‚àë‚àë
‚àë‚àë
pi‚Ä≤ j ‚Ä≤
(2)
Œ∏i‚Ä≤ j ‚Ä≤ , Œ∑ij =
log pij =
i‚Ä≤ ‚â§i j ‚Ä≤ ‚â§j

i‚Ä≤ ‚â•i j ‚Ä≤ ‚â•j

Matrix
a11 a12 a13 a14

Constraints for balancing
Œ∑11 Œ∑12 Œ∑13 Œ∑14

a21 a22 a23 a24

Œ∑21 Œ∏22 Œ∏23 Œ∏24

a31 a32 a33 a34

Œ∑31 Œ∏32 Œ∏33 Œ∏34

a41 a42 a43 a44

Œ∑41 Œ∏42 Œ∏43 Œ∏44 Invariant

Figure 2. Matrix balancing with two parameters Œ∏ and Œ∑.

for each
‚àë i, j ‚àà [n], where
‚àë we normalized entries as pij =
aij / ij aij so that ij pij = 1. We assume for simplicity that each entry is strictly larger than zero. The assumption will be removed in Section 5.
(t)

The key to our approach is that we update Œ∏ij with i = 1
or j = 1 by Newton‚Äôs method at each iteration t = 1, 2, . . .
(t)
while fixing Œ∏ij with i, j Ã∏= 1 so that Œ∑ij satisfies the following condition (Figure 2):
(t)

Œ∑i1 = (n ‚àí i + 1)/n,

(t)

Œ∑1j = (n ‚àí j + 1)/n.

Note that the rows and columns sum not to 1 but to 1/n due
to the normalization. The update formula is described as
Ô£Æ (t)
Ô£π
Ô£Æ (t+1) Ô£π Ô£Æ (t) Ô£π
Œ∏11
Œ∏11
Œ∑11 ‚àí (n ‚àí 1 + 1)/n
Ô£Ø
Ô£∫
Ô£Ø . Ô£∫ Ô£Ø . Ô£∫
..
Ô£Ø
Ô£∫
Ô£Ø .. Ô£∫ Ô£Ø .. Ô£∫
.
Ô£Ø
Ô£Ø
Ô£∫ Ô£Ø Ô£∫
Ô£∫
Ô£Ø (t)
Ô£∫
Ô£Ø (t+1) Ô£∫ Ô£Ø (t) Ô£∫
Ô£ØŒ∏1n Ô£∫ Ô£ØŒ∏1n Ô£∫
‚àí1 Ô£ØŒ∑1n ‚àí (n ‚àí n + 1)/nÔ£∫
Ô£∫ , (3)
Ô£Ø (t+1) Ô£∫ = Ô£Ø (t) Ô£∫ ‚àí J Ô£Ø (t)
Ô£Ø Œ∑21 ‚àí (n ‚àí 2 + 1)/n Ô£∫
Ô£ØŒ∏21 Ô£∫ Ô£ØŒ∏21 Ô£∫
Ô£Ø
Ô£∫
Ô£Ø
Ô£∫ Ô£Ø Ô£∫
Ô£Ø
Ô£∫
Ô£Ø .. Ô£∫ Ô£Ø .. Ô£∫
..
Ô£∞
Ô£∞ . Ô£ª Ô£∞ . Ô£ª
Ô£ª
.
(t)
(t)
(t+1)
Œ∑n1 ‚àí (n ‚àí n + 1)/n
Œ∏n1
Œ∏n1
where J is the Jacobian matrix given as
(t)

J(ij)(i‚Ä≤ j ‚Ä≤ ) =

‚àÇŒ∑ij

(t)

‚àÇŒ∏i‚Ä≤ j ‚Ä≤

= Œ∑max{i,i‚Ä≤ } max{j,j ‚Ä≤ } ‚àín2 Œ∑ij Œ∑i‚Ä≤ j ‚Ä≤ , (4)

which is derived from our theoretical result in Theorem 3.
Since J is a (2n‚àí1)√ó(2n‚àí1) matrix, the time complexity
of each update is O(n3 ), which is needed to compute the
inverse of J.
(t+1)

(t+1)

(t+1)

After updating to Œ∏ij , we can compute pij
and Œ∑ij
by Equation (2). Since this update does not ensure the
‚àë (t+1)
(t+1)
condition
= 1, we again update Œ∏11
as
ij pij
‚àë
(t+1)
(t+1)
(t+1)
(t+1)
Œ∏11
= Œ∏11
‚àí log ij pij
and recompute pij
(t+1)

and Œ∑ij

for each i, j ‚àà [n].

By iterating the above update process in Equation (3) until
convergence, A = (aij ) with aij = npij becomes doubly
stochastic.

3. Numerical Experiments
We evaluate the efficiency of our algorithm compared to the
two prominent balancing methods, the standard SinkhornKnopp algorithm (Sinkhorn, 1964) and the state-of-the-art

104
102
10

104
102
10
10‚Äì2
10‚Äì4

10

50

500
5000
n
Newton (proposed)

10

50

Sinkhorn

n

500

5000

BNEWT

Figure 3. Results on Hessenberg matrices. The BNEWT algorithm (green) failed to converge for n ‚â• 200.

105

109

Running time (sec.)

106

Number of iterations

108

Running time (sec.)

Number of iterations

Tensor Balancing on Statistical Manifold

106
103
10
20

100

200
300
n
Newton (proposed)

103
101
10‚Äì1
10‚Äì3
20

100

Sinkhorn

n

200

300

BNEWT

Figure 5. Results on Trefethen matrices. The BNEWT algorithm
(green) failed to converge for n ‚â• 200.

Residual

10
10‚Äì1
‚Äì

10 3
Newton
Sinkhorn
BNEWT

10‚Äì5
10‚Äì7
0

500

1000
1500
2000
Number of iterations

2500

3000

Figure 4. Convergence graph on H20 .

algorithm BNEWT (Knight & Ruiz, 2013), which uses
Newton‚Äôs method-like iterations with conjugate gradients.
All experiments were conducted on Amazon Linux AMI
release 2016.09 with a single core of 2.3 GHz Intel Xeon
CPU E5-2686 v4 and 256 GB of memory. All methods
were implemented in C++ with the Eigen library and
compiled with gcc 4.8.31 . We have carefully implemented
BNEWT by directly translating the MATLAB code provided in (Knight & Ruiz, 2013) into C++ with the Eigen
library for fair comparison, and used the default parameters. We measured the residual of a matrix A‚Ä≤ = (a‚Ä≤ij ) by
the squared norm ‚à•(A‚Ä≤ 1 ‚àí 1, A‚Ä≤T 1 ‚àí 1)‚à•2 , where each entry a‚Ä≤ij is obtained as npij in our algorithm, and ran each
of three algorithms until the residual is below the tolerance
threshold 10‚àí6 .
Hessenberg Matrix. The first set of experiments used a
Hessenberg matrix, which has been a standard benchmark
for matrix balancing (Parlett & Landis, 1982; Knight &
Ruiz, 2013). Each entry of an n √ó n Hessenberg matrix
Hn = (hij ) is given as hij = 0 if j < i ‚àí 1 and hij = 1
otherwise. We varied the size n from 10 to 5, 000, and
measured running time (in seconds) and the number of iterations of each method.
Results are plotted in Figure 3. Our balancing algorithm
with the Newton‚Äôs method (plotted in blue in the figures)
1

An implementation of algorithms for matrices and third
order tensors is available at: https://github.com/
mahito-sugiyama/newton-balancing

is clearly the fastest: It is three to five orders of magnitude
faster than the standard Sinkhorn-Knopp algorithm (plotted
in red). Although the BNEWT algorithm (plotted in green)
is competitive if n is small, it suddenly fails to converge
whenever n ‚â• 200, which is consistent with results in the
original paper (Knight & Ruiz, 2013) where there is no result for the setting n ‚â• 200 on the same matrix. Moreover,
our method converges around 10 to 20 steps, which is about
three and seven orders of magnitude smaller than BNEWT
and Sinkhorn-Knopp, respectively, at n = 100.
To see the behavior of the rate of convergence in detail, we
plot the convergence graph in Figure 4 for n = 20, where
we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT
algorithm, which contrasts with our quick convergence.
Trefethen Matrix. Next, we collected a set of Trefethen
matrices from a collection website2 , which are nonnegative diagonal matrices with primes. Results are plotted in
Figure 5, where we observe the same trend as before: Our
algorithm is the fastest and about four orders of magnitude
faster than the Sinkhorn-Knopp algorithm. Note that larger
matrices with n > 300 do not have total support, which
is the necessary condition for matrix balancing (Knight &
Ruiz, 2013), while the BNEWT algorithm fails to converge
if n = 200 or n = 300.

4. Theoretical Analysis
In the following, we provide theoretical support to our algorithm by formulating the problem as a projection within
a statistical manifold, in which a matrix corresponds to an
element, that is, a probability distribution, in the manifold.
We show that a balanced matrix forms a submanifold and
matrix balancing is projection of a given distribution onto
the submanifold, where the Jacobian matrix in Equation (4)
is derived from the gradient of the manifold.
2
http://www.cise.ufl.edu/research/sparse/
matrices/

Tensor Balancing on Statistical Manifold

4.1. Formulation
We introduce our log-linear probabilistic model, where the
outcome space is a partially ordered set, or a poset (Gierz
et al., 2003). We prepare basic notations and the key mathematical tool for posets, the MoÃàbius inversion formula, followed by formulating the log-linear model.

A probability
‚àë vector is treated as a mapping p : S ‚Üí (0, 1)
such that x‚ààS p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.
Using the zeta and the MoÃàbius functions, let us introduce
two mappings Œ∏ : S ‚Üí R and Œ∑ : S ‚Üí R as
‚àë
Œ∏(x) =
¬µ(s, x) log p(s),
(6)
s‚ààS

4.1.1. M OÃàBIUS I NVERSION
A poset (S, ‚â§), the set of elements S and a partial order
‚â§ on S, is a fundamental structured space in computer
science. A partial order ‚Äú‚â§‚Äù is a relation between elements in S that satisfies the following three properties:
For all x, y, z ‚àà S, (1) x ‚â§ x (reflexivity), (2) x ‚â§ y,
y ‚â§ x ‚áí x = y (antisymmetry), and (3) x ‚â§ y,
y ‚â§ z ‚áí x ‚â§ z (transitivity). In what follows, S is always finite and includes the least element (bottom) ‚ä• ‚àà S;
that is, ‚ä• ‚â§ x for all x ‚àà S. We denote S \ {‚ä•} by S + .
Rota (1964) introduced the MoÃàbius inversion formula on
posets by generalizing the inclusion-exclusion principle.
Let Œ∂ : S √ó S ‚Üí {0, 1} be the zeta function defined as
{
1 if s ‚â§ x,
Œ∂(s, x) =
0 otherwise.
The MoÃàbius function ¬µ : S √óS ‚Üí Z satisfies Œ∂¬µ = I, which
is inductively defined for all x, y with x ‚â§ y as
Ô£±
if x = y,
Ô£≤ 1 ‚àë
‚àí
¬µ(x,
s)
if x < y,
¬µ(x, y) =
x‚â§s<y
Ô£≥
0
otherwise.

‚àë

x‚â§s‚â§y

Œ∂(x, s)¬µ(s, y) =

s‚ààS

‚àë

(5)

x‚â§s‚â§y

with the Kronecker delta Œ¥ such that Œ¥xy = 1 if x = y and
Œ¥xy = 0 otherwise. Then for any functions f , g, and h with
the domain S such that
‚àë
‚àë
g(x) =
Œ∂(s, x)f (s) =
f (s),
h(x) =

‚àë

s‚ààS

s‚â§x

Œ∂(x, s)f (s) =

‚àë

f (s),

s‚â•x

f is uniquely recovered with the MoÃàbius function:
‚àë
‚àë
f (x) =
¬µ(s, x)g(s), f (x) =
¬µ(x, s)h(s).
s‚ààS

Œ∂(x, s)p(s) =

s‚ààS

‚àë

p(s).

From the MoÃàbius inversion formula, we have
‚àë
‚àë
log p(x) =
Œ∂(s, x)Œ∏(s) =
Œ∏(s),
s‚ààS

p(x) =

(7)

s‚â•x

‚àë

(8)

s‚â§x

¬µ(x, s)Œ∑(s).

(9)

s‚ààS

They are generalization of the log-linear model (Agresti,
2012) that gives the probability p(x) of an n-dimensional
binary vector x = (x1 , . . . , xn ) ‚àà {0, 1}n as
‚àë
‚àë
‚àë
Œ∏ijk xi xj xk
log p(x) =
Œ∏ i xi +
Œ∏ij xi xj +
i

i<j

i<j<k

+ ¬∑ ¬∑ ¬∑ + Œ∏1...n x1 x2 . . . xn ‚àí œà,
where Œ∏ = (Œ∏1 , . . . , Œ∏12...n ) is a parameter vector, œà is a
normalizer, and Œ∑ = (Œ∑ 1 , . . . , Œ∑ 12...n ) represents the expectation of variable combinations such that
Œ∑ i = E[xi ] = Pr(xi = 1),
Œ∑ 1...n = E[x1 . . . xn ] = Pr(x1 = ¬∑ ¬∑ ¬∑ = xn = 1).

¬µ(s, y) = Œ¥xy

s‚ààS

‚àë

Œ∑ ij = E[xi xj ] = Pr(xi = xj = 1), i < j, . . .

From the definition, it follows that
‚àë
‚àë
Œ∂(s, y)¬µ(x, s) =
¬µ(x, s) = Œ¥xy ,
s‚ààS

Œ∑(x) =

They coincide with Equations (8) and (7) when we let
S = 2V with V = {1, 2, . . . , n}, each x ‚àà S as the set
of indices of ‚Äú1‚Äù of x, and the order ‚â§ as the inclusion relationship, that is, x ‚â§ y if and only if x ‚äÜ y. Nakahara
et al. (2006) have pointed out that Œ∏ can be computed from
p using the inclusion-exclusion principle in the log-linear
model. We exploit this combinatorial property of the loglinear model using the MoÃàbius inversion formula on posets
and extend the log-linear model from the power set 2V to
any kind of posets (S, ‚â§). Sugiyama et al. (2016) studied a
relevant log-linear model, but the relationship with MoÃàbius
inversion formula has not been analyzed yet.
4.2. Dually Flat Riemannian Manifold

s‚ààS

This is called the MoÃàbius inversion formula and is at the
heart of enumerative combinatorics (Ito, 1993).
4.1.2. L OG -L INEAR M ODEL ON P OSETS
We consider a probability vector p on (S, ‚â§) that gives a
discrete probability distribution with the outcome space S.

We theoretically analyze our log-linear model introduced in
Equations (6), (7) and show that they form dual coordinate
systems on a dually flat manifold, which has been mainly
studied in the area of information geometry (Amari, 2001;
Nakahara & Amari, 2002; Amari, 2014; 2016). Moreover,
we show that the Riemannian metric and connection of our
model can be analytically computed in closed forms.

Tensor Balancing on Statistical Manifold

In the following, we denote by Œæ the function Œ∏ or Œ∑ and by
‚àá the gradient operator with respect to S + = S \ {‚ä•}, i.e.,
(‚àáf (Œæ))(x) = ‚àÇf /‚àÇŒæ(x) for x ‚àà S + , and denote by S the
set of probability distributions specified by probability vectors, which forms a statistical manifold. We use uppercase
letters P, Q, R, . . . for points (distributions) in S and their
lowercase letters p, q, r, . . . for the corresponding probability vectors treated as mappings. We write Œ∏P and Œ∑P if they
are connected with p by Equations (6) and (7), respectively,
and abbreviate subscripts if there is no ambiguity.
4.2.1. D UALLY F LAT S TRUCTURE
We show that S has the dually flat Riemannian structure
induced by two functions Œ∏ and Œ∑ in Equation (6) and (7).
We define œà(Œ∏) as
œà(Œ∏) = ‚àíŒ∏(‚ä•) = ‚àí log p(‚ä•),

(10)

which corresponds to the normalizer of p. It is a convex
function since we have
Ô£´
Ô£∂
‚àë
‚àë
œà(Œ∏) = log
exp Ô£≠
Œ∏(s) Ô£∏
‚àë

‚ä•<s‚â§x

x‚ààS

from log p(x) = ‚ä•<s‚â§x Œ∏(s) ‚àí œà(Œ∏). We apply the Legendre transformation to œà(Œ∏) given as
(
)
‚àë
‚Ä≤
‚Ä≤
‚Ä≤
œÜ(Œ∑) = max
Œ∏
Œ∑
‚àí
œà(Œ∏
)
,
Œ∏
Œ∑
=
Œ∏‚Ä≤ (x)Œ∑(x). (11)
‚Ä≤
Œ∏

x‚ààS +

Proof. They can be directly derived from our definitions
(Equations (6) and (11)) as
(‚àë
)
‚àë
exp
Œ∏(s)
‚àë
y‚â•x
‚ä•<s‚â§y
‚àÇœà(Œ∏)
(‚àë
) =
=‚àë
p(s) = Œ∑(x),
‚àÇŒ∏(x)
exp
Œ∏(s)
y‚ààS

(

‚ä•<s‚â§y

s‚â•x

)

‚àÇœÜ(Œ∑)
‚àÇ
=
Œ∏Œ∑ ‚àí œà(Œ∏) = Œ∏(x).
‚àÇŒ∑(x)
‚àÇŒ∑(x)
Moreover, we can confirm the orthogonality of Œ∏ and Œ∑ as
[
] ‚àë
‚àÇ log p(s) ‚àÇ log p(s)
E
=
Œ∂(x, s)¬µ(s, y) = Œ¥xy .
‚àÇŒ∏(x)
‚àÇŒ∑(y)
s‚ààS

The last equation holds from Equation (5), hence the
MoÃàbius inversion directly leads to the orthogonality.
The Bregman divergence is known to be the canonical divergence (Amari, 2016, Section 6.6) to measure the difference between two distributions P and Q on a dually flat
manifold, which is defined as
D [P, Q] = œà(Œ∏P ) + œÜ(Œ∑Q ) ‚àí Œ∏P Œ∑Q .
‚àë
In our case, since we ‚àë
have œÜ(Œ∑Q ) = x‚ààS q(x) log q(x)
and Œ∏P Œ∑Q ‚àíœà(Œ∏P ) = x‚ààS q(x) log p(x) from Theorem 1
and Equation (12), it is given as
‚àë
q(x)
D [P, Q] =
q(x) log
,
p(x)
x‚ààS

Then œÜ(Œ∑) coincides with the negative entropy.

which coincides with the Kullback‚ÄìLeibler divergence (KL
divergence) from Q to P : D [P, Q] = DKL [Q, P ].

Theorem 1 (Legendre dual).
‚àë
œÜ(Œ∑) =
p(x) log p(x).

4.2.2. R IEMANNIAN S TRUCTURE
Next we analyze the Riemannian structure on S and show
that the MoÃàbius inversion formula enables us to compute
the Riemannian metric of S.

x‚ààS

Proof. From Equation (5), we have
Ô£´
Ô£∂
‚àë
‚àë
‚àë
Ô£≠
Œ∏‚Ä≤ Œ∑ =
¬µ(s, x) log p‚Ä≤ (s)
p(s) Ô£∏
‚ä•<s‚â§x

x‚ààS +

=

‚àë

Theorem 3 (Riemannian metric). The manifold (S, g(Œæ))
is a Riemannian manifold with the Riemannian metric g(Œæ)
such that for all x, y ‚àà S +
]
Ô£±‚àë[
Ô£¥
Ô£¥
Œ∂(x, s)Œ∂(y, s)p(s) ‚àí Œ∑(x)Œ∑(y) if Œæ = Œ∏,
Ô£¥
Ô£≤
s‚ààS
gxy (Œæ) = ‚àë
Ô£¥
Ô£¥
Ô£¥
¬µ(s, x)¬µ(s, y)p(s)‚àí1
if Œæ = Œ∑.
Ô£≥

s‚â•x
‚Ä≤

‚Ä≤

p(x) ( log p (x) ‚àí log p (‚ä•) ) .

x‚ààS +

Thus it holds that
Œ∏‚Ä≤ Œ∑ ‚àí œà(Œ∏‚Ä≤ ) =

‚àë

p(x) log p‚Ä≤ (x).

x‚ààS

Proof. Since the Riemannian metric is defined as

Hence it is maximized with p(x) = p‚Ä≤ (x).

g(Œ∏) = ‚àá‚àáœà(Œ∏),

Since they are connected with each other by the Legendre
transformation, they form a dual coordinate system ‚àáœà(Œ∏)
and ‚àáœÜ(Œ∑) of S (Amari, 2016, Section 1.5), which coincides with Œ∏ and Œ∑ as follows.
Theorem 2 (dual coordinate system).
‚àáœà(Œ∏) = Œ∑,

‚àáœÜ(Œ∑) = Œ∏.

s‚ààS

(12)

(13)

g(Œ∑) = ‚àá‚àáœÜ(Œ∑),

when Œæ = Œ∏ we have
‚àÇ2
‚àÇ
œà(Œ∏) =
Œ∑(y)
‚àÇŒ∏(x)‚àÇŒ∏(y)
‚àÇŒ∏(x)
Ô£´
Ô£∂
‚àë
‚àÇ ‚àë
Œ∂(y, s) exp Ô£≠
Œ∏(u) ‚àí œà(Œ∏)Ô£∏
=
‚àÇŒ∏(x)

gxy (Œ∏) =

s‚ààS

‚ä•<u‚â§s

Tensor Balancing on Statistical Manifold

=

‚àë

Œ∂(x, s)Œ∂(y, s)p(s) ‚àí |S|Œ∑(x)Œ∑(y).

s‚ààS

When Œæ = Œ∑, it follows that
‚àÇ2
‚àÇ
œÜ(Œ∑) =
Œ∏(y)
‚àÇŒ∑(x)‚àÇŒ∑(y)
‚àÇŒ∑(x)
Ô£´
Ô£∂
‚àë
‚àÇ ‚àë
=
¬µ(s, y) log Ô£≠
¬µ(s, u)Œ∑(u)Ô£∏
‚àÇŒ∑(x)
s‚â§y
u‚â•s
‚àë
=
¬µ(s, x)¬µ(s, y)p(s)‚àí1 .

gxy (Œ∑) =

s‚ààS

Since g(Œæ) coincides with the Fisher information matrix,
[
]
‚àÇ
‚àÇ
E
log p(s)
log p(s) = gxy (Œ∏),
‚àÇŒ∏(x)
‚àÇŒ∏(y)
[
]
‚àÇ
‚àÇ
E
log p(s)
log p(s) = gxy (Œ∑).
‚àÇŒ∑(x)
‚àÇŒ∑(y)
Then the Riemannian (Levi‚ÄìChivita) connection Œì(Œæ) with
respect to Œæ, which is defined as
(
)
1 ‚àÇgyz (Œæ) ‚àÇgxz (Œæ) ‚àÇgxy (Œæ)
Œìxyz (Œæ) =
+
‚àí
2
‚àÇŒæ(x)
‚àÇŒæ(y)
‚àÇŒæ(z)
for all x, y, z ‚àà S , can be analytically obtained.
+

Theorem 4 (Riemannian connection). The Riemannian
connection Œì(Œæ) on the manifold (S, g(Œæ)) is given in the
following for all x, y, z ‚àà S + ,
Ô£±
)(
)
1 ‚àë(
Ô£¥
Ô£¥
Œ∂(x,
s)
‚àí
Œ∑(x)
Œ∂(y,
s)
‚àí
Œ∑(y)
Ô£¥
Ô£¥ 2
Ô£¥
)
Ô£¥
s‚ààS (
Ô£≤
Œ∂(z, s) ‚àí Œ∑(z) p(s)
if Œæ = Œ∏,
Œìxyz (Œæ) =
Ô£¥
Ô£¥
Ô£¥
1‚àë
Ô£¥
Ô£¥
¬µ(s, x)¬µ(s, y)¬µ(s, z)p(s)‚àí2 if Œæ = Œ∑.
‚àí
Ô£¥
Ô£≥ 2
s‚ààS

Proof. Connections Œìxyz (Œ∏) and Œìxyz (Œ∑) can be obtained
by directly computing ‚àÇgyz (Œ∏)/‚àÇŒ∏(x) and ‚àÇgyz (Œ∑)/‚àÇŒ∑(x),
respectively.
4.3. The Projection Algorithm
Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be
formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the
target model (Amari, 2016). Here we define projection of
distributions on posets and show that Newton‚Äôs method can
be applied to perform projection as the Jacobian matrix can
be analytically computed.
4.3.1. D EFINITION
Let S(Œ≤) be a submanifold of S such that
S(Œ≤) = {P ‚àà S | Œ∏P (x) = Œ≤(x), ‚àÄx ‚àà dom(Œ≤)} (14)

specified by a function Œ≤ with dom(Œ≤) ‚äÜ S + . Projection
of P ‚àà S onto S(Œ≤), called m-projection, which is defined
as the distribution PŒ≤ ‚àà S(Œ≤) such that
{
Œ∏PŒ≤ (x) = Œ≤(x)
if x ‚àà dom(Œ≤),
Œ∑PŒ≤ (x) = Œ∑P (x) if x ‚àà S + \ dom(Œ≤),
is the minimizer of the KL divergence from P to S(Œ≤):
PŒ≤ = argmin DKL [P, Q].
Q‚ààS(Œ≤)

The dually flat structure with the coordinate systems Œ∏ and
Œ∑ guarantees that the projected distribution PŒ≤ always exists and is unique (Amari, 2009, Theorem 3). Moreover,
the Pythagorean theorem holds in the dually flat manifold,
that is, for any Q ‚àà S(Œ≤) we have
DKL [P, Q] = DKL [P, PŒ≤ ] + DKL [PŒ≤ , Q].
We can switch Œ∑ and Œ∏ in the submanifold S(Œ≤) by changing DKL [P, Q] to DKL [Q, P ], where the projected distribution PŒ≤ of P is given as
{
Œ∏PŒ≤ (x) = Œ∏P (x) if x ‚àà S + \ dom(Œ≤),
Œ∑PŒ≤ (x) = Œ≤(x) if x ‚àà dom(Œ≤),
This projection is called e-projection.
Example 1 (Boltzmann machine). Given a Boltzmann machine represented as an undirected graph G = (V, E) with
a vertex set V and an edge set E ‚äÜ {{i, j} | i, j ‚àà V }.
The set of probability distributions that can be modeled by
a Boltzmann machine G coincides with the submanifold
S B = {P ‚àà S | Œ∏P (x) = 0 if |x| > 2 or x Ã∏‚àà E},
with S = 2V . Let PÃÇ be an empirical distribution estimated from a given dataset. The learned model is the mprojection of the empirical distribution PÃÇ onto S B , where
the resulting distribution PŒ≤ is given as
{
Œ∏PŒ≤ (x) = 0
if |x| > 2 or x Ã∏‚àà E,
Œ∑PŒ≤ (x) = Œ∑PÃÇ (x) if |x| = 1 or x ‚àà E.
4.3.2. C OMPUTATION
Here we show how to compute projection of a given probability distribution. We show that Newton‚Äôs method can be
used to efficiently compute the projected distribution PŒ≤ by
(0)
(0)
(1)
(2)
iteratively updating PŒ≤ = P as PŒ≤ , PŒ≤ , PŒ≤ , . . . until
converging to PŒ≤ .
(0)

Let us start with the m-projection with initializing PŒ≤

=

(t)

P . In each iteration t, we update Œ∏PŒ≤ (x) for all x ‚àà domŒ≤
(t)

while fixing Œ∑PŒ≤ (x) = Œ∑P (x) for all x ‚àà S + \ dom(Œ≤),
which is possible from the orthogonality of Œ∏ and Œ∑. Using
(t+1)
Newton‚Äôs method, Œ∑PŒ≤ (x) should satisfy
(
) ‚àë
(
)
(t)
(t+1)
(t)
Œ∏PŒ≤ (x) ‚àí Œ≤(x) +
Jxy Œ∑PŒ≤ (y) ‚àí Œ∑PŒ≤ (y) = 0,
y‚ààdom(Œ≤)

Tensor Balancing on Statistical Manifold

for every x ‚àà dom(Œ≤), where Jxy is an entry of the
|dom(Œ≤)| √ó |dom(Œ≤)| Jacobian matrix J and given as
(t)

Jxy =

‚àÇŒ∏PŒ≤ (x)
(t)
‚àÇŒ∑PŒ≤ (y)

=

‚àë

¬µ(s, x)¬µ(s, y)pŒ≤ (s)‚àí1
(t)

s‚ààS

from Theorem 3. Therefore, we have the update formula
for all x ‚àà dom(Œ≤) as
(
)
‚àë
(t+1)
(t)
(t)
‚àí1
Œ∑PŒ≤ (x) = Œ∑PŒ≤ (x) ‚àí
Jxy
Œ∏PŒ≤ (y) ‚àí Œ≤(y) .
y‚ààdom(Œ≤)
(t)

In e-projection, update Œ∑PŒ≤ (x) for x ‚àà dom(Œ≤) while fix(t)

ing Œ∏PŒ≤ (x) = Œ∏P (x) for all x ‚àà S + \ dom(Œ≤). To ensure
(t)

Œ∑PŒ≤ (‚ä•) = 1, we add ‚ä• to dom(Œ≤) and Œ≤(‚ä•) = 1. We
(t)

update Œ∏PŒ≤ (x) at each step t as
)
‚àë ‚àí1 ( (t)
(t+1)
(t)
J ‚Ä≤ xy Œ∑PŒ≤ (y) ‚àí Œ≤(y) ,
Œ∏PŒ≤ (x) = Œ∏PŒ≤ (x) ‚àí
y‚ààdom(Œ≤)

J ‚Ä≤ xy =

(t)
‚àÇŒ∑PŒ≤ (x)
(t)
‚àÇŒ∏PŒ≤ (y)

=

‚àë
s‚ààS

(t)

Œ∂(x, s)Œ∂(y, s)pŒ≤ (s)
(t)

(t)

‚àí |S|Œ∑PŒ≤ (x)Œ∑PŒ≤ (y).
(t)

In this case, we also need to update Œ∏PŒ≤ (‚ä•) as it is not
guaranteed to be fixed. Let us define
(
)
(t+1)
exp Œ∏PŒ≤ (s)
‚àè
‚Ä≤(t+1)
(t)
(
) Œ∂(s, x).
pŒ≤
(x) = pŒ≤ (x)
(t)
s‚ààdom(Œ≤) exp Œ∏PŒ≤ (s)
Since we have

)
(
(t+1)
exp Œ∏PŒ≤ (‚ä•)
(t+1)
(
) p‚Ä≤(t+1)
pŒ≤
(x) =
(x),
Œ≤
(t)
exp Œ∏PŒ≤ (‚ä•)

it follows that
(t+1)

(t)

(‚ä•) ‚àí Œ∏PŒ≤ (‚ä•)
(
)
(
)
‚àë ‚Ä≤(t+1)
(t)
= ‚àí log exp Œ∏PŒ≤ (‚ä•) +
pŒ≤
(x) ,
Œ∏PŒ≤

x‚ààS +

The time complexity of each iteration is O(|dom(Œ≤)|3 ),
which is required to compute the inverse of the Jacobian
matrix.
Global convergence of the projection algorithm is always
guaranteed by the convexity of a submanifold S(Œ≤) defined
in Equation (14). Since S(Œ≤) is always convex with respect
to the Œ∏- and Œ∑-coordinates, it is straightforward to see that
our e-projection is an instance of the Bregman algorithm
onto a convex region, which is well known to always converge to the global solution (Censor & Lent, 1981).

5. Balancing Matrices and Tensors
Now we are ready to solve the problem of matrix and tensor
balancing as projection on a dually flat manifold.

5.1. Matrix Balancing
Recall that the task of matrix balancing is to find r, s ‚àà Rn
that satisfy (RAS)1 = 1 and (RAS)T 1 = 1 with R =
diag(r) and S = diag(s) for a given nonnegative square
n√ón
matrix A = (aij ) ‚àà R‚â•0
.
Let us define S as
S = {(i, j) | i, j ‚àà [n] and aij Ã∏= 0},
(15)
where we remove zero entries from the outcome space S as
our formulation cannot treat zero‚àëprobability, and give each
probability as p((i, j)) = aij / ij aij . The partial order
‚â§ of S is naturally introduced as
x = (i, j) ‚â§ y = (k, l) ‚áî i ‚â§ j and k ‚â§ l,
(16)
resulting in ‚ä• = (1, 1). In addition, we define Œπk,m for
each k ‚àà [n] and m ‚àà {1, 2} such that
Œπk,m = min{ x = (i1 , i2 ) ‚àà S | im = k },
where the minimum is with respect to the order ‚â§. If Œπk,m
does not exist, we just remove the entire kth row if m = 1
or kth column if m = 2 from A. Then we switch rows and
columns of A so that the condition
Œπ1,m ‚â§ Œπ2,m ‚â§ ¬∑ ¬∑ ¬∑ ‚â§ Œπn,m
(17)
is satisfied for each m ‚àà {1, 2}, which is possible for any
matrices. Since we have {
‚àën
p((k, j)) if m = 1,
‚àëj=1
Œ∑(Œπk,m ) ‚àí Œ∑(Œπk+1,m ) =
n
if m = 2
i=1 p((i, k))
if the condition (17) is satisfied, the probability distribution
is balanced if for all k ‚àà [n] and m ‚àà {1, 2}
n‚àík+1
.
Œ∑(Œπk,m ) =
n
Therefore, we obtain the following result.
Matrix balancing as e-projection:
Given a matrix A ‚àà Rn√ón with its normalized probabil‚àë
ity distribution P ‚àà S such that p((i, j)) = aij / ij aij .
Define the poset (S, ‚â§) by Equations (15) and (16) and let
S(Œ≤) be the submanifold of S such that
S(Œ≤) = {P ‚àà S | Œ∑P (x) = Œ≤(x) for all x ‚àà dom(Œ≤)},
where the function Œ≤ is given as
dom(Œ≤) = {Œπk,m ‚àà S | k ‚àà [n], m ‚àà {1, 2}},
n‚àík+1
.
Œ≤(Œπk,m ) =
n
Matrix balancing is the e-projection of P onto the submanifold S(Œ≤), that is, the balanced matrix (RAS)/n is the
distribution PŒ≤ such that
{
Œ∏PŒ≤ (x) = Œ∏P (x) if x ‚àà S + \ dom(Œ≤),
Œ∑PŒ≤ (x) = Œ≤(x) if x ‚àà dom(Œ≤),
which is unique and always exists in S, thanks to its dually
flat structure. Moreover, two balancing vectors r and s are
( i
) {
‚àë
ri if m = 1,
exp
Œ∏PŒ≤ (Œπk,m ) ‚àí Œ∏P (Œπk,m ) =
ai if m = 2,
k=1
‚àë
for every i ‚àà [n] and r = rn/ ij aij .
‚ñ†

Tensor Balancing on Statistical Manifold

5.2. Tensor Balancing
Next, we generalize our approach from matrices to tensors.
For an N th order tensor A = (ai1 i2 ...iN ) ‚àà Rn1 √ón2 √ó¬∑¬∑¬∑√ónN
and a vector b ‚àà Rnm , the m-mode product of A and b is
defined as
nm
‚àë
(A √óm b)i1 ...im‚àí1 im+1 ...iN =
ai1 i2 ...iN bim .
im =1

We define tensor balancing as follows: Given a tensor A ‚àà
Rn1 √ón2 √ó¬∑¬∑¬∑√ónN with n1 = ¬∑ ¬∑ ¬∑ = nN = n, find (N ‚àí 1)
order tensors R1 , R2 , . . . , RN such that
A‚Ä≤ √óm 1 = 1 (‚àà Rn1 √ó¬∑¬∑¬∑√ónm‚àí1 √ónm+1 √ó¬∑¬∑¬∑√ónN ) (18)
‚àën
for all m ‚àà [N ], i.e., im =1 a‚Ä≤i1 i2 ...iN = 1, where each
entry a‚Ä≤i1 i2 ...iN of the balanced tensor A‚Ä≤ is given as
‚àè
a‚Ä≤i1 i2 ...iN = ai1 i2 ...iN
Rim1 ...im‚àí1 im+1 ...iN .
m‚àà[N ]

A tensor A‚Ä≤ that satisfies Equation (18) is called multistochastic (Cui et al., 2014). Note that this is exactly the
same as the matrix balancing problem if N = 2.
It is straightforward to extend matrix balancing to tensor
balancing as e-projection onto a submanifold. Given a tensor A ‚àà Rn1 √ón2 √ó¬∑¬∑¬∑√ónN with its normalized probability
distribution P such that
/‚àë
p(x) = ai1 i2 ...iN
(19)
aj1 j2 ...jN
j1 j2 ...jN

for all x = ‚àë
(i1 , i2 , . . . , iN ). The objective is to obtain
n
PŒ≤ such that im =1 pŒ≤ ((i1 , . . . , iN )) = 1/(nN ‚àí1 ) for all
m ‚àà [N ] and i1 , . . . , iN ‚àà [n]. In the same way as matrix
balancing, we define S as

{
}
S = (i1 , i2 , . . . , iN ) ‚àà [n]N  ai1 i2 ...i Ã∏= 0
N

with removing zero entries and the partial order ‚â§ as
x = (i1 . . . iN ) ‚â§ y = (j1 . . . jN ) ‚áî ‚àÄm ‚àà [N ], im ‚â§ jm .
In addition, we introduce Œπk,m as
Œπk,m = min{ x = (i1 , i2 , . . . , iN ) ‚àà S | im = k }.
and require the condition in Equation (17).
Tensor balancing as e-projection:
Given a tensor A ‚àà Rn1 √ón2 √ó¬∑¬∑¬∑√ónN with its normalized
probability distribution P ‚àà S given in Equation (19). The
submanifold S(Œ≤) of multistochastic tensors is given as
S(Œ≤) = {P ‚àà S | Œ∑P (x) = Œ≤(x) for all x ‚àà dom(Œ≤)},
where the domain of the function Œ≤ is given as
dom(Œ≤) = { Œπk,m | k ‚àà [n], m ‚àà [N ] }
and each value is described using the zeta function as
‚àë
1
Œ≤(Œπk,m ) =
Œ∂(Œπk,m , Œπl,m ) N ‚àí1 .
n
l‚àà[n]

Tensor balancing is the e-projection of P onto the submanifold S(Œ≤), that is, the multistochastic tensor is the distribution PŒ≤ such that
{
Œ∏PŒ≤ (x) = Œ∏P (x) if x ‚àà S + \ dom(Œ≤),
Œ∑PŒ≤ (x) = Œ≤(x) if x ‚àà dom(Œ≤),
which is unique and always exists in S, thanks to its dually
flat structure. Moreover, each balancing tensor Rm is
Rim1 ...im‚àí1 im+1 ...iN
Ô£´
Ô£∂
im‚Ä≤
‚àë ‚àë
= expÔ£≠
Œ∏PŒ≤ (Œπk,m‚Ä≤ ) ‚àí Œ∏P (Œπk,m‚Ä≤ )Ô£∏
m‚Ä≤ Ã∏=m k=1

for every m ‚àà [N ] and R1 = R1 nN ‚àí1 /
to recover a multistochastic tensor.

‚àë
j1 ...jN

aj1 ...jN
‚ñ†

Our result means that the e-projection algorithm based on
Newton‚Äôs method proposed in Section 4.3 converges to the
unique balanced tensor whenever S(Œ≤) Ã∏= ‚àÖ holds.

6. Conclusion
In this paper, we have solved the open problem of tensor
balancing and presented an efficient balancing algorithm
using Newton‚Äôs method. Our algorithm quadratically converges, while the popular Sinkhorn-Knopp algorithm linearly converges. We have examined the efficiency of our
algorithm in numerical experiments on matrix balancing
and showed that the proposed algorithm is several orders
of magnitude faster than the existing approaches.
We have analyzed theories behind the algorithm, and
proved that balancing is e-projection in a special type of
a statistical manifold, in particular, a dually flat Riemannian manifold studied in information geometry. Our key
finding is that the gradient of the manifold, equivalent to
Riemannian metric or the Fisher information matrix, can be
analytically obtained using the MoÃàbius inversion formula.
Our information geometric formulation can model several
machine learning applications such as statistical analysis
on a DAG structure. Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study
in future work.

Acknowledgements
The authors sincerely thank Marco Cuturi for his valuable comments. This work was supported by JSPS
KAKENHI Grant Numbers JP16K16115, JP16H02870
(MS), JP26120732 and JP16H06570 (HN). The research
of K.T. was supported by JST CREST JPMJCR1502,
RIKEN PostK, KAKENHI Nanostructure and KAKENHI
JP15H05711.

Tensor Balancing on Statistical Manifold

References
Agresti, A. Categorical data analysis. Wiley, 3 edition,
2012.
Ahmed, M., De Loera, J., and Hemmecke, R. Polyhedral
Cones of Magic Cubes and Squares, volume 25 of Algorithms and Combinatorics, pp. 25‚Äì41. Springer, 2003.
Akartunalƒ±, K. and Knight, P. A. Network models and
biproportional rounding for fair seat allocations in the
UK elections. Annals of Operations Research, pp. 1‚Äì19,
2016.

the National Academy of Sciences, 108(23):9679‚Äì9684,
2011.
Gierz, G., Hofmann, K. H., Keimel, K., Lawson, J. D., Mislove, M., and Scott, D. S. Continuous Lattices and Domains. Cambridge University Press, 2003.
Idel, M. A review of matrix scaling and sinkhorn‚Äôs normal
form for matrices and positive maps. arXiv:1609.06349,
2016.
Ito, K. (ed.). Encyclopedic Dictionary of Mathematics. The
MIT Press, 2 edition, 1993.

Amari, S. Information geometry on hierarchy of probability distributions. IEEE Transactions on Information
Theory, 47(5):1701‚Äì1711, 2001.

Knight, P. A. The Sinkhorn‚ÄìKnopp algorithm: Convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261‚Äì275, 2008.

Amari, S. Information geometry and its applications: Convex function and dually flat manifold. In Nielsen, F.
(ed.), Emerging Trends in Visual Computing: LIX Fall
Colloquium, ETVC 2008, Revised Invited Papers, pp.
75‚Äì102. Springer, 2009.

Knight, P. A. and Ruiz, D. A fast algorithm for matrix
balancing. IMA Journal of Numerical Analysis, 33(3):
1029‚Äì1047, 2013.

Amari, S. Information geometry of positive measures
and positive-definite matrices: Decomposable dually flat
structure. Entropy, 16(4):2131‚Äì2145, 2014.
Amari, S. Information Geometry and Its Applications.
Springer, 2016.
Balinski, M. Fair majority voting (or how to eliminate gerrymandering). American Mathematical Monthly, 115(2):
97‚Äì113, 2008.
Censor, Y. and Lent, A. An iterative row-action method for
interval convex programming. Journal of Optimization
Theory and Applications, 34(3):321‚Äì353, 1981.
Chang, H., Paksoy, V. E., and Zhang, F. Polytopes of
stochastic tensors. Annals of Functional Analysis, 7(3):
386‚Äì393, 2016.
Cui, L.-B., Li, W., and Ng, M. K. Birkhoff‚Äìvon Neumann
theorem for multistochastic tensors. SIAM Journal on
Matrix Analysis and Applications, 35(3):956‚Äì973, 2014.
Cuturi, M. Sinkhorn distances: Lightspeed computation of
optimal transport. In Advances in Neural Information
Processing Systems 26, pp. 2292‚Äì2300, 2013.
Frogner, C., Zhang, C., Mobahi, H., Araya, M., and Poggio, T. A. Learning with a Wasserstein loss. In Advances
in Neural Information Processing Systems 28, pp. 2053‚Äì
2061, 2015.
Ganmor, E., Segev, R., and Schneidman, E. Sparse loworder interaction network underlies a highly correlated
and learnable neural population code. Proceedings of

Lahr, M. and de Mesnard, L. Biproportional techniques
in input-output analysis: Table updating and structural
analysis. Economic Systems Research, 16(2):115‚Äì134,
2004.
Lamond, B. and Stewart, N. F. Bregman‚Äôs balancing
method. Transportation Research Part B: Methodological, 15(4):239‚Äì248, 1981.
Livne, O. E. and Golub, G. H. Scaling by binormalization.
Numerical Algorithms, 35(1):97‚Äì120, 2004.
Marshall, A. W. and Olkin, I. Scaling of matrices to achieve
specified row and column sums. Numerische Mathematik, 12(1):83‚Äì90, 1968.
Miller, R. E. and Blair, P. D. Input-Output Analysis: Foundations and Extensions. Cambridge University Press, 2
edition, 2009.
Moon, T. K., Gunther, J. H., and Kupin, J. J. Sinkhorn
solves sudoku. IEEE Transactions on Information Theory, 55(4):1741‚Äì1746, 2009.
Nakahara, H. and Amari, S. Information-geometric measure for neural spikes. Neural Computation, 14(10):
2269‚Äì2316, 2002.
Nakahara, H., Nishimura, S., Inoue, M., Hori, G., and
Amari, S. Gene interaction in DNA microarray data is
decomposed by information geometric measure. Bioinformatics, 19(9):1124‚Äì1131, 2003.
Nakahara, H., Amari, S., and Richmond, B. J. A comparison of descriptive models of a single spike train by
information-geometric measure. Neural computation, 18
(3):545‚Äì568, 2006.

Tensor Balancing on Statistical Manifold

Parikh, A. Forecasts of input-output matrices using the
R.A.S. method. The Review of Economics and Statistics,
61(3):477‚Äì481, 1979.
Parlett, B. N. and Landis, T. L. Methods for scaling to doubly stochastic form. Linear Algebra and its Applications,
48:53‚Äì79, 1982.
Rao, S. S. P., Huntley, M. H., Durand, N. C., Stamenova,
E. K., Bochkov, I. D., Robinson, J. T., Sanborn, A. L.,
Machol, I., Omer, A. D., Lander, E. S., and Aiden, E. L.
A 3D map of the human genome at kilobase resolution
reveals principles of chromatin looping. Cell, 159(7):
1665‚Äì1680, 2014.
Rota, G.-C. On the foundations of combinatorial theory I:
Theory of MoÃàbius functions. Z. Wahrseheinlichkeitstheorie, 2:340‚Äì368, 1964.
Sinkhorn, R. A relationship between arbitrary positive matrices and doubly stochastic matrices. The Annals of
Mathematical Statistics, 35(2):876‚Äì879, 06 1964.
Sinkhorn, R. and Knopp, P. Concerning nonnegative matrices and doubly stochastic matrices. Pacific Journal of
Mathematics, 21(2):343‚Äì348, 1967.
Solomon, J., de Goes, F., PeyreÃÅ, G., Cuturi, M., Butscher,
A., Nguyen, A., Du, T., and Guibas, L. Convolutional
Wasserstein distances: Efficient optimal transportation
on geometric domains. ACM Transactions on Graphics,
34(4):66:1‚Äì66:11, 2015.
Soules, G. W. The rate of convergence of sinkhorn balancing. Linear Algebra and its Applications, 150:3‚Äì40,
1991.
Sugiyama, M., Nakahara, H., and Tsuda, K. Information
decomposition on structured space. In 2016 IEEE International Symposium on Information Theory, pp. 575‚Äì
579, July 2016.
Wu, H.-J. and Michor, F. A computational strategy to adjust
for copy number in tumor Hi-C data. Bioinformatics, 32
(24):3695‚Äì3701, 2016.

