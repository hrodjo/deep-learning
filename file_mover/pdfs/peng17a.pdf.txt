Asynchronous Distributed Variational Gaussian Process for Regression

Hao Peng 1 Shandian Zhe 1 Xiao Zhang 1 Yuan Qi 2

Abstract
Gaussian processes (GPs) are powerful nonparametric function estimators. However, their
applications are largely limited by the expensive
computational cost of the inference procedures.
Existing stochastic or distributed synchronous
variational inferences, although have alleviated
this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world
large applications, where the data sizes are often orders of magnitudes larger, say, billions. To
solve this problem, we propose ADVGP, the first
Asynchronous Distributed Variational Gaussian
Process inference for regression, on the recent
large-scale machine learning platform, PARAM ETER S ERVER. ADVGP uses a novel, flexible
variational framework based on a weight space
augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon the efficiency of the existing variational methods. With ADVGP, we effortlessly
scale up GP regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the
popular linear models.

1. Introduction
Gaussian processes (GPs) (Rasmussen & Williams, 2006)
are powerful non-parametric Bayesian models for function estimation. Without imposing any explicit parametric
form, GPs merely induce a smoothness assumption via the
definition of covariance function, and hence can flexibly infer various, complicated functions from data. In addition,
GPs are robust to noise, resist overfitting and produce uncertainty estimations. However, a crucial bottleneck of GP
1

Purdue University, West Lafayette, IN, USA 2 Ant Financial Service Group.
Correspondence to: Hao Peng
<pengh@alumni.purdue.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

models is their expensive computational cost: exact GP inference requires O(n3 ) time complexity and O(n2 ) space
complexity (n is the number of training samples), which
limits GPs to very small applications, say, a few hundreds
of samples.
To mitigate this limitation, many approximate inference algorithms have been developed (Williams & Seeger, 2001;
Seeger et al., 2003; Quiñonero-Candela & Rasmussen,
2005; Snelson & Ghahramani, 2005; Deisenroth & Ng,
2015). Most methods use sparse approximations. Basically, we first introduce a small set of inducing points; and
then we develop an approximation that transfers the expensive computations from the entire large training data, such
as the covariance and inverse covariance matrix calculations, to the small set of the inducing points. To this end,
a typical strategy is to impose some simplified modeling
assumption. For example, FITC (Snelson & Ghahramani,
2005) makes a fully conditionally independent assumption.
Recently, Titsias (2009) proposed a more principled, variational sparse approximation framework, where the inducing points are also treated as variational parameters. The
variational framework is less prone to overfitting and often
yields a better inference quality (Titsias, 2009; Bauer et al.,
2016). Based on the variational approximation, Hensman
et al. (2013) developed a stochastic variational inference
(SVI) algorithm, and Gal et al. (2014) used a tight variational lower bound to develop a distributed inference algorithm with the M AP R EDUCE framework.
While SVI and the distributed variational inference have
successfully scaled up GP models to millions of samples
(O(106 )), they are still insufficient for real-world largescale applications, in which the data sizes are often orders
of magnitude larger, say, over billions of samples (O(109 )).
Specifically, SVI (Hensman et al., 2013) sequentially processes data samples and requires too much time to complete
even one epoch of training. The distributed variational algorithm in (Gal et al., 2014) uses the M AP R EDUCE framework and requires massive synchronizations during training, where a large amount of time is squandered when the
M APPERS or R EDUCERS are waiting for each other, or the
failed nodes are restarted.
To tackle this problem, we propose Asynchronous
Distributed Variational Gaussian Process inference (AD-

Asynchronous Distributed Variational Gaussian Process for Regression

VGP), which enables GP regression on applications with
(at least) billions of samples. To the best of our knowledge,
this is the first variational inference that scales up GPs to
this level. The contributions of our work are summarized
as follows: first, we propose a novel, general variational
GP framework using a weight space augmentation (Section
3). The framework allows flexible constructions of feature mappings to incorporate various low-rank structures
and to fulfill different variational model evidence lower
bounds (ELBOs). Furthermore, due to the simple standard normal prior of the random weights, the framework
enables highly efficient, asynchronous proximal gradientbased optimization, with convergence guarantees as well
as fast, element-wise and parallelizable variational posterior updates. Second, based on the new framework, we
develop a highly efficient, asynchronous variational inference algorithm in the recent distributed machine learning
platform, PARAMETER S ERVER (Li et al., 2014a) (Section
4). The asynchronous algorithm eliminates an enormous
amount of waiting time caused by the synchronous coordination, and fully exploits the computational power and network bandwidth; as a result, our new inference, ADVGP,
greatly improves on both the scalability and efficiency of
the prior variational algorithms, while still maintaining a
similar or better inference quality. Finally, in a real-world
application with billions of samples, we effortlessly train
a GP regression model with ADVGP and achieve an excellent prediction accuracy, with 15% improvement over
the popular linear regression implemented in Vowpal Wabbit (Agarwal et al., 2014), the state-of-the-art large-scale
machine learning software widely used in industry.

2. Gaussian Processes Review
In this paper, we focus on Gaussian process (GP) regression. Suppose we aim to infer an underlying function
f : Rd → R from an observed dataset D = {X, y}, where
> >
X = [x>
1 , . . . , xn ] is the input matrix and y is the output
vector. Each row of X, namely xi (1 ≤ i ≤ n), is a ddimensional input vector. Correspondingly, each element
of y, namely yi , is an observed function value corrupted by
some random noise. Note that the function f can be highly
nonlinear. To estimate f from D, we place a GP prior over
f . Specifically, we treat the collection of all the function
values as one realization of the Gaussian process. Therefore, the finite projection of f over the inputs X, i.e., f =
[f (x1 ), . . . , f (xn )] follows
 a multivariate Gaussian distribution: f ∼ N f |f̄ , Knn , where f̄ = [f¯(x1 ), . . . , f¯(xn )]
are the mean function values and Knn is the n × n covariance matrix. Each element of Knn is a covariance function k(·, ·) of two input vectors, i.e., [Knn ]i,j = k(xi , xj ).
We can choose any symmetric positive semidefinite kernel as the covariance function, e.g., the ARD kernel:

k(xi , xj ) = a20 exp − 12 (xi − xj )> diag(η)(xi − xj ) ,

where η = [1/a21 , ..., 1/a2d ]. For simplicity, we usually use
the zero mean function, namely f¯(·) = 0.
Given f , we use an isotropic Gaussian model to sample the
observed noisy output y: p(y|f ) = N (y|f , β −1 I). The
joint probability of GP regression is

p(y, f |X) = N f |0, Knn N (y|f , β −1 I).
(1)
Further, we can obtain the marginal distribution of y,
namely the model evidence, by marginalizing out f :

p(y|X) = N y|0, Knn + β −1 I .
(2)
The inference of GP regression aims to estimate the appropriate kernel parameters and noise variance from the training data D = {X, y}, such as {a0 , η} in ARD kernel and
β −1 . To this end, we can maximize the model evidence in
(2) with respect to those parameters. However, to maximize
(2), we need to calculate the inverse and the determinant of
the n × n matrix Knn + β −1 I to evaluate the multivariate
Gaussian term. This will take O(n3 ) time complexity and
O(n2 ) space complexity and hence is infeasible for a large
number of samples, i.e., large n.
For prediction, given a test input x∗ , since the test output f ∗
and training output f can be treated as another GP projection on X and x∗ , the joint distribution of f ∗ and f is also a
multivariate Gaussian distribution. Then by marginalizing
out f , we can obtain the posterior distribution of f ∗ :
p(f ∗ |x∗ , X, y) = N (f ∗ |α, v),

(3)

where
−1 −1
α = k>
I) y,
n∗ (Knn + β

(4)

−1 −1
v = k(x∗ , x∗ ) − k>
I) kn∗ ,
n∗ (Knn + β

(5)

and kn∗ = [k(x∗ , x1 ), . . . , k(x∗ , xn )]> . Note that the calculation also requires the inverse of Knn + β −1 I and hence
takes O(n3 ) time complexity and O(n2 ) space complexity.

3. Variational Framework Using Weight
Space Augmentation
Although GPs allow flexible function inference, they have
a severe computational bottleneck. The training and prediction both require O(n3 ) time complexity and O(n2 ) space
complexity (see (2), (4) and (5)), making GPs unrealistic
for real-world, large-scale applications, where the number
of samples (i.e., n) are often billions or even larger. To
address this problem, we propose ADVGP that performs
highly efficient, asynchronous distributed variational inference and enables the training of GP regression on extremely large data. ADVGP is based on a novel variational
GP framework using a weight space augmentation, which
is discussed below.

Asynchronous Distributed Variational Gaussian Process for Regression

First, we construct an equivalent augmented model by introducing an m × 1 auxiliary random weight vector w
(m  n). We assume w is sampled from the standard
normal prior distribution: p(w) = N (w|0, I). Given w,
we sample an n × 1 latent function values f from
p(f |w) = N (f |Φw, Knn − ΦΦ> ),

(6)

where Φ is an n × m matrix: Φ = [φ(x1 ), . . . , φ(xn )]> .
Here φ(·) represents a feature mapping that maps the original d-dimensional input into an m-dimensional feature
space. Note that we need to choose an appropriate φ(·)
to ensure the covariance matrix in (6) is symmetric positive semidefinite. Flexible constructions of φ(·) enable us
to fulfill different variational model evidence lower bounds
(ELBO) for large-scale inference, which we will discuss
more in Section 5.
Given f , we sample the observed output y from the
isotropic Gaussian model p(y|f ) = N (y|f , β −1 I). The
joint distribution of our augmented model is then given by
p(y, f , w|X)
=N (w|0, I)N (f |Φw, Knn − ΦΦ> )N (y|f , β −1 I). (7)
This model is equivalent to the original GP regression—
when we marginalize out w, we recover the joint distribution in (1); we can further marginalize out f to recover the
model evidence in (2). Note that our model is distinct from
the traditional weight space view of GP regression (Rasmussen & Williams, 2006): the feature mapping φ(·) is not
equivalent to the underlying (nonlinear) feature mapping
induced by the covariance function (see more discussions
in Section 5). Instead, φ(·) is defined for computational
purpose only—that is, to construct a tractable variational
evidence lower bound (ELBO), shown as follows.
Now, we derive the tractable ELBO based on the weight
space augmented model in (7). The derivation is similar
to (Titsias, 2009; Hensman et al., 2013). Specifically, we
first consider the conditional
distribution p(y|w). Because
R
log p(y|w) = log p(y|f )p(f |w)df = loghp(y|f )ip(f |w) ,
where h·ip(θ) denotes the expectation under the distribution p(θ), we can use Jensen’s inequality to obtain a lower
bound:
log p(y|w) = loghp(y|f )ip(f |w) ≥ hlog p(y|f )ip(f |w)
n
X
β
(8)
=
log N (yi |φ> (xi )w, β −1 ) − k̃ii ,
2
i=1
where k̃ii is the ith diagonal element of Knn − ΦΦ> .
Next, we introduce a variational posterior q(w) to construct

the variational lower bound of the log model evidence,


p(y|w)p(w)
log p(y) = log
q(w)
q(w)
≥ hlog p(y|w)iq(w) − KL(q(w)kp(w)).

(9)

where KL(·k·) is the Kullback–Leibler divergence. Replacing log p(y|w) in (9) by the right side of (8), we obtain
the following lower bound,
log p(y) ≥ L = −KL (q(w)kp(w))
n
X



β
+
log N (yi |φ> (xi )w, β −1 ) q(w) − k̃ii .
2
i=1

(10)

Note that this is a variational lower bound: the equality is
obtained when ΦΦT = Knn and q(w) = p(w|y). To
achieve equality, we need to set m = n and have φ(·)
map the d-dimensional input into an n-dimensional feature
space. In order to reduce the computational cost, however,
we can restrict m to be very small and choose any family
of mappings φ(·) that satisfy Knn − ΦΦ>  0. The flexible choices of φ(·) allows us to explore different approximations in a unified variational framework. For example,
in our practice, we introduce an m × d inducing matrix
Z = [z1 , . . . , zm ]> and define
φ(x) = L> km (x),

(11)

where km (x) = [k(x, z1 ), . . . , k(x, zm )]> and L is
the lower triangular Cholesky factorization of the inverse
kernel matrix over Z, i.e., [Kmm ]i,j = k(zi , zj ) and
>
>
=
K−1
mm = LL . It can be easily verified that ΦΦ
>
−1
Knm Kmm Knm , where Knm is the cross kernel matrix
between X and Z, i.e., [Knm ]ij = k(xi , zj ). Therefore
Knn − ΦΦ is always positive semidefinite, because it can
be viewed
as a Schur
h
i complement of Knn in the block maK>
mm
nm . We discuss other choices of φ(·) in
trix K
Knm
Knn
Section 5.

4. Delayed Proximal Gradient Optimization
for ADVGP
A major advantage of our variational GP framework is the
capacity of using the asynchronous, delayed proximal gradient optimization supported by PARAMETER S ERVER (Li
et al., 2014b), with convergence guarantees and scalability to huge data. PARAMETER S ERVER is a well-known,
general platform for asynchronous machine learning algorithms for extremely large applications. It has a bipartite architecture where the computing nodes are partitioned into
two classes: server nodes store the model parameters and
worker nodes the data. PARAMETER S ERVER assumes the
learning procedure minimizes a non-convex loss function

Asynchronous Distributed Variational Gaussian Process for Regression

with the following composite form:
Xr
L(θ) =
Gk (θ) + h(θ)
k=1

(12)

where θ are the model parameters. Here Gk (θ) is a (possibly non-convex) function associated with the data in worker
k and therefore can be calculated by worker k independently; h(θ) is a convex function with respect to θ.
To efficiently minimize the loss function in (12), PARAM ETER S ERVER uses a delayed proximal gradient updating
method to perform asynchronous optimization. To illustrate it, let us first review the standard proximal gradient descent. Specifically, for each iteration
t, we first take a gradiP
ent descent step according to k Gk (θ) and then perform
a proximal operation to project θ toward
P the minimum
of h(·), i.e., θ (t+1) = Proxγt [θ (t) − γt k ∇Gk (θ (t) )],
where γt is the step size. The proximal operation is defined
as
1
kθ ∗ − θk22 .
Proxγt [θ] = argmin h(θ ∗ ) +
∗
2γ
t
θ

(13)

The standard proximal gradient descent guarantees to find
a local minimum solution. However, the computation is
inefficient, even in parallel: in each iteration, the server
nodes wait until the worker nodes finish calculating each
∇Gk (θ (t) ); then the workers wait for the servers to finish
the proximal operation. This synchronization wastes much
time and computational resources. To address this issue,
PARAMETER S ERVER uses a delayed proximal gradient updating approach to implement asynchronous computation.
Specifically, we set a delay limit τ ≥ 0. At any iteration t, the servers do not enforce all the workers to finish iteration t; instead, as long as each worker has finished an iteration no earlier than t − τ , the servers will
proceed to perform
updates, i.e., θ (t+1) =
P the proximal
(t)
(tk )
Proxγt [θ −γt k ∇Gk (θ )] (t−τ ≤ tk ≤ t), and notify all the workers with the new parameters θ (t+1) . Once
received the updated parameters, the workers compute and
push the local gradient to the servers immediately. Obviously, this delay mechanism can effectively reduce the wait
between the server and worker nodes. By setting different τ , we can adjust the degree of the asynchronous computation: when τ = 0, we have no asynchronization and
return to the standard, synchronous proximal gradient descent; when τ = ∞, we are totally asynchronous and there
is no wait at all.
A highlight is that given the composite form of the nonconvex loss function in (12), the above asynchronous delayed proximal gradient descent guarantees to converge according to Theorem 1.
Theorem 1. (Li et al., 2013) Assume the gradient of the
function Gk is Lipschitz continuous, that is, there is a constant Ck such that k∇Gk (θ) − ∇Gk (θ 0 )k ≤ Ck ||θ − θ 0 ||

Pr
for any θ, θ 0 , and k = 1, ..., r. Define C =
k=1 Ck .
Also, assume we allow a maximum delay for the updates by
τ and a significantly-modified filter on pulling the parameters with threshold O(t−1 ). For any  > 0, the delayed
proximal gradient descent converges to a stationary point
if the learning rate γt satisfies γt ≤ ((1 + τ )C + )−1 .
Now, let us return to our variational GP framework. A
major benefit of our framework is that the negative variational evidence lower bound (ELBO) for GP regression has
the same composite form as (12). Thereby we can apply
the asynchronous proximal gradient descent for GP inference on PARAMETER S ERVER. Specifically, we explicitly
assume q(w) = N (w|µ, Σ) and obtain the negative variational ELBO (see (10))
Xn
−L =
gi + h
(14)
i=1

where
gi = − log N (yi |φ> (xi )µ, β −1 ) +
+
h=

β >
φ (xi )Σφ(xi )
2

β
k̃ii ,
2


1
− ln |Σ| − m + tr(Σ) + µ> µ .
2

(15)

Instead of directly updating Σ, we consider U, the upper
triangular Cholesky factor of Σ, i.e., Σ = U> U. This not
only simplifies the proximal operation but also ensures the
positive definiteness of Σ during computation. The partial
derivatives of gi with respect to µ and U are

∂gi
= β −yi φ(xi ) + φ(xi )φ> (xi )µ ,
∂µ
∂gi
= βtriu[Uφ(xi )φ> (xi )],
∂U

(16)
(17)

where triu[·] denotes the operator that keeps the upper triangular part of a matrix but leaves any other element zero.It
can be verified that the partial derivatives of gi with respect
to µ and U are Lipschitz continuous and h is also convex
with respect to µ and U. According to Theorem 1, minimizing −L (i.e., maximizing L) with respect to the variational parameters, µ and U, using the asynchronous proximal gradient method can guarantee convergence. For other
parameters, such as kernel parameters and inducing points,
h is simply a constant. As a result, the delayed proximal
updates for these parameters reduce to the delayed gradient
descent optimization such as in (Agarwal & Duchi, 2011).
We now present the details of ADVGP implementation on
PARAMETER S ERVER. We first partition the data for r
workers and allocate the model parameters (such as the kernel parameters, the parameters of q(w) and the inducing
points Z) to server nodes. At any iteration t, the server
nodes aggregate all the local gradients and perform the

Asynchronous Distributed Variational Gaussian Process for Regression

proximal operation in (13), as long as each worker k has
computed and pushed the local gradient on its own data
subset Dk for some prior iteration tk (t − τ ≤ tk ≤ t),
P
(t )
(t )
∇Gk k = i∈Dk ∇gi k . Note that the proximal operation is only performed for the parameters of q(w), namely
µ and U; since h is constant for the other model parameters, such as the kernel parameters and the inducing points,
their gradient descent updates remain unchanged. Minimizing (13) by setting the derivatives to zero, we obtain the
proximal updates for each element in µ and U:

(t+1)

µi

0(t+1)

= µi

(t+1)
Uij

=

(t+1)

=

Uii

/(1 + γt ),

(18)

0(t+1)
Uij
/(1

+ γt ),
q
0(t+1)
0(t+1) 2
Uii
+ (Uii
) + 4(1 + γt )γt
2(1 + γt )

(19)
, (20)

where
0(t+1)
µi

0(t+1)

Uij

=

(t)
µi

(t)

− γt

= Uij − γt

Xr
k=1

Xr
k=1

(t )

∂Gk k

(tk )

,

∂µi

(t )

∂Gk k

(t )

∂Uij k

.

The proximal operation comprises of element-wise, closedform computations, therefore making the updates of the
variational posterior q(w) highly parallelizable and efficient. The gradient calculation for the other parameters,
including the kernel parameters and inducing points, although quite complicated, is pretty standard and we give
the details in the supplementary material (Appendix A). Finally, ADVGP is summarized in Algorithm 1.
Algorithm 1 Delayed Proximal Gradient for ADVGP
Worker k at iteration tk
1: Block until servers have new parameters ready.
2: Pull the parameters from servers and update the current
version (or iteration) tk .
(t )
3: Compute the gradient ∇Gk k on data Dk .
(tk )
4: Push the gradient ∇Gk to servers.
Servers at iteration t
1: if Each worker k completes iteration tk ≥ t − τ then
P
(t )
2:
Aggregate gradients to obtain ∇G(t) = ∇Gk k .
3:
Update µ and U using (18), (19) and (20).
4:
Update the other parameters using gradient descent.
5:
Notify all blocked workers of the new parameters
and the version (i.e., t + 1).
6:
Proceed to iteration t + 1.
7: end if

5. Discussion and Related Work
Exact GP inference requires computing the full covariance matrix (and its inverse), and therefore is infeasible
for large data. To reduce the computational cost, many
sparse GP inference methods use a low-rank structure to
approximate the full covariance. For example, Williams
& Seeger (2001); Peng & Qi (2015) used the Nyström approximation; Bishop & Tipping (2000) used relevance vectors, constructed from covariance functions evaluated on a
small subset of the training data. A popular family of sparse
GPs introduced a small set of inducing inputs and targets,
viewed as statistical summary of the data, and define an approximate model by imposing some conditional independence between latent functions given the inducing targets;
the inference of the inexact model is thereby much easier.
Quiñonero-Candela & Rasmussen (2005) provided a unified view of those methods, such as SoR (Smola & Bartlett,
2001), DTC (Seeger et al., 2003), PITC (Schwaighofer &
Tresp, 2003) and FITC (Snelson & Ghahramani, 2005).
Despite the success of those methods, their inference procedures often exhibit undesirable behaviors, such as underestimation of the noise and clumped inducing inputs (Bauer
et al., 2016). To obtain a more favorable approximation,
Titsias (2009) proposed a variational sparse GP framework,
where the approximate posteriors and the inducing inputs
are both treated as variational parameters and estimated by
maximizing a variational lower bound of the true model
evidence. The variational framework is less prone to overfitting and often yields a better inference quality (Titsias,
2009; Bauer et al., 2016). Based on Titsias (2009)’s work,
Hensman et al. (2013) developed a stochastic variational inference for GP (SVIGP) by parameterizing the variational
distributions explicitly. Gal et al. (2014) reparameterized
the bound of Titsias (2009) and developed a distributed optimization algorithm with M AP R EDUCE framework. Further, Dai et al. (2014) developed a GPU acceleration using
the similar formulation, and Matthews et al. (2017) developed GPflow library, a TensorFlow implementation that exploit GPU hardwares.
To further enable GPs on real-world, extremely large applications, we proposed a new variational GP framework using a weight space augmentation. The proposed augmented
model, introducing an extra random weight vector w with
standard normal prior, is distinct from the traditional GP
weight space view (Rasmussen & Williams, 2006) and the
recentering tricks used in GP MCMC inferences (Murray &
Adams, 2010; Filippone et al., 2013; Hensman et al., 2015).
In the conventional GP weight space view, the weight vector is used to combine the nonlinear feature mapping induced by the covariance function and therefore can be infinite dimensional; in the recentering tricks, the weight vector is used to reparameterize the latent function values, to

Asynchronous Distributed Variational Gaussian Process for Regression

dispose of the dependencies on the hyper-parameters, and
to improve the mixing rate. In our framework, however, the
weight vector w has a fixed, much smaller dimension than
the number of samples (m  n), and is used to introduce
an extra feature mapping φ(·) — φ(·) plays the key role
to construct a tractable variational model evidence lower
bound (ELBO) for large scale GP inference.
The advantages of our framework are mainly twofold.
First, by using the feature mapping φ(·), we are flexible
to incorporate various low rank structures, and meanwhile
still cast them into a principled variational inference framework. For example, in addition to (11), we can define
−1/2

φ(x) = diag(λ)

Q> km (x),

(21)

where Q are λ are eigenvectors and eigenvalues of Kmm .
Then φ(·) is actually a scaled Nyström approximation for
eigenfunctions of the kernel used in GP regression. This
actually fulfills a variational version of the EigenGP approximation (Peng & Qi, 2015). Further, we can extend
(21) by combining q Nyström approximations. Suppose
we have q groups of inducing inputs {Z1 , . . . , Zq }, where
each Zl consists of ml inducing inputs. Then the feature
mapping can be defined by
φ(x) =

q
X

q −1/2 diag(λl )

−1/2

Q>
l kml (x),

(22)

l=1

where λl and Ql are the eigenvalues and eigenvectors of
the covariance matrix for Zl . This leads to a variational
sparse GP based on the ensemble Nyström approximation (Kumar et al., 2009). It can be trivially verified that
both (21) and (22) satisfied Knn − ΦΦ>  0 in (6).
In addition, we can also relate ADVGP to GP models with
pre-defined feature mappings, for instance, Relevance Vector Machines (RVMs) (Bishop & Tipping, 2000), by setting
φ(x) = diag(α1/2 )km (x), where α is an m × 1 vector.
Note that to ensure Knn −ΦΦ>  0, we have to add some
constraint over the range of each αi in α.
The second major advantage of ADVGP is that our variational ELBO is consistent with the composite non-convex
loss form favored by PARAMETER S ERVER, therefore we
can utilize the highly efficient, distributed asynchronous
proximal gradient descent in PARAMETER S ERVER to scale
up GPs to extremely large applications (see Section 6.3).
Furthermore, the simple element-wise and closed-form
proximal operation enables exceedingly efficient and parallelizable variational posterior update on the server side.

6. Experiments
6.1. Predictive Performance
First, we evaluated the inference quality of ADVGP in
terms of predictive performance. To this end, we used the

US Flight data1 (Hensman et al., 2013), which recorded the
arrival and departure time of the USA commercial flights
between January and April in 2008. We performed two
groups of tests: in the first group, we randomly chose 700K
samples for training; in the second group, we randomly selected 2M training samples. Both groups used 100K samples for testing. We ensured that the training and testing
data are non-overlapping.
We compared ADVGP with two existing scalable variational inference algorithms: SVIGP (Hensman et al., 2013)
and DistGP (Gal et al., 2014). SVIGP employs an online
training, and DistGP performs a distributed synchronous
variational inference. We ran all the methods on a computer node with 16 CPU cores and 64 GB memory. While
SVIGP uses a single CPU core, DistGP and ADVGP use
all the CPU cores to perform parallel inference. We used
ARD kernel for all the methods, with the same initialization of the kernel parameters. For SVIGP, we set the minibatch size to 5000, consistent with (Hensman et al., 2013).
For DistGP, we tested two optimization frameworks: local gradient descent (DistGP-GD) and L-BFGS (DistGPLBFGS). For ADVGP, we initialized µ = 0, U = I, and
used ADADELTA (Zeiler, 2012) to adjust the step size for
the gradient descent before the proximal operation. To
choose an appropriate delay τ , we sampled another set of
training and test data, based on which we tuned τ from
{0, 8, 16, 24, 32, 40}. These tunning datasets do not overlap the test data in the evaluation. Note that when τ = 0,
the computation is totally synchronous; larger τ results in
more asynchronous computation. We chose τ = 32 as it
produced the best performance on the tunning datasets.
Table 1 and Table 2 report the root mean square errors (RMSEs) of all the methods using different numbers of inducing
points, i.e., m ∈ {50, 100, 200}. As we can see, ADVGP
exhibits better or comparable prediction accuracy in all the
cases. Therefore, while using asynchronous computation,
ADVGP maintains the same robustness and quality for inference. Furthermore, we examined the prediction accuracy of each method along with the training time, under
the settings m ∈ {100, 200}. Figure 1 shows that during
the same time span, ADVGP achieves the highest performance boost (i.e., RMSE is reduced faster than the competing methods), which demonstrates the efficiency of ADVGP. It is interesting to see that in a short period of time
since the beginning, SVIGP reduces RMSE as fast as ADVGP; however, after that, RMSE of SVIGP is constantly
larger than ADVGP, exhibiting an inferior performance. In
addition, DistGP-LBFGS converges earlier than both ADVGP and SVIGP. However, RMSE of DistGP-LBFGS is
larger than both ADVGP and SVIGP at convergence. This
implies that the L-BFGS optimization converged to a sub1

http://stat-computing.org/dataexpo/2009/

Asynchronous Distributed Variational Gaussian Process for Regression

33.5

32.5
0

34.5
RMSE

RMSE

34.5

38.5
ADVGP
DistGP−GD
DistGP−LBFGS
SVIGP

2000
3000
Time (s)

4000

(A) n = 700K, m = 100

32.5
0

37.5

2500

5000
7500
Time (s)

10000

(B) n = 700K, m = 200

ADVGP
DistGP−GD
DistGP−LBFGS
SVIGP

37.5

36.5

33.5

1000

38.5
ADVGP
DistGP−GD
DistGP−LBFGS
SVIGP

RMSE

35.5
ADVGP
DistGP−GD
DistGP−LBFGS
SVIGP

RMSE

35.5

36.5

35.5
0

0.5

1
Time (s)

1.5

2

35.5
0

1.5

4

x 10

(C) n = 2M, m = 100

3
Time (s)

4.5

6
4

x 10

(D) n = 2M, m = 200

Figure 1. Root mean square errors (RMSEs) for US flight data as a function of training time.

optimal solution.

35
τ =5
τ =10
τ =20
τ =40
τ =80
τ =160

Table 1. Root mean square errors (RMSEs) for 700K/100K US
Flight data.

Method
Prox GP
GD Dist GP
LBFG Dist GP
SVIGP

m = 50
32.9080
32.9411
33.0707
33.1054

m = 100
32.7543
32.8069
33.2263
32.9499

m = 200
32.6143
32.6521
32.8729
32.7802

Table 2. RMSEs for 2M/100K US Flight data.

Method
Prox GP
GD Dist GP
LBFG Dist GP
SVIGP

m = 50
36.1156
36.0142
35.9809
36.2019

m = 100
35.8347
35.9487
36.1676
35.9517

m = 200
35.7017
35.7971
36.0749
35.8599

We also studied how the delay limit τ affects the performance of ADVGP. Practically, when many machines are
used, some worker may always be slower than the others due to environmental factors, e.g., unbalanced workloads. To simulate this scenario, we intentionally introduced a latency by assigning each worker a random sleep
time of 0, 10 or 20 seconds at initialization; hence a
worker would pause for its given sleep time before each
iteration. In our experiment, the average per-iteration
running time was only 0.176 seconds; so the fastest
worker could be hundreds of iterations ahead of the slowest one in the asynchronous setting. We examined τ =
0, 5, 10, 20, 40, 80, 160 and plotted RMSEs as a function of
time in Figure 2. Since RMSE of the synchronous case
(τ = 0) is much larger than the others, we do not show it
in the figure. When τ is larger, ADVGP’s performance is
more fluctuating. Increasing τ , we first improved the prediction accuracy due to more efficient CPU usage; however,
later we observed a decline caused by the excessive asynchronization that impaired the optimization. Therefore, to
use ADVGP for workers at various paces, we need to carefully choose the appropriate delay limit τ .

RMSE

34.5
34
33.5
33
32.5
0

2000

4000
6000
Time (s)

8000

10000

Figure 2. Root mean square errors (RMSEs) as a function of time
for different delay limits τ

6.2. Scalability
Next, we examined the scalability of our asynchronous
inference method, ADVGP. To this end, we used the
700K/100K dataset and compared with the synchronous
inference algorithm DistGP (Gal et al., 2014). For a fair
comparison, we used the local gradient descent version of
DistGP, i.e., DistGP-GD. We conducted two experiments
on 4 c4.8xlarge instances of Amazon EC2 cloud, where
we set the number of inducing points m = 100. In the
first experiment, we fixed the size of the training data, and
increased the number of CPU cores from 4 to 128. We
examined the per-iteration running time of both ADVGP
and DistGP-GD. Figure 3(A) shows that while both decreasing with more CPU cores, the per-iteration running
time of ADVGP is much less than that of DistGP-GD. This
demonstrates the advantage of ADVGP in computational
efficiency. In addition, the per-iteration running time of
ADVGP decays much more quickly than that of DistGPGD as the number of cores approaches 128. This implies
that even the communication cost becomes dominant, the
asynchronous mechanism of ADVGP still effectively reduces the latency and maintains a high usage of the computational power. In the second experiment, we simultaneously increased the number of cores and the size of training data. We started from 87.5K samples and 16 cores

Asynchronous Distributed Variational Gaussian Process for Regression

and gradually increased them to 700K samples and 128
cores. As shown in Figure 3(B), the average per-iteration
time of DistGP-GD grows linearly; in contrast, the average per-iteration time of ADVGP stays almost constant.
We speculate that without synchronous coordination, ADVGP can fully utilize the network bandwidth so that the
increased amount of messages, along with the growth of
the data size, affect little the network communication efficiency. This demonstrates the advantage of asynchronous
inference from another perspective.

100

10-1

10
Number of cores

100

Number of cores
64
96

128

ADVGP
DistGP-GD

1.5
1

700

600

0.5
0

1

32

600
0

2

4
Dataset size

(A)

6

8
5

×10

(B)

Figure 3. Scalability tests on 700K US flight data. (A) Periteration time as a function of available cores in log-scale. (B)
Per-iteration time when scaling the computational resources proportionally to dataset size.

ADVGP
Mean
Vowpal Wabbit

500

500
RMSE

101

0

RMSE

ADVGP
DistGP-GD

Time per iteration (s)

Time per iteration (s)

2

processes. The delay limit τ was selected as 20. We used
Vowpal Wabbit to train a linear regression model, with default settings. We also took the average traveling time over
the training data to obtain a simple mean prediction. In Figure 4(A), we report RMSEs of the linear regression and the
mean prediction, as well as the GP regression along with
running time. As we can see, ADVGP greatly outperforms
the competing methods. Only after 6 minutes, ADVGP
has improved RMSEs of the linear regression and the mean
prediction by 9% and 41%, respectively; the improvements
continued for about 30 minutes. Finally, ADVGP reduced
the RMSEs of the linear regression and the mean prediction
by 22% and 49%, respectively. The RMSEs are {ADVGP:
333.4, linear regression: 424.8, mean-prediction: 657.7}.

ADVGP
Mean
Vowpal Wabbit

400
400
300
0

2000
4000
Time (s)

6000

(A) 100M training samples

300
0

2000
4000
Time (s)

6000

(B) 1B training samples

Figure 4. RMSE as a function of training time on NYC Taxi Data.

6.3. NYC Taxi Traveling Time Prediction
Finally, we applied ADVGP for an extremely large problem: the prediction of the taxi traveling time in New York
city. We used the New York city yellow taxi trip dataset
2
, which consist of 1.21 billions of trip records from January 2009 to December 2015. We excluded the trips that
are outside the NYC area or more than 5 hours. The average traveling time is 764 seconds and the standard derivation is 576 seconds. To predict the traveling time, we used
the following 9 features: time of the day, day of the week,
day of the month, month, pick-up latitude, pick-up longitude, drop-off latitude, drop-off longitude, and travel distance. We used Amazon EC2 cloud, and ran ADVGP on
multiple Amazon c4.8xlarge instances, each with 36 vCPUs and 60 GB memory. We compared with the linear
regression model implemented in Vowpal Wabbit (Agarwal
et al., 2014). Vowpal Wabbit is a state-of-the-art large scale
machine learning software package and has been used in
many industrial-scale applications, such as click-throughrate prediction (Chapelle et al., 2014).
We first randomly selected 100M training samples and
500K test samples. We set m = 50 and initialized the
inducing points as the the K-means cluster centers from a
subset of 2M training samples. We trained a GP regression
model with ADVGP, using 5 Amazon instances with 200
2
http://www.nyc.gov/html/tlc/html/about/
trip_record_data.shtml

To further verify the advantage of GP regression in extremely large applications, we used 1B training and 1M
testing samples. We used 50 inducing points, initialized
by the K-means cluster centers from a 1M training subset.
We ran ADVGP using 28 Amazon instances with 1000 processes and chose τ = 100. As shown in Figure 4(B), the
RMSE of GP regression outperforms the linear models by a
large margin. After 12 minutes, ADVGP has improved the
RMSEs of the linear regression and the mean prediction by
8% and 40%, respectively; the improvement kept growing
for about 1.5 hours. At the convergence, ADVGP outperforms the linear regression and the mean prediction by 15%
and 44%, respectively. The RMSEs are {ADVGP: 309.7,
linear regression: 362.8, mean-prediction: 556.3}. In addition, the average per-iteration time of ADVGP is only 0.21
seconds. These results confirm the power of the nonlinear
regression in extremely large real-world scenarios, comparing with linear models, while the latter are much easier to
be scaled up and hence more popular.

7. Conclusion
We have presented ADVGP, an asynchronous, distributed
variational inference algorithm for GP regression, which
enables real-world extremely large applications. ADVGP
is based on a novel variational GP framework, which allows
flexible construction of low rank approximations and can
relate to many sparse GP models.

Asynchronous Distributed Variational Gaussian Process for Regression

References
Agarwal, Alekh and Duchi, John C. Distributed delayed
stochastic optimization. In Advances in Neural Information Processing Systems 24, pp. 873–881. Curran Associates, Inc., 2011.
Agarwal, Alekh, Chapelle, Oliveier, Dudı́k, Miroslav, and
Langford, John. A reliable effective terascale linear
learning system. Journal of Machine Learning Research,
15:1111–1133, 2014.
Bauer, Matthias, van der Wilk, Mark, and Rasmussen,
Carl Edward. Understanding probabilistic sparse Gaussian process approximations. In Advances in Neural Information Processing Systems 29, pp. 1525–1533, 2016.
Bishop, Christopher M. and Tipping, Michael E. Variational relevance vector machines. In Proceedings of the
16th Conference in Uncertainty in Artificial Intelligence
(UAI), 2000.
Chapelle, Olivier, Manavoglu, Eren, and Rosales, Romer.
Simple and scalable response prediction for display advertising. ACM Transactions on Intelligent Systems and
Technology (TIST), 5(4):61:1–61:34, December 2014.
ISSN 2157-6904.
Dai, Zhenwen, Damianou, Andreas, Hensman, James, and
Lawrence, Neil D. Gaussian process models with parallelization and GPU acceleration. In NIPS Workshop on
Software Engineering for Machine Learning, 2014.
Deisenroth, Marc and Ng, Jun W. Distributed Gaussian
processes. In Proceedings of the 32nd International
Conference on Machine Learning, pp. 1481–1490, 2015.
Filippone, Maurizio, Zhong, Mingjun, and Girolami, Mark.
A comparative evaluation of stochastic-based inference
methods for gaussian process models. Machine Learning, 93(1):93–114, 2013.
Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl. Distributed variational inference in sparse Gaussian process
regression and latent variable models. In Advances in
Neural Information Processing Systems 27, pp. 3257–
3265, 2014.
Hensman, James, Fusi, Nicolo, and Lawrence, Neil D.
Gaussian processes for big data. In Proceedings of
the Conference on Uncertainty in Artificial Intelligence
(UAI), 2013.
Hensman, James, Matthews, Alexander G, Filippone, Maurizio, and Ghahramani, Zoubin. Mcmc for variationally
sparse gaussian processes. In Advances in Neural Information Processing Systems, pp. 1648–1656, 2015.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
Ensemble Nyström method. In Bengio, Y., Schuurmans,
D., Lafferty, J. D., Williams, C. K. I., and Culotta, A.
(eds.), Advances in Neural Information Processing Systems 22, pp. 1060–1068. Curran Associates, Inc., 2009.
Li, Mu, Andersen, David G, and Smola, Alexander J. Distributed delayed proximal gradient methods. In NIPS
Workshop on Optimization for Machine Learning, 2013.
Li, Mu, Andersen, David G, Park, Jun Woo, Smola,
Alexander J, Ahmed, Amr, Josifovski, Vanja, Long,
James, Shekita, Eugene J, and Su, Bor-Yiing. Scaling distributed machine learning with the parameter
server. In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), pp. 583–
598, 2014a.
Li, Mu, Andersen, David G, Smola, Alexander, and Yu,
Kai. Communication efficient distributed machine learning with the parameter server. In Neural Information
Processing Systems 27, 2014b.
Matthews, Alexander G de G, van der Wilk, Mark, Nickson, Tom, Fujii, Keisuke, Boukouvalas, Alexis, LeónVillagrá, Pablo, Ghahramani, Zoubin, and Hensman,
James. GPflow: A Gaussian process library using TensorFlow. Journal of Machine Learning Research, 18
(40):1–6, 2017.
Murray, Iain and Adams, Ryan P. Slice sampling covariance hyperparameters of latent gaussian models. In Advances in Neural Information Processing Systems 24, pp.
1732–1740, 2010.
Peng, Hao and Qi, Yuan. EigenGP: Sparse Gaussian process models with adaptive eigenfunctions. In Proceedings of the 24th International Joint Conference on Artificial Intelligence, pp. 3763–3769, 2015.
Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward. A unifying view of sparse approximate Gaussian
process regression. The Journal of Machine Learning
Research, 6:1939–1959, 2005.
Rasmussen, Carl E. and Williams, Christopher K. I. Gaussian Processes for Machine Learning. The MIT Press,
2006.
Schwaighofer, Anton and Tresp, Volker. Transductive and
inductive methods for approximate Gaussian process regression. In Advances in Neural Information Processing
Systems 15, pp. 953–960. MIT Press, 2003.
Seeger, Matthias, Williams, Christopher, and Lawrence,
Neil. Fast forward selection to speed up sparse Gaussian
process regression. In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics,
2003.

Asynchronous Distributed Variational Gaussian Process for Regression

Smola, Alexander J. and Bartlett, Peter L. Sparse greedy
Gaussian process regression. In Advances in Neural Information Processing Systems 13. MIT Press, 2001.
Snelson, Edward and Ghahramani, Zoubin. Sparse Gaussian processes using pseudo-inputs. In Advances in
Neural Information Processing Systems, pp. 1257–1264,
2005.
Titsias, Michalis K. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of
the Twelfth International Conference on Artificial Intelligence and Statistics, pp. 567–574, 2009.
Williams, Christopher and Seeger, Matthias. Using the
Nyström method to speed up kernel machines. In Advances in Neural Information Processing Systems 13, pp.
682–688, 2001.
Zeiler, Matthew D. ADADELTA: an adaptive learning rate
method. arXiv:1212.5701, 2012.

