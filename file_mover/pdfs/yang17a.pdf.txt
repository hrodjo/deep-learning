High-dimensional Non-Gaussian Single Index Models via Thresholded Score
Function Estimation
Zhuoran Yang 1 Krishnakumar Balasubramanian 1 Han Liu 1

Abstract
We consider estimating the parametric component of single index models in high dimensions.
Compared with existing work, we do not require
the covariate to be normally distributed. Utilizing
Stein‚Äôs Lemma, we propose estimators based on
the score function of the covariate. Moreover, to
handle score function and response variables that
are heavy-tailed, our estimators are constructed
via carefully thresholding their empirical counterparts. Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators. Extensive
numerical experiments are provided to back up
our theory.

1. Introduction
Estimators for high-dimensional parametric (linear) models have been developed and analyzed extensively in
the last two decades (see for example (BuÃàhlmann &
van de Geer, 2011; Vershynin, 2015) for comprehensive
overviews). While being a useful testbed for illustrating
conceptual phenomenon, they often suffer from a lack of
flexibility in modeling real-world situations. On the other
hand, completely nonparametric models, although flexible, suffer from the curse of dimensionality unless restrictive additive sparsity or smoothness assumptions are imposed (Ravikumar et al., 2009; Yuan et al., 2016). An interesting compromise between the parametric and nonparametric models is provided by the so-called semiparametric
index models (Horowitz, 2009). Here, the response and the
covariate are linked through a low-dimensional nonparametric function that takes in as input a linear transformation of the covariate. The nonparametric component is also
called as the link function and the linear components are
1

Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Han Liu <hanliu@princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

called as the indices.
In this work, we focus on the simplest family of such models, the single index models (SIMs), which assume that the
response Y and the covariate X satisfy Y = f (hX, Œ≤ ‚àó i) +
, where Œ≤ ‚àó is the true signal,  is the mean-zero random
noise, and f is a univariate link function. (see ¬ß2 for the
precise definition). They form the basis of more complicated models such as Multiple Index Models (MIMs) (Diaconis & Shahshahani, 1984) and Deep Neural Networks
(DNNs) (LeCun et al., 2015), which are cascades of MIMs.
Moreover, we focus on the task of estimating the parametric (linear) component Œ≤ ‚àó without the knowledge of
the nonparametric part f in the high-dimensional setting,
where the number of samples is much smaller than the dimensionality of Œ≤ ‚àó .
Estimating the parametric component without depending
on the specific form of the nonparametric part appears naturally in several situations. For example, in one-bit compressed sensing (Boufounos & Baraniuk, 2008) and sparse
generalized linear models (Loh & Wainwright, 2015), we
are interested in recovering the true signal vector based on
nonlinear measurements. Furthermore, in a DNN, the activation function is pre-specified and the task is to estimate
the linear components, which are used for prediction in the
test stage. Performing nonlinear least-squares in this setting, leads to nonconvex optimization problems that are invariably sub-optimal without further assumptions. Hence,
developing estimators for the linear component that are
both statistically accurate and computationally efficient for
a class of activation functions provide a compelling alternative. Understanding such estimators for SIMs is hence
crucial for understanding the more complicated DNNs.
Although SIMs appear to be a simple extension of the
standard linear models, most existing work in the highdimensional setting assume X follows a Gaussian distribution for estimating Œ≤ ‚àó without the knowledge of the nonparametric part. It is not clear whether those estimation
methods are still valid and optimal when X is drawn from
a more general class of distributions. To relax the Gaussian
assumption, we study the setting where the distribution of
X is non-Gaussian but known a priori.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

1.1. Challenges of the Single Index Models

1400

There are significant challenges that appear when we are
dealing with estimators for SIMs. They can be summarized as assumptions on either the link function or the data
distribution (for example, non-Gaussian assumption).

1200

1. Knowledge of link function: Suppose the link function is known, for example, f (u) = u2 which corresponds to the phase retrieval model (see (Jaganathan
et al., 2015) for a survey and history of this model).
Then using an M-estimator to estimate Œ≤ ‚àó is a natural
procedure (Jaganathan et al., 2015). But computationally the problem becomes nonconvex and one need to
resort to either SDP based convex relaxations that are
computationally expensive or do non-convex alternating minimization that require Gaussian assumptions
on the data for successful initialization in the highdimensional setting (Cai et al., 2015). Furthermore,
if the link function is changed, it might become challenging or impossible to obtain provably computable
estimators.

600

2. Knowledge of data distribution: Now suppose we
want to be agnostic about the link function, i.e., we
want to estimate the linear component for a general
class of link functions. Then it becomes necessary to
make assumptions about the distribution from which
the covariates are sampled from. In particular, assuming the covariate has Gaussian and symmetric elliptical distributions respectively, (Plan & Vershynin,
2016) and (Goldstein et al., 2016) propose estimators
in the high-dimensional setting for a large class of unknown link functions.
As mentioned previously, our estimators are based on
Stein‚Äôs Lemma for non-Gaussian distributions, which utilizes the score function. Estimating with the score function
is challenging due to their heavy tails. In order to illustrate
that, consider the univariate histograms provided in Figure1. The dark shaded, more concentrated one corresponds to
the histogram of 10000 i.i.d. samples from Gamma distribution with scale and shape parameters set to 5 and 0.2
respectively. The transparent histogram corresponds to the
distribution of the score function of the same Gamma distribution. Note that even when the actual Gamma distribution is well concentrated, the distribution of the corresponding score function is well-spread and heavy-tailed. In the
high dimensional setting, in order to estimate with the score
functions, we require certain vectors or matrices based on
the score functions to be well-concentrated in appropriate
norms. In order to achieve that, we construct robust estimators via careful truncation arguments to balance the bias
(due to thresholding)-variance (of the estimator) tradeoff
and achieve the required concentration.

1000
800

400
200
0
-10

0

10

20

30

40

50

Figure 1: Histogram of Score Function based on 10000
independent samples from the Gamma distribution with
shape 5 and scale 0.2. The dark histogram (we recommend
the reader to zoom in to notice it) concentrated around zero
corresponds to the Gamma distribution and the transparent
histogram corresponds to the distribution of the score of the
same Gamma distribution.
1.2. Related Work
There is a significant body of work on SIMs in the lowdimensional setting. They are based on assumptions on
either the distribution of the covariate or the link functions. Assuming a monotonic link function, (Han, 1987;
Sherman, 1993) propose the maximum rank correlation
estimator exploiting the relationship between monotonic
functions and rank-correlations. Furthermore, (Li & Duan,
1989) propose an estimator for a wide class of unknown
link functions under the assumption that the covariate follows a symmetric elliptical distribution. This assumption is
restrictive as often times the covariates are not from a symmetric distribution. For example, in several economic applications where the covariates are usually highly skewed
and heavy-tailed (Horowitz, 2009). A line of work for
estimation in SIMs is proposed by Ker-Chau Li which is
based on sliced inverse regression (Li, 1991) and principal Hessian directions (Li, 1992) . These estimators are
based on similar symmetry assumptions and involve computing second-order (conditional and unconditional) moments which are difficult to estimate in high-dimensions
without restrictive assumptions.
The success of Lasso and related linear estimators in highdimensions (BuÃàhlmann & van de Geer, 2011), also enabled the exploration of high-dimensional SIMs. Although,
this is very much work in progress. As mentioned previously, (Plan & Vershynin, 2016) show that the Lasso estimator works for the SIMs in high dimensions when the
data is Gaussian. A more tighter albeit an asymptotic results under the same setting was proved in (Thrampoulidis
et al., 2015). Very recently (Goldstein et al., 2016) extend

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

the results of (Li & Duan, 1989) to the high dimensional
setting but it suffers from similar problems as mentioned
in the low-dimensional setting. For the case of monotone
nonparametric component, (Yang et al., 2015) analyze a
non-convex least squares approach under the assumption
that the data is sub-Gaussian. However, the success of
their method hinges on the knowledge of the link function. Furthermore, (Jiang & Liu, 2014; Lin et al., 2015;
Zhu et al., 2006) analyze the sliced inverse regression estimator in the high-dimensional setting concentrating mainly
on support recovery and consistency properties. Similar
to the low-dimensional case, the assumptions made on the
covariate distribution restrict them from several real-world
applications involving non-Gaussian or non-symmetric covariate, for example high-dimensional problems in economics (Fan et al., 2011). Furthermore, several results
are established on a case-by-case basis for fixed link function. Specifically (Boufounos & Baraniuk, 2008; Ai et al.,
2014) and (Davenport et al., 2014) consider 1-bit compressed sensing and matrix completion respectively, where
the link is assumed to be the sign function. Also, (Waldspurger et al., 2015) and (Cai et al., 2015) propose and analyze convex and non-convex estimators for phase retrieval
respectively, in which the link is the square function. All
the above works, except (Ai et al., 2014) make Gaussian
assumptions on the data and are specialized for the specific
link functions. The non-asymptotic result obtained in (Ai
et al., 2014) is under sub-Gaussian assumptions, but the estimator is not consistent. Finally, there is a line of work
focussing on estimating both the parametric and the nonparametric component (Kalai & Sastry, 2009; Kakade et al.,
2011; Alquier & Biau, 2013; Radchenko, 2015). We do not
focus on this situation in this paper as mentioned before.
To summarize, all the above works require restrictive assumption on either the data distribution or on the link function. We propose and analyze an estimator for a class of
(unknown) link functions for the case when the covariates
are drawn from a non-Gaussian distribution ‚Äì under the assumption that we know the distribution a priori. Note that
in several situations, one could fit specialized distributions,
to real-world data that is often times skewed and heavytailed, so that it provides a good generative model of the
data. Also, mixture of Gaussian distribution, with the number of components selected appropriately, approximates the
set of all square integrable distributions to arbitrary accuracy (see for example (McLachlan & Peel, 2004)). Furthermore, since this is a density estimation problem it is
unlabeled and there is no issue of label scarcity. Hence it is
possible to get accurate estimate of the distribution in most
situations of interest. Thus our work is complementary to
the existing literature and provides an estimator for a class
of models that is not addressed in the previous works. We
conclude this section with a summary of our main contri-

butions in this paper:
‚Ä¢ We propose estimators for the parametric component
of a sparse SIM and low-rank SIM for a class of unknown link function under the assumption that the covariate distribution is non-Gaussian but known a priori.
‚Ä¢ We show that it is possible to recover a s-sparse ddimensional vector and a rank-r, d1 √ó d2 dimensional
matrix with number of samples of the order of s log d
and r(d1 + d2 ) log(d1 + d2 ) respectively under significantly mild moment assumptions in the SIM setting.
‚Ä¢ We provide numerical simulation results that confirm
our theoretical predictions.

2. Single Index Models
In this section, we introduce the notation and define the
single index models. Throughout this work, we use [n] to
denote the set {1, . . . , n}. In addition, for a vector v ‚àà Rd ,
we denote by kvkp the `p -norm of v for any p ‚â• 1. We
use S d‚àí1 to denote the unit sphere in Rd , which is defined
as S d‚àí1 = {v ‚àà Rd : kvk2 = 1}. In addition, we define
the support of v ‚àà Rd as supp(v) = {j ‚àà [d], vj 6= 0}.
Moreover, we denote the nuclear norm, operator norm, and
Frobenius norm of a matrix A ‚àà Rd1 √ód2 by k¬∑k? , k¬∑kop , and
k ¬∑ kfro , respectively. We denote by vec(A) the vectorization of matrix A, which is a vector in Rd1 ¬∑d2 . For two matrices A, B ‚àà Rd1 √ód2 we define the trace inner product as
hA, Bi = Trace(A> B). Note that it can be viewed as the
standard inner product between vec(A) and vec(B). In addition, for an univariate function g : R ‚Üí R, we denote by
g ‚ó¶ (v) and g ‚ó¶ (A) the output of applying g to each element
of a vector v and a matrix A, respectively. Finally, for a random variable X ‚àà R with density p, we use p‚äód : Rd ‚Üí R
to denote the joint density of {X1 , ¬∑ ¬∑ ¬∑ , Xd }, which are d
identical copies of X.
Now we are ready to define the statistical model. Let
f : R ‚Üí R be an univariate function and Œ≤ ‚àó be the parameter of interest, which is a structured vector or a matrix.
The single index model in general is formulated as
Y = f (hX, Œ≤ ‚àó i) + ,
(2.1)
where X is the covariate, Y ‚àà R is the response, and
 is the exogenous noise that is independent of X. We
assume that  is centered and has bounded fourth moment,
i.e., Ep0 () = 0 and E(4 ) ‚â§ C for an absolute constant
C > 0. Note in particular that this allows for heavy-tailed
noise as well. In addition, we assume that the entries
of X are i.i.d. random variables with density p0 . This
assumption could be further relaxed using more sophisticated concentration arguments; here we focus on the i.i.d.
setting to clearly present the main message of this paper.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

Let {(Yi , Xi )}ni=1 be n i.i.d. observations of the SIM. Our
goal is to consistently estimate Œ≤ ‚àó without the knowledge
of f . In particular, we focus on the case when Œ≤ ‚àó is either
sparse or low-rank, which are defined as follows.
Sparse single index model: In this setting, we assume
that Œ≤ ‚àó = (Œ≤1‚àó , ¬∑ ¬∑ ¬∑ , Œ≤d‚àó )> is a sparse vector in Rd with s‚àó
nonzero entries, such that s‚àó  n  d. Moreover, for the
model in (2.1) to be identifiable, we further assume Œ≤ ‚àó lies
on the unit sphere S d‚àí1 as the norm of Œ≤ ‚àó can always be
absorbed in the unknown link function f .
Low-rank single index model: In this setting, we assume
that Œ≤ ‚àó ‚àà Rd1 √ód2 has rank r‚àó  min{d1 , d2 }. In this
scenario, X ‚àà Rd1 √ód2 and the inner product in (2.1) is
hX, Œ≤ ‚àó i = Trace(X > Œ≤ ‚àó ). For model identifiability, we
further assume that kŒ≤ ‚àó kF = 1, similar to the sparse case.

3. Estimation via Score Functions
Our estimator is primarily motivated by an interesting phenomenon illustrated in (Plan & Vershynin, 2016) for the
Gaussian setting. Below, we first briefly summarize the
result from (Plan & Vershynin, 2016) and then provide
our alternative justification for the same result via Stein‚Äôs
Lemma. We mainly leverage this alternative justification
and propose our estimators for the more general setting we
consider. Assuming for simplicity, we work in the onedimensional setting and are given n i.i.d. samples from the
SIM. Consider the least-squares estimator
n

1X
2
(Yi ‚àí Xi Œ≤) .
Œ≤bLS = argmin
n
Œ≤‚ààR
i=1
Note that the above estimator is the standard least-squares
estimator assuming a linear model (i.e., identity link function). The surprising observation from (Plan & Vershynin,
2016) is that, under the crucial assumption that X is standard Gaussian, Œ≤bLS is a good estimator of Œ≤ ‚àó (up to a scaling) even when the data is generated from the nonlinear
SIM. The same holds true for the high-dimensional setting
when the minimization is performed in an appropriately
constrained norm-ball (for example, the `1 -ball). Hence the
theory developed for the linear setting could be leveraged to
understand the performance in the SIM setting. Below, we
give an alternative justification for the above estimator as an
implication of Stein‚Äôs Lemma in the Gaussian case, which
is summarized as follows.
Proposition 3.1 (Gaussian Stein‚Äôs Lemma (Stein, 1972)).
Let X ‚àº N (0, 1) and g : R ‚Üí R be a continuos function
such that E|g 0 (X)| ‚â§ ‚àû. Then we have E[g(X)X] =
E[g 0 (X)].
Note that in our context for SIMs, we have E[f 0 (X)] ‚àù Œ≤ ‚àó
and E[f (X)X] = E[Y ¬∑ X]. Now consider the following

estimator, which is based on performing least-squares on
the sample version of the above proposition:
n

1X
Œ≤bSL = argmin
(Yi Xi ‚àí Œ≤)2
n
Œ≤‚ààR
i=1
Note that Œ≤bLS and Œ≤bSL are the same estimators assuming
X ‚àº N (0, 1), as n ‚Üí ‚àû. This observation leads to an alternative interpretation of the estimator proposed by (Plan
& Vershynin, 2016) via Stein‚Äôs Lemma for Gaussian random variables. Thus it provides an alternative justification
for why the linear least-squares estimator should work in
the SIM setting. This observation naturally leads to leveraging non-Gaussian versions of Stein‚Äôs Lemma for dealing
with non-Gaussian covariates.
We now describe our estimator for the non-Gaussian setting
based on the above observation. We first define the score
function associate to a density. Let p : Rd ‚Üí R be a probability density function defined on Rd . The score function
Sp : Rd ‚Üí R associated to p is defined as
Sp (x) = ‚àí‚àáx [log p(x)] = ‚àí‚àáx p(x)/p(x).
Note that in the above definition, the derivative is taken
with respect to x. This is different from the more traditional
definition of the score function where the density belongs
to a parametrized family and the derivative is taken with respect to the parameters. In the rest of the paper to simplify
the notation, we omit the subscript x from ‚àáx . We also
omit the subscript p from Sp when the underlying density
p is clear from the context.
We now describe a version of Stein‚Äôs Lemma that is applicable for non-Gaussian random variables. Note from
the motivating example for the Gaussian case that while
utilizing the Stein‚Äôs Lemma for SIM estimation, assumptions on the function in Stein‚Äôs Lemma translate directly
to those on the link function in SIM. We now introduce a
version of Stein‚Äôs Lemma that applies to non-Gaussian random variables and for continuously differentiable functions
from (Stein et al., 2004). A more general version of the
Stein‚Äôs Lemma that applies to a class of regular functions
is available in (Stein et al., 2004). We assume continuously
differentiable functions in the Stein‚Äôs Lemma below as they
cover a wide range of practical SIM such as generalized linear models and single-layer neural networks.
Lemma 3.2 (Non-Gaussian Stein‚Äôs Lemma (Stein et al.,
2004)). Let g : Rd ‚Üí R be continuously differentiable
function and X ‚àà Rd be a random vector with density
p : Rd ‚Üí R, which is also continuously differentiable. Under the assumption that the expectations E[g(X) ¬∑ S(X)]
and E[‚àág(X)] are both well-defined, we have the follow-

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

ing generalized Stein‚Äôs identity
Z
E[g(X) ¬∑ S(X)] = ‚àí
g(x) ¬∑ ‚àáp(x)dx
Rd
Z
=
‚àág(x) ¬∑ p(x)dx = E[‚àág(X)].

too can be heavy-tailed. Thus the naive method of using the
sample version of (3.3) to estimate Œ≤ ‚àó leads to sub-optimal
statistical rates of convergence.
(3.1)

Rd

Recall that in the two single index models introduced in
¬ß2, X in (2.1) has i.i.d. entries with density p0 . To unify
both the vector and matrix settings, in the low-rank SIM,
we identify X with vec(X) ‚àà Rd where d = d1 ¬∑ d2 . In
this case, X has density p = p‚äód
0 and the corresponding
score function S : Rd ‚Üí Rd is given by
S(x) = ‚àí‚àá log p(x) = ‚àí‚àáp(x)/p(x) = s0 ‚ó¶ (x), (3.2)

To improve concentration and obtain optimal rates of convergence, we replace Y ¬∑ S(X) with a transformed random
variable T (Y, X), which will be defined precisely in ¬ß4 for
the sparse and low-rank cases. In particular, T (Y, X) is a
carefully truncated version of Y ¬∑ S(X), introduced and analyzed in (Catoni et al., 2012; Fan et al., 2016) for related
problems, that enables us to obtain well-concentrated estimators. Thus our final estimator Œ≤b is defined as the solution
to the following regularized optimization problem
minimize L(Œ≤) + Œª ¬∑ R(Œ≤),
Œ≤‚ààRd

p00 /p0

where the univariate function s0 =
is applied to each
entry of x. Thus S(X) has i.i.d. entries. In addition, by
Lemma 3.2, we have E[S(X)] = 0 by setting g to be a
constant function in (3.1). Moreover, in the context of SIMs
specified in (2.1), we have


E[Y ¬∑ S(X)] = E f (hX, Œ≤ ‚àó i) ¬∑ S(X)

(3.4)

n

L(Œ≤) = hŒ≤, Œ≤i ‚àí


2X
hT (Yi , Xi ), Œ≤ ,
n i=1

where Œª > 0 is the regularization parameter which will be
specified later and R(¬∑) is the `1 -norm in the vector case
and the nuclear norm in the matrix case.

= E[f 0 (hX, Œ≤ ‚àó i)] ¬∑ Œ≤ ‚àó ,
as long as the density and the link function satisfy the conditions stated in Lemma 3.2. This implies that optimization
problem

	
minimize hŒ≤, Œ≤i ‚àí 2E[Y ¬∑ hS(X), Œ≤i]
(3.3)
Œ≤‚ààRd

has solution Œ≤ = ¬µ ¬∑ Œ≤ ‚àó , where ¬µ = E[f 0 (hX, Œ≤ ‚àó i)]. Hence
the above program could be used to obtain the unknown Œ≤ ‚àó
as long as ¬µ 6= 0. Before we proceed to describe the sample version of the above program, we make the following
brief remark. The requirement ¬µ 6= 0 rules out in particular the use of our approach for non-Gaussian phase retrieval (where f (u) = u2 ) as in that case we have ¬µ = 0
when X is centered. But we emphasize that the same holds
true in the Gaussian and elliptical setting as well, as noted
in (Plan & Vershynin, 2016) and (Goldstein et al., 2016).
Their methods also fail to recover the true Œ≤ ‚àó when the SIM
model corresponds to phase retrieval. We refer the reader to
¬ß6 for a discussion on overcoming this limitation.
Finally, we use a sample version of the above program as
an estimator for the unknown Œ≤ ‚àó . In order to deal with the
high-dimensional setting, we consider a regularized version
of the above formulation. More specifically, we use the
`1 -norm and nuclear norm regularization in the vector and
matrix settings respectively. However, a major difficulty in
the sample setting for this procedure is that E[Y ¬∑ S(X)]
and its empirical counterpart may not be close enough due
to a lack of concentration. Recall our discussion from ¬ß1.1
that even if the random variable X is light-tailed, its scorefunction S(x) might be arbitrarily heavy-tailed. Furthermore, bounded-fourth moment assumption on the noise, Y

4. Theoretical Results
In this section, we state our main results in Theorem 4.2
and Theorem 4.3,which establish the statistical rates of
convergence of the estimator defined in ¬ß3. The proof
for both Theorems is presented in the supplementary material. Before doing so, we introduce our main moment
assumption for the single index model. This assumption is
made apart from the assumptions made on the noise and
the link function in ¬ß2 and ¬ß3 respectively. Recall that
each entry of the score function defined in (3.2) is equal
to s0 (u) = ‚àíp00 (u)/p0 (u). We first state the assumption
and make a few remarks about it.
Assumption 4.1. There exists an absolute constant M >
0 such that E(Y 4 ) ‚â§ M and Ep0 [s40 (U )] ‚â§ M , where
random variable U ‚àà R has density p0 .
Consider the assumption E(Y 4 ) ‚â§ M . By CauchySchwarz inequality we have E(Y 4 ) ‚â§ 4E(4 ) +
4E[f 4 (hX, Œ≤ ‚àó )]. Note that we assum  to be centered, independent of X and has bounded fourth moment (see ¬ß2). If
the covariate X has bounded fourth moment along the direction of true parameter, since f (¬∑) is continuously differentiable, f (hX, Œ≤ ‚àó i) has bounded fourth moment as well
if f (¬∑) is defined on a compact subset of R. Hence the
condition E(Y 4 ) ‚â§ M is relatively easy to satisfy and significantly milder than assuming that Y is bounded or has
lighter tails. Furthermore, Ep0 [s40 (U )] ‚â§ M is relatively
mild and satisfied by a wide class of random variables.
Specifically random variables that are non-symmetric and
non-Gaussian satisfy this property thereby allowing our approach to work with covariates not previously possible.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

We believe it is highly non-trivial to weaken this condition
without losing significantly in the rates of convergence that
we discuss below.
4.1. Sparse Single Index Model
Under the above assumptions, we first state our theorem
on the sparse SIM. As discussed in ¬ß3, Y ¬∑ S(X) can by
heavy-tailed and hence we apply truncation to achieve concentration. Denote the j-th entry of the score function S
in (3.2) as Sj : Rd ‚Üí R, j ‚àà [d]. We define the truncated
response and score function as
Ye = sign(Y ) ¬∑ (|Y | ‚àß œÑ ),


Sej (x) = sign[Sj (x)] ¬∑ |Sj (x)| ‚àß œÑ ,

(4.1)

where œÑ > 0 is a predetermined threshold value. We define
Yei similarly for all Yi , i ‚àà [n]. Then we define the estimator Œ≤b as the solution to the optimization problem in (3.4)
e i ) and R(Œ≤) = kŒ≤k1 . Here we
with T (Yi , Xi ) = Yei ¬∑ S(X
apply elementwise truncation in T to ensure the sample average of T converges to E[Y ¬∑ S(X)] in the `‚àû -norm for an
appropriately chosen œÑ . Note that the `‚àû -norm is the dual
norm of the `1 -norm. Such a convergence requirement in
the dual norm is standard in the analysis of regularized M estimators (Negahban et al., 2012) to achieve optimal rates.
The following theorem characterizes the convergence rates
b
of Œ≤.
Theorem 4.2 (Signal recovery for the sparse single index
model). For the sparse SIM defined in ¬ß2, we assume that
Œ≤ ‚àó ‚àà Rd has s‚àó nonzero entries. Under Assumption 4.1,
we let œÑ = 2(M ¬∑ log d/n)1/4 in (4.1)
p and set the regularization parameter Œª in (3.4) as C M ¬∑ log d/n, where
C > 0 is an absolute constant. Then with probability at
least 1‚àíd‚àí2 , the `1 -regularized estimator Œ≤b defined in (3.4)
satisfies
‚àö
kŒ≤b ‚àí ¬µŒ≤ ‚àó k2 ‚â§ s‚àó ¬∑ Œª, kŒ≤b ‚àí ¬µŒ≤ ‚àó k1 ‚â§ 4s‚àó ¬∑ Œª.
From this theorem, the `1p
- and `2 -convergence rates of Œ≤b
‚àó
‚àó
b
arep
kŒ≤ ‚àí ¬µŒ≤ k1 = O(s log d/n) and kŒ≤b ‚àí ¬µŒ≤ ‚àó k2 =
O( s‚àó log d/n), respectively. These rates match the convergence rates of sparse generalized linear models (Loh
& Wainwright, 2015) and sparse single index models with
Gaussian and symmetric elliptical covariates (Plan & Vershynin, 2016; Goldstein et al., 2016) which are known to
be minimax-optimal for this problem via matching lower
bounds.
4.2. Low-rank Single Index Model
We next state our theorem for the low-rank SIM. In this
case, we apply the nuclear norm regularization to promote
low-rankness. Note that by definition, T is matrix-valued.

Since the dual norm of the nuclear norm is the operator
norm, we need the sample average of T to converge to
E[Y ¬∑S(X)] in the operator norm rapidly to achieve optimal
rates of convergence. To achieve such a goal, we leverage
the truncation argument from (Catoni et al., 2012; Minsker,
2016) to construct T (Y, X).
Let œÜ : R ‚Üí R be a non-decreasing function such that
‚àí log(1‚àíx+x2 /2) ‚â§ œÜ(x) ‚â§ log(1+x+x2 /2), ‚àÄx ‚àà R.
Based on œÜ, we define a linear mapping œà : Rd1 √ód2 ‚Üí
Rd1 √ód2 as follows. For any A ‚àà Rd1 √ód2 , let


0 A
e
A=
A> 0
e In
and let Œ•ŒõŒ•> be the eigenvalue decomposition of A.
addition, let B = Œ•[œÜ ‚ó¶ (Œõ)]Œ•> , where œà is applied elementwisely on Œõ. Then we write B in block from as


B11 B12
B=
B21 B22
and define
 œà(A) = B12 . Finally, we define T (Y, X) =
1/Œ∫ ¬∑ œà Œ∫ ¬∑ Y ¬∑ S(X) , where Œ∫ > 0 will be specified later.
Therefore, our final estimator Œ≤b ‚àà Rd1 √ód2 is defined as the
solution to the optimization problem in (3.4) with R(Œ≤) =
kŒ≤k? . We note here the minimization in (3.4) is taken over
Rd1 √ód2 . The following theorem quantifies the convergence
rates of the proposed estimator.
Theorem 4.3 (Signal recovery for the low-rank single index model). For the low-rank single index model defined
in ¬ß2, we assume that rank(Œ≤ ‚àó ) = r‚àó . Under Assumption
4.1, we let
p
2 n ¬∑ log(d1 + d2 )
Œ∫= p
(d1 + d2 )M
in T (Y, X). Moreover,
the regularization parameter Œª in
p
(3.4) is set to C M ¬∑ (d1 + d2 ) ¬∑ log(d1 + d2 )/n, where
C > 0 is an absolute constant. Then with probability at
least 1 ‚àí (d1 + d2 )‚àí2 , the nuclear norm regularized estimator Œ≤b satisfies
‚àö
kŒ≤b ‚àí ¬µŒ≤ ‚àó kfro ‚â§ 3 r‚àó ¬∑ Œª, kŒ≤b ‚àí ¬µŒ≤ ‚àó k? ‚â§ 12r‚àó ¬∑ Œª.
By pthis theorem, we have kŒ≤b ‚àí ¬µŒ≤ ‚àó kfro
=
O( r‚àóp
(d1 + d2 ) ¬∑ log(d1 + d2 )/n) and kŒ≤b ‚àí ¬µŒ≤ ‚àó k? =
O(r‚àó ¬∑ (d1 + d2 ) ¬∑ log(d1 + d2 )/n). Note that the rate
obtained is minimax-optimal up to a logarithmic factor.
Furthermore, it matches the rates for low-rank single
index models with Gaussian and symmetric elliptical
distributions up to a logarithmic factor (Plan & Vershynin,
2016; Goldstein et al., 2016).

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

5. Numerical Experiments
We assess the finite sample performance of the proposed
estimators on simulated data. Throughout this section, we
let  ‚àº N (0, 1) and set the link function
‚àö in (2.1) as one of
f1 (u) = 3u + 10 sin(u) and f2 (u) = 2u + 4 exp(‚àí2u2 ),
which are plotted in Figure 2. We set p0 to be one of
(i) Gamma distribution with shape parameter 5 and scale
parameter 1, (ii) Student‚Äôs t-distribution with 5 degrees of
freedom, and (iii) Rayleigh distribution with scale parameter 2. To measure the estimation accuracy, we use the cosine distance
b Œ≤ ‚àó ) = 1 ‚àí kŒ≤k
b ‚àí1 |hŒ≤,
b Œ≤ ‚àó i|,
cos Œ∏(Œ≤,
‚Ä¢
where ‚Ä¢ stands for the Euclidean norm in the vector case
and the Frobenius norm when Œ≤ ‚àó is a matrix. Here we report the cosine distance rather than kŒ≤b ‚àí ¬µŒ≤ ‚àó k‚Ä¢ to compare the performances for X having different distributions,
where ¬µ may have different values.
For the vector case, we fix d = 2000, s‚àó = 5 and vary
n. The support of Œ≤ ‚àó is chosen uniformly random among
‚àó
all subsets
‚àö of {1, . . . , d}. For each j ‚àà supp(Œ≤ ), we set
‚àó
‚àó
Œ≤j = 1/ s ¬∑ Œ≥j , where each Œ≥j is an i.i.d. Rademacher
random variable.
p In addition, the regularization parameter
Œª is set to 4 log d/n.
p We plot the cosine distance against
the signal strength s‚àó log d/n in Figure 4-(a) and (b) for
f1 and f2 respectively, based on 200 independent trials for
each n. As shown in this figure, the estimation error grows
sublinearly as a function of the signal strength.
As for the matrix case, we fix d1 = d2 = 20, r‚àó = 3 and let
n vary. The signal parameter Œ≤ ‚àó is equal to U SV > , where
U, V ‚àà Rd√ód are random orthogonal matrices and S is a
diagonal matrix with r‚àó nonzero entries.
‚àö Moreover, we set
the nonzero diagonal entries of S as 1/ r‚àó , which implies
‚àó
kŒ≤
p kfro = 1. We set the regularization parameter as Œª =
2 (d1 + d2 ) log(d1 + d2 )/n. Furthermore, we use the
proximal gradient descent algorithm (with the learning rate
fixed to 0.05) to solve the nuclear norm regularization problem in (3.4). To present the result,
p we plot the cosine distant
against the signal strength r‚àó (d1 + d2 ) log(d1 + d2 )/n
in Figure 4-(b) based on 200 independent trials. As shown
in this figure, the error is bounded by a linear function of
the signal strength, which corroborates Theorem 4.3.

6. Conclusion
In this paper, we consider SIMs in the high-dimensional
non-Gaussian setting and proposed estimators based on
Stein‚Äôs Lemma for a wider class of unknown link functions and covariate distributions. We consider both sparse
and low-rank models and propose minimax rate-optimal
estimators under fairly mild assumptions. An interesting
avenue of future work is the problem of phase retrieval

with non-Gaussian data. Our current approach requires
that ¬µ 6= 0 which is not applicable. The main reason this
happens is we use a first-order version of Stein‚Äôs Lemma.
Such a problem could overcome by second-order Stein‚Äôs
Lemma (Janzamin et al., 2014). Obtaining rate-optimal
estimators based on second-order score functions require
addressing several challenges. Concentrating on phase retrieval (and sparse phase retrieval) we plan to report our
results for the above problem in the near future.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation
40

15

2a + 4 exp(!2a2 )

20
10

-10

p

0

-20

f (u) =

f (u) = 3 " u + 10 sin(u)

30

-30
-40
-10

-5

0

5

10

10
5
0
-5
-10
-15
-10

-5

0

u

Figure 2: Plot of the link functions f1 (u) = 3u + 10 ¬∑ sin(u) (left) and f2 (u) =
are nonlinear and not monotone.
0.3
0.25

5

10

u

‚àö

2u + 4 exp(‚àí2u2 ) (right). Both functions

0.1
Gamma(5; 2)
t(5)
Rayleigh(2)

0.09
0.08

Gamma(5; 2)
t(5)
Rayleigh(2)

0.07

b -$)
cos 3(-;

b -$)
cos 3(-;

0.2

0.06

0.15

0.05
0.04

0.1

0.03
0.02

0.05

0.01
0
0.1

0.2

p 0.3
s$ log d=n

0.4

0
0.1

0.5

0.2

p 0.3
s$ log d=n

0.4

0.5

Figure 3: Cosine distances between the true parameter and the estimated parameter in the sparse SIM with the link function
in 2.1 set to f1 (left) and f2 (right). Here we set d= 2000. s‚àó = 5 and vary n.
0.6
0.5

0.6
Gamma(5; 2)
t(5)
Rayleigh(2)

0.5

b -$)
cos 3(-;

0.4

b -$)
cos 3(-;

0.4
0.3

0.3

0.2

0.2

0.1
0
0.1

Gamma(5; 2)
t(5)
Rayleigh(2)

0.1

0.2 p

0.3

r$ (d

1

0.4

0.5

+ d2 ) log(d1 + d2 )=n

0.6

0.7

0
0.1

0.2 p

0.3

0.4

0.5

0.6

0.7

r$ (d1 + d2 ) log(d1 + d2 )=n

Figure 4: Cosine distances between the true parameter and the estimated parameter in the low-rank SIM with link function
in 2.1 set to f1 (left) and f2 (right). Here we set d1 = d2 = 20. r‚àó = 3 and vary n.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

References
Ai, Albert, Lapanowski, Alex, Plan, Yaniv, and Vershynin,
Roman. One-bit compressed sensing with non-gaussian
measurements. Linear Algebra and its Applications,
441:222‚Äì239, 2014.
Alquier, Pierre and Biau, GeÃÅrard. Sparse single-index
model. The Journal of Machine Learning Research, 14
(1):243‚Äì280, 2013.
Boucheron, SteÃÅphane, Lugosi, GaÃÅbor, and Massart, Pascal.
Concentration inequalities: A nonasymptotic theory of
independence. Oxford university press, 2013.
Boufounos, Petros T and Baraniuk, Richard G. 1-bit compressive sensing. In Information Sciences and Systems,
2008. CISS 2008. 42nd Annual Conference on, pp. 16‚Äì
21. IEEE, 2008.
BuÃàhlmann, Peter and van de Geer, Sara. Statistics for highdimensional data: methods, theory and applications.
Springer Science & Business Media, 2011.
Cai, T Tony, Li, Xiaodong, and Ma, Zongming. Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow. arXiv preprint
arXiv:1506.03382, 2015.

Horowitz, Joel L. Semiparametric and nonparametric
methods in econometrics, volume 12. Springer, 2009.
Jaganathan, Kishore, Eldar, Yonina C, and Hassibi, Babak.
Phase retrieval: An overview of recent developments.
arXiv preprint arXiv:1510.07713, 2015.
Janzamin, Majid, Sedghi, Hanie, and Anandkumar, Anima. Score function features for discriminative learning: Matrix and tensor framework. arXiv preprint
arXiv:1412.2863, 2014.
Jiang, B. and Liu, J. S. Variable selection for general index models via sliced inverse regression. The Annals of
Statistics, 42(5):1751‚Äì1786, 2014.
Kakade, Sham M, Kanade, Varun, Shamir, Ohad, and
Kalai, Adam. Efficient learning of generalized linear
and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pp.
927‚Äì935, 2011.
Kalai, Adam Tauman and Sastry, Ravi. The isotron algorithm: High-dimensional isotonic regression. In Conference on Learning Theory, 2009.
LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep
learning. Nature, 521(7553):436‚Äì444, 2015.

Catoni, Olivier et al. Challenging the empirical mean
and empirical variance: a deviation study. Annales de
l‚ÄôInstitut Henri PoincareÃÅ, ProbabiliteÃÅs et Statistiques, 48
(4):1148‚Äì1185, 2012.

Li, Ker-Chau. Sliced inverse regression for dimension reduction. Journal of the American Statistical Association,
86(414):316‚Äì327, 1991.

Davenport, Mark A, Plan, Yaniv, van den Berg, Ewout, and
Wootters, Mary. 1-bit matrix completion. Information
and Inference, 3(3):189‚Äì223, 2014.

Li, Ker-Chau. On principal Hessian directions for data visualization and dimension reduction: Another application of Stein‚Äôs lemma. Journal of the American Statistical Association, 87(420):1025‚Äì1039, 1992.

Diaconis, P. and Shahshahani, M. On nonlinear functions
of linear combinations. SIAM Journal on Scientific and
Statistical Computing, 5(1):175‚Äì191, 1984.
Fan, J., Lv, J., and Qi, L. Sparse high-dimensional models
in economics. Annual review of economics, 3(1):291‚Äì
317, 2011.
Fan, Jianqing, Wang, Weichen, and Zhu, Ziwei. Robust low-rank matrix recovery.
arXiv preprint
arXiv:1603.08315, 2016.
Goldstein, Larry, Minsker, Stanislav, and Wei, Xiaohan.
Structured signal recovery from non-linear and heavytailed measurements. arXiv preprint arXiv:1609.01025,
2016.
Han, Aaron K. Non-parametric analysis of a generalized
regression model: the maximum rank correlation estimator. Journal of Econometrics, 35(2-3):303‚Äì316, 1987.

Li, Ker-Chau and Duan, Naihua. Regression analysis under link violation. The Annals of Statistics, 17(3):1009‚Äì
1052, 1989.
Lin, Q., Zhao, Z., and Liu, J. S. On consistency and sparsity
for sliced inverse regression in high dimensions. arXiv
preprint arXiv:1507.03895, 2015.
Loh, Po-Ling and Wainwright, Martin J. Regularized mestimators with nonconvexity: Statistical and algorithmic theory for local optima. Journal of Machine Learning Research, 16:559‚Äì616, 2015.
McLachlan, Geoffrey and Peel, David. Finite mixture models. John Wiley & Sons, 2004.
Minsker, Stanislav. Sub-gaussian estimators of the mean
of a random matrix with heavy-tailed entries. arXiv
preprint arXiv:1605.07129, 2016.

Non-Gaussian Single Index Models via Thresholded Score Function Estimation

Negahban, Sahand N., Ravikumar, Pradeep, Wainwright,
Martin J., and Yu, Bin. A unified framework for highdimensional analysis of M -estimators with decomposable regularizers. Statistical Science, 27(4):538‚Äì557, 11
2012.
Plan, Yaniv and Vershynin, Roman. The generalized lasso
with non-linear observations. IEEE Transactions on information theory, 62(3):1528‚Äì1537, 2016.
Radchenko, Peter. High dimensional single index models.
Journal of Multivariate Analysis, 139:266‚Äì282, 2015.
Ravikumar, Pradeep, Lafferty, John, Liu, Han, and Wasserman, Larry. Sparse additive models. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
71(5):1009‚Äì1030, 2009.
Sherman, Robert P. The limiting distribution of the maximum rank correlation estimator. Econometrica: Journal
of the Econometric Society, 61(1):123‚Äì137, 1993.
Stein, C. A bound for the error in the normal approximation to the distribution of a sum of dependent random
variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume
2: Probability Theory. The Regents of the University of
California, 1972.
Stein, Charles, Diaconis, Persi, Holmes, Susan, Reinert,
Gesine, et al. Use of exchangeable pairs in the analysis of
simulations. In Stein‚Äôs Method. Institute of Mathematical
Statistics, 2004.
Thrampoulidis, Christos, Abbasi, Ehsan, and Hassibi,
Babak. Lasso with non-linear measurements is equivalent to one with linear measurements. Advances in Neural Information Processing Systems, 2015.
Vershynin, Roman. Estimation in high dimensions: a geometric perspective. In Sampling theory, a renaissance,
pp. 3‚Äì66. Springer, 2015.
Waldspurger, IreÃÄne, dAspremont, Alexandre, and Mallat,
SteÃÅphane. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149(12):47‚Äì81, 2015.
Yang, Zhuoran, Wang, Zhaoran, Liu, Han, Eldar, Yonina C,
and Zhang, Tong. Sparse nonlinear regression: Parameter estimation and asymptotic inference. International
Conference on Machine Learning, 2015.
Yuan, Ming, Zhou, Ding-Xuan, et al. Minimax optimal
rates of estimation in high dimensional additive models.
The Annals of Statistics, 44(6):2564‚Äì2593, 2016.

Zhu, Lixing, Miao, Baiqi, and Peng, Heng. On sliced inverse regression with high-dimensional covariates. Journal of the American Statistical Association, 101(474):
630‚Äì643, 2006.

