Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence
and Cluster Recovery

Ashkan Panahi 1 Devdatt Dubhashi 2 Fredrik D. Johansson 3 Chiranjib Bhattacharyya 4

Abstract
Standard clustering methods such as K–means,
Gaussian mixture models, and hierarchical clustering, are beset by local minima, which are
sometimes drastically suboptimal. Moreover the
number of clusters K must be known in advance.
The recently introduced sum–of–norms (SON)
or Clusterpath convex relaxation of k-means and
hierarchical clustering shrinks cluster centroids
toward one another and ensure a unique global
minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to
solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which
have a similar form to the unifying proximity
condition introduced in the approximation algorithms community (that covers paradigm cases
such as Gaussian mixtures and planted partition
models). We give experimental results to confirm
that our algorithm scales much better than previous methods while producing clusters of comparable quality.

1. Introduction
Clustering is perhaps the most fundamental problem in unsupervised learning. Many clustering algorithms have been
proposed in the literature (Jain et al., 1999), including Kmeans, spectral clustering, Gaussian mixture models and
hierarchical clustering, to solve problems with respect to
a wide range of cluster shapes. However, much research
has pointed out that these methods all suffer from instabilities. For example, the formulation of K-means is NP-hard
and the typical way to solve it is the Lloyds method, which
1
ECE, North Carolina State University, Raleigh, NC 2 CSE,
Chalmers University of Technology, Göteborg, Sweden 3 IMES,
MIT, Cambridge, MA 4 CSA, IISc, Bangalore, India. Correspondence to: Ashkan Panahi <panahi1986@gmail.com>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

requires randomly initializing the clusters. However, one
needs to know the number of clusters in advance and different initializations may lead to significantly different final
cluster results.
Lindsten et al. (2011) and Hocking et al. (2011) proposed
the following convex optimization procedure for clustering,
called SON (“Sum of norms” clustering) by the former and
Clusterpath by the latter;
minm

{ui ∈R

n
X
1X
kxi − ui k22 + λ
kui − uj k2
} 2
i=1
i<j

(1)

The main idea of the formulation is that if input data points
xi and xj belong to the same cluster, then their corresponding centroids ui and uj should be forced to be the same.
Intuitively, this is due to the fact that the second term is a
regularization term that enforces zeroes in the vector consisting of entries kui − uj k and can be seen as a generalization of the fused Lasso penalty. From another point of
view, the regularization term can be seen as an `1,2 norm,
i.e., the sum of `2 norms. Such a group norm is known to
encourage block sparse solutions (Bach et al., 2012). Thus
for many pairs (i, j), we expect to enforce ui = uj .
Lindsten et al. (2011) used an off–the–shelf convex solver,
CVX to generate solution paths. Hocking et al. (2011) introduced three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function separates, and they solve the convex clustering problem by the exact path following method designed
for the fused lasso. For the `1 and `2 norms, they employ
subgradient descent in conjunction with active sets. Recently, Chi & Lange (2015); Chen et al. (2015) introduce
two similar generic frameworks for minimizing the convex
clustering objective function with an arbitrary norm. One
approach solves the problem by the alternating direction
method of multipliers (ADMM), while the other solves it
by the alternating minimization algorithm (AMA). However both algorithms have issues with scalablity.
Moreover, none of these papers provide any theoretical
guarantees about the cluster recovery property of the algorithm. The first theoretical result on cluster recovery was
shown by Zhu et al. (2010): if the samples are drawn from

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

two cubes, each being one cluster, then SON can provably
recover the clusters provided that the distance between the
two cubes is larger than a threshold which depends (linearly) on the size of the cube and the ratio of numbers of
samples in each cluster. Unfortunately, the conditions for
recovery represent an extremely narrow special case: only
two clusters which both have to be cubes. Moreover in
their paper, there is no algorithm or analysis of the speed of
convergence. No other theoretical guarantees for SON are
known previously.
Here we develop a new algorithm in the spirit of recent
advances in stochastic methods for large scale optimization
(Bottou et al., 2016) to solve the optimization problem (1).
We give a convergence analysis and provide quite general
cluster recovery guarantees.
There has been a flurry of advances (Johnson & Zhang,
2013; Defazio et al., 2014; Schmidt et al., 2016) in developing algorithms for solving optimization problems for the
case when the objective consists of the sum of two convex functions: one is the average of a large number of
smooth component functions, and the other is a general
convex function that admits a proximal mapping (and the
whole objective function is strongly convex). The optimization (1) is of this form but here we exploit the structure
of (1) further by observing that the second function can also
be split into component functions. This results in an incremental algorithm with proximal iterations consisting of
very simple and natural steps. Our algorithm can be seen
as a special case of the methods of Bertsekas (2011). We
compute the proximal operator in closed form to yield very
simple and cheap iterations. Using the fact that the proximal operator is non-expansive, we refine and strengthen
Bertsekas’ convergence results. The stochastic incremental
nature of our algorithm makes it highly suited to large scale
problems (Bottou et al., 2016) in contrast to the methods in
Chi & Lange (2015); Chen et al. (2015).
We show that the SON formualation (1) provides strong
cluster recovery properties that go far beyond the special
case considered in Zhu et al. (2010). Our cluster recovery
conditions are similar in spirit to the unifying general conditions recently formulated in A. Kumar (2010); P.Awasthi
(2012) of the form that the means of the clusters are well–
separated, i.e., the distance between the means of any two
clusters is at least Ω(k) standard deviations (the notion of
standard deviations is based on the spectral norm of the matrix whose rows represent the difference between a point
and the mean of the cluster to which it belongs). Besides
containing the result of Zhu et al. (2010) as a special case,
the condition essentially recovers the well known cluster
recovery conditions for paradigm examples such as mixtures of Gaussians and planted partition models. The algorithms in A. Kumar (2010); P.Awasthi (2012) are based on

an SVD-based initialization followed by applying Lloyd’s
K–means algorithm, so K must be known in advance. Our
method does not need to know K and is independent of any
initialization.
A summary of our contributions are:
• We develop a new incremental proximal algorithm for
the SON optimization problem (1).
• We give a convergence analysis for our algorithm
that refines and strengthens the analysis in Bertsekas
(2011).
• We show that the SON formulation (1) provides strong
cluster recovery guarantees that is far more general
than previously known recovery results, essentially
similar to the recently discovered unifying center separation conditions.
• We give experimental results giving evidence that our
algorithm produces clusters of comparable quality to
previous methods but scales much better to large scale
problems.

2. Related Work
The SON formulation first appeared in (Lindsten et al.,
2011) and in closely related forms in Hocking et al. (Hocking et al., 2011). Lindsten et al (Lindsten et al., 2011) used
an off–the–shelf convex solver, CVX to generate solution
paths. Hocking et al. (Hocking et al., 2011) introduced
three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function
separates, and they solve the convex clustering problem
by the exact path following method designed for the fused
lasso. For the `1 and `2 norms, they employ subgradient
descent in conjunction with active sets. Neither provides
any theoretical results on cluster recovery. Chi et al (Chi
& Lange, 2015; Chen et al., 2015) introduce two similar
generic frameworks for minimizing the convex clustering
objective function with an arbitrary norm. One approach
solves the problem by the alternating direction method of
multipliers (ADMM), while the other solves it by the alternating minimization algorithm (AMA). The first (and only)
theoretical results on cluster recovery are in (Zhu et al.,
2010) but this is a very simple special case of exactly two
cube shaped clusters that are well separated. This work
also does not develop a specialized algorithm for the SON
formulation.

3. Cluster Recovery
To express our results, we first review few definitions:
Definition 1. Take a finite set X = {x1 , x2 , . . . , xn } ⊂
Rm and its partitioning V = {V1 , V2 , . . . , VK }, where each

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

Vk is a subset of X. We say that a map φ on X perfectly
recovers V when φ(xi ) = φ(xj ) is equivalent to xi , xj
belonging to the same cluster, or in other words, there exist
distinct vectors v1 , v2 , . . . , vK such that φ(xi ) = vα holds
whenever xi ∈ Vα .
Definition 2. For any set S ⊂ Rm , its diameter is defined
as
D(S) = sup{kx − yk2 | x, y ∈ S}.

where

P
cα =

nα
We prove the following results, which clearly imply Theorem 1:
1. Suppose that for every α ∈ [K],
max kxi − xj k

Moreover, for any finite set T ⊂ Rm we define its separation as
d(T ) = min{kx − yk2 | x, y ∈ S, x 6= y}

c(T ) =

|V |

.

α6=β

Finally, for any family of mutually disjoint finite sets T =
{Ti ⊂ Rm }, we define C(T ) = {c(Ti )}.
Definition 3. Take a finite set X = {x1 , x2 , . . . , xn } ⊂
Rm and its partitioning V = {V1 , V2 , . . . , VK }. We call a
partitioning W = {W1 , W2 , . . . , WL } of X a coarsening
of V if each partition Wl is obtained by taking the union of
a number of partitions Vk . Further, W is called the trivial
coarsening of V if W has exactly one element, i.e. W =
{X}. Otherwise, it is called a non-trivial coarsening.
Based on the above definitions, our result can be explained
as follows:
Theorem 1. Consider a finite set X = {xi ∈ Rm |
i = 1, 2, ..., n} of vectors and its partitioning V =
{V1 , V2 , . . . , VK }. Take the SON optimization in (1). Denote its optimal solution by {ūi } and define the map φ :
xi → φ(xi ) = ūi .
1. If
D(V )
d(C(V))
√ ,
≤λ≤
|V |
2n K
then the map φ perfectly recovers V.

α −ck
3. If max kcn−n
≥ λ where c =
α

α

V ∈V

xi /n, then at least

i=1

To prove the above, notice that the solution of the centroid
optimization satisfies
X
cα − vα = λ
nβ zα,β
β

where kzα,β k ≤ 1, zα,β = −zβ,α and whenever vα 6= vβ ,
v −v
the relation zα,β = kvαα−vββk2 holds. Now, for the solution
ui = vα for i ∈ Vα , define

zα,β α 6= β
0
,
zij
=
xi −xj
α=β
λnα
0
where i ∈ Vα , j ∈ Vβ . It is easy to see that kzij
k2 ≤ 1,
0
0
0
zij = −zji and whenever ui 6= uj , we have that zij
=
ui −uj
kui −uj k2 . Further for each i,

λ

X
j

0
zi,j
=λ

X
β

zα,β nβ +

X xi − xj
nα

j∈Vα

= cα − vα + xi − cα = xi − vα = xi − ui

2. If
max
V ∈V

This shows that the local optimality conditions for the SON
optimization holds and proves item a.

D(V )
kc(X) − c(V )k2
≤ λ ≤ max
,
V ∈V
|V |
|X| − |V |

then the map φ perfectly recovers a non-trivial coarsening of V.
Proof. We introduce associated centroid optimization:
K

{vα ∈R

n
P

two centroids vα are distinct.

max

minm

≤ λ.

nα

d
2. If all cα s are distinct and 2n√
≥ λ where d =
K
min kcα − cβ k, then all centroids vα are distinct.

x

x∈T

i,j∈Vα

Then, ui = vα for i ∈ Vα is a global solution of the
SON clustering.

and its Euclidean centroid as
P

xi

i∈Vα

X
1X
kvα − cα k22 nα + λ
nα nβ kcα − cα k2
} 2
i=1
α6=β

(2)

For item b, denote the solution of the centroid optimization
by vα (λ) and notice that the solution of SON consists of
distinct elements vα = cα and is continuous at λ = 0.
Hence, vα :s remain distinct in an interval λ ∈ [0, λ1 ).
Take λ0 as the supremum of all possible λ1 :s. Hence, the
solution in λ ∈ [0, λ0 ) contains distinct element and at
λ = λ0 contains two equal elements (otherwise, one can
extend [0, λ0 ) to some [0, λ0 +), which is against λ being
supremum). Now, notice that for λ ∈ [0 λ0 ) the objective

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

function is smooth at the optimal point. Hence, vα (λ) is
differentiable and satisfies


dvα
∂g
= H −1
δ=
(3)
dλ α
∂λ
where [. ]α and [. ]α,β denote block vectors and block matrices respectively. Moreover, H and g are the Hessian and
the gradient of the objective function at the optimal point. It
is possible,√by explicitly expanding H and g, to show that
kδk2 ≤ n K (see the supplementary material for more
detailed derivations).
Hence,


√
 dvα 


 dλ  ≤ kδk2 ≤ Kn
2
This yields for λ < λ0 to



 
Zλ 


dvβ
dvα
kvα (λ) − vβ (λ)k2 = 
−
dλ
cα − cβ +

dλ
dλ


0

2


Zλ 
 dvα
dvβ 
 dλ

−
≥ kcα − cβ k2 − 
dλ
dλ 2
0

√
≥ d − 2nλ K
Since at λ = λ0 , we
some α 6= β, we
√ have that vα = vβ for √
get that d − 2nλ0 K ≤ 0 or λ0 ≥ d/2n K. this proves
item b.
For item c, Take a value of λ, where v1 = v2 = . . . = vK .
It is simple to see that in this case vα = c. The optimality
condition leads to
X
c − cα = λ
zα,β nβ
β6=α

Hence, kc − cα k2 ≤ λ(n − nα ). This proves item c.
Remark 1. The study in Zhu et al. (2010) establishes some
results for the special case of two clusters in rectangular
boxes. In this special case, we observe that our result improves theirs.
Proof. Consider the notation in Zhu et al. (2010) with two
clusters V1 , V2 and notice that λ = α/2 (α denotes regularization parameter in Zhu et al. (2010)). Moreover,
D(Vi ) ≤ ksi k as ksi k is the diameter of the rectangle surrounding Vi . We observe that


ni −1
2n
+1
2
3−i
w1,2
D(Vi )
ni
≥ D(Vi )
≥
n
n3−i + ni
ni

w

for i = 1, 2, which shows that the condition λ ≥ n1,2 in
Zhu et al. (2010) is tighter than λ ≥ max D(V )/|V | in
ours. On the other hand,
kc(Vi ) − c(X)k
n − ni

=
=
=

kc(Vi ) −

c(V1 )n1 +c(V2 )n2
k
n1 +n2

n − ni
kc(V1 ) − c(V2 )k2
n
d(C(V))
n

(4)

Hence, the condition λ ≤ d(C(V))
in Zhu et al. (2010) is the
n
i )−c(X)k
.
same as our condition λ ≤ kc(Vn−n
i
Remark 2. The second result in Theorem 1 reflects a hierarchical structure in the SON clusters: Under weaker condition than the first part, SON may merge some clusters and
provide larger clusters than the true ones. In a recursive
way, SON clustering can be applied to each of these large
clusters to refine them, which improves the guarantees in
Theorem 1. We postpone careful study of this method to
future work.
3.1. Comparison with Center Separation Conditions
Recently, there have been a number of theoretical results of
the form that if we have data points generated by a mixture of K probability distributions, then one can cluster
the data points into the K clusters, one corresponding to
each component, provided the means of the different components are well–separated. There are different notions of
well-separated, but mainly, the (best known) results can be
qualitatively stated as: “If the means of every pair of densities are at least poly(K) standard deviations apart, then we
can learn the mixture in polynomial time.”. These results
generally make heavy use of the generative model and particular properties of the distributions (Indeed, many of them
specialize to Gaussians or independent Bernoulli trials).
Kumar and Kannan (A. Kumar, 2010) and Awasthi and
Sheffet (P.Awasthi, 2012) unified these into a general deterministic condition which can be roughly stated as follows: “If the means of every pair of clusters are at least
Ω(K) times standard deviations apart, then we can learn
the mixture in polynomial time.” Here the spectral norm of
the matrix A − C scaled by √1n plays the role of standard
deviation, where A is the data matrix and C is the matrix of
cluster centers. More formally, for any two distinct clusters
α, β,


1
1
kc(Vα ) − c(Vβ )k2 ≥ K √
+√
kA − Ck (5)
nα
nβ
Our condition is similar in spirit:
kc(Vα ) − c(Vβ )k2 ≥

√


K

n
d(Vα )
nα


(6)

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

If nα ≥ wn for all clusters α, then this becomes

3.1.3. R EGULAR AND D IRECTED C LUSTERS

√
kc(Vα ) − c(Vβ )k2 ≥

K
d(Vα ).
w

(7)

In the sequel, we specialize the above discussion in a number of examples and provide an explicit comparison of our
result with the center separation condition. In some cases,
our condition is slightly tighter than the center separation
guarantees, but we remind that the latter is obtained by
applying K-means and a SVD-based initialization, which
can be intractable in large problems, while our techniques
scales with the problem size more suitably.
3.1.1. M IXTURES OF G AUSSIANS
Suppose we have a mixture of K Gaussians in d dimensions with mixture weights w1 , · · · , wK , let w := mini wi
and let µ1 , · · · , µK denote their means respectively. If we
have n = Ω(poly(d/w)) points sampled from this mixture
distribution, then with high probability, the center separation condition is satisfied if:
cKσ
kµr − µs k ≥ √ polylog(d/w).
w
Here σ is the maximum variance in any direction of any of
the Gaussians. Our cluster recovery condition (7) is satisfied if:
cKσ
kµr − µs k ≥
polylog(n).
w
3.1.2. P LANTED PARTITION M ODEL
In the planted partition model of McSherry, a set of n points
is implicitly partitioned into K groups. There is an (unknown) K × K matrix of probabilities P . We are given a
graph G on these n points, where an edge between two
vertices from groups r and s is present with probability
Pr,s . We can consider these n points x1 , · · · , xn ∈ Rn
where coordinate j in xi is 1 if (i, j) ∈ G and 0 otherwise.
The center µr of cluster r has in coordinate j the value
Pr,ψ(j) , where ψ(j) is the cluster vertex j belongs to. Kumar and Kannan show that the center separation condition
holds with probability at least 1 − δ if:

Besides the stochastic models, we take a closer look at the
result in A. Kumar (2010) and identify deterministic cases
where the SON has better performance than the proved
bounds for K-means. These cases essentially guarantee that
the term kA−Ck in (5) remains large and the bound therein
becomes highly restrictive:
Definition 4. We say
{V1 , V2 , . . . , VK } of X
(δ, γ)−expanded if

where c is a large constant, w is such that every group has
size at least w · n and σ 2 := maxr,s Pr,s . Our center separation condition (7) is satisfied if:
kµr − µs k ≥ c

σ2 K √
n
w

a

partition V
=
{x1 , x2 , . . . , xn } is

|{x ∈ V | kx − c(V )k2 ≥ δ}| ≥ γ|V |.
We further say that this partition is (w, D, , γ)-regular if
for all V ∈ V we have D(V ) ≥ D, |V | ≥ wn and it is
(D, γ)−expanded.
Definition 5. We say that a set X = {x1 , x2 , . . . , xn } is
θ−directed if there exists a unit vector v ∈ Rm such that
X
x∈X\{c(X)}

|v T (x − c(X))|2
≥ θ|X|
kx − c(X)k22

For a (w, D, , γ)-regular
partition, the bound in (5) implies
√
2cKD γn
d(C(V)) ≥ √mwn . This is because

kA − Ck2 = σmax

n
X

Tr

!
δi δiT

≥

i=1
n
P

=

kδi k22

i=1

= γ2 D2

m

n
P

i=1

δi δiT

m



(8)

n
m

where δi = xi − c(Vα ) for i ∈ Vα . Notice
that our condi√
2nD K
tions can be implied by d(C(V)) ≥ wn . Hence,SON
can improve K-means if m ≤ wKc2 γ2 , which means that
the number of clusters K is large and the smallest fraction
of cluster size w is Ω(1).
If the (w, D, , γ)-regular partition is further θ−directed
we may improve the previous bounds as
!
n
X
X
T
σmax
δi δi ≥
|v T δi |2 ≥ γ2 D2 nθ
i=1

1
n
kµr − µs k ≥ cσ 2 K( + log )
w
δ

that
=

x∈X
√

γθn
√
Hence (5) implies d(C(V)) ≥ 2cKD
. This means that
wn
SON improves K-means if wK ≥ c2 12 γθ , i.e. the number
of clusters is higher than a fixed value.

4. Stochastic Splitting Algorithm
Our implementation is identical to the so-called proximalbased incremental technique in Bertsekas (2011), which is

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

performed in a way that it requires little amount of calculations (precisely O(m) and independent of other parameters) in each iteration. The proximal-based incremental
method is a variant of the stochastic gradient technique, in
problems where many terms in the objective function are
not differentiable, and the local gradient steps are replaced
by local proximal operators. To perform the proximalbased incremental method, we first write the SON objective
function as
X
Φ(u1 , u2 , . . . , un ) =
φij (ui , uj )
i<j

the probability sense (uniform convergence), while the result in Bertsekas (2011) is pointwise. Second, we prove
guaranteed speed of convergence with probability one. We
present these results by the following theorem. In this theorem, we consider fixed data dimension m and bounded data
vectors (i.e. kxk k ≤ C for some absolute constant C).
Theorem 2.
1. Assume that {µk } is non-increasing
∞
P
0

where
φij (ui , uj ) =

1
1
kxi −ui k22 + kxj −uj k22 +λkui −uj k.
2n
2n

Then, we introduce and explicitly calculate the proximal
(µ)
operator Πij of φij with step size µ as
(µ)

Πij (ui , uj ) =
arg min
φij (u0i , u0j ) +
0
0
ui ,uj

1
0
2µ kui

− ui k22 +

1
0
2µ kuj

= Tλµ (ui + µxi , uj + µxj ),

− uj k22
(9)

where we also introduce the pairwise soft-thresholding operator Tη (y, z) =

( 
y−z
z−y
,
z
+
η
ky − zk ≥ 2η
y + η kz−yk
ky−zk2
,
 2
y+z y+z
,
ky − zk < 2η
2
2
(10)
and the final equality is obtained by the local optimality
conditions and straightforward calculations. Our algorithm
simply consists in iteratively applying randomly selected
proximal operators. This is depicted in Algorithm 1.
Algorithm 1 Stochastic Splitting Algorithm
Input: The data vectors {xk }nk=1 and step sizes
{µk }∞
k=1
Initialization: Set u1 , u2 , . . . , un arbitrarily (we use
u1 = u2 = . . . = un = 0)
for k = 1, 2, . . . do
Select a pair (i, j) with i < j uniformly randomly.
(µ )
Update (ui , uj ) ← Πij k (ui , uj )
end for

∞
P

µk = ∞ and

0

µ2k < ∞. Then, the sequence Uk converges to Ũ

in the following strong probability sense:


∀ > 0; lim Pr sup kUl − Ũk2F >  = 0 (11)
k→∞

l≥k

2. Take µk = kµα1 for k = 1, 2, . . . and 23 < α < 1. For
sufficiently small values of  > 0 the relation


n4
kUl − Ũk2F = O 3α−2−
l
holds for every l, n with probability 1.
Proof. We skip many steps in our proof for lack of space.
These steps can be found in the supplement. Denote by Uk
a matrix where the ith column is the value of ui at the k th
iteration. Define
ψµ (U) = E (Uk+1 | Uk = U, µk = µ) ,

(12)

Starting from Ū0 = U0 (the initialization of the algorithm), we define the characteristic sequence {Ūk }∞
k=0 by
the following iteration:
Ūk+1 = ψµk (Ūk )
Our proof is based on the following two results, which we
prove in the supplementary material:
i We have that

Pr sup kUk − Ūk k2F +
k

∞
X

∞
P

!
µ2l > λ

≤

l=k

k=0

µ2k

λ
(13)

4.1. Convergence Analysis
Convergence of proximal-based incremental method is
discussed in Bertsekas (2011). We further elaborate
on the convergence by further exploitation of the nonexpansiveness property of proximal operators. This allows
us to complement the result in Bertsekas (2011) in the following two directions: First, we establish convergence in

ii Define Ũ as the unique optimal solution of the SON
optimization and suppose that {µk } is a non-increasing
sequence. There exists a universal constant a such that
kŪk − Ũk2F is upper bounded by
a

k−1
X
l=0

µ2l e

− n22

k−1
P
s=l+1

µs

+ kU0 −

Ũk2F e

− n22

k−1
P
s=0

µs

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

To prove Theorem 2, define U k = {Ūkl }∞
l=0 as the sequence obtained by starting from Ūk0 = Uk and applying

120

Exact
Stochastic Splitting

100

Ūkl+1 = ψµl+k (Ūkl )
Take arbitrary (non-zero) positive numbers , δ. Take λ
∞
P
such that λ ≥ 2δ
µ2l . Take some values l0 , k which we

t (s)

80
60
40

l=0

specialize later. Now, we define two outcomes H1 and H2 :

20

H1 : ∀k ≥ 0; kUk − Ũk2F ≤ λ

0

H2 : ∀l ≥

0; kŪkl

n


− Ul+k k ≤
4

From item (i), it is simple to see that Pr(H1c ) and Pr(H2c )
are less than δ/2. Furthermore we can show by (ii) that
under H1 ∩ H2 and suitable l0 , k:
∀l > l0 ; kUl+k − Ũk22 ≤ 2(kUl+k − Ūkl k2F +kŪkl − Ũk2F )


≤ 2( + ) = 
4 4
This is detailed in the supplement. We conclude that

which proves part (1) of Theorem.

Qr : sup kUl+kr −
l≥0

where we introduce µ1 = bn2 and A = 4an4 b2 for simplicity. This leads to
1−α

l>l0 +k

For part (2), define kr = rγ , λr = r−β , where γ =
β < γ(2α − 1) − 1, and the outcomes:

Figure 2. Running times of the exact SON clustering algorithm
(implemented in CVX) and stochastic splitting for samples from
a mixture of two Gaussians with increasing sample size.

kUkr+1 − Ũk2F ≤ LeLkr

Pr( sup kUl − Ũk22 > ) ≤ Pr(H1c ) + Pr(H2c ) ≤ δ

Ūkl r k2F

100 200 300 400 500 600

1− 2
1−α ,

2λr + A

lr
X
t=0

1−α
−Lkr+1

kUkr − Ũk2F +

1−α
1−α
1
eL(kr +t) −Lkr+1
(t + kr )2α

where L is a suitable constant with different values at different occurrences. Postponing few more steps to the supplementary material, we obtain that
kUkr − Ũk2F ≤

> λr .

L log r
L
≤ β−

β−
2
r
r

Take kr < l ≤ kr+1 . We obtain that
∞
P

Pr(Qr ) < ∞. Hence by
r=1
c
c
Qr0 , Qr0 +1 , Qcr0 +2 , . . . simultane-

By item (i), we have that

Borel-Cantelli lemma,
ously hold for some r0 with probability 1. For simplicity
and without loss of generality, we assume that r0 = 0 as it
does not affect the asymptotic rate. Then for any r > 0, we
have that
sup kUl+kr − Ūkl r k2F ≤ λr

kUl − Ũk22 ≤ 2(kUkr − Ũk22 + kUkr − Ul k22 )
L
L
L
≤ β− ≤ β−
rβ−
r
l γ
By taking β = γ(2α − 1) − 1, we obtain part (2).
≤ 2λr +

l≥0

5. Experiments

In particular,
kUkr+1 − Ūklrr k2F ≤ λr
where lr = kr+1 − kr . From item (ii), we also conclude
that
kŪklrr

−

Ũk2F

≤A

lX
r −1
t=0

+kUkr −

lrP
−1

1

−2a
1
(s+kr )α
s=t+1
e
2α
(t + kr )

Ũk2F e

−2a

lrP
−1
s=0

1
(s+kr )α

We evaluate the proposed stochastic splitting algorithm in
the task of clustering points generated by Gaussians mixture models. We compare the results to the exact algorithm
proposed by Lindsten et al. (2011) in terms of a) the quality of the produced clustering and b) the time spent solving
the optimization problem. The results of both algorithms
are dense embeddings of the points that are then thresholded to form clusters. The clusters are the largest subsets
of nodes such that the maximum pairwise distance within
the subset is less than τ . The stochastic splitting algorithm
is implemented as in Algorithm 1. We observed in practice

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

0.8
0.6
0.4
0.2
0

1.2

Exact

1

Stochastic Splitting

Adjusted Rand

Stochastic Splitting

0.8
0.6
0.4
0.2
0

0

2

4

6

6

(a) σ 2 = 0.01

8

10
#10-3

1.2

Exact

1

Stochastic Splitting

Adjusted Rand

1.2

Exact

1

Adjusted Rand

Adjusted Rand

1.2

0.8
0.6
0.4
0.2
0

0

2

4

6

6

8

10
#10-3

(b) σ 2 = 0.02

Exact

1

Stochastic Splitting

0.8
0.6
0.4
0.2
0

0

2

4

6

6

8

10
#10-3

(c) σ 2 = 0.05

0

2

4

6

6

8

10
#10-3

(d) σ 2 = 0.1

Figure 1. Adjusted Rand index for different choices of λ. Each plot represents quality of the clustering produced by solving the SON
objective exactly or with stochastic
√ splitting. The different plots represent clustering of 200 samples from a mixture of two Gaussians in
R2 with fixed separation d = 2 and variance σ 2 .

that a heuristic for adaptively setting the step-size improved
robustness and rate of convergence. Specifically, the step
size was reduced by a constant factor whenever the average
change in the objective over successive rounds in a small
window was positive. If the same average was negative,
but small in absolute value, the step size was increased by
a small constant factor.
The data is generated from Gaussian mixture models with
two components
in R2 where the means are separated by
√
d = 2 and the variance σ 2 is varied. The number of samples is also varied, to illustrate the computational gains of
the stochastic splitting method. As pointed out by Lindsten
et al. (2011), the choice of the regularization parameter λ
is perhaps the most challenging hurdle in applying SON
clustering. Choosing λ too high might result in a single
large cluster, and choosing it too low may cause each point
to be represented by its own cluster. While this problem
is of great importance in applications, we focus on the relative performance of the Lindsten et al. (2011) algorithm
(CVX) and stochastic splitting (SS). We report the adjusted
Rand index (Rand, 1971) as measure of cluster quality, and
would like to emphasize that this does not rely on identifying the number of clusters beforehand.
Results The results of the experiments are presented in
Figures 1 and 2. We see in Figure 1 that the quality of the
clustering produced by the stochastic splitting algorithm is
comparable to that of the exact algorithm. This pattern is
consistant across choices of σ, where a high σ implies low
sample separation between the clusters. We also note that
the range of λ for which the stochastic splitting algorithm
achieves as good result as the exact algorithm is less wide
than for the exact. We believe this is due to the stochastic
nature of the algorithm which makes the resulting embedding clusters less separated than in the exact version. Deviations from the optimal embedding could be magnified
by the thresholding step, effectively making the stochastic
algorithm more sensitive to the choice of threshold, and in
effect the quality more sensitive to the choice of λ. In these

experiments, the same threshold was used for both algorithms, but tailored choices could be considered given an
appropriate selection criterion.
Furthermore, we see in Figure 2 that the running time of
the stochastic splitting algorithm is lower than that of the
exact algorithm, and grows significantly slower. While the
stochastic splitting algorithm could in principle be implemented in time constant in the number of samples, and instead determined by the number of iterations, the adaptive
stepsize used to improve performance requires evaluation
of the objective value which scales with the number of samples. This could be improved by subsampling the terms in
the objective function, but this was not done here.

6. Conclusions
We developed a stochastic incremental algorithm based on
proximal iterations for the SON convex relaxtion of clustering that is highly suited to large scale problems and gave
an analysis of its convergence propertis. We also gave quite
general theoretical guaranteees for exact recovery of clusters similar to the unifying proximity condition in approximation algorithms that covers paradigm models for clustering data.
It has not escaped our attention that our algorithm can easily be adapted to incorporate similarity weights as used in
Chi & Lange (2015); Chen et al. (2015); Hocking et al.
(2011) and that it is amenable to acceleration using variance reduction and other techniques. The cluster recovery
conditions can also be extended to cover almost perfect recovery i.e. correctly clustering all except a small fraction
of points. A more complete experimental evaluation of our
algorithm and comparison to others will be included in a
longer version of the paper.

Acknowledgements
This work is supported in part by the Swedish Foundation
for Strategic Research (SSF).

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

References
A. Kumar, R. Kannan. Clustering with spectral norm and the kmeans algorithm. In FOCS, 2010.
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Optimization with sparsity-inducing penalties. Foundation and Trends
in Machine Learning, 1(4):1–106, 2012.
Bertsekas, D. Incremental proximal methods for large scale convex optimization. Math. Program., 129(163), 2011.
Bottou, Léon, Curtis, Frank E., and Nocedal, Jorge. Optimization methods for large-scale machine learning. Technical report, arXiv:1606.04838, June 2016. URL http://leon.
bottou.org/papers/tr-optml-2016.
Chen, Gary K., Chi, Eric C., Ranola, John M.O., and Lange,
Kenneth. Convex clustering: An attractive alternative to
hierarchical clustering. PLoS Computational Biology, 11(5):
e1004228, 2015. doi: 10.1371/journal.pcbi.1004228. URL
http://journals.plos.org/ploscompbiol/
article?id=10.1371/journal.pcbi.1004228.
Chi, Eric C. and Lange, Kenneth. Splitting methods for convex clustering. Journal of Computational and Graphical
Statistics, 24(4):994–1013, 2015. doi: 10.1080/10618600.
2014.948181. URL http://dx.doi.org/10.1080/
10618600.2014.948181.
Defazio, A., F. Bach, F., and Lacoste-Julien, S. A fast incremental
gradient method with support for non-strongly convex composite objectives. In NIPS, pp. 1646–1654, 2014.
Hocking, T., Vert, J-P., Bach, F., and Joulin., A. Clusterpath:
an algorithm for clustering using convex fusion penalties. In
ICML, 2011.
Jain, A. K., Murty, M. N., and Flynn, P. J. Data clustering: A
review. ACM Comput. Surv., 31(3):264–323, September 1999.
ISSN 0360-0300. doi: 10.1145/331499.331504. URL http:
//doi.acm.org/10.1145/331499.331504.
Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS, pp. 315–
323, 2013.
Lindsten, F., Ohlsson, H., and Ljung, L. Clustering using sum-ofnorms regularization: With application to particle filter output
computation. 2011.
P.Awasthi, O Sheffet. Improved spectral norm bounds for clustering. In APPROX-RANDOM, 2012.
Rand, William M. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association,
66(336):846–850, 1971.
Schmidt, M., Roux, N. Le, and Bach, F. Minimizing finite sums
with the stochastic average gradient. Mathematical Programming, 2016.
Zhu, C., Xu, H., Leng, C., and Yan, S. Convex optimization procedure for clustering: Theoretical revisit. In NIPS, 2010.

