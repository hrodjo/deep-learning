Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence
and Cluster Recovery

Ashkan Panahi 1 Devdatt Dubhashi 2 Fredrik D. Johansson 3 Chiranjib Bhattacharyya 4

Abstract
Standard clustering methods such as K‚Äìmeans,
Gaussian mixture models, and hierarchical clustering, are beset by local minima, which are
sometimes drastically suboptimal. Moreover the
number of clusters K must be known in advance.
The recently introduced sum‚Äìof‚Äìnorms (SON)
or Clusterpath convex relaxation of k-means and
hierarchical clustering shrinks cluster centroids
toward one another and ensure a unique global
minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to
solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which
have a similar form to the unifying proximity
condition introduced in the approximation algorithms community (that covers paradigm cases
such as Gaussian mixtures and planted partition
models). We give experimental results to confirm
that our algorithm scales much better than previous methods while producing clusters of comparable quality.

1. Introduction
Clustering is perhaps the most fundamental problem in unsupervised learning. Many clustering algorithms have been
proposed in the literature (Jain et al., 1999), including Kmeans, spectral clustering, Gaussian mixture models and
hierarchical clustering, to solve problems with respect to
a wide range of cluster shapes. However, much research
has pointed out that these methods all suffer from instabilities. For example, the formulation of K-means is NP-hard
and the typical way to solve it is the Lloyds method, which
1
ECE, North Carolina State University, Raleigh, NC 2 CSE,
Chalmers University of Technology, GoÃàteborg, Sweden 3 IMES,
MIT, Cambridge, MA 4 CSA, IISc, Bangalore, India. Correspondence to: Ashkan Panahi <panahi1986@gmail.com>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

requires randomly initializing the clusters. However, one
needs to know the number of clusters in advance and different initializations may lead to significantly different final
cluster results.
Lindsten et al. (2011) and Hocking et al. (2011) proposed
the following convex optimization procedure for clustering,
called SON (‚ÄúSum of norms‚Äù clustering) by the former and
Clusterpath by the latter;
minm

{ui ‚ààR

n
X
1X
kxi ‚àí ui k22 + Œª
kui ‚àí uj k2
} 2
i=1
i<j

(1)

The main idea of the formulation is that if input data points
xi and xj belong to the same cluster, then their corresponding centroids ui and uj should be forced to be the same.
Intuitively, this is due to the fact that the second term is a
regularization term that enforces zeroes in the vector consisting of entries kui ‚àí uj k and can be seen as a generalization of the fused Lasso penalty. From another point of
view, the regularization term can be seen as an `1,2 norm,
i.e., the sum of `2 norms. Such a group norm is known to
encourage block sparse solutions (Bach et al., 2012). Thus
for many pairs (i, j), we expect to enforce ui = uj .
Lindsten et al. (2011) used an off‚Äìthe‚Äìshelf convex solver,
CVX to generate solution paths. Hocking et al. (2011) introduced three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function separates, and they solve the convex clustering problem by the exact path following method designed
for the fused lasso. For the `1 and `2 norms, they employ
subgradient descent in conjunction with active sets. Recently, Chi & Lange (2015); Chen et al. (2015) introduce
two similar generic frameworks for minimizing the convex
clustering objective function with an arbitrary norm. One
approach solves the problem by the alternating direction
method of multipliers (ADMM), while the other solves it
by the alternating minimization algorithm (AMA). However both algorithms have issues with scalablity.
Moreover, none of these papers provide any theoretical
guarantees about the cluster recovery property of the algorithm. The first theoretical result on cluster recovery was
shown by Zhu et al. (2010): if the samples are drawn from

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

two cubes, each being one cluster, then SON can provably
recover the clusters provided that the distance between the
two cubes is larger than a threshold which depends (linearly) on the size of the cube and the ratio of numbers of
samples in each cluster. Unfortunately, the conditions for
recovery represent an extremely narrow special case: only
two clusters which both have to be cubes. Moreover in
their paper, there is no algorithm or analysis of the speed of
convergence. No other theoretical guarantees for SON are
known previously.
Here we develop a new algorithm in the spirit of recent
advances in stochastic methods for large scale optimization
(Bottou et al., 2016) to solve the optimization problem (1).
We give a convergence analysis and provide quite general
cluster recovery guarantees.
There has been a flurry of advances (Johnson & Zhang,
2013; Defazio et al., 2014; Schmidt et al., 2016) in developing algorithms for solving optimization problems for the
case when the objective consists of the sum of two convex functions: one is the average of a large number of
smooth component functions, and the other is a general
convex function that admits a proximal mapping (and the
whole objective function is strongly convex). The optimization (1) is of this form but here we exploit the structure
of (1) further by observing that the second function can also
be split into component functions. This results in an incremental algorithm with proximal iterations consisting of
very simple and natural steps. Our algorithm can be seen
as a special case of the methods of Bertsekas (2011). We
compute the proximal operator in closed form to yield very
simple and cheap iterations. Using the fact that the proximal operator is non-expansive, we refine and strengthen
Bertsekas‚Äô convergence results. The stochastic incremental
nature of our algorithm makes it highly suited to large scale
problems (Bottou et al., 2016) in contrast to the methods in
Chi & Lange (2015); Chen et al. (2015).
We show that the SON formualation (1) provides strong
cluster recovery properties that go far beyond the special
case considered in Zhu et al. (2010). Our cluster recovery
conditions are similar in spirit to the unifying general conditions recently formulated in A. Kumar (2010); P.Awasthi
(2012) of the form that the means of the clusters are well‚Äì
separated, i.e., the distance between the means of any two
clusters is at least ‚Ñ¶(k) standard deviations (the notion of
standard deviations is based on the spectral norm of the matrix whose rows represent the difference between a point
and the mean of the cluster to which it belongs). Besides
containing the result of Zhu et al. (2010) as a special case,
the condition essentially recovers the well known cluster
recovery conditions for paradigm examples such as mixtures of Gaussians and planted partition models. The algorithms in A. Kumar (2010); P.Awasthi (2012) are based on

an SVD-based initialization followed by applying Lloyd‚Äôs
K‚Äìmeans algorithm, so K must be known in advance. Our
method does not need to know K and is independent of any
initialization.
A summary of our contributions are:
‚Ä¢ We develop a new incremental proximal algorithm for
the SON optimization problem (1).
‚Ä¢ We give a convergence analysis for our algorithm
that refines and strengthens the analysis in Bertsekas
(2011).
‚Ä¢ We show that the SON formulation (1) provides strong
cluster recovery guarantees that is far more general
than previously known recovery results, essentially
similar to the recently discovered unifying center separation conditions.
‚Ä¢ We give experimental results giving evidence that our
algorithm produces clusters of comparable quality to
previous methods but scales much better to large scale
problems.

2. Related Work
The SON formulation first appeared in (Lindsten et al.,
2011) and in closely related forms in Hocking et al. (Hocking et al., 2011). Lindsten et al (Lindsten et al., 2011) used
an off‚Äìthe‚Äìshelf convex solver, CVX to generate solution
paths. Hocking et al. (Hocking et al., 2011) introduced
three distinct algorithms for the three most commonly encountered norms. For the `1 norm, the objective function
separates, and they solve the convex clustering problem
by the exact path following method designed for the fused
lasso. For the `1 and `2 norms, they employ subgradient
descent in conjunction with active sets. Neither provides
any theoretical results on cluster recovery. Chi et al (Chi
& Lange, 2015; Chen et al., 2015) introduce two similar
generic frameworks for minimizing the convex clustering
objective function with an arbitrary norm. One approach
solves the problem by the alternating direction method of
multipliers (ADMM), while the other solves it by the alternating minimization algorithm (AMA). The first (and only)
theoretical results on cluster recovery are in (Zhu et al.,
2010) but this is a very simple special case of exactly two
cube shaped clusters that are well separated. This work
also does not develop a specialized algorithm for the SON
formulation.

3. Cluster Recovery
To express our results, we first review few definitions:
Definition 1. Take a finite set X = {x1 , x2 , . . . , xn } ‚äÇ
Rm and its partitioning V = {V1 , V2 , . . . , VK }, where each

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

Vk is a subset of X. We say that a map œÜ on X perfectly
recovers V when œÜ(xi ) = œÜ(xj ) is equivalent to xi , xj
belonging to the same cluster, or in other words, there exist
distinct vectors v1 , v2 , . . . , vK such that œÜ(xi ) = vŒ± holds
whenever xi ‚àà VŒ± .
Definition 2. For any set S ‚äÇ Rm , its diameter is defined
as
D(S) = sup{kx ‚àí yk2 | x, y ‚àà S}.

where

P
cŒ± =

nŒ±
We prove the following results, which clearly imply Theorem 1:
1. Suppose that for every Œ± ‚àà [K],
max kxi ‚àí xj k

Moreover, for any finite set T ‚äÇ Rm we define its separation as
d(T ) = min{kx ‚àí yk2 | x, y ‚àà S, x 6= y}

c(T ) =

|V |

.

Œ±6=Œ≤

Finally, for any family of mutually disjoint finite sets T =
{Ti ‚äÇ Rm }, we define C(T ) = {c(Ti )}.
Definition 3. Take a finite set X = {x1 , x2 , . . . , xn } ‚äÇ
Rm and its partitioning V = {V1 , V2 , . . . , VK }. We call a
partitioning W = {W1 , W2 , . . . , WL } of X a coarsening
of V if each partition Wl is obtained by taking the union of
a number of partitions Vk . Further, W is called the trivial
coarsening of V if W has exactly one element, i.e. W =
{X}. Otherwise, it is called a non-trivial coarsening.
Based on the above definitions, our result can be explained
as follows:
Theorem 1. Consider a finite set X = {xi ‚àà Rm |
i = 1, 2, ..., n} of vectors and its partitioning V =
{V1 , V2 , . . . , VK }. Take the SON optimization in (1). Denote its optimal solution by {uÃÑi } and define the map œÜ :
xi ‚Üí œÜ(xi ) = uÃÑi .
1. If
D(V )
d(C(V))
‚àö ,
‚â§Œª‚â§
|V |
2n K
then the map œÜ perfectly recovers V.

Œ± ‚àíck
3. If max kcn‚àín
‚â• Œª where c =
Œ±

Œ±

V ‚ààV

xi /n, then at least

i=1

To prove the above, notice that the solution of the centroid
optimization satisfies
X
cŒ± ‚àí vŒ± = Œª
nŒ≤ zŒ±,Œ≤
Œ≤

where kzŒ±,Œ≤ k ‚â§ 1, zŒ±,Œ≤ = ‚àízŒ≤,Œ± and whenever vŒ± 6= vŒ≤ ,
v ‚àív
the relation zŒ±,Œ≤ = kvŒ±Œ±‚àívŒ≤Œ≤k2 holds. Now, for the solution
ui = vŒ± for i ‚àà VŒ± , define

zŒ±,Œ≤ Œ± 6= Œ≤
0
,
zij
=
xi ‚àíxj
Œ±=Œ≤
ŒªnŒ±
0
where i ‚àà VŒ± , j ‚àà VŒ≤ . It is easy to see that kzij
k2 ‚â§ 1,
0
0
0
zij = ‚àízji and whenever ui 6= uj , we have that zij
=
ui ‚àíuj
kui ‚àíuj k2 . Further for each i,

Œª

X
j

0
zi,j
=Œª

X
Œ≤

zŒ±,Œ≤ nŒ≤ +

X xi ‚àí xj
nŒ±

j‚ààVŒ±

= cŒ± ‚àí vŒ± + xi ‚àí cŒ± = xi ‚àí vŒ± = xi ‚àí ui

2. If
max
V ‚ààV

This shows that the local optimality conditions for the SON
optimization holds and proves item a.

D(V )
kc(X) ‚àí c(V )k2
‚â§ Œª ‚â§ max
,
V ‚ààV
|V |
|X| ‚àí |V |

then the map œÜ perfectly recovers a non-trivial coarsening of V.
Proof. We introduce associated centroid optimization:
K

{vŒ± ‚ààR

n
P

two centroids vŒ± are distinct.

max

minm

‚â§ Œª.

nŒ±

d
2. If all cŒ± s are distinct and 2n‚àö
‚â• Œª where d =
K
min kcŒ± ‚àí cŒ≤ k, then all centroids vŒ± are distinct.

x

x‚ààT

i,j‚ààVŒ±

Then, ui = vŒ± for i ‚àà VŒ± is a global solution of the
SON clustering.

and its Euclidean centroid as
P

xi

i‚ààVŒ±

X
1X
kvŒ± ‚àí cŒ± k22 nŒ± + Œª
nŒ± nŒ≤ kcŒ± ‚àí cŒ± k2
} 2
i=1
Œ±6=Œ≤

(2)

For item b, denote the solution of the centroid optimization
by vŒ± (Œª) and notice that the solution of SON consists of
distinct elements vŒ± = cŒ± and is continuous at Œª = 0.
Hence, vŒ± :s remain distinct in an interval Œª ‚àà [0, Œª1 ).
Take Œª0 as the supremum of all possible Œª1 :s. Hence, the
solution in Œª ‚àà [0, Œª0 ) contains distinct element and at
Œª = Œª0 contains two equal elements (otherwise, one can
extend [0, Œª0 ) to some [0, Œª0 +), which is against Œª being
supremum). Now, notice that for Œª ‚àà [0 Œª0 ) the objective

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

function is smooth at the optimal point. Hence, vŒ± (Œª) is
differentiable and satisfies


dvŒ±
‚àÇg
= H ‚àí1
Œ¥=
(3)
dŒª Œ±
‚àÇŒª
where [. ]Œ± and [. ]Œ±,Œ≤ denote block vectors and block matrices respectively. Moreover, H and g are the Hessian and
the gradient of the objective function at the optimal point. It
is possible,‚àöby explicitly expanding H and g, to show that
kŒ¥k2 ‚â§ n K (see the supplementary material for more
detailed derivations).
Hence,


‚àö
 dvŒ± 


 dŒª  ‚â§ kŒ¥k2 ‚â§ Kn
2
This yields for Œª < Œª0 to



 
ZŒª 


dvŒ≤
dvŒ±
kvŒ± (Œª) ‚àí vŒ≤ (Œª)k2 = 
‚àí
dŒª
cŒ± ‚àí cŒ≤ +

dŒª
dŒª


0

2


ZŒª 
 dvŒ±
dvŒ≤ 
 dŒª

‚àí
‚â• kcŒ± ‚àí cŒ≤ k2 ‚àí 
dŒª
dŒª 2
0

‚àö
‚â• d ‚àí 2nŒª K
Since at Œª = Œª0 , we
some Œ± 6= Œ≤, we
‚àö have that vŒ± = vŒ≤ for ‚àö
get that d ‚àí 2nŒª0 K ‚â§ 0 or Œª0 ‚â• d/2n K. this proves
item b.
For item c, Take a value of Œª, where v1 = v2 = . . . = vK .
It is simple to see that in this case vŒ± = c. The optimality
condition leads to
X
c ‚àí cŒ± = Œª
zŒ±,Œ≤ nŒ≤
Œ≤6=Œ±

Hence, kc ‚àí cŒ± k2 ‚â§ Œª(n ‚àí nŒ± ). This proves item c.
Remark 1. The study in Zhu et al. (2010) establishes some
results for the special case of two clusters in rectangular
boxes. In this special case, we observe that our result improves theirs.
Proof. Consider the notation in Zhu et al. (2010) with two
clusters V1 , V2 and notice that Œª = Œ±/2 (Œ± denotes regularization parameter in Zhu et al. (2010)). Moreover,
D(Vi ) ‚â§ ksi k as ksi k is the diameter of the rectangle surrounding Vi . We observe that


ni ‚àí1
2n
+1
2
3‚àíi
w1,2
D(Vi )
ni
‚â• D(Vi )
‚â•
n
n3‚àíi + ni
ni

w

for i = 1, 2, which shows that the condition Œª ‚â• n1,2 in
Zhu et al. (2010) is tighter than Œª ‚â• max D(V )/|V | in
ours. On the other hand,
kc(Vi ) ‚àí c(X)k
n ‚àí ni

=
=
=

kc(Vi ) ‚àí

c(V1 )n1 +c(V2 )n2
k
n1 +n2

n ‚àí ni
kc(V1 ) ‚àí c(V2 )k2
n
d(C(V))
n

(4)

Hence, the condition Œª ‚â§ d(C(V))
in Zhu et al. (2010) is the
n
i )‚àíc(X)k
.
same as our condition Œª ‚â§ kc(Vn‚àín
i
Remark 2. The second result in Theorem 1 reflects a hierarchical structure in the SON clusters: Under weaker condition than the first part, SON may merge some clusters and
provide larger clusters than the true ones. In a recursive
way, SON clustering can be applied to each of these large
clusters to refine them, which improves the guarantees in
Theorem 1. We postpone careful study of this method to
future work.
3.1. Comparison with Center Separation Conditions
Recently, there have been a number of theoretical results of
the form that if we have data points generated by a mixture of K probability distributions, then one can cluster
the data points into the K clusters, one corresponding to
each component, provided the means of the different components are well‚Äìseparated. There are different notions of
well-separated, but mainly, the (best known) results can be
qualitatively stated as: ‚ÄúIf the means of every pair of densities are at least poly(K) standard deviations apart, then we
can learn the mixture in polynomial time.‚Äù. These results
generally make heavy use of the generative model and particular properties of the distributions (Indeed, many of them
specialize to Gaussians or independent Bernoulli trials).
Kumar and Kannan (A. Kumar, 2010) and Awasthi and
Sheffet (P.Awasthi, 2012) unified these into a general deterministic condition which can be roughly stated as follows: ‚ÄúIf the means of every pair of clusters are at least
‚Ñ¶(K) times standard deviations apart, then we can learn
the mixture in polynomial time.‚Äù Here the spectral norm of
the matrix A ‚àí C scaled by ‚àö1n plays the role of standard
deviation, where A is the data matrix and C is the matrix of
cluster centers. More formally, for any two distinct clusters
Œ±, Œ≤,


1
1
kc(VŒ± ) ‚àí c(VŒ≤ )k2 ‚â• K ‚àö
+‚àö
kA ‚àí Ck (5)
nŒ±
nŒ≤
Our condition is similar in spirit:
kc(VŒ± ) ‚àí c(VŒ≤ )k2 ‚â•

‚àö


K

n
d(VŒ± )
nŒ±


(6)

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

If nŒ± ‚â• wn for all clusters Œ±, then this becomes

3.1.3. R EGULAR AND D IRECTED C LUSTERS

‚àö
kc(VŒ± ) ‚àí c(VŒ≤ )k2 ‚â•

K
d(VŒ± ).
w

(7)

In the sequel, we specialize the above discussion in a number of examples and provide an explicit comparison of our
result with the center separation condition. In some cases,
our condition is slightly tighter than the center separation
guarantees, but we remind that the latter is obtained by
applying K-means and a SVD-based initialization, which
can be intractable in large problems, while our techniques
scales with the problem size more suitably.
3.1.1. M IXTURES OF G AUSSIANS
Suppose we have a mixture of K Gaussians in d dimensions with mixture weights w1 , ¬∑ ¬∑ ¬∑ , wK , let w := mini wi
and let ¬µ1 , ¬∑ ¬∑ ¬∑ , ¬µK denote their means respectively. If we
have n = ‚Ñ¶(poly(d/w)) points sampled from this mixture
distribution, then with high probability, the center separation condition is satisfied if:
cKœÉ
k¬µr ‚àí ¬µs k ‚â• ‚àö polylog(d/w).
w
Here œÉ is the maximum variance in any direction of any of
the Gaussians. Our cluster recovery condition (7) is satisfied if:
cKœÉ
k¬µr ‚àí ¬µs k ‚â•
polylog(n).
w
3.1.2. P LANTED PARTITION M ODEL
In the planted partition model of McSherry, a set of n points
is implicitly partitioned into K groups. There is an (unknown) K √ó K matrix of probabilities P . We are given a
graph G on these n points, where an edge between two
vertices from groups r and s is present with probability
Pr,s . We can consider these n points x1 , ¬∑ ¬∑ ¬∑ , xn ‚àà Rn
where coordinate j in xi is 1 if (i, j) ‚àà G and 0 otherwise.
The center ¬µr of cluster r has in coordinate j the value
Pr,œà(j) , where œà(j) is the cluster vertex j belongs to. Kumar and Kannan show that the center separation condition
holds with probability at least 1 ‚àí Œ¥ if:

Besides the stochastic models, we take a closer look at the
result in A. Kumar (2010) and identify deterministic cases
where the SON has better performance than the proved
bounds for K-means. These cases essentially guarantee that
the term kA‚àíCk in (5) remains large and the bound therein
becomes highly restrictive:
Definition 4. We say
{V1 , V2 , . . . , VK } of X
(Œ¥, Œ≥)‚àíexpanded if

where c is a large constant, w is such that every group has
size at least w ¬∑ n and œÉ 2 := maxr,s Pr,s . Our center separation condition (7) is satisfied if:
k¬µr ‚àí ¬µs k ‚â• c

œÉ2 K ‚àö
n
w

a

partition V
=
{x1 , x2 , . . . , xn } is

|{x ‚àà V | kx ‚àí c(V )k2 ‚â• Œ¥}| ‚â• Œ≥|V |.
We further say that this partition is (w, D, , Œ≥)-regular if
for all V ‚àà V we have D(V ) ‚â• D, |V | ‚â• wn and it is
(D, Œ≥)‚àíexpanded.
Definition 5. We say that a set X = {x1 , x2 , . . . , xn } is
Œ∏‚àídirected if there exists a unit vector v ‚àà Rm such that
X
x‚ààX\{c(X)}

|v T (x ‚àí c(X))|2
‚â• Œ∏|X|
kx ‚àí c(X)k22

For a (w, D, , Œ≥)-regular
partition, the bound in (5) implies
‚àö
2cKD Œ≥n
d(C(V)) ‚â• ‚àömwn . This is because

kA ‚àí Ck2 = œÉmax

n
X

Tr

!
Œ¥i Œ¥iT

‚â•

i=1
n
P

=

kŒ¥i k22

i=1

= Œ≥2 D2

m

n
P

i=1

Œ¥i Œ¥iT

m



(8)

n
m

where Œ¥i = xi ‚àí c(VŒ± ) for i ‚àà VŒ± . Notice
that our condi‚àö
2nD K
tions can be implied by d(C(V)) ‚â• wn . Hence,SON
can improve K-means if m ‚â§ wKc2 Œ≥2 , which means that
the number of clusters K is large and the smallest fraction
of cluster size w is ‚Ñ¶(1).
If the (w, D, , Œ≥)-regular partition is further Œ∏‚àídirected
we may improve the previous bounds as
!
n
X
X
T
œÉmax
Œ¥i Œ¥i ‚â•
|v T Œ¥i |2 ‚â• Œ≥2 D2 nŒ∏
i=1

1
n
k¬µr ‚àí ¬µs k ‚â• cœÉ 2 K( + log )
w
Œ¥

that
=

x‚ààX
‚àö

Œ≥Œ∏n
‚àö
Hence (5) implies d(C(V)) ‚â• 2cKD
. This means that
wn
SON improves K-means if wK ‚â• c2 12 Œ≥Œ∏ , i.e. the number
of clusters is higher than a fixed value.

4. Stochastic Splitting Algorithm
Our implementation is identical to the so-called proximalbased incremental technique in Bertsekas (2011), which is

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

performed in a way that it requires little amount of calculations (precisely O(m) and independent of other parameters) in each iteration. The proximal-based incremental
method is a variant of the stochastic gradient technique, in
problems where many terms in the objective function are
not differentiable, and the local gradient steps are replaced
by local proximal operators. To perform the proximalbased incremental method, we first write the SON objective
function as
X
Œ¶(u1 , u2 , . . . , un ) =
œÜij (ui , uj )
i<j

the probability sense (uniform convergence), while the result in Bertsekas (2011) is pointwise. Second, we prove
guaranteed speed of convergence with probability one. We
present these results by the following theorem. In this theorem, we consider fixed data dimension m and bounded data
vectors (i.e. kxk k ‚â§ C for some absolute constant C).
Theorem 2.
1. Assume that {¬µk } is non-increasing
‚àû
P
0

where
œÜij (ui , uj ) =

1
1
kxi ‚àíui k22 + kxj ‚àíuj k22 +Œªkui ‚àíuj k.
2n
2n

Then, we introduce and explicitly calculate the proximal
(¬µ)
operator Œ†ij of œÜij with step size ¬µ as
(¬µ)

Œ†ij (ui , uj ) =
arg min
œÜij (u0i , u0j ) +
0
0
ui ,uj

1
0
2¬µ kui

‚àí ui k22 +

1
0
2¬µ kuj

= TŒª¬µ (ui + ¬µxi , uj + ¬µxj ),

‚àí uj k22
(9)

where we also introduce the pairwise soft-thresholding operator TŒ∑ (y, z) =

( 
y‚àíz
z‚àíy
,
z
+
Œ∑
ky ‚àí zk ‚â• 2Œ∑
y + Œ∑ kz‚àíyk
ky‚àízk2
,
 2
y+z y+z
,
ky ‚àí zk < 2Œ∑
2
2
(10)
and the final equality is obtained by the local optimality
conditions and straightforward calculations. Our algorithm
simply consists in iteratively applying randomly selected
proximal operators. This is depicted in Algorithm 1.
Algorithm 1 Stochastic Splitting Algorithm
Input: The data vectors {xk }nk=1 and step sizes
{¬µk }‚àû
k=1
Initialization: Set u1 , u2 , . . . , un arbitrarily (we use
u1 = u2 = . . . = un = 0)
for k = 1, 2, . . . do
Select a pair (i, j) with i < j uniformly randomly.
(¬µ )
Update (ui , uj ) ‚Üê Œ†ij k (ui , uj )
end for

‚àû
P

¬µk = ‚àû and

0

¬µ2k < ‚àû. Then, the sequence Uk converges to UÃÉ

in the following strong probability sense:


‚àÄ > 0; lim Pr sup kUl ‚àí UÃÉk2F >  = 0 (11)
k‚Üí‚àû

l‚â•k

2. Take ¬µk = k¬µŒ±1 for k = 1, 2, . . . and 23 < Œ± < 1. For
sufficiently small values of  > 0 the relation


n4
kUl ‚àí UÃÉk2F = O 3Œ±‚àí2‚àí
l
holds for every l, n with probability 1.
Proof. We skip many steps in our proof for lack of space.
These steps can be found in the supplement. Denote by Uk
a matrix where the ith column is the value of ui at the k th
iteration. Define
œà¬µ (U) = E (Uk+1 | Uk = U, ¬µk = ¬µ) ,

(12)

Starting from UÃÑ0 = U0 (the initialization of the algorithm), we define the characteristic sequence {UÃÑk }‚àû
k=0 by
the following iteration:
UÃÑk+1 = œà¬µk (UÃÑk )
Our proof is based on the following two results, which we
prove in the supplementary material:
i We have that

Pr sup kUk ‚àí UÃÑk k2F +
k

‚àû
X

‚àû
P

!
¬µ2l > Œª

‚â§

l=k

k=0

¬µ2k

Œª
(13)

4.1. Convergence Analysis
Convergence of proximal-based incremental method is
discussed in Bertsekas (2011). We further elaborate
on the convergence by further exploitation of the nonexpansiveness property of proximal operators. This allows
us to complement the result in Bertsekas (2011) in the following two directions: First, we establish convergence in

ii Define UÃÉ as the unique optimal solution of the SON
optimization and suppose that {¬µk } is a non-increasing
sequence. There exists a universal constant a such that
kUÃÑk ‚àí UÃÉk2F is upper bounded by
a

k‚àí1
X
l=0

¬µ2l e

‚àí n22

k‚àí1
P
s=l+1

¬µs

+ kU0 ‚àí

UÃÉk2F e

‚àí n22

k‚àí1
P
s=0

¬µs

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

To prove Theorem 2, define U k = {UÃÑkl }‚àû
l=0 as the sequence obtained by starting from UÃÑk0 = Uk and applying

120

Exact
Stochastic Splitting

100

UÃÑkl+1 = œà¬µl+k (UÃÑkl )
Take arbitrary (non-zero) positive numbers , Œ¥. Take Œª
‚àû
P
such that Œª ‚â• 2Œ¥
¬µ2l . Take some values l0 , k which we

t (s)

80
60
40

l=0

specialize later. Now, we define two outcomes H1 and H2 :

20

H1 : ‚àÄk ‚â• 0; kUk ‚àí UÃÉk2F ‚â§ Œª

0

H2 : ‚àÄl ‚â•

0; kUÃÑkl

n


‚àí Ul+k k ‚â§
4

From item (i), it is simple to see that Pr(H1c ) and Pr(H2c )
are less than Œ¥/2. Furthermore we can show by (ii) that
under H1 ‚à© H2 and suitable l0 , k:
‚àÄl > l0 ; kUl+k ‚àí UÃÉk22 ‚â§ 2(kUl+k ‚àí UÃÑkl k2F +kUÃÑkl ‚àí UÃÉk2F )


‚â§ 2( + ) = 
4 4
This is detailed in the supplement. We conclude that

which proves part (1) of Theorem.

Qr : sup kUl+kr ‚àí
l‚â•0

where we introduce ¬µ1 = bn2 and A = 4an4 b2 for simplicity. This leads to
1‚àíŒ±

l>l0 +k

For part (2), define kr = rŒ≥ , Œªr = r‚àíŒ≤ , where Œ≥ =
Œ≤ < Œ≥(2Œ± ‚àí 1) ‚àí 1, and the outcomes:

Figure 2. Running times of the exact SON clustering algorithm
(implemented in CVX) and stochastic splitting for samples from
a mixture of two Gaussians with increasing sample size.

kUkr+1 ‚àí UÃÉk2F ‚â§ LeLkr

Pr( sup kUl ‚àí UÃÉk22 > ) ‚â§ Pr(H1c ) + Pr(H2c ) ‚â§ Œ¥

UÃÑkl r k2F

100 200 300 400 500 600

1‚àí 2
1‚àíŒ± ,

2Œªr + A

lr
X
t=0

1‚àíŒ±
‚àíLkr+1

kUkr ‚àí UÃÉk2F +

1‚àíŒ±
1‚àíŒ±
1
eL(kr +t) ‚àíLkr+1
(t + kr )2Œ±

where L is a suitable constant with different values at different occurrences. Postponing few more steps to the supplementary material, we obtain that
kUkr ‚àí UÃÉk2F ‚â§

> Œªr .

L log r
L
‚â§ Œ≤‚àí

Œ≤‚àí
2
r
r

Take kr < l ‚â§ kr+1 . We obtain that
‚àû
P

Pr(Qr ) < ‚àû. Hence by
r=1
c
c
Qr0 , Qr0 +1 , Qcr0 +2 , . . . simultane-

By item (i), we have that

Borel-Cantelli lemma,
ously hold for some r0 with probability 1. For simplicity
and without loss of generality, we assume that r0 = 0 as it
does not affect the asymptotic rate. Then for any r > 0, we
have that
sup kUl+kr ‚àí UÃÑkl r k2F ‚â§ Œªr

kUl ‚àí UÃÉk22 ‚â§ 2(kUkr ‚àí UÃÉk22 + kUkr ‚àí Ul k22 )
L
L
L
‚â§ Œ≤‚àí ‚â§ Œ≤‚àí
rŒ≤‚àí
r
l Œ≥
By taking Œ≤ = Œ≥(2Œ± ‚àí 1) ‚àí 1, we obtain part (2).
‚â§ 2Œªr +

l‚â•0

5. Experiments

In particular,
kUkr+1 ‚àí UÃÑklrr k2F ‚â§ Œªr
where lr = kr+1 ‚àí kr . From item (ii), we also conclude
that
kUÃÑklrr

‚àí

UÃÉk2F

‚â§A

lX
r ‚àí1
t=0

+kUkr ‚àí

lrP
‚àí1

1

‚àí2a
1
(s+kr )Œ±
s=t+1
e
2Œ±
(t + kr )

UÃÉk2F e

‚àí2a

lrP
‚àí1
s=0

1
(s+kr )Œ±

We evaluate the proposed stochastic splitting algorithm in
the task of clustering points generated by Gaussians mixture models. We compare the results to the exact algorithm
proposed by Lindsten et al. (2011) in terms of a) the quality of the produced clustering and b) the time spent solving
the optimization problem. The results of both algorithms
are dense embeddings of the points that are then thresholded to form clusters. The clusters are the largest subsets
of nodes such that the maximum pairwise distance within
the subset is less than œÑ . The stochastic splitting algorithm
is implemented as in Algorithm 1. We observed in practice

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

0.8
0.6
0.4
0.2
0

1.2

Exact

1

Stochastic Splitting

Adjusted Rand

Stochastic Splitting

0.8
0.6
0.4
0.2
0

0

2

4

6

6

(a) œÉ 2 = 0.01

8

10
#10-3

1.2

Exact

1

Stochastic Splitting

Adjusted Rand

1.2

Exact

1

Adjusted Rand

Adjusted Rand

1.2

0.8
0.6
0.4
0.2
0

0

2

4

6

6

8

10
#10-3

(b) œÉ 2 = 0.02

Exact

1

Stochastic Splitting

0.8
0.6
0.4
0.2
0

0

2

4

6

6

8

10
#10-3

(c) œÉ 2 = 0.05

0

2

4

6

6

8

10
#10-3

(d) œÉ 2 = 0.1

Figure 1. Adjusted Rand index for different choices of Œª. Each plot represents quality of the clustering produced by solving the SON
objective exactly or with stochastic
‚àö splitting. The different plots represent clustering of 200 samples from a mixture of two Gaussians in
R2 with fixed separation d = 2 and variance œÉ 2 .

that a heuristic for adaptively setting the step-size improved
robustness and rate of convergence. Specifically, the step
size was reduced by a constant factor whenever the average
change in the objective over successive rounds in a small
window was positive. If the same average was negative,
but small in absolute value, the step size was increased by
a small constant factor.
The data is generated from Gaussian mixture models with
two components
in R2 where the means are separated by
‚àö
d = 2 and the variance œÉ 2 is varied. The number of samples is also varied, to illustrate the computational gains of
the stochastic splitting method. As pointed out by Lindsten
et al. (2011), the choice of the regularization parameter Œª
is perhaps the most challenging hurdle in applying SON
clustering. Choosing Œª too high might result in a single
large cluster, and choosing it too low may cause each point
to be represented by its own cluster. While this problem
is of great importance in applications, we focus on the relative performance of the Lindsten et al. (2011) algorithm
(CVX) and stochastic splitting (SS). We report the adjusted
Rand index (Rand, 1971) as measure of cluster quality, and
would like to emphasize that this does not rely on identifying the number of clusters beforehand.
Results The results of the experiments are presented in
Figures 1 and 2. We see in Figure 1 that the quality of the
clustering produced by the stochastic splitting algorithm is
comparable to that of the exact algorithm. This pattern is
consistant across choices of œÉ, where a high œÉ implies low
sample separation between the clusters. We also note that
the range of Œª for which the stochastic splitting algorithm
achieves as good result as the exact algorithm is less wide
than for the exact. We believe this is due to the stochastic
nature of the algorithm which makes the resulting embedding clusters less separated than in the exact version. Deviations from the optimal embedding could be magnified
by the thresholding step, effectively making the stochastic
algorithm more sensitive to the choice of threshold, and in
effect the quality more sensitive to the choice of Œª. In these

experiments, the same threshold was used for both algorithms, but tailored choices could be considered given an
appropriate selection criterion.
Furthermore, we see in Figure 2 that the running time of
the stochastic splitting algorithm is lower than that of the
exact algorithm, and grows significantly slower. While the
stochastic splitting algorithm could in principle be implemented in time constant in the number of samples, and instead determined by the number of iterations, the adaptive
stepsize used to improve performance requires evaluation
of the objective value which scales with the number of samples. This could be improved by subsampling the terms in
the objective function, but this was not done here.

6. Conclusions
We developed a stochastic incremental algorithm based on
proximal iterations for the SON convex relaxtion of clustering that is highly suited to large scale problems and gave
an analysis of its convergence propertis. We also gave quite
general theoretical guaranteees for exact recovery of clusters similar to the unifying proximity condition in approximation algorithms that covers paradigm models for clustering data.
It has not escaped our attention that our algorithm can easily be adapted to incorporate similarity weights as used in
Chi & Lange (2015); Chen et al. (2015); Hocking et al.
(2011) and that it is amenable to acceleration using variance reduction and other techniques. The cluster recovery
conditions can also be extended to cover almost perfect recovery i.e. correctly clustering all except a small fraction
of points. A more complete experimental evaluation of our
algorithm and comparison to others will be included in a
longer version of the paper.

Acknowledgements
This work is supported in part by the Swedish Foundation
for Strategic Research (SSF).

Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery

References
A. Kumar, R. Kannan. Clustering with spectral norm and the kmeans algorithm. In FOCS, 2010.
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Optimization with sparsity-inducing penalties. Foundation and Trends
in Machine Learning, 1(4):1‚Äì106, 2012.
Bertsekas, D. Incremental proximal methods for large scale convex optimization. Math. Program., 129(163), 2011.
Bottou, LeÃÅon, Curtis, Frank E., and Nocedal, Jorge. Optimization methods for large-scale machine learning. Technical report, arXiv:1606.04838, June 2016. URL http://leon.
bottou.org/papers/tr-optml-2016.
Chen, Gary K., Chi, Eric C., Ranola, John M.O., and Lange,
Kenneth. Convex clustering: An attractive alternative to
hierarchical clustering. PLoS Computational Biology, 11(5):
e1004228, 2015. doi: 10.1371/journal.pcbi.1004228. URL
http://journals.plos.org/ploscompbiol/
article?id=10.1371/journal.pcbi.1004228.
Chi, Eric C. and Lange, Kenneth. Splitting methods for convex clustering. Journal of Computational and Graphical
Statistics, 24(4):994‚Äì1013, 2015. doi: 10.1080/10618600.
2014.948181. URL http://dx.doi.org/10.1080/
10618600.2014.948181.
Defazio, A., F. Bach, F., and Lacoste-Julien, S. A fast incremental
gradient method with support for non-strongly convex composite objectives. In NIPS, pp. 1646‚Äì1654, 2014.
Hocking, T., Vert, J-P., Bach, F., and Joulin., A. Clusterpath:
an algorithm for clustering using convex fusion penalties. In
ICML, 2011.
Jain, A. K., Murty, M. N., and Flynn, P. J. Data clustering: A
review. ACM Comput. Surv., 31(3):264‚Äì323, September 1999.
ISSN 0360-0300. doi: 10.1145/331499.331504. URL http:
//doi.acm.org/10.1145/331499.331504.
Johnson, R. and Zhang, T. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS, pp. 315‚Äì
323, 2013.
Lindsten, F., Ohlsson, H., and Ljung, L. Clustering using sum-ofnorms regularization: With application to particle filter output
computation. 2011.
P.Awasthi, O Sheffet. Improved spectral norm bounds for clustering. In APPROX-RANDOM, 2012.
Rand, William M. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association,
66(336):846‚Äì850, 1971.
Schmidt, M., Roux, N. Le, and Bach, F. Minimizing finite sums
with the stochastic average gradient. Mathematical Programming, 2016.
Zhu, C., Xu, H., Leng, C., and Yan, S. Convex optimization procedure for clustering: Theoretical revisit. In NIPS, 2010.

