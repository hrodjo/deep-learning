Provably Optimal Algorithms for Generalized Linear Contextual Bandits

Lihong Li 1 Yu Lu 2 Dengyong Zhou 1

Abstract
Contextual bandits are widely used in Internet
services from news recommendation to advertising, and to Web search. Generalized linear
models (logistical regression in particular) have
demonstrated stronger performance than linear
models in many applications where rewards are
binary. However, most theoretical analyses on
contextual bandits so far are on linear bandits. In
this work, we propose an upper confidence bound
based algorithm for generalized linear
√ contextual bandits, which achieves an Õ( dT ) regret
over T rounds with d dimensional feature vectors. This regret matches the minimax lower
bound, up to logarithmic terms,
√and improves on
the best previous result by a d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp
finite-sample confidence bound for maximumlikelihood estimates in generalized linear models, which may be of independent interest. We
also analyze a simpler upper confidence bound
algorithm, which is useful in practice, and prove
it to have optimal regret for certain cases.

1. Introduction
Contextual bandit problems are originally motivated by applications in clinical trials (Woodroofe, 1979). When a
standard treatment and a new treatment are available for
a certain disease, the doctor needs to decide, in a sequetial
manner, which of them to use based on the patient’s profiles
such as age, general physical status or medicine history.
With the development of modern technologies, contextual
bandit problems have more applications, especially in webbased recommendation, advertising and search (Agarwal
1

Microsoft Research, Redmond, WA 98052 2 Department
of Statistics, Yale University, New Haven, CT, USA. Correspondence to:
Lihong Li <lihongli@microsoft.com>,
Yu Lu <yu.lu@yale.edu>,
Dengyong Zhou <denzho@microsoft.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

et al., 2009; Li et al., 2010; 2012). In the problem of personalized news recommendation, the website must recommend news articles that are most interesting to users that
visit the website. The problem is especially challenging
for breaking news, as little data are available to make good
prediction about user interest. A trade-off naturally occurs
in this kind of sequential decision making problems. One
needs to balance exploitation—choosing actions that performed well in the past—and exploration—choosing actions that may potentially give better outcomes.
In this paper, we study the following stochastic, K-armed
contextual bandit problem. Suppose at each of the T
rounds, an agent is presented with a set of K actions, each
of which is associated with a context (a d-dimensional feature vector). By choosing an action based on the rewards
obtained from previous rounds and on the contexts, the
agent will receive a stochastic reward generated from some
unknown distribution conditioned on the context and the
chosen action. The goal of the agent is to maximize the
expected cumulative rewards over T rounds. The most
studied model in contextual bandits literature is the linear
model (Auer, 2002; Dani et al., 2008; Rusmevichientong
& Tsitsiklis, 2010; Chu et al., 2011; Abbasi-Yadkori et al.,
2011), in which the expected rewards at each round is a linear combination of features in the context vector. The linear model is theoretically convenient to work with. However, in practice, we usually have binary rewards (click or
not, treatment working or not). Logistic regression model
based algorithms have been shown to have substantial improvements over linear models (Li et al., 2012). We therefore consider generalized linear models (GLM) in the contextual bandit setting, in which linear, logistic and probit
regression serve as three important special cases.
The celebrated work of Lai & Robbins (1985) first introduces the upper confidence bound (UCB) approach to efficient exploration. Later, the idea of confidence bound has
been successfully applied to many stochastic bandits problems, from K-arm bandits problems (Auer et al., 2002a;
Bubeck & Cesa-Bianchi, 2012) to linear bandits (Auer,
2002; Abbasi-Yadkori et al., 2011). UCB-type algorithms
are both efficient and provable optimal in K-arm bandits
and K-armed linear bandits. However, most study are limited to the linear case. While some UCB-type algorithms
using GLMs perform well empirically (Li et al., 2012),

Generalized Linear Contextual Bandits

there is little theoretical study of them. A natural question
arises: can we find an efficient algorithm to achieve the optimal convergence rate for generalized linear bandits?
Our Contributions In this paper, we propose a GLM
version of the UCB algorithm called SupCB-GLM
that
√
achieves a regret over T rounds of order Õ( dT ). This rate
improves
√ the state-of-the-art results of Filippi et al. (2010)
by a d factor, assuming the number of actions is fixed.
Moreover, it matches the GLM bandits problem’s minimax lower bound indicated by the linear bandits problem
and thus is optimal. SupCB-GLM is inspired by the seminal work of Auer (2002), which introduced a technique to
construct independence samples in linear contextual bandits. A key observation in proving this result is that the
`2 confidence ball of the unknown parameter is insufficient
to calculate a sharp upper confidence bound, yet what we
need is the confidence interval in all directions. Thus, we
prove a finite sample normality type confidence bound for
the maximum likelihood estimator of GLM. To the best of
our knowledge, this is the first non-asymptotic normality
type result for the GLM and might be of its own theoretical
value. We also analyze a simple version of UCB algorithm
called UCB-GLM that is widely used in practice. We prove
it also achieves the optimal regret bound under a reasonable assumption. These results shed light on explaining the
good empirical performance of GLM bandits in practice.
Related Work The study of GLM bandits problem goes
back at least to Sarkar (1991), who considered discounted
regrets rather than cumulative regerts. They prove that a
myopic rule without exploration is asymptotically optimal.
Recently, Filippi et al. (2010) study the same stochastic
GLM bandit problem considered here. They propose the
GLM-UCB algorithm, similar
to our Algorithm 1, which
√
achieves a regret of Õ(d T ) after T rounds. However, as
we believe the optimal regret for stochastic GLM bandits
should be the same as linear case√when the number of actions is small, their rates misses a d term than the optimal
rates.
Another line of research focuses on using EXP-type algorithms, which can be applied to almost any model
classes (Auer et al., 2002b). These algorithms, which
choose actions using a carefully randomized policy, use
importance sampling to reduce a bandit problem to its fullinformation analogue. Later variants of the EXP4 algorithm (Beygelzimer
et al., 2010; Agarwal et al., 2014) give
√
an Õ( dKT ) regret that is near-optimal√ with respect to
T . However, these regret bounds have a K dependence.
Moreover, these algorithms can be expensive to run: they
either have a computational complexity exponential in d for
our GLM case, or need to make a large number of calls to
a nontrivial optimization oracle.

Organization Section 2 introduces the generalized linear
bandit problem. Section 3 gives a brief review of the statistical properties of generalized linear model, and gives a
sharp non-asymptotic normality-type result for GLM parameter estimation which can be of independent value.
With this tool, Section 4 presents our algorithms and the
main theoretical results. Section 5 concludes the paper with
further discussions, including several open problems. All
proofs are given in the supplementary materials.
Notations For a vector x ∈ Rd , we use kxk to denote its
`2 - norm and x0 its transpose. Bd := {x ∈ Rd : kxk ≤ 1}
is the unit ball centered at the origin. The weighted `2 -norm
associated √
with a positive-definite matrix A is defined by
kxkA := x0 Ax. The minimum and maximum singular
values of a matrix A are written as λmin (A) and kAk, respectively. The trace of a matrix A is tr (A). For two symmetric matrices A and B of the same dimensions, A  B
means that A − B is positive semi-definite. For a realvalued function f , we use f˙ and f¨ to denote its first and
second derivatives. Finally, [n] := {1, 2, . . . , n}.

2. Problem Setting
We consider the stochastic K-armed contextual bandit
problem. Let T be the number of total rounds. At round
t, the agent observes a context consisting of a set of K
feature vectors, {xt,a | a ∈ [K]} ⊂ Rd , which is drawn
IID from an unknown distribution ν, with kxt,a k ≤ 1.
Each feature vector xt,a is associated with an unknown
stochastic reward yt,a ∈ [0, 1]. The agent selects one action, denoted at , and observes the corresponding reward
yt,at . Finally, we make a regularity assumption about the
distributionPν: there exists a constant σ0 > 0 such that
1
2
0
λmin (E[ K
a∈[K] xt,a xt,a ]) ≥ σ0 for all t.
In this paper, we are concerned with the generalized linear
model, or GLM, in which there is an unknown θ∗ ∈ Rd and
a fixed, strictly increasing link function µ : R → R such
that E[Y | X] = µ(X 0 θ∗ ), where X is the chosen action’s
feature and Y the corresponding reward. One can verify
that linear and logistic models are special cases of GLM
with µ(x) = x and µ(x) = 1/(1 + e−x ), respectively.
The agent’s goal is to maximize the cumulative expected
rewards over T rounds. Suppose the agent takes action at
at round t. Then the agent’s strategy can be evaluated by
comparing its expected reward to the best expected reward.
To do so, define the optimal action at round t by a∗t =
argmaxa∈[K] µ(x0t,a θ∗ ). Then, the agent’s total regret of
following strategy π can be expressed as follows
T 

X
RT (π) :=
µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ ) .
t=1

Note that RT (π) is in general a random variable due to the

Generalized Linear Contextual Bandits

possible randomness in π. Denote by Xt = xt,at , Yt =
yt,at , and our model can be written as
Yt = µ(Xt0 θ∗ ) + t ,

(1)

where {t , t ∈ [T ]} are independent zero-mean noise.
Here, Xt is a random variable because the agent chooses
current action based on previous rewards. Formally, we assume there is an increasing sequence of sigma fields {Fn }
such that t is Ft -measurable with E [ t | Ft−1 ] = 0.
An example of Fn will be the sigma-field generated by
{X1 , Y1 , . . . , Xn , Yn }. Also, we assume the noise t is
sub-Gaussian with parameter σ, where σ is some positive,
universal constant; that is, for all t,


2 2
E eλt | Ft−1 ≤ eλ σ /2 .
(2)
In practice, when we have bounded reward Yt ∈ [0, 1], the
noise t is also bounded and hence satisfies (2) with some
appropriate σ value. In addition to the boundedness assumption on the rewards and feature vectors, we also need
the following assumption on the link function µ.
Assumption 1. κ := inf {kxk≤1, kθ−θ∗ k≤1} µ̇(x0 θ) > 0.
As we shall see in Section 3, the asymptotic normality of
maximum-likelihood estimates implies the necessity of this
assumption. Note that this assumption is weaker than Assumption 1 in Filippi et al. (2010), as it only requires to
control the local behavior of µ̇(x0 θ) near θ∗ .
Assumption 2. µ is twice differentiable. Its first and second order derivatives are upper-bounded by Lµ and Mµ ,
respectively.
It can be verified that Assumption 2 holds for the logistic
link function, where we may choose Lµ = Mµ = 1/4.

3. Generalized Linear Models
To motivate the algorithms proposed in this paper, we first
briefly review the classical likelihood theory of generalized
linear models. In the canonical generalized linear model
(McCullagh & Nelder, 1989), the conditional distribution
of Y given X is from the exponential family, and its density, parameterized by θ ∈ Θ, can be written as


Y X 0 θ∗ − m(X 0 θ∗ )
+ h(Y, η) . (3)
P(Y | X) = exp
g(η)
Here, η ∈ R+ is a known scale parameter; m, g and h
are three normalization functions mapping from R to R.
The exponential family (3) is a very broad family of distributions including the Gaussian, binomial, Poisson, gamma
and inverse-Gaussian distributions. It follows from standard properties of exponential families (Brown, 1986) that
m is infinitely differentiable satisfying ṁ(X 0 θ∗ ) = E[ Y |

X ] = µ(X 0 θ∗ ) and m̈(X 0 θ∗ ) = V(Y | X). It can be
checked that the data generated from (3) automatically satisfies the sub-Gaussian condition (2).
Suppose we have independent samples of Y1 , Y2 , . . . , Yn
condition on X1 , X2 , . . . , Xn . The log-likelihood function
of θ under model (3) is

n 
X
Yt Xt0 θ − m(Xt0 θ)
+ c(Yt , η)
log `(θ) =
v(η)
t=1
n

=

1 X
[Yt Xt0 θ − m(Xt0 θ)] + constant .
v(η) t=1

Consequently, the maximum likelihood estimate (MLE)
may be defined by
θ̂n ∈ argmax
θ∈Θ

n
X

[Yt Xt0 θ − m(Xt0 θ)] .

t=1

From classical likelihood theory (Lehmann & Casella,
1998), we know that when the sample size n goes to infinity, the MLE θ̂n is asymptotically
normal, that is, θ̂n −θ∗ →
Pn
−1
N (0, Iθ∗ ), where Iθ = t=1 µ̇(Xt0 θ)Xt Xt0 is the Fisher
Information Matrix. Note that if µ̇(Xt0 θ∗ ) → 0, the asymptotic variance of x0 θ̂ can go to infinity for some x. This
suggests the necessity of Assumption 1.
As we will see later, the normality result is crucial in our
regret analysis of GLM bandits. However, to the best of
our knowledge, there is no non-asymptotic normality results of the MLE for GLM. In the following, we present a
finite-sample version of the classical asymptotic normality
results, which can be of independent interest.
Pn
Theorem 1. Define Vn = t=1 Xt Xt0 , and let δ > 0 be
given. Furthermore, assume that


512Mµ2 σ 2
1
2
d + log
.
(4)
λmin (Vn ) ≥
κ4
δ
Then, with probability at least 1 − 3δ, the maximumlikelihood estimator satisfies, for any x ∈ Rd , that
3σ p
|x0 (θ̂n − θ∗ )| ≤
log(1/δ) kxkVn−1 .
(5)
κ
This theorem characterizes the behavior of MLE on every
direction. It implies that x0 (θ̂n − θ∗ ) has a sub-Gaussian
tail bound for any x ∈ Rd . It also provides a rigorous justification of the asymptotic upper confidence bound derived
heuristically by Filippi et al. (2010, Section 4.2).
The proof of the theorem is given in the appendix. It consists of two main steps, as is typical for proving normalitytype results of MLEs (Van der Vaart, 2000). We first show
the n−1/2 -consistency of θ̂ to θ∗ . Then, by using a secondorder Taylor expansion or Newton-step, we can prove the
desired normality of θ̂.

Generalized Linear Contextual Bandits

The condition (4) on λmin (Vn ) is necessary for the consistency of estimating linear models (Lai & Wei, 1982; Bickel
et al., 2009) and generalized linear models (Fahrmeir &
Kaufmann, 1985; Chen et al., 1999). It can be satisfied under mild conditions such as the proposition below, which
will be useful for our analysis.
Pn
0
Proposition 1. Define Vn =
t=1 Xt Xt , where Xt is
drawn iid from some distribution ν with support in the unit
ball, Bd . Furthermore, let Σ := E[Xt Xt0 ] be the second
moment matrix, and B and δ > 0 be two positive constants. Then, there exist positive, universal constants C1
and C2 such that λmin (Vn ) ≥ B with probability at least
1 − δ, as long as
n ≥

!2
p
√
C1 d + C2 log(1/δ)
2B
+
.
λmin (Σ)
λmin (Σ)

Proof Sketch. We give a proof sketch here, and the full
proof is found in the appendix. In the following, for simplicity, we will drop the subscript n when there is no ambiguity. Therefore, Vn is denoted V and so on. We will need a
technical lemma, which is an existing result in random matrix theory. The version we presented here is adapted from
Equation (5.23) of Theorem 5.39 from Vershynin (2012).
Lemma 1. Let A ∈ Rn×d be a matrix whose rows Ai
are independent sub-Gaussian isotropic random vectors in
Rd with parameter σ, namely, E exp(x0 (Ai − EAi )) ≤
2
exp(σ 2 kxk /2) for any x ∈ Rd . Then, there exist positive, universal constants C1 and C2 such that, for every t ≥ 0, the following holds with probability
at √
least
p
2
2
d/n
+
t/
n):
1 − 2 exp(−C
t
),
where
ε
=
σ
(C
1
2
 1 A0 A − Id  ≤ max{ε, ε2 } .
n
Let X be a random vector drawn from the distribution
ν. Define Z := Σ−1/2 X. P
Then Z is isotropic, namely,
n
E[ZZ 0 ] = Id . Define U = t=1 Zt Zt0 = Σ−1/2 V Σ−1/2 .
From Lemma 1, we have that, for any t, with probability
√
at least
1 − 2 exp(−C2 t2 ), λmin (U ) ≥ n − C1 σ 2 nd −
√
σ 2 t n, where σ is the sub-Gaussian parameter of Z, which


−1/2
is upper-bounded by Σ−1/2  = λmin (Σ) (see, e.g., Vershynin (2012)). We thus can rewrite the above inequality
(which holds with probability 1 − δ as

√
√ 
2
λmin (U ) ≥ n − λ−1
nd + t n .
min (Σ) C1 σ
This implies the following lower bound:
p
√
λmin (V ) ≥ λmin (Σ)n − C1 nd − C2 n log(1/δ) .
Finally, simple calculations show that the last expression is
no less than B as long as n is no smaller than the expression
stated in the proposition, finishing the proof.

4. Algorithms and Main Results
In this section, we are going to present two algorithms.
While the first algorithm is computationally more efficient,
the second algorithm has a provable optimal regret bound.
4.1. Algorithm UCB-GLM
The idea of upper confidence bounds (UCB) is highly
effective in dealing with the exploration and exploitation trade-off in many parametric bandit problems, including K-arm bandits (Auer et al., 2002a) and linear bandits (Abbasi-Yadkori et al., 2011; Auer, 2002; Chu et al.,
2011; Dani et al., 2008). For the generalized linear model
considered here, since µ is a strictly increasing function,
our goal is equivalent to choosing a ∈ [K] to maximize
x0t,a θ∗ at round t. Suppose θ̂t is our current estimator of θ∗
after round t. An exploitation action is to take the action
that maximizes the estimated mean value, while an exploration action is to choose the one that has the largest variance. Thus, to balance exploitation and exploration, we
can simply choose the action that maximizes the sum of estimated mean and variance, which can be interpreted as an
upper confidence bound of x0t,a θ̂t . This leads to the algorithm UCB-GLM (Algorithm 1).
Algorithm 1 UCB-GLM
Input: the total rounds T , tuning parameter τ and α.
Initialization:
Pτ randomly choose at ∈ [K] for t ∈ [τ ], set
Vτ +1 = i=1 Xt Xt0
For t = τ + 1, τ + 2, . . . , T do
1. Calculate the maximum-likelihood estimator θ̂t by
solving the equation
t−1
X
(Yi − µ(Xi0 θ))Xi = 0
(6)
i=1



0
θ̂t + α kXt,a kV −1
2. Choose at = argmaxa∈[K] Xt,a
t

3. Observe Yt , let Xt ← Xt,at , Vt+1 ← Vt + Xt Xt0
End For
UCB-GLM take two parameters. At the initialization stage,
we randomly choose actions to ensure a unique solution
of (6). The choice of τ in the theorem statement follows
from Proposition 1 with B = 1. It should be noted that the
IID assumption about contextual (i.e., the distribution ν)
is only needed to ensure Vτ +1 is invertable (similar to the
first phase in the algorithm of Filippi et al. (2010)); the rest
of our analysis does not depend on this stochastic assumption. The same may be achieved by using regularization
(see, e.g., Abbasi-Yadkori et al. (2011)). Another tuning
parameter α is used to control the amount of exploration.
The larger the α is, the more exploration will be used.
As mentioned earlier, the feature vectors Xt depend on the

Generalized Linear Contextual Bandits

previous rewards. Consequently, the rewards {Yi , i ∈ [t]}
may not be independent given {Xi , i ∈ [t]}. We instead use results on self-normalized martingales (AbbasiYadkori et al., 2011), together with a finite-time normality
result like Theorem 1, to prove the next theorem.

Lemma 3. Suppose λmin (Vτ +1 ) ≥ 1. For any δ ∈
[1/T, 1), define event
(
)
r
σ d
E∆ := k∆t kVt ≤
log(1 + 2t/d) + log(1/δ) .
κ 2

Theorem 2. Fix any δ > 0. There exists a universal
constant
q C > 0, such that if we run UCB-GLM with

Then, event E∆ holds for all t ≥ τ with probability at least
1 − δ.

σ
κ

Cσ0−2 (d +

d
2

log(1 + 2T /d) + log(1/δ) and τ =
α=
log(1/δ)), then, with probability at least 1 − 2δ, the regret
of the algorithm is upper bounded by
 
2Lµ σd
T √
RT ≤ τ +
log
T.
κ
dδ

√
The theorem shows an Õ(d T ) regret bound that is independent of K. Indeed, this rate matches the minimax
lower bound up to logarithm factor for the infinite actions
contextual bandit problems (Dani et al., 2008). By choosing δ = 1/T and using the fact that RT ≤ T , this highprobability result
√implies a bound on the expected regret:
E[RT ] = Õ(d T ). Our result improves√the previous
regret bound of Filippi et al. (2010) by a log T factor.
Moreover, the algorithm proposed in Filippi et al. (2010)
involves a projection step, which is computationally more
expensive comparing to UCB-GLM. Finally, this algorithm
works well in practice. We give a heuristic argument for its
strong performance in Section 5, under a specific condition
that sometimes are satisfied.
Proof of Theorem 2. We first bound the one-step regret. To
do so, fix t and let Xt∗ = xt,a∗t and ∆t = θ̂t − θ∗ , where
a∗t ∈ arg maxa∈[K] µ(x0t,a θ∗ ) is an optimal action at round
t. The selection of at in UCB-GLM implies
hXt∗ , θ̂t i + α kXt∗ kV −1 ≤ hXt , θ̂t i + α kXt kV −1 . (7)
t

t

Then, we have

q
We now choose α = σκ d2 log(1 + 2T /d) + log(1/δ). If
event E∆ holds for all t ≥ τ , then,
hXt∗ , θ∗ i − hXt , θ∗ i


≤ α kXt kV −1 − kXt∗ kV −1 + kXt∗ − Xt kV −1
t

≤ 2α kXt k

≤ α kXt kV −1 −
t

≤ α kXt kV −1
t

α kXt∗ kV −1
t

t

.

Combining the above with Lemma 2 yields
s
 
T
X

T
∗ ∗
∗
hXt , θ i − hXt , θ i ≤ 2α 2T d log
d
t=τ +1
 
2dσ
T √
≤
T . (8)
log
κ
dδ
Note that µ is an increasing Lipschitz function with Lipschitz constant Lµ and the µ function is bounded between
0 and 1. The regret of algorithm UCB-GLM can be upper
bounded as
RT

=

τ 
X

µ (hXt∗ , θ∗ i) − µ (hXt , θ∗ i)



t=1
T


X
+
µ (hXt∗ , θ∗ i) − µ (hXt , θ∗ i)
t=τ +1

≤ τ + Lµ

hXt∗ , θ∗ i − hXt , θ∗ i = hXt∗ − Xt , θ̂t i − hXt∗ − Xt , θ̂t − θ∗ i

t

Vt−1

T

X


hXt∗ , θ∗ i − hXt , θ∗ i .

t=τ +1

hXt∗

−
− Xt , ∆t i
The proof can be finished by applying (8) and the specified

∗
∗
− kXt kV −1 + kXt − Xt kV −1 k∆t kVt , value of τ to the bound above.
t

t

where the last inequality is due to Cauchy-Schwartz inequality. We have the following two lemmas to bound
k∆t kVt and kXt kV −1 , respectively. Their proofs are det
ferred to the appendix.
Lemma 2. Let {Xt }∞
in Rd satisfying
t=1 be a sequence
Pt−1
kXt k ≤ 1. Define X0 = 0 and Vt = s=0 Xs Xs0 . Suppose there is an integer m such that λmin (Vm+1 ) ≥ 1, then
for all n > 0,
s


m+n
X
n+m
.
kXt kV −1 ≤ 2nd log
t
d
t=m+1

4.2. Algorithm SupCB-GLM
While the algorithm UCB-GLM performs sufficiently well
in practice (Li et al., 2012), it is
√ unclear whether it can
achieve the optimal rates of O( dT log K), when K is
fixed and small. As mentioned in Section 4.1, the key
technical difficulty in analyzing UCB-GLM is the dependence between samples. Inspired by a technique developed
by Auer (2002) to create independent samples for linear
contextual bandits, we propose another algorithm SupCBGLM (Algorithm 3), which uses algorithm CB-GLM (Algorithm 2) as a sub-routine.

Generalized Linear Contextual Bandits

Algorithm 2 CB-GLM

Algorithm 3 SupCB-GLM

Input: parameter α, index set Ψ(t), and candidate set A.
1. Let θ̂t be the solution of
X
[Yi − µ(Xi0 θ)] Xi = 0

Input: tuning parameter α, τ , the number of trials T .

i∈Ψ(t)

2. Vt =

P

i∈Ψ(t)

Xi Xi0

Initialization:
for t ∈ [τ ], randomly choose at ∈ [K].
Set S = blog2 T c, F = {a1, · · · , aτ } and Ψ0 = Ψ1 =
· · · = ΨS = ∅.
For t = τ + 1, τ + 2, · · · , T do
1. Initialize A1 = [K] and s = 1.

3. For a ∈ A, do
wt,a = α kxt,a kV −1 ,
t

mt,a = hxt,a , θ̂t i

End For

This algorithm also relies on the idea of confidence bound
to do exploration. At round t, the algorithm screens the
(s)
candidate actions based on the value of wt,a through S
stages until an action is chosen. At stage s, we set the
(s)
confidence level at stage s to be 2−s . If wt,a > 2−s for
some a, we need to do more exploration on xt,a and thus
we choose this action. Otherwise, the actions are filtered
in step 2d such that the actions passed to the next stage are
close enough to the optimal action. Since all the widths
(s)
(s)
are smaller than 2−s , if mt,a < mt,j − 2 · 2−s for some
j ∈ As , the action a can not be the optimal action. The filter process terminates when we have
√ already got accurate
estimate of all x0t,a θ∗ up to the 1/ T level and we do not
need to do exploration. Thus in step 2c we just choose the
action that maximizes the estimated mean value.
Our algorithm is different from the algorithm SupLinRel
in Auer (2002) that we directly maximize the mean, rather
than the upper confidence bound, in steps c and d. This
modification leads to a simpler algorithm and a cleaner regret analysis. Also, we would like to point out that, unlike
SpectralEliminator (Valko et al., 2014), the algorithm can
easily handle a changing action set.
The following result, adapted from Auer (2002, lemma 14),
shows how the algorithm SupCB-GLM will give us independent samples. For the sake of completeness, we also
present its proof here.
Lemma 4. For all s ∈ [S] and t ∈ [T ], given {xi,ai , i ∈
Ψs (t)}, the rewards {yi,ai , i ∈ Ψs (t)} are independent
random variables.
Proof of Lemma 4. Since a trial t can only be added to
Ψs (t) in step 2b of algorithm SupCB-GLM, the event {t ∈
Ψs } only depends on the results of trials τ ∈ ∪σ<s Ψσ (t)
(s)
(s)
and on wt,a . From the definition of wt,a , we know it only
depends on the feature vectors xi,ai , i ∈ Ψs (t) and on xt,i .
This implies the lemma.

2. While at =Null
a. Run CB-GLM with α and Ψs ∪ F to calculate
(s)
(s)
mt,a and wt,a for all a ∈ As .
(s)

b. If wt,a > 2−s for some a ∈ As ,
set at = a, update Ψs = Ψs ∪ {t}
√
(s)
c. Else if wt,a ≤ 1/ T for all a ∈ As ,
(s)

set at = argmax mt,a , update Ψ0 = Ψ0 ∪ {t}
a∈As

(s)

d. Else if wt,a ≤ 2−s for all a ∈ As ,
(s)

(s)

As+1 = {a ∈ As , mt,a ≥ max mt,j − 2 · 2−s },
j∈As

s ← s + 1.
End For

With Lemma 4, we are able to apply the non-asymptotic
normality result (5) and thus to prove our regret bound of
Algorithm SupCB-GLM.
Theorem 3. For any√
0 < δ < 1, if we run
p the SupCB-GLM
algorithm with τ = dT and α = 3σ
2 log(T K/δ) for
κ
T ≥ T0 rounds, where
 2


σ
3 log(T K/δ)
T0 = Ω
max
d
,
,
(9)
κ4
d
the regret of the algorithm is bounded as
p
√
RT ≤ 45(σLµ /κ) log T log(T K/δ) log(T /d) dT ,
with probability at least 1 − δ. With δ = 1/T , we obtain


p
E[RT ] = O (log T )1.5 dT log K .
√
The theorem demonstrates an Õ( dT log K) regret bound
for the algorithm SupCB-GLM.√It has been proved in Chu
et al. (2011, Theorem 2) that dT is the minimax lower
bound of the expected regret for K-armed linear bandits,
a special of the GLM bandits considered here. Therefore,

Generalized Linear Contextual Bandits

the regret of our SupCB-GLM algorithm is optimal up to
logarithm terms of T and K. To the best of our knowledge,
this is the first algorithm which achieves the (near-)optimal
rate of GLM bandits.
It is worthwhile to compare Theorem 3 with the result in
Theorem 2. When K = o(2d ) is small, the rate of SupCB√
GLM is faster, and we improve the previous rates by a d
factor. Here,√
we give a briefly illustration of how we get rid
of the extra d factor. Both in Theorem 2 and in Filippi
et al. (2010), |x0 (θ̂n − θ∗ )| is upper bounded by using the
Cauchy-Schwartz inequality,




(10)
|x0 (θ̂n − θ∗ )| ≤ kxkVn−1 θ̂n − θ∗  .
Vn

for some
√ constant c with probability at least 1 −
exp(− dT ). By Theorem 1 and union bound, we have
the desired result under condition (9).
Lemma 6. Suppose that event EX holds, and that in round
t, the action at is chosen at stage st . Then, a∗t ∈ As for all
s ≤ st . Furthermore, we have
µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ )

(8Lµ )/2√st if at is selected in step 2b
≤
(2Lµ )/ T if at is selected in step 2c .
P
Define Vs,t = t∈Ψs (T ) Xt Xt0 , then by Lemma 2,
X
X
(s)
wt,at =
α(δ)kxt,at kV −1
s,t

Lemma 3 in the supplementary material establishes that


p


θ̂n − θ∗  ≤ C2 d log(T /δ).
Vn

√

This will lead to an extra d factor compared to (5). By
using the Cauchy-Schwartz inequality (10), we only make
use of the fact that θ̂n is close to θ∗ in the `2 sense. However, (5) tells us that actually θ̂n is close to θ∗ in every
direction.√This is the reason why we are able to remove
the extra d factor to achieve a near-optimal regret. It also
explains why the bound in Theorem 2 is tight when K is
large. As K goes large, it is likely there is a direction x for
which (10) is tight.
Proof of Theorem 3. To facilitate our proof, we first
present two technical lemmas. Lemma 5 follows from
Lemma 4, Theorem 1, Theorem 5.39 of Vershynin (2012)
and a union bound. The proof of Lemma 6 is deferred to
the appendix.
√
Lemma 5. Fix
δ > 0. Choose in SupCB-GLM τ = dT
p
and α = 3σ
2 log(T K/δ). Suppose T satisfies condition
κ
(9). Define the following event:
EX

:=

(s)
{|mt,a

−

x0t,a θ∗ |

≤

(s)
wt,a ,

t∈Ψs (T )

≤

t∈Ψs (T )

Combining the above two inequalities gives us
p
|Ψs (T )| ≤ 2s α(δ) 2d log (T /d)|Ψs (T )|.

(12)

Let Ψ0 be the collection of trials such that at is chosen in
step 2c. Since we have chose S = log2 T , each t ∈ [τ +
1, T ] must be in one of Ψs and hence,
√ {τ, τ + 1, . . . , T } =
Ψ0 ∪ ∪Ss=1 Ψs (T ) . If we set τ = dT , we have
RT =

τ 
X


µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ )

t=1

+

T

X

µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ )



t=τ +1

≤

τ+

X

µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ )



t∈Ψ0

+

(11)

Proof of Lemma 5. By Lemma 4, we have independent
samples now. Then to apply Theorem 1, the key is to
lower bound the minimum eigenvalue of Vt . Note that
√ we
randomly select the feature vectors at the first τ = dT
rounds, that is, they are independent. Moreover, the feature vectors are bounded. Thus, X1 , X2 , . . . , Xτ are independent sub-Gaussian with parameter 1. It follows from
Proposition 1 that
√
λmin (Vt ) ≥ λmin (Vτ ) ≥ c dT

p
α(δ) 2d log(T /d)|Ψs (n)| .

On the other hand, by the step 2b of SupCB-GLM,
X
(s)
wt,at ≥ 2−s |Ψs (T )|.

S
X
X 


µ(x0t,a∗t θ∗ ) − µ(x0t,at θ∗ )

s=1 t∈Ψs (T )

∀t ∈ [τ + 1, T ], s ∈ [S], a ∈ [K]}
Then, event EX holds with probability at least 1 − δ.

t∈Ψs (T )

≤
≤

≤
≤

S
2Lµ X
dT + T · √ +
Lµ · 23−s · |Ψs (T )|
T
s=1
√
√
dT + 2Lµ T
S r
X
T
+8Lµ α(δ)
2d log |Ψs (T )|
d
s=1
p
√
√
√
dT + 2Lµ T + 8Lµ α(δ) 2d log(T /d) ST
p
√
45(σLµ /κ) log T log(T K/δ) log(T /d) dT ,

√

with probability at least 1 − δ. Here, the first inequality is
due to the assumption that 0 ≤ µ ≤ 1. The second inequality is Lemma 6. The third inequality is the inequality (12)

Generalized Linear Contextual Bandits

and the fourth inequality is implied by Cauchy-Schwartz.
This completes the proof of the high-probability result.

follows that,
T
X

1
p

5. Discussions

t=τ +1

λmin (Vt )

=

T
X

√
Θ(t−1/2 ) = O( T ).

t=τ +1

In this paper, we propose two algorithms for K-armed bandits with generalized linear models. While the first algorithm, UCB-GLM, achieves the optimal rate for the case
of infinite number of arms, the second algorithm SupCBGLM is provable optimal for the case of finite number actions at each round. However, it remains open whether
UCB-GLM can achieve the optimal rate for small K.

It should be cautioned that, since we do not know the distribution of our feature vectors, we cannot assume the above
gap exists. It is therefore challenging to make the above
arguments rigorous. In fact, when studying the ARIMA
model in time series, Lai & Wei (1982, Example 1) provide an example such that λmin (Vt ) = O(log t).

5.1. A better regret bound for UCB-GLM

5.2. Open Questions

A key quantity in determining the regret of UCB-GLM is
the minimum eigenvalue of Vt . If we make an addition
assumption on the minimum
eigenvalue of Vt , we will be
√
able to prove an O( dT ) regret bound for UCB-GLM.

Computational efficient algorithms. While UCB-GLM
and SupCB-GLM enjoy good theoretical properties, they
can be expensive in some applications. First, they require
inverting a d × d matrix in every step, a costly operation
when d is large. Second, at step t, the MLE is computed
using Θ(t) samples, meaning that the per-step complexity
grows at least linearly with t for a straightforward implementation of the algorithms. It is therefore interesting to
investigate more scalable alternatives. It is possible to use a
first-order, iterative optimization procedure to amortize the
cost, analogous to the approach of Agarwal et al. (2014).

Theorem 4. We run algorithm UCB-GLM with τ =
8σ 2
κ2 d log T and α ≤ Lµ σ/κ. For any δ ∈ [1/T, 1), suppose there is an universal constant c such that
T
X

√
−1/2
λmin (Vt ) ≤ c T .

(13)

t=τ +1

holds with probability at least 1 − δ, and


σR
2
T =Ω
d log T .
κLµ

(14)

Then, the regret of the algorithm is bounded by
RT ≤

CLµ σ p
dT log(T /δ)
κ

with probability at least 1 − 2δ, where C is a positive, universal constant.
This theorem provides some insights of why UCB-GLM
performs well in practice. Although the condition in (13) is
hard to check and may be violated in some cases, for example, in K-armed bandits, we provide a heuristic argument
to justify this assumption in a range of problems. When
t is large enough, our estimator θ̂t is very close to θ∗ . If
we assume there is a positive gap between hxt,a∗t , θ∗ i and
hxt,a , θ∗ i √
for all a 6= a∗t , we will have at = a∗t after, for
example, T steps. Since {xt,a , a ∈ [K]} are independent
for t ∈ [T ], {xt,a∗t } are also independent samples. Then
Vt /t will be well-approximated by the covariance matrix of
xt,a∗t , which we denote by Σ0 . In many problem in practice, especially when features are dense, it is unlikely the
feature vector xt,a∗t lies in a low-dimensional subspace of
Rd . It implies that Σ0 has full rank, and that we will have
λmin (Vt ) = Θ(t · λmin (Σ0 )) when t is large enough. It

K-dependent lower bound. Currently, all the lower
bound results on (generalized) linear bandits have no dependence on K, the number of arms. The minimax lower
bound will be of particularly interest because all current
lower bound results assume that K ≤ d. Although it will
at most be a logarithm dependence on K, it is still a theoretically interesting question.
Randomized algorithms with optimal regret rate. As
opposed to the deterministic, UCB-style algorithms studied in this paper, randomized algorithms like EXP4 (Auer
et al., 2002b) and Thompson Sampling (Thompson, 1933)
have advantages in certain situations, for example, when reward observations are delayed (Chapelle & Li, 2012). Recently developed techniques for analyzing Bayes regret in
BLM bandits (Russo & Van Roy, 2014) may be useful to
analyze the cumulative regret considered here.

Generalized Linear Contextual Bandits

References
Abbasi-Yadkori, Yasin, Pál, Dávid, and Szepesvári, Csaba.
Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pp.
2312–2320, 2011.
Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford,
John, Li, Lihong, and Schapire, Robert E. Taming the
monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31th International Conference
on Machine Learning (ICML), pp. 1638–1646, 2014.
Agarwal, Deepak, Chen, Bee-Chung, Elango, Pradheep,
Motgi, Nitin, Park, Seung-Taek, Ramakrishnan, Raghu,
Roy, Scott, and Zachariah, Joe. Online models for content optimization. In Advances in Neural Information
Processing Systems 21, pp. 17–24, 2009.
Auer, Peter. Using confidence bounds for exploitationexploration trade-offs. The Journal of Machine Learning
Research, 3:397–422, 2002.

Chu, Wei, Li, Lihong, Reyzin, Lev, and Schapire, Robert E.
Contextual bandits with linear payoff functions. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 208–214,
2011.
Dani, Varsha, Hayes, Thomas P, and Kakade, Sham M.
Stochastic linear optimization under bandit feedback. In
Proceedings of the 21st Annual Conference on Learning
Theory (COLT), pp. 355–366, 2008.
Fahrmeir, Ludwig and Kaufmann, Heinz. Consistency and
asymptotic normality of the maximum likelihood estimator in generalized linear models. The Annals of Statistics, 13(1):342–368, 1985.
Filippi, Sarah, Cappe, Olivier, Garivier, Aurélien, and
Szepesvári, Csaba. Parametric bandits: The generalized
linear case. In Advances in Neural Information Processing Systems 23, pp. 586–594, 2010.

Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235–256, 2002a.

Lai, Tze Leung and Robbins, Herbert. Asymptotically efficient adaptive allocation rules. Advances in Applied
Mathematics, 6(1):4–22, 1985.

Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77,
2002b.

Lai, Tze Leung and Wei, Ching Zong. Least squares estimates in stochastic regression models with applications
to identification and control of dynamic systems. The
Annals of Statistics, 10(1):154–166, 1982.

Beygelzimer, Alina, Langford, John, Li, Lihong, Reyzin,
Lev, and Schapire, Robert E. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial
Intelligence and Statistics (AISTATS), pp. 19–26, 2010.

Lehmann, Erich Leo and Casella, George. Theory of Point
Estimation, volume 31 of Springer Texts in Statistics.
Springer Science & Business Media, 1998.

Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexandre B. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.
Brown, Lawrence D. Fundamentals of Statistical Exponential Families with Applications in Statistical Decision
Theory, volume 9 of Lecture Notes-Monograph Series.
Institute of Mathematical Statistics, 1986.
Bubeck, Sébastien and Cesa-Bianchi, Nicolo. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
Chapelle, Olivier and Li, Lihong. An empirical evaluation
of Thompson sampling. In Advances in Neural Information Processing Systems 24, pp. 2249–2257, 2012.
Chen, Kani, Hu, Inchi, and Ying, Zhiliang. Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs. The
Annals of Statistics, 27(4):1155–1163, 1999.

Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th
International Conference on World Wide Web (WWW),
pp. 661–670. ACM, 2010.
Li, Lihong, Chu, Wei, Langford, John, Moon, Taesup, and
Wang, Xuanhui. An unbiased offline evaluation of contextual bandit algorithms with generalized linear models. JMLR Workshop and Conference Proceedings, 26:
19–36, 2012.
McCullagh, Peter and Nelder, John A. Generalized Linear
Models, volume 37. CRC press, 1989.
Pollard, David. Empirical processes: Theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i–86. JSTOR, 1990.
Rusmevichientong, Paat and Tsitsiklis, John N. Linearly
parameterized bandits. Mathematics of Operations Research, 35(2):395–411, 2010.

Generalized Linear Contextual Bandits

Russo, Daniel and Van Roy, Benjamin. Learning to optimize via posterior sampling. Mathematics of Operations
Research, 39(4):1221–1243, 2014.
Sarkar, Jyotirmoy. One-armed bandit problems with covariates. The Annals of Statistics, 19(4):1978–2002,
1991.
Thompson, William R. On the likelihood that one unknown
probability exceeds another in view of the evidence of
two samples. Biometrika, 25(3–4):285–294, 1933.
Valko, Michal, Munos, Rémi, Kveton, Branislav, and
Kocák, Tomáš. Spectral bandits for smooth graph functions. In Proceedings of the 31th International Conference on Machine Learning (ICML), pp. 46–54, 2014.
Van der Vaart, Aad W. Asymptotic Statistics, volume 3.
Cambridge university press, 2000.
Vershynin, Roman. Introduction to the non-asymptotic
analysis of random matrices. In Eldar, Yonina C. and
Kutyniok, Gitta (eds.), Compressed Sensing: Theory and
Applications, pp. 210–268. Cambridge University Press,
2012.
Woodroofe, Michael. A one-armed bandit problem with a
concomitant variable. Journal of the American Statistical Association, 74(368):799–806, 1979.

