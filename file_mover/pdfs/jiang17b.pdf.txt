Uniform Convergence Rates for Kernel Density Estimation

Heinrich Jiang 1

Abstract
Kernel density estimation (KDE) is a popular
nonparametric density estimation method. We
(1) derive finite-sample high-probability density
estimation bounds for multivariate KDE under
mild density assumptions which hold uniformly
in x ∈ Rd and bandwidth matrices. We apply these results to (2) mode, (3) density level
set, and (4) class probability estimation and attain optimal rates up to logarithmic factors. We
then (5) provide an extension of our results under the manifold hypothesis. Finally, we (6) give
uniform convergence results for local intrinsic dimension estimation.

1. Introduction
KDE (Rosenblatt, 1956; Parzen, 1962) is a foundational aspect of nonparametric statistics. It is a powerful method to
estimate the probability density function of a random variable. Moreover, it is simple to compute and has played
a significant role in a very wide range of practical applications. Its convergence properties have been studied for a
long time with most of the work dedicated to its asymptotic
behavior or mean-squared risk (Tsybakov, 2008). However, there is still a surprising amount not yet fully understood about its convergence behavior. In this paper, we
focus on the uniform finite-sample facet of KDE convergence theory. We handle the multivariate KDE setting in
Rd which allows a d × d bandwidth matrix H. This generalizes the scalar bandwidth h > 0 i.e. H = h2 I. Such
a generalization is significant to multivariate statistics e.g.
Silverman (1986); Simonoff (1996).
Our work begins by using VC-based Bernstein-type uniform convergence bounds to attain finite-sample rates for
a fixed unknown density f over Rd (Theorem 1). These
bounds hold with high-probability under general assumptions on f and the kernel i.e. we only require f to be
1
Google.
Correspondence to:
rich.jiang@gmail.com>.
th

Heinrich Jiang <hein-

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

bounded as well as decay assumptions on the kernel functions. Moreover, these bounds hold uniformly over Rd and
bandwidth matrices H.
We then show the versatility of our results by applying it to
the related areas of KDE rates under `∞ , mode estimation,
density level-set estimation, and class probability estimation. We then extend our analysis to the manifold setting.
Finally, we provide uniform finite-sample results for local
intrinsic dimension estimation. Each of these contributions
are significant on their own.

2. Contributions and Related Works
`∞ bounds for KDE: It must first be noted that bounding
|fbh − f |∞ where fbh is the KDE of f for scalar h > 0
is a more difficult problem than for example bounding the
mean-squared error Ef [(fbh − f )2 ]. Gine & Guillon (2002)
and Einmahl & Mason (2005) give asymptotic convergence
results on KDE for |fˆh − Ef fˆh |∞ . In their work about
density clustering, Rinaldo & Wasserman (2010) extends
the results of the former to obtain high-probability finitesample bounds. This is to our knowledge the strongest and
most general uniform finite-sample result about KDE thus
far.
b
We
p show a general bound of form |fh (x) − f (x)| . x +
d
log n/nh where x is a function of the kernel and the
smoothness of f at x which holds with probability 1 − 1/n
uniformly over x ∈ Rd and h (Theorem 1). An almost
direct consequence is that if we take f to be α-Hölder continuous then under the optimal choice for h ≈ n−1/(2α+d) ,
we have |fˆh − f |∞ . n−α/(2α+d) with probability 1 − 1/n
(Theorem 2). This matches the known lower bound (Tsybakov, 2008).
When comparing our finite-sample results to that of Rinaldo & Wasserman (2010), there are a few notable differences. Our results hold uniformly across bandwidths and
the probability that the bounds hold are independent of the
bandwidth (in fact, holds with probability 1 − 1/n). Our
results also extends to general bandwidth matrix H.
This can be significant to analyze KDE-based procedures
with adaptive bandwidths– i.e. when the bandwidths
change depending on the region. Then the need for bounds
which hold simultaneously over bandwidth choices be-

Uniform Convergence Rates for Kernel Density Estimation

comes clear. Such an example includes adaptive or variable KDE (Terrell & Scott, 1992; Botev et al., 2010) which
extends KDE to bandwidths that vary over the data space.
Thus our result for uniform finite-sample KDE bounds can
be seen as a refinement to existing results.
Mode estimation Estimating the modes of a distribution
has a long history e.g. Parzen (1962); Chernoff (1964);
Eddy (1980); Silverman (1981); Cheng (1995); Abraham
et al. (2004); Li et al. (2007); Dasgupta & Kpotufe (2014);
Genovese et al. (2015); Jiang & Kpotufe (2017). The
modes can be viewed as the central tendancies of a distribution and this line of work has played a significant role in
areas such as clustering, image segmentation, and anomaly
detection.
Much of the early work focused on the estimator
argmaxx∈Rd fbh (x). While many useful insights have come
from studying this, it is difficult to algorithmically compute. Abraham et al. (2004) turned to the simple estimator argmaxx∈X fbh (x) and showed that it behaves asymptotically as argmaxx∈Rd fbh (x) where X is the data. In this
paper, we show that this estimator is actually a rate-optimal
estimator of the mode under finite samples with appropriate bandwidth choice. This would not have been possible
without the appropriate bounds on KDE. This approach is
similar to that of Dasgupta & Kpotufe (2014), who apply
their k-NN density estimation bounds to show that the kNN analogue of the estimator is rate-optimal.
Another approach to mode estimation that must be noted
is mean-shift (Fukunaga & Hostetler, 1975; Cheng, 1995;
Comaniciu & Meer, 2002; Arias-Castro et al., 2016), which
is a popular clustering algorithm amongst practitioners
based on performing a gradient-ascent of the KDE. Its theoretical analysis however is still far from complete; the difficulty comes from analyzing KDE’s ability to estimate gradients. Here we are focused on density estimation rather
than density derivative estimation so our results do not appear immediately applicable to mean-shift.
Density level-set estimation The problem of density-level
set estimation has been extensively studied e.g. Carmichael
et al. (1968); Hartigan (1975); Cuevas & Fraiman (1997);
Tsybakov (1997); Cadre (2006); Rigollet & Vert (2009);
Singh et al. (2009); Rinaldo & Wasserman (2010); Steinwart (2011); Jiang (2017). It involves estimating {x :
f (x) ≥ λ} for some λ > 0 and density f based on samples drawn from f . This turns out to be one of the earliest
and still currently most popular means of modeling clusters
in the context of density-based clustering. The level-sets
also influenced much of the work on hierarchical clustering (Chaudhuri & Dasgupta, 2010).
Naturally, we must use some density estimator to get a handle on λ. It turns out that in order to obtain the most gen-

eral uniform recovery bounds (e.g. finite-sample Hausdorff
rates (Singh et al., 2009)), one also needs similar uniform
density estimation bounds. The strongest known results
thus far use density estimators that are often impractical
(e.g. histogram density estimator) in order to obtain these
theoretical rates over a practical one such as KDE. Much
of the work, especially ones using more practical density
estimators have focused on bounding metrics such as symmetric set difference, which are computed as an expectation over f . This is considerably weaker than the Hausdorff metric, which imposes a uniform guarantee over each
estimated point and each point in the level-set.
We show that a simple KDE-based estimator is consistent
under the Hausdorff metric; moreoever when the bandwidth is appropriately chosen, it attains the minimax optimal rate established by Tsybakov (1997).
Class probability estimation Class probability estimation
involves estimating the probability distribution over a set
of classes for a given input. In other words, it is an approach to classification which involves first estimating the
marginal density f (Y |X) (where X is the observation and
Y is its category) and then choosing the category with highest probability. This density-based approach to classification has been studied in many places under nonparametric assumptions. e.g. Rigollet (2007); Chaudhuri et al.
(2009). However, there are still aspects about its convergence properties that haven’t been fully understood. In the
current work, we give uniform rates on the approximation
of f (Y |X). Much of the related work assume the binary
classification case and derive a hard classifier based on the
marginal density and compare the risk between that and
the Bayes-optimal classifier. Our work differs in that we
give uniform bounds on the recovery of the marginal density, which is a considerably stronger notion of consistency.
This is important in situations where a worst-case bound on
classifier performance is required.
Density Estimation on Manifolds Density estimation on
manifolds has received much less attention than the fulldimensional counterpart. However, understanding density
estimation in situations where the intrinsic dimension can
be much lower than the ambient dimension is becoming
ever more important: modern systems are able to capture
data at an increasing resolution while the number of degrees of freedom stays relatively constant. One of the limiting aspects of density-based approaches is their performance in high dimensions. It takes an exponential in dimension number of samples to estimate the density – this
is the so-called curse of dimensionality. Here we give results whose rates of convergence depend on the dimension
of the manifold dM compared to a much higher ambient dimension d; thus the convergence properties become much
more attractive under the manifold hypothesis.

Uniform Convergence Rates for Kernel Density Estimation

Local Intrinsic Dimension Estimation Many learning algorithms require the intrinsic dimension as an input in order
to take advantage of the lower dimensional structure that
arises. There has been much work on estimating the intrinsic dimension of the data given finite samples e.g. (Kégl,
2003). However, the more interesting problem of estimating the local intrinsic dimension has received much less attention. The bulk of the work in this area e.g. (Costa et al.,
2005; Houle, 2013; Amsaleg et al., 2015) provide interesting estimators, but are unable to establish strong finitesample guarantees under nonparametric assumptions. In
this paper, we consider a simple notion of local intrinsic
dimension based on the doubling dimension and utilize a
simple estimator. We then give a uniform finite-sample
convergence result for the estimator under nonparametric
assumptions. To the best of our knowledge, this is perhaps
the strongest finite-sample result obtained this far for this
problem.

3. Background and Setup
Definition 1. Let f be a probability density over Rd with
corresponding distribution F. Let X = {X1 , ..., Xn } be n
i.i.d. samples drawn from it and let FnP
denote the empirical
n
distribution w.r.t. X. i.e. Fn (A) = n1 i=1 1{Xi ∈ A}.
We only require that f is bounded.
Assumption 1. ||f ||∞ < ∞.
Definition 2. Define kernel function K : Rd → R≥0 where
R≥0 denotes the non-negative real numbers such that
Z
K(u)du = 1.
Rd

We make the following mild regularity assumptions on K.
Assumption 2. (Spherically Symmetric and nonincreasing) There exists non-increasing function
k : R≥0 → R≥0 such that K(u) = k(|u|) for u ∈ Rd .
Assumption 3. (Exponential Decays) There exists
ρ, Cρ , t0 > 0 such that for t > t0 ,
k(t) ≤ Cρ · exp(−tρ ).
Remark 1. These assumptions allow the popular kernels
such as Gaussian, Exponential, Silverman, uniform, triangular, tricube, Cosine, and Epanechnikov.
Assumption 3 implies the next result which will be useful
later on. The proof is elementary and is omitted.
Lemma 1. For all m > 0, we have
Z
K(u)|u|m du < ∞.
Rd

Definition 3 (Bandwidth matrix). H is a valid bandwidth
matrix if it is a positive definite and symmetric d×d matrix.
H0 is a unit bandwidth matrix if it is a valid bandwidth
matrix and |H0 | = 1.
Let σ1 (H0 ) ≥ · · · ≥ σd (H0 ) > 0 be the eigenvalues of
H0 .
Remark 2. In the scalar bandwidth case, H0 = I.
Remark 3. It will be useful later on that if H = h2 H0
where H0 is a unit bandwidth, then for u ∈ Rd ,
p
p
σd (H0 ) · h · |u| ≤ |H1/2 u| ≤ σ1 (H0 ) · h · |u|.
Definition 4 (Kernel Density Estimation). Given a kernel
K and h > 0 and H0 , the KDE for H := h2 H0 is given
by
n


X
1
K H−1/2 (x − Xi )
fbH (x) := · |H|−d/2
n
i=1
!
n
X
1
H0 −1/2 (x − Xi )
=
K
.
n · hd i=1
h

4. Uniform Convergence Bounds
The following is a paraphrase of Bousquet et al. (2004),
which was given in Chaudhuri & Dasgupta (2010).
Lemma 2. Let G be a class of functions from X to {0, 1}
with VC dimension d < ∞, and F a probability distribution on X. Let E denote expectation with respect to F.
Suppose n points are drawn independently at random from
F; let En denote expectation with respect to this sample.
Then with probability at least 1 − 1/n, the following holds
for all g ∈ G:
p
p
− min{βn En g, βn2 + βn Eg}
p
p
≤ Eg − En g ≤ min{βn2 + βn En g, βn Eg},
p
where βn ≥ 4(d + 3) log 2n/n.
Chaudhuri & Dasgupta (2010) takes G to be the indicator
functions over balls. Dasgupta & Kpotufe (2014) uses this
to provide similar bounds for the k-NN density estimator
as in this paper. Here, we extend this idea to ellipsoids
by taking G = B (the indicator functions over ellipsoids),
which has VC dimension (d2 + 3d)/2 as determined by
Akama & Irie (2011).
Lemma 3. Define ellipsoid BH0 (x, r) := {x0 ∈ Rd :
|H0 −1/2 (x − x0 )| ≤ r}, and B := {BH0 (x, r) : x ∈
Rd , r > 0, H0 is a unit bandwidth}. With probability at
least 1 − 1/n, the following holds uniformly for every
B ∈ B and γ ≥ 0:
√
F(B) ≥ γ ⇒ Fn (B) ≥ γ − βn γ − βn2 ,
√
F(B) ≤ γ ⇒ Fn (B) ≤ γ + βn γ + βn2 ,

Uniform Convergence Rates for Kernel Density Estimation

p
where βn = 8d log n/n.
Remark 4. We could have alternatively used a fixed confidence δ so that our results would hold with probability at
least 1 − δ. This would p
only require a modification of βn
(e.g. by taking βn = 4d 2(log n + log(1/δ))/n). In this
paper, we have simply taken δ = 1/n.

In order to prove Theorem 1, we first define the following
two functions which serve to approximate K as a step-wise
linear combination of uniform kernels.
Definition 6. Let ∆ > 0.
K∆ (u) :=

∞
X
(k(j∆) − k((j + 1)∆)) · 1 {|u| < j∆} ,
j=0

∞
X
K∆ (u) :=
(k(j∆) − k((j + 1)∆)) · 1 {|u| < (j + 1)∆} .

5. KDE Bound
Define the following which characterizes how much the
density can respectively decrease and increase from x in
B(x, r).

j=0

Then it is clear that the following holds for all ∆ > 0.
K∆ (u) ≤ K(u) ≤ K∆ (u).

Definition 5.
ǔx (r) := f (x) −
ûx (r) :=

inf

x0 ∈B(x,r)

sup

f (x0 ).

f (x0 ) − f (x).

x0 ∈B(x,r)

The first are general upper and lower bounds for fbH .
Theorem 1. [Uniform Upper and Lower Bounds for fbH ]
Let vd be the volume of the unit ball in Rd . Then the following holds uniformly in x ∈ Rd ,  > 0, unit bandwidths H0 ,
and h > (log n/n)1/d with probability at least 1 − 1/n.
Let H := h2 H0 .
r
log n
b
,
fH (x) > f (x) −  − C
n · hd
p
R
if Rd K(u) · ǔx (h|u|/ σd (H0 ))du < , and
r
log n
fbH (x) < f (x) +  + C
,
n · hd
p
R
if Rd K(u) · ûx (h|u|/ σd (H0 ))du < , where C =
p

R∞
8d ·vd · ||f ||∞ 0 k(t) · td/2 dt + 1 + 64d2 · k(0).
√
Remark 5. The conditions on ǔx (h|u|/ σd ) and
√
ûx (h|u|/ σd ) can be interpreted as a bound on
their
expectations over the probability measure K (i.e.
R
K(u)du
= 1). These conditions can be satisfied by
d
R
taking h sufficiently small.
Remark 6. The parameter  allows us the amount of slack
in the estimation errors. This is useful in a few aspects.
Oftentimes, we don’t require tight bounds, especially when
reasoning about low density regions thus having a large 
allows us to satisfy the conditions more easily. In the case
that we want tight bounds, the additive error controlled by
the pointwise smoothness of the density can be encoded in
, so to not require global smoothness assumptions.
Remark 7. Besides the ||f ||∞ factor, the value of C at the
end of the theorem statement is a quantity which can be
known without any a priori knowledge of f . We can bound
||f ||∞ in terms of known quantities given smoothness assumptions near argmaxx f (x). This is used in later results
where knowing a value of C is important.

The next Lemma is useful in computing the expectations of
functions over the kernel measure.
Lemma 4. Suppose g is an integrable function over R≥0
and let vd denote the volume of a unit ball in Rd . Then
Z
Z ∞
K(u)g(|u|)du = vd ·
k(t) · td g(|u|)du.
Rd

0

Proof of Lemma 4. Let Sd = 2π d/2 /Γ(d/2) denote the
surface area of the unit ball in Rd .
Z
Z ∞
K(u)g(|u|)du = Sd
k(t) · td−1 · g(|t|)dt
Rd
0
Z
Z ∞
Sd ∞
(k(t) · g(|t|))td dt = vd
(k(t) · g(|t|))td dt,
=
d 0
0
where the second last equality follows from integration by
parts and the last follows from the fact that vd = Sd /d.
The following follows immediately from Lemma 4.
Corollary 1.
Z
Z ∞
K(u)ǔx (h|u|)du = vd ·
k(t) · td · ǔx (ht)dt,
d
ZR
Z0 ∞
K(u)ûx (h|u|)du = vd ·
k(t) · td · ûx (ht)dt.
Rd

0

Proof of Theorem 1. Assume that the event that Lemma 3
holds, which occurs with probability at least 1 − 1/n. We
first show the lower bound for fbH (x). Define
!
n
H0 −1/2 (x − Xi )
1 X
b
K∆
.
f∆,H (x) :=
n · hd i=1
h
It is clear that fbH (x) ≥ fb∆,H (x) for all x ∈ Rd . Let us use
the following shorthand ∆k,j := k(j∆). We have
∞
1 X
(∆k,j − ∆k,j+1 ) · Fn (BH0 (x, jh∆)) .
fb∆,H (x) = d
h j=0

Uniform Convergence Rates for Kernel Density Estimation

We next get a handle on each Fn (BH0 (x, jh∆)). We have
d

F (BH0 (x, jh∆)) ≥ vd · (jh∆) · Fj ,
p
where Fj := max{0, f (x) − ǔx (jh∆/ σd (H0 ))}. Thus,
by Lemma 3, we have
Fn (BH0 (x, jh∆))

p
√
≥ vd · (jh∆)d · Fj − βn vd · (jh∆)d/2 · Fj − βn2
p
≥ vd · (jh∆)d · Fj − βn vd · ||f ||∞ · (jh∆)d/2 − βn2 .

Therefore,

∞
X

−

− βn2

jh∆

d

(∆k,j − ∆k,j+1 )(j∆) · ǔx

j=0

p

fb∆,H (x)
∞
1 X
= d
(∆k,j − ∆k,j+1 ) · Fn (BH0 (x, (j + 1)h∆)) .
h j=0
F(BH0 (x, jh∆)) ≤ vd · (jh∆)d · Fj
p
where Fj = min{||f ||∞ , f (x) + û(jh∆/ σd (H0 ))}.
Thus by Lemma 3 we have

j=0

βn

It is clear that fbH (x) ≤ fb∆,H (x) for all x ∈ Rd and

We next get a handle on each Fn (BH0 (x, jh∆)). We have

fb∆,h (x)
∞
X
≥ vd
(∆k,j − ∆k,j+1 )(j∆)d · f (x)

− vd

This gives us the lower bound. Next we derive an upper
bound. Let us redefine


n
1 X
x − Xi
b
f∆,H (x) :=
Km
.
n · hd i=1
h

p

!

Fn (BH0 (x, jh/m))
≤ vd (jh∆)d Fj + βn (jh∆)d/2

σd (H0 )

∞
vd · ||f ||∞ X
·
(∆k,j − ∆k,j+1 )(j∆)d/2
hd/2
j=0

k(0)
.
hd

p

vd · Fj + βn2 .

Using this, we now have
fb∆,H (x)
∞
X
≤ vd
(∆k,j − ∆k,j+1 )((j + 1))d · f (x)
j=0

We handle each term separately. For the first term, we have
lim vd

∆→0

Z

∞
X

0

Finally, we have
∞
X

(∆k,j − ∆k,j+1 )(j∆)d/2

j=0
∞

=

p

(j + 1)h∆
p
σd (H0 )

!

∞
vd · ||f ||∞ X
·
(∆k,j − ∆k,j+1 )((j + 1)∆)d/2
hd/2
j=0

k(0)
.
hd
We proceed the same way as the other direction. Thus taking ∆ → 0 we get
p
Z ∞
vd · ||f ||∞
β
n
fb∆,H (x) ≤ f (x) +  +
·
k(t) · td/2 dt
hd/2
0
2 k(0)
+ βn d .
h
The result follows.
+ βn2

where the last equality follows from Lemma 4. Next, we
have
!
∞
X
jh∆
d
lim vd
(∆k,j − ∆k,j+1 )(j∆) · ǔx p
∆→0
σd (H0 )
j=0
Z ∞
p
= vd
k(t) · td · ǔx (th/ σd (H0 ))dt < .

Z

+

βn

k(t)td dt = 1.

0

lim

j=0

(∆k,j − ∆k,j+1 )(j∆)d

j=0
∞

= vd

∆→0

∞
X
+ vd
(∆k,j − ∆k,j+1 )((j + 1)∆)d · ûx

k(t) · td/2 dt < ∞.

0

Thus, taking ∆ → 0 we get
p
Z ∞
βn vd · ||f ||∞
b
fH (x) ≥ f (x) −  −
·
k(t) · td/2 dt
hd/2
0
k(0)
− βn2 d .
h

6. Sup-norm Bounds for KDE
Theorem 2. [`∞ bound for α-Hölder continuous functions] If f is Hölder-continuous (i.e. |f (x) − f (x0 )| ≤
Cα |x − x|α for x, x0 ∈ Rd and 0 < α ≤ 1), then there
exists positive constant C 0 ≡ C 0 (C, Cα , α, K) such that
the following holds with probability at least 1 − 1/n uniformly in h > (log n/n)1/d and unit bandwidths H0 . Let
H := h2 H0 .
!
r
α
h
log
n
0
sup |fbH (x) − f (x)| < C ·
+
.
n · hd
σd (H0 )α/2
x∈Rd

Uniform Convergence Rates for Kernel Density Estimation

Remark 8. Taking h = n−1/(2α+d) in the above r.h.s. optimizes the rates to n−α/(2α+d) (ignoring log factors).
Remark 9. We can attain similar results (although not
uniform in bandwidth) by a straightforward application
of Theorem 3.1 of Sriperumbudur & Steinwart (2012) or
Proposition 9 of Rinaldo & Wasserman (2010).

7. Mode Estimation Results
The goal of this section is to utilize the KDE to estimate
the mode of a uni-modal distribution from its samples. We
borrow the estimator from Abraham et al. (2004)
x̂ := argmaxx∈X fbH (x),
where H := h2 I.
We adopt the mode estimation framework assumptions
from Dasgupta & Kpotufe (2014) which are summarized
below.
Definition 7. x0 is a mode of f if f (x) < f (x0 ) for all
x ∈ B(x0 , r)\{x0 } for some r > 0.
Assumption 4. • f has a single mode x0 .
• f is twice differentiable in a neighborhood around x0 .
• f has a negative-definite Hessian at x0 .
These assumptions lead to the following.
Lemma 5 ((Dasgupta & Kpotufe, 2014)). Let f satisfy Assumption 4. Then there exists Ĉ, Č, r0 , λ > 0 such that the
following holds.
Č · |x0 − x|2 ≤ f (x0 ) − f (x) ≤ Ĉ · |x0 − x|2
for all x ∈ Ax where A0 is a connected component of {x :
f (x) ≥ λ} and A0 contains B(x0 , r0 ).
We obtain the following result for the estimation error of x̂.
Theorem 3. Suppose that Assumptions 1, 2, 3, 4 hold.
Choose h such that (log n)2/ρ · h → 0 and log n/(nhd ) →
0 as n → ∞. Then, for n sufficiently large depending on
d, ||f ||∞ , K, Ĉ, Č, r0 the following holds with probability
least 1 − 1/n.
)
(
r
log n
32Ĉ
4/ρ
2
2
(log n) · h , 17 · C
.
|x̂ − x0 | ≤ max
n · hd
Č
Remark 10. Taking h = n−1/(4+d) optimizes the above
expression so that |x̂ − x0 | . n−1/(4+d) (ignoring log factors) which matches the lower bound rate for mode estimation as established in Tsybakov (1990).
Remark 11. This result can be extended to multi-modal
distributions as done by Dasgupta & Kpotufe (2014) by using the connected components of nearest neighbor graphs
at appropriate empirical density levels to isolate the modes
away from each other.

8. Density Level Set Estimation Results
In this section, we estimate the density level set Lf (λ) :=
{x : f (x) ≥ λ} where λ > 0 is given. We make the following standard regularity assumptions e.g. (Singh et al.,
2009). To simplify the analysis, let us take H = h2 I. It is
clear that the results that follow can be extended to arbitrary
H0 .
Assumption 5 (β-regularity). Let 0 < β < ∞. There exists 0 < λ0 < λ and Čβ , Ĉβ , r̄ > 0 such that the following
holds for x ∈ Lf (λ0 )\Lf (λ).
Čβ · d(x, Lf (λ))β ≤ λ − f (x) ≤ Ĉβ · d(x, Lf (λ))β ,
where d(x, A) := inf x0 ∈A {|x − x0 |}. and B(Lf (λ), r̄) ⊆
Lf (λ0 ) where B(A, r) := {x : d(x, A) ≤ r}.
Then we consider following estimator.
)
(
r
log n
b
b
.
Lf := x ∈ X : fH (x) > λ − C̃
n · hd
where C̃ is obtained by taking C and replacing the ||f ||∞
factor by 1+5 maxx∈Xn0 fbH (x) where Xn0 is a fixed sample of size n0 . Then, C̃ can be viewed as a constant w.r.t.
n and can be known without any a priori knowledge of f
while ensuring that C̃ ≥ max{1, 2C}.
We use the following Hausdorff metric.
Definition 8 (Hausdorff Distance).
dH (A, B) := max{sup d(x, B), sup d(x, A)}.
x∈A

x∈B

Theorem 4. Suppose that Assumptions 1, 2, 3, 5 hold and
that f is α-Hölder continuous for some 0 < α ≤ 1. Choose
h such that (log n)2/ρ · h → 0 and log n/(nhd ) → 0
as n → ∞. Then, for n sufficiently large depending on
d, C, C̃, K, Ĉβ , Čβ , β, r̄ the following holds with probability least 1 − 1/n.
1/(2β) !

log
n
b f , Lf (λ)) ≤ C 00 (log n)2/ρ · h +
dH (L
,
n · hd
where C 00 ≡ C 00 (C, C̃, Ĉβ , Čβ , C̃, β).
Remark 12. Choosing h = n−β/(2β+d) gives us a densitylevel set estimation rate of O(n−1/(2β+d) ). This matches
the lower bound (ignoring log factors) determined by Tsybakov (1997).
Remark 13. This result can be extended so that we can recover each component separately (i.e. identify which points
correspond to which connected components of Lf (λ)).
Similar to the mode estimation result, this can be done using nearest neighbor graphs at the appropriate level to isolate the connected components of Lf (λ) away from each
other. This has been done extensively in the related area of
cluster tree estimation e.g. (Chaudhuri & Dasgupta, 2010).

Uniform Convergence Rates for Kernel Density Estimation

Remark 14. The global α-Hölder continuous assumption
is not required and is only here for simplicity. Smoothness
in a neighborhood around a maximizer of f is sufficient so
that for n0 large enough, C̃ ≥ 2C.

works e.g. Audibert et al. (2007); Chaudhuri & Dasgupta
(2014). Note that misclassification rate for a hard classifier
is a slightly different but very related to what is done here,
which is directly estimating the marginal density.

9. Class Probability Estimation Results

10. Extension to Manifolds

We consider the setting where we have observations from
compact subset X ⊂ Rd and labels y ∈ {1, ..., L}. Given
a label y, an instance x ∈ Rd has density fy (x) where
fy is w.r.t. the uniform measure on Rd . Instance-label
pairs (X, Y ) are thus drawn according to a mixture distribution where Y is chosen from {1, ..., L} with correspondPL
ing probabilities π1 , ..., πL (i.e. j=1 πj = 1) and then X
is chosen according to fY .

We make the following regularity assumptions which are
standard among works in manifold learning e.g. (Baraniuk
& Wakin, 2009; Genovese et al., 2012; Balakrishnan et al.,
2013).

Then given x ∈ X , we can define the marginal distribution
as follows.
g(x) := (g1 (x), ..., gL (x)),

• M is a dM -dimensional smooth compact Riemannian
manifold without boundary embedded in compact subset X ⊆ RD .
• The volume of M is bounded above by a constant.
• M has condition number 1/τ , which controls the curvature and prevents self-intersection.

πy fy (x)
.
gy (x) := f (Y = y|X = x) = P
j πj fj (x)
The goal of class probability estimation is to learn g based
on samples (x1 , y1 ), ..., (xn , yn ). We define our estimator
naturally as follows. Let fbh,y be the KDE of fy w.r.t. to
bandwidth matrix H = h2 I.
ĝh (x) := (ĝh,1 (x), ..., ĝh,L (x)),
n
π̂y fbh,y (x)
1X
ĝh,y (x) := P
and π̂y :=
1[y = yi ].
b
n j=1
j π̂j fh,j (x)
We make the following regularity assumption on fy .
Assumption 6. (α-Hölder densities) For each y
{1, ..., L} and x ∈ Rd we have

Assumption 7. F is supported on M where:

∈

|fy (x) − fy (x0 )| ≤ Cα |x − x0 |α ,
where 0 < α ≤ 1.
We state the result below:

Let f be the density of F with respect to the uniform measure on M .
In this section, we assume that our density estimator is w.r.t.
to dM instead of the ambient dimension d.
!
n
X
H0 −1/2 (x − Xi )
1
b
K
.
fH (x) :=
n · hdM i=1
h
Remark 16. It is then the case that we must know the intrinsic dimension dM . There are numerous known techniques for doing so e.g. (Kegl, 2002; Levina & Bickel,
2004; Hein & Audibert, 2005; Farahmand et al., 2007).
Next, we need the following guarantee on the volume of
the intersection of a Euclidean ball and M ; this is required
to get a handle on the true mass of the ball under F in
later arguments. The upper and lower bounds follow from
Chazal (2013) and Lemma 5.3 of Niyogi et al. (2008). The
proof can be found in (Jiang, 2017).

Theorem 5. Suppose that Assumptions 1, 2, 3, 6 hold.
Then for n sufficiently large depending on miny πy , there
exists positive constants C 00 ≡ C 00 (L, C, Cα , α, K) and
C̃ ≡ C̃(miny πy , L) such that the following holds with
probability at least 1−C̃/n uniformly in h > (log n/n)1/d .
!
r
log
n
.
sup ||ĝh (x) − g(x)||∞ ≤ C 00 · hα +
n · hd
x∈Rd

Lemma 6 (Ball Volume). If 0 < r < min{τ /4dM , 1/τ },
and x ∈ M then

Remark 15. This corresponds to an optimized rate of
e −α/(2α+d) ). This matches the lower bounds up to
O(n
log factors for misclassification as established in related

Theorem 6. [Manifold Case Uniform Upper
and Lower Bounds for fbH ] There exists CM ≡
CM (dM , d, K, ||f ||∞ , τ ) such that the following holds

1 − τ 2 r2 ≤

voldM (B(x, r) ∩ M )
≤ 1 + 4dM r/τ,
vdM rdM

where voldM is the volume w.r.t. the uniform measure on
M.
We then give analogues to Theorem 1 and Theorem 2.

Uniform Convergence Rates for Kernel Density Estimation

uniformly in x ∈ M ,  > 0, unit bandwidths H0 , and
h > (log n/n)1/dM with probability at least 1 − 1/n. Let
H := h2 H0 .
!
r
log
n
2
,
fbH (x) > f (x) −  − CM h +
n · hdM
if

R
Rd

K(u) · ǔx (h|u|/

p
σd (H0 ))du < , and
r

fbH (x) < f (x) +  + CM
if

R
Rd

K(u) · ûx (h|u|/

h+

log n
n · hdM

,

Remark 17. The extra h2 and h term in the lower and
upper bounds respectively come from the approximation of
the volume of the full-dimensional balls w.r.t. the uniform
measure on M in Lemma 6.
Proof Sketch of Theorem 6. The proof mirrors that of the
full dimensional case so we only highlight the differences.
For the lower bound, instead of

we have

= vdM (jhδ)

dM

dM +2

Fj − h

Fj (1 − τ 2 (jhδ)2 )
2

vdM τ ||f ||∞ (jδ)

We can then define our estimator of local intrinsic dimension at x ∈ X as follows:


Fn (B(x, 2h))
c
IDn,h (x) := log2
.
Fn (B(x, h))
The following is a uniform convergence result for
cn,h (x).
ID
Theorem 8. Define the following


F(B(x, 2h))
IDh (x) := log2
.
F(B(x, h))

d

F (BH0 (x, jhδ)) ≥ vd · (jhδ) · Fj ,

dM

In this section, we only assume a distribution F on Rd
whose support is defined as X := {x ∈ Rd : F(B(x, h)) >
0 ∀h > 0} and X is assumed to be compact. We use the
following notion of intrinsic dimension, which is based on
the doubling dimension and adapted from previous works
such as Houle (2013).
Definition 9. For x ∈ X , define the following local intrinsic dimension wherever the quantity exists


F(B(x, 2h))
.
ID(x) := lim log2
h→0
F(B(x, h))

!

p
σd (H0 ))du < .

F (BH0 (x, jhδ)) ≥ vdM (jhδ)

11. Local Intrinsic Dimension Estimation

dM +2

.

The first term can be treated in the same way as before,
while the second term contributes in an extra term with an
h2 factor after taking the total summation.
For the upper bound, instead of
F(BH0 (x, jhδ)) ≤ vd · (jhδ)d · Fj ,
we have
F(BH0 (x, jhδ)) ≤ vdM · (jhδ)dM · Fj (1 + 4dM (jhδ)/τ ).
Similary, this contributes an extra term with an h factor
after taking the total summation.
Theorem 7. [Manifold Case `∞ bound for α-Hölder continuous functions] If f is Hölder-continuous (i.e. |f (x) −
f (x0 )| ≤ Cα |x − x|α for x, x0 ∈ Rd with 0 <
0
α ≤ 1), then there exists positive constant CM
≡
0
CM (||f ||∞ , Cα , α, K, τ, dM , d, σd (H0 )) such that the following holds with probability at least 1 − 1/n uniformly in
h satisfying (log n/n)1/dM < h < 1.
!
r
log
n
0
α
sup |fbH (x) − f (x)| < CM · h +
.
n · hdM
x∈M

Suppose that
>
0 and n satisfy βn
<
p h
1
0
F(B(x0 , h)). Then the following holds
10 inf x ∈X
with probability at least 1 − 1/n uniformly in x ∈ X .
cn,h (x) − IDh (x)| ≤
|ID

inf

x0 ∈X

6βn
p
.
F(x0 , 2h)

Remark 18. The r.h.s. goes to 0 as n → ∞. Moreover, if IDh (x) converges to ID(x) uniformly in x ∈ X ,
then simultaneously taking h → 0 and n → ∞ such that

−1
p
βn · inf x0 ∈X F(x0 , 2h)
→ 0 gives us a finite-sample
uniform convergence rate for local intrinsic dimension estimation.
Remark 19. If we assume a global intrinsic dimension dp
<
0 and a density, the condition βn
1
0 , h)) can be interpreted as log n → 0
0 ∈X
inf
F(B(x
x
d
10
q nh 0
log n
and the r.h.s. of the bound is on the order of nh
d0 .
In fact, this result is similar to the uniform convergence
results for the KDE for
qestimating
 the smoothed density.
log
n
e.g. |fbh − fh |∞ = O
when (ignoring some log
nhd
factors) nhd → ∞ where fh is the density convolved with
the uniform kernel with bandwidth h. It is interesting that
an analogous result comes up when estimating the intrinsic
dimension with our notion of smoothed ID.

Uniform Convergence Rates for Kernel Density Estimation

References
Abraham, C., Biau, G., and Cadre., B. On the asymptotic
properties of a simple estimate of the mode. ESAIM:
Probability and Statistics, 2004.

Chaudhuri, Probal, Ghosh, Anil K, and Oja, Hannu. Classification based on hybridization of parametric and nonparametric classifiers. IEEE transactions on pattern
analysis and machine intelligence, 31(7):1153–1164,
2009.

Akama, Yohji and Irie, Kei. Vc dimension of ellipsoids.
arxiv, 2011.

Chazal, F. An upper bound for the volume of geodesic balls
in submanifolds of euclidean spaces. 2013.

Amsaleg, Laurent, Chelly, Oussama, Furon, Teddy, Girard,
Stéphane, Houle, Michael E, Kawarabayashi, Ken-ichi,
and Nett, Michael. Estimating local intrinsic dimensionality. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 29–38. ACM, 2015.

Cheng, Y. Mean shift, mode seeking, and clustering. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 1995.

Arias-Castro, Ery, Mason, David, and Pelletier, Bruno. On
the estimation of the gradient lines of a density and the
consistency of the mean-shift algorithm. Journal of Machine Learning Research, 2016.
Audibert, Jean-Yves, Tsybakov, Alexandre B, et al. Fast
learning rates for plug-in classifiers. The Annals of statistics, 35(2):608–633, 2007.
Balakrishnan, S., Narayanan, S., Rinaldo, A., Singh, A.,
and Wasserman, L. Cluster trees on manifolds. In Advances in Neural Information Processing Systems, pp.
2679–2687, 2013.
Baraniuk, Richard G and Wakin, Michael B. Random projections of smooth manifolds. Foundations of computational mathematics, 9(1):51–77, 2009.
Botev, Zdravko I, Grotowski, Joseph F, Kroese, Dirk P,
et al. Kernel density estimation via diffusion. The Annals
of Statistics, 38(5):2916–2957, 2010.
Bousquet, O., Boucheron, S., and Lugosi, G. Introduction
to statistical learning theory. Lecture Notes in Artificial
Intelligence, 2004.
Cadre, Benoit. Kernel estimation of density level sets.
Journal of Multivariate Analysis, 2006.
Carmichael, J., George, G., and Julius, R. Finding natural
clusters. Systematic Zoology, 1968.
Chaudhuri, K. and Dasgupta, S. Rates for convergence for
the cluster tree. Advances in Neural Information Processing Systems, 2010.
Chaudhuri, Kamalika and Dasgupta, Sanjoy. Rates of convergence for nearest neighbor classification. In Advances
in Neural Information Processing Systems, pp. 3437–
3445, 2014.

Chernoff, Herman. Estimation of the mode. Annals of the
Institute of Statistical Mathematics, 1964.
Comaniciu, D and Meer, P. Mean shift: A robust approach
toward feature space analysis. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2002.
Costa, Jose A, Girotra, Abhishek, and Hero, AO. Estimating local intrinsic dimension with k-nearest neighbor
graphs. In Statistical Signal Processing, 2005 IEEE/SP
13th Workshop on, pp. 417–422. IEEE, 2005.
Cuevas, A. and Fraiman, R. A plug-in approach to support
estimation. Annals of Statistics, 1997.
Dasgupta, S. and Kpotufe, S. Optimal rates for k-nn density
and mode estimation. Advances in Neural Information
Processing Systems, 2014.
Eddy, William F. Optimum kernel estimators of the mode.
Annals of Statistics, 1980.
Einmahl, Uwe and Mason, David M. Uniform in bandwidth consistency of kernel-type function estimators.
Annals of Statistics, 2005.
Farahmand, A., Szepesvari, C., and Audibert, J. Manifoldadaptive dimension estimation. ICML, 2007.
Fukunaga and Hostetler. The estimation of the gradient of
a density function, with applications in pattern recognition. IEEE Transactions on Information Theory, 1975.
Genovese,
Christopher,
Perone-Pacifico,
Marco,
Verdinelli, Isabella, and Wasserman, Larry. Minimax manifold estimation. Journal of machine learning
research, 13(May):1263–1291, 2012.
Genovese, Christopher R., Verdinelli, Marco PeronePacificoand Isabella, and Wasserman, Larry. Nonparametric inference for density modes. Series B Statistical Methodology, 2015.
Gine and Guillon. Rates of strong uniform consistency
for multivariate kernel density estimators. Ann. Inst. H.
Poincare Probab. Statist., 2002.

Uniform Convergence Rates for Kernel Density Estimation

Hartigan, J. Clustering algorithms. Wiley, 1975.
Hein, Matthias and Audibert, Jean-Yves. Intrinsic dimensionality estimation of submanifolds in rd. ICML, 2005.
Houle, Michael E. Dimensionality, discriminability, density and distance distributions. In Data Mining Workshops (ICDMW), 2013 IEEE 13th International Conference on, pp. 468–473. IEEE, 2013.
Jiang, Heinrich. Density level set estimation on manifolds with dbscan. International Conference on Machine
Learning (ICML), 2017.
Jiang, Heinrich and Kpotufe, Samory. Modal-set estimation with an application to clustering. AISTATS, 2017.
Kegl, Balazs. Intrinsic dimension estimation using packing
numbers. NIPS, 2002.
Kégl, Balázs. Intrinsic dimension estimation using packing
numbers. In Advances in neural information processing
systems, pp. 697–704, 2003.
Levina, Elizaveta and Bickel, Peter J. Maximum likelihood
estimation of intrinsic dimension. NIPS, 2004.
Li, J., Ray, S., and Lindsay, B. A nonparametric statistical
approach to clustering via mode identification. Journal
of Machine Learning Research, 2007.
Niyogi, P., Smale, S., and Weinberger, S. Finding the homology of submanifolds with high confidence from random samples. Discrete and Computational Geometry,
2008.
Parzen, Emanual. On estimation of a probability density
function and mode. Annals of mathematical statistics,
1962.
Rigollet, P. and Vert, R. Fast rates for plug-in estimators of
density level sets. Bernoulli, 15(4):1154–1178, 2009.
Rigollet, Philippe. Generalization error bounds in semisupervised classification under the cluster assumption.
Journal of Machine Learning Research, 8(Jul):1369–
1392, 2007.
Rinaldo, A. and Wasserman, L. Generalized density clustering. Annals of Statistics, 2010.
Rosenblatt, M. Remarks on some nonparametric estimates
of a density function. Ann. Math. Statist., 1956.
Silverman, B. Density Estimation for Statistics and Data
Analysis. CRC Press, 1986.
Silverman, B. W. Using kernel density estimates to investigate multimodality. Journal of the Royal Statistical Society. Series B (Methodological), 1981.

Simonoff, Jeffrey S.
Springer, 1996.

Smoothing Methods in Statistics.

Singh, Aarti, Scott, Clayton, and Nowak, Robert. Adaptive hausdorff estimation of density level sets. Annals of
Statistics, 2009.
Sriperumbudur, Bharath K. and Steinwart, Ingo. Consistency and rates for clustering with dbscan. AISTATS,
2012.
Steinwart, I. Adaptive density level set clustering. 24th
Annual Conference on Learning Theory, 2011.
Terrell, George R and Scott, David W. Variable kernel density estimation. The Annals of Statistics, pp. 1236–1265,
1992.
Tsybakov, A. Recursive estimation of the mode of a multivariate distribution. Problemy Peredachi Informatsii,
1990.
Tsybakov, A. Introduction to nonparametric estimation.
Springer, 2008.
Tsybakov, A. B. On non-parametric estimation of density
level sets. Annals of Statistics, 1997.

