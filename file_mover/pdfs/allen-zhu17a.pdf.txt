Natasha: Faster Non-Convex Stochastic Optimization
via Strongly Non-Convex Parameter

Zeyuan Allen-Zhu 1

Abstract
Given a non-convex function f (x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate
stationary points. The performance of our new
methods depend on the smallest (negative) eigenvalue âˆ’Ïƒ of the Hessian. This parameter Ïƒ
captures how strongly non-convex f (x) is, and
is analogous to the strong convexity parameter
for convex optimization. At least in theory, our
methods outperform known results for a range of
parameter Ïƒ, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold Ïƒ0 so
that the (currently) fastest methods for Ïƒ > Ïƒ0
and for Ïƒ < Ïƒ0 have different behaviors: the former scales with n2/3 and the latter scales with
n3/4 .

1

Introduction

We study the problem of composite non-convex minimization:
n
o
n
1X
fi (x)
min F (x) := Ïˆ(x) + f (x) := Ïˆ(x) +
n i=1
xâˆˆRd
(1.1)
where each fi (x) is nonconvex but smooth, and Ïˆ(Â·) is
proper convex, possibly nonsmooth, but relatively simple.
We are interested in finding a point x that is an approximate
local minimum of F (x).
Pn
â€¢ The finite-sum structure f (x) = n1 i=1 fi (x) arises
prominently in large-scale machine learning tasks. In
particular, when minimizing loss over a training set,
each example i corresponds to one loss function fi (Â·)
in the summation. This finite-sum structure allows one
to perform stochastic gradient descent with respect to a
Future version of this paper shall be found at http://
arxiv.org/abs/1702.00763. 1 Microsoft Research. Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

random âˆ‡fi (x).
â€¢ The so-called proximal term Ïˆ(x) adds more generality to the model. For instance, if Ïˆ(x) is the indicator
function of a convex set, then problem (1.1) becomes
constraint minimization; if Ïˆ(x) = kxk1 , then we can
allow problem (1.1) to perform feature selection. In
general, Ïˆ(x) has to be a simple function where the
1
kx âˆ’ x0 k2 }
projection operation arg minx {Ïˆ(x) + 2Î·
is efficiently computable. At a first reading of this paper, one can assume Ïˆ(x) â‰¡ 0 for simplicity.
Many non-convex machine learning problems fall into
problem (1.1). Most notably, training deep neural networks
and classifications with sigmoid loss correspond to (1.1)
where neither fi (x) or f (x) is convex. However, our understanding to this challenging non-convex problem is very
limited.
1.1

Strongly Non-Convex Optimization

Let L be the smoothness parameter for each fi (x), meaning
all the eigenvalues of âˆ‡2 fi (x) lie in [âˆ’L, L].1
We denote byP
Ïƒ âˆˆ [0, L] the strong-nonconvexity parameter
n
of f (x) = n1 i=1 fi (x), meaning that
all the eigenvalues of âˆ‡2 f (x) lie in [âˆ’Ïƒ, L].
We emphasize that parameter Ïƒ is analogous to the strongconvexity parameter Âµ for convex optimization, where all
the eigenvalues of âˆ‡2 f (x) lie in [Âµ, L] for some Âµ > 0.
We wish to find an Îµ-approximate stationary point (a.k.a.
critical point) of F (x), that is
a point x satisfying kG(x)k â‰¤ Îµ
where G(x) is the so-called gradient mapping of F (x) (see
Section 2 for a formal definition). In the special case of
Ïˆ(Â·) â‰¡ 0, gradient mapping G(x) is the same as gradient
âˆ‡f (x), so x satisfies kâˆ‡f (x)k â‰¤ Îµ.
Since f (Â·) is Ïƒ-strongly nonconvex, any Îµ-approximate stationary point is automatically also an (Îµ, Ïƒ)-approximate
local minimum â€” meaning that the Hessian of the output
point âˆ‡2 f (x)  âˆ’ÏƒI is approximately positive semidefinite (PSD).
1
This definition also applies to functions f (x) that are not
twice differentiable, see Section 2 for details.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

1.2

Motivations and Remarks
complexity (log-scale)

â€¢ We focus on strongly non-convex optimization because
introducing this parameter Ïƒ allows us to perform a
more refined study of non-convex optimization. If Ïƒ
equals L then L-strongly nonconvex optimization is
equivalent to the general non-convex optimization.
â€¢ We focus only on finding stationary points as opposed to local minima, because in a recent study â€”
see Appendix Aâ€” researchers have shown that finding
(Îµ, Î´)-approximate local minima reduces to finding Îµapproximate stationary points in an O(Î´)-strongly nonconvex function.
â€¢ Parameter Ïƒ is often not constant and can be much
smaller thanâˆšL. For instance, second-order methods often find (Îµ, Îµ)-approximate localâˆšminima (Nesterov,
2008) and this corresponds to Ïƒ = Îµ.
1.3

Known Results

Despite the widespread use of nonconvex models in machine learning and related fields, our understanding to nonconvex optimization is still very limited. Until recently,
nearly all research papers have been mostly focusing on either Ïƒ = 0 or Ïƒ = L:
â€¢ If Ïƒ = 0, the accelerated SVRG method (ShalevShwartz, 2016; Allen-Zhu & Yuan, 2016) finds x satisfying F (x)pâˆ’ F (xâˆ— ) â‰¤ Îµ, in gradient complexity

e n + n3/4 L/Îµ .2 This result is irrelevant to this
O
paper because f (x) is simply convex.
â€¢ If Ïƒ = L, the SVRG method (Allen-Zhu & Hazan,
2016) finds an Îµ-approximate stationary point of F (x)
in gradient complexity O(n + n2/3 L/Îµ2 ).
â€¢ If Ïƒ = L, gradient descent finds an Îµ-approximate stationary point in gradient complexity O(nL/Îµ2 ).
â€¢ If Ïƒ = L, stochastic gradient descent finds an Îµ-approx.
stationary point in gradient complexity O(L2 /Îµ4 ).
Throughout this paper, we refer to gradient complexity
as the total number of stochastic gradient computations
âˆ‡fi (x) and proximal computations y â† ProxÏˆ,Î· (x) :=
1
arg miny {Ïˆ(y) + 2Î·
ky âˆ’ xk2 }.3
Very recently, it was observed by two independent
groups (Agarwal et al., 2017; Carmon et al., 2016) â€”
although implicitly, see Section 2.1â€” that for solving the
Ïƒ-strongly nonconvex problem, one can repeatedly regularize F (x) to make it Ïƒ-strongly convex, and then apply
the accelerated SVRG method to minimize this regularized
2

e to hide poly-logarithmic factors in n, L, 1/Îµ.
We use O
Some authors also refer to them as incremental first-order oracle (IFO) and proximal oracle (PO) calls. In most machine learning applications, each IFO and PO call can be implemented to run
in time O(d) where d is the dimension of the model, or even in
time O(s) if s is the average sparsity of the data vectors.
3

ğœ=0

ğ‘›ğ¿
ğœ€2

ğ‘›ğœ
ğœ€2
ğ‘›2/3 ğ¿2 ğœ
ğœ€2
ğ‘›3/4 ğ¿ğœ
ğœ€2

1/2

ğœ = ğ¿/ ğ‘›

ğ‘›2/3 ğ¿
ğœ€2

1/3

repeatSVRG
Natasha
SVRG
gradient descent
ğœ=ğ¿

Figure 1: Comparison to prior works

function. Under mild assumption Ïƒ â‰¥ Îµ2 , this approach
â€¢ finds an Îµ-approximate
stationary point in gradient
âˆš

LÏƒ
e nÏƒ+n3/4
complexity O
.
Îµ2
We call this method repeatSVRG in this paper. Unfortunately, repeatSVRG is even slower than the vanilla SVRG
for Ïƒ = L by a factor n1/3 , see Figure 1.
Remark on SGD. Stochastic gradient descent (SGD) has
a slower convergence rate (i.e., in terms of 1/Îµ4 ) than other
cited first-order methods (i.e., in terms of 1/Îµ2 ), see for
instance (Ghadimi & Lan, 2015). However, the complexity
of SGD does not depend on n and thus is incomparable to
gradient descent, SVRG, or repeatSVRG.4 This is one of
the main motivations to study how to reduce the complexity
of non-SGD methods, especially in terms of n.
1.4

Our New Results

In this paper, we identify an interesting dichotomy with respect to the spectrum of the nonconvexity parameter
âˆš Ïƒ âˆˆ
[0, L]. In particular, we showed that if Ïƒ â‰¥ L/ n, then
our new method Natasha finds an Îµ-approximate stationary point of F (x) in gradient complexity

1 n2/3 (L2 Ïƒ)1/3 
O n log +
.
Îµ
Îµ2
In other words, together with repeatSVRG, we have improved the gradient complexity for Ïƒ-stringly nonconvex
optimization to5

n 3/4 âˆšLÏƒ n2/3 (L2 Ïƒ)1/3 o
e min n
O
,
Îµ2
Îµ2
âˆš
and the first term in the min is smaller
âˆš if Ïƒ < L/ n and
the second term is smaller if Ïƒ > L/ n. We illustrate our
4

In practice, there are examples in non-convex empirical risk
minimization (Allen-Zhu & Hazan, 2016) and in training neural
networks (Allen-Zhu & Hazan, 2016; Reddi et al., 2016) where
SVRG can outperform SGD. Of course, for deep learning tasks,
SGD remains to be the best practical method of choice.
5
We remark here that this is under mild assumptions for Îµ being sufficiently small. For instance, the result of (Agarwal et al.,
2017; Carmon et al., 2016) requires Îµ2 â‰¤ Ïƒ. In our result, the
term n log 1Îµ disappears when Îµ6 â‰¤ L2 Ïƒ/n.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

performance improvement in Figure 1. Our result matches
that of SVRG for Ïƒ = L, and has a much simpler analysis.
Additional Results. One can take a step further and ask
what if each function fi (x) is (`1 , `2 )-smooth for parameters `1 , `2 â‰¥ Ïƒ, meaning that all the eigenvalues of âˆ‡2 fi (x)
lie in [âˆ’`2 , `1 ].
We show that a variant of our method, which we call
Natashafull , solves this more refined problem of (1.1) with
2/3
1/3 
as
total gradient complexity O n log 1Îµ + n (`1Îµ2`2 Ïƒ)
long as `Ïƒ1 `22 â‰¤ n2 .
Remark 1.1. In applications, `1 and `2 can be of very different magnitudes. The most influential example is finding
the leading eigenvector of a symmetric matrix. Using the
so-called shift-and-invert reduction (Garber et al., 2016),
computing the leading eigenvector reduces to the convex version of problem (1.1), where each fi (x) is (Î», 1)smooth for Î»  1. Other examples include all the applications that are built on shift-and-invert, including high
rank SVD/PCA (Allen-Zhu & Li, 2016), canonical component analysis (Allen-Zhu & Li, 2017a), online matrix learning (Allen-Zhu & Li, 2017b), and approximate local minima algorithms (Agarwal et al., 2017; Carmon et al., 2016).
Mini-Batch. Our result generalizes trivially to the minibatch stochastic setting, where in each iteration one computes âˆ‡fi (x) for b random choices of index i âˆˆ [n] and average them. The stated gradient complexities of Natasha
and Natashafull can be adjusted so that the factor n2/3 is
replaced with n2/3 b1/3 .
1.5

Our New Idea. In this paper, we propose Natasha and
Natashafull , two methods that are no longer black-box reductions to SVRG. Both of them still divide iterations into
e the
epochs of length n, and compute gradient estimators âˆ‡
same way as SVRG. However, we do not apply compute
e directly.
xt âˆ’ Î± âˆ‡
â€¢ In our base algorithm Natasha, we divide each epoch
into p sub-epochs, each with a starting vector b
x. Our
Ïƒ2
1/3
n)
.
Then,
we
theory suggests the choice p â‰ˆ ( L
2
e
e
replace the use of âˆ‡ with âˆ‡ + 2Ïƒ(xt âˆ’ b
x). This is
equivalent to replacing f (x) with its regularized version
f (x) + Ïƒkx âˆ’b
xk2 , where the center b
x varies across subepochs. We provide pseudocode in Algorithm 1 and illustrate it in Figure 2.
We view this additional term 2Ïƒ(xt âˆ’ b
x) as a type of
retraction, which stabilizes the algorithm by moving
the vector a bit in the backward direction towards b
x.
â€¢ In our full algorithm Natashafull , we add one more ingredient on top of Natasha. That is, we perform upe with respect to a difdates zt+1 â† ProxÏˆ,Î± (zt âˆ’ Î±âˆ‡)
x
ferent sequence {zt }, and then define xt = 21 zt + 12 b
e at points xt . We
and compute gradient estimators âˆ‡
provide pseudocode in Algorithm 2 in the appendix.
We view this averaging xt = 21 zt + 21 b
x as another type
of retraction, which stabilizes the algorithm by moving towards b
x. The technique of computing gradients at
points xt but moving a different sequence of points zt is
related to the Katyusha momentum recently developed
for convex optimization (Allen-Zhu, 2017).

Our Techniques

Let us first recall the main idea behind stochastic variancereduced methods, such as SVRG (Johnson & Zhang, 2013).
The SVRG method divides iterations into epochs, each of
e for each epoch,
length n. It maintains a snapshot point x
and computes the full gradient âˆ‡f (e
x) only for snapshots.
Then, in each iteration t at point xt , SVRG defines gradient
e = âˆ‡fi (xt ) âˆ’ âˆ‡fi (e
estimator âˆ‡
x) + âˆ‡f (e
x) which satisfies
e
Ei [âˆ‡] = âˆ‡f (xt ), and performs proximal update xt+1 â†

e for some learning rate Î±. (Recall that
ProxÏˆ,Î± xt âˆ’ Î±âˆ‡
e
if Ïˆ(Â·) â‰¡ 0 then we would have xt+1 â† xt âˆ’ Î±âˆ‡.)
In nearly all the aforementioned results for nonconvex optimization, researchers have either directly applied
SVRG (Allen-Zhu & Hazan, 2016) (for the case Ïƒ = L),
or repeatedly applied SVRG (Agarwal et al., 2017; Carmon
et al., 2016) (for general Ïƒ âˆˆ [0, L]). This puts some limitation in the algorithmic design, because SVRG requires
each epoch to be of length exactly n.6
6

The epoch length of SVRG is always n (or a constant multiple of n in practice), because this ensures the computation of
e is of amortized gradient complexity O(1). The per-iteration
âˆ‡
complexity of SVRG is thus the same as the traditional stochastic

1.6

Other Related Work

Methods based on variance-reduced stochastic gradients
were first introduced for convex optimization. The first
such method is SAG by Schmidt et al (Schmidt et al.,
2013). The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by (Johnson & Zhang, 2013; Zhang
et al., 2013), and the SAGA-like one introduced by (Defazio et al., 2014). In nearly all applications, the results
proven for SVRG-like estimators and SAGA-like estimators are simply exchangeable (therefore, the results of this
paper naturally generalize to SAGA-like estimators).
The first â€œnon-convex useâ€ of variance reduction is by
Shalev-Shwartz (Shalev-Shwartz, 2016) who assumes that
each fi (x) is non-convex but their average f (x) is still convex. This result has been slightly improved to several more
refined settings (Allen-Zhu & Yuan, 2016). The first truly
non-convex use of variance reduction (i.e., for f (x) being
also non-convex) is independently by (Allen-Zhu & Hazan,
2016) and (Reddi et al., 2016). First-order methods only
gradient descent (SGD).

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter
next ğ’™
ğ’™

â€¦

â€¦

â€¦
â€¦

â€¦
regularized by ğœ ğ‘¥ âˆ’ ğ‘¥

â€¦
2

regularized by ğœ ğ‘¥ âˆ’ ğ‘¥

â€¦
2

regularized by ğœ ğ‘¥ âˆ’ ğ‘¥

2

Figure 2: One full epoch of Natasha. The n iterations are divided into p sub-epochs, each consisting of m = n/p steps.

find stationary points (unless there is extra assumption on
the randomness of the data), and converge no faster than
1/Îµ2 .
When the second-order Hessian information is used, one
can (1) find local minima instead of stationary points, and
(2) improve the 1/Îµ2 rate to 1/Îµ1.5 . The first such result is by cubic-regularized Newtonâ€™s method (Nesterov,
2008); however, its per-iteration complexity is very high.
Very recently, two independent groups of authors tackled
this problem from a somewhat similar viewpoint (Carmon
et al., 2016; Agarwal et al., 2017): if the computation
of

Hessian-vector multiplications (i.e., âˆ‡2 fi (x) v) is on the
same order of the computation
of gradients âˆ‡fi (x),7 then
âˆš
one can obtain a (Îµ, Îµ)-approximate local minimum in
3/4 
n
e 1.5
gradient complexity O
+ Îµn1.75 , if we use big-O to
Îµ
also hide dependencies on the smoothness parameters.
Other related papers include Ge et al. (Ge et al., 2015)
where the authors showed that a noise-injected version of
SGD converges to local minima instead of critical points,
as long as the underlying function is â€œstrict-saddle.â€ Their
theoretical running time is a large polynomial in the dimension. Lee et al. (Lee et al., 2016) showed that gradient
descent, starting from a random point, almost surely converges to a local minimum if the function is â€œstrict-saddleâ€.
The rate of convergence required is somewhat unknown.

2

Preliminaries

Throughout this paper, we denote by k Â· k the Euclidean
norm. We use i âˆˆR [n] to denote that i is generated from
[n] = {1, 2, . . . , n} uniformly at random. We denote by
âˆ‡f (x) the full gradient of function f if it is differentiable,
and âˆ‚f (x) any subgradient if f is only Lipschitz continuous at point x. We let xâˆ— be any minimizer of F (x).
Recall some definitions on strong convexity (SC), strongly
nonconvexity, and smoothness.
Definition 2.1. For a function f : Rd â†’ R,
7

A lot of interesting problems satisfy this property, including
training neural nets.

â€¢ f is Ïƒ-strongly convex if âˆ€x, y âˆˆ Rd , it satisfies
Ïƒ
f (y) â‰¥ f (x) + hâˆ‚f (x), y âˆ’ xi + kx âˆ’ yk2 .
2
â€¢ f is Ïƒ-strongly nonconvex if âˆ€x, y âˆˆ Rd , it satisfies
Ïƒ
f (y) â‰¥ f (x) + hâˆ‚f (x), y âˆ’ xi âˆ’ kx âˆ’ yk2 .
2
d
â€¢ f is (`1 , `2 )-smooth if âˆ€x, y âˆˆ R , it satisfies
`1
2 kx

âˆ’ yk2 â‰¥ f (y)
`2
â‰¥ f (x) + hâˆ‡f (x), y âˆ’ xi âˆ’ kx âˆ’ yk2 .
2
â€¢ f is L-smooth if it is (L, L)-smooth.
f (x) + hâˆ‡f (x), y âˆ’ xi +

The (`1 , `2 )-smoothness parameters were introduced
in (Allen-Zhu & Yuan, 2016) to tackle the convex setting
of problem (1.1). The notion of strong nonconvexity is
also known as â€œalmost convexity (Carmon et al., 2016)â€
or â€œlower smoothness (Allen-Zhu & Yuan, 2016).â€ We refrain from using the name â€œalmost convexityâ€ because it
coincides with several other non-equivalent definitions in
optimization literatures.
Definition 2.2. Given a parameter Î· > 0, the gradient
mapping of F (Â·) in (1.1) at point x is

1
GÎ· (x) :=
x âˆ’ x0
Î·

	
1
where x0 = arg miny Ïˆ(y) + hâˆ‡f (x), yi + 2Î·
ky âˆ’ xk2 .
In particular, if Ïˆ(Â·) â‰¡ 0, then GÎ· (x) â‰¡ âˆ‡f (x).
The following theorem for the SVRG method can be found
for instance in (Allen-Zhu & Yuan, 2016), which is built on
top of the results (Shalev-Shwartz, 2016; Lin et al., 2015;
Frostig et al., 2015):
Pn
Theorem 2.3 (SVRG). Let G(y) := Ïˆ(y) + n1 i=1 gi (y)
be Ïƒ-strongly convex, then the SVRG method finds a point
y satisfying G(y) âˆ’ G(y âˆ— ) â‰¤ Îµ

2
1
â€¢ with gradient complexity O (n + L
Ïƒ 2 ) log Îµ , if each
gi (Â·) is L-smooth (for L â‰¥ Ïƒ); or
â€¢ with gradient complexity O (n + `Ïƒ1 `22 ) log
gi (Â·) is (`1 , `2 )-smooth (for `1 , `2 â‰¥ Ïƒ).

1
Îµ



, if each

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

If one performs
the running times become
p acceleration,

e n + n3/4 L/Ïƒ and O
e n + n3/4 (`1 `2 Ïƒ 2 )1/4 .
O
2.1

RepeatSVRG

We recall the idea behind a simple algorithm â€”that we call
repeatSVRGâ€” which finds the Îµ-approximate stationary
points for problem (1.1) when f (x) is Ïƒ-strongly nonconvex. The algorithm is divided into stages. In each stage t,
consider a modified function Ft (x) := F (x) + Ïƒkx âˆ’ xt k2 .
It is easy to see that Ft (x) is Ïƒ-strongly convex, so one can
apply the accelerated SVRG method to minimize Ft (x).
Let xt+1 be any sufficiently accurate approximate minimizer of Ft (x).8
Now, one can prove (c.f. Section 4) that xt+1 is an
O(Ïƒkxt âˆ’ xt+1 k)-approximate stationary point for F (x).
Therefore, if Ïƒkxt âˆ’ xt+1 k â‰¤ Îµ we can stop the algorithm
because we have already found an O(Îµ)-approximate stationary point. If Ïƒkxt âˆ’ xt+1 k > Îµ , then it must satisfy that F (xt ) âˆ’ F (xt+1 ) â‰¥ Ïƒkxt âˆ’ xt+1 k2 â‰¥ â„¦(Îµ2 /Ïƒ),
but this cannot happen for more than T = O ÎµÏƒ2 (F (x0 ) âˆ’
F âˆ— ) stages. Therefore, the total gradient complexity is
T multiplied with the complexity of
paccelerated SVRG
e + n3/4 L/Ïƒ) according to
in each stage (which is O(n
Theorem 2.3).
Remark 2.4. The complexity of repeatSVRG can be inferred from (Agarwal et al., 2017; Carmon et al., 2016), but
is not explicitly stated. For instance, the paper (Carmon
et al., 2016) does not allow F (x) to have a non-smooth
proximal term Ïˆ(x), and applies accelerated gradient descent instead of accelerated SVRG.

3

Our Algorithms

We introduce two variants of our algorithms: (1) the base
method Natasha targets on the simple regime when f (x)
and each fi (x) are both L-smooth, and (2) the full method
Natashafull targets on the more refined regime when f (x)
is L-smooth but each fi (x) is (`1 , `2 )-smooth.
Both methods follow the general idea of variance-reduced
stochastic gradient descent: in each inner-most iteration,
e that is of the form
they compute a gradient estimator âˆ‡
e
e =
âˆ‡ = âˆ‡f (e
x)âˆ’âˆ‡fi (e
x)+âˆ‡fi (x) and satisfies EiâˆˆR [n] [âˆ‡]
e is a snapshot point that is changed once
âˆ‡f (x). Here, x
every n iterations (i.e., for each different k = 1, 2, . . . , T 0
in the pseudocode), and we call it a full epoch for every
distinct k. Notice that the amortized gradient complexity
e is O(1) per-iteration.
for computing âˆ‡
Base Method. In Natasha (see Algorithm 1), as illustrated by Figure 2, we divide each full epoch into p subepochs s = 0, 1, . . . , p âˆ’ 1, each of length m = n/p. In
8

Since the accelerated SVRG method has a linear convergence
rate for strongly convex functions, the complexity to find such
xt+1 only depends logarithmically on this accuracy.

each sub-epoch s, we start with a point x0 = b
x, and replace
f (x) with its regularized version f s (x) := f (x) + Ïƒkx âˆ’
b
xk2 . Then, in each iteration t of the sub-epoch s, we
e with respect to f s (xt ),
â€¢ compute gradient estimator âˆ‡

e yi +
â€¢ perform update xt+1 = arg miny Ïˆ(y) + hâˆ‡,
	
1
2
ky
âˆ’
x
k
with
learning
rate
Î±.
t
2Î±
Effectively, the introduction of the regularizer Ïƒkx âˆ’ b
xk2
makes sure that when performing update xt â† xt+1 , we
also move a bit towards point b
x (i.e., retraction by regularization). Finally, when the sub-epoch is done, we define b
x
to be a random one from {x0 , . . . , xmâˆ’1 }.
Full Method. In Natashafull (see full version), we also
divide each full epoch into p sub-epochs. In each sub-epoch
s, we start with a point x0 = z0 = b
x and define f s (x) :=
2
f (x) + Ïƒkx âˆ’ b
xk . However, this time in each iteration t,
we
e with respect to f s (xt ),
â€¢ compute gradient estimator âˆ‡

e yi +
â€¢ perform update zt+1 = arg miny Ïˆ(y) + hâˆ‡,
	
1
2
with learning rate Î±, and
2Î± ky âˆ’ zt k
x.
â€¢ choose xt+1 = 21 zt+1 + 12 b
Effectively, the regularizer Ïƒkxâˆ’b
xk2 makes sure that when
performing updates, we move a bit towards point b
x (i.e.,
retraction by regularization); at the same time, the choice
xt+1 = 21 zt+1 + 12 b
x also helps us move towards point b
x
(i.e., retraction by the so-called â€œKatyusha momentumâ€9 ).
Finally, when the sub-epoch is over, we define b
x to be a
random one from the set {x0 , . . . , xmâˆ’1 }, and move to the
next sub-epoch.

4

A Sufficient Stopping Criterion

In this section, we present a sufficient condition for finding
approximate stationary points in a Ïƒ-strongly nonconvex
function. Lemma 4.1 below states that, if we regularize the
original function and define G(x) := F (x) + Ïƒkx âˆ’ b
xk2
for an arbitrary point b
x, then the minimizer of G(x) is an
approximate saddle-point for F (x).
Lemma 4.1. Suppose G(y) = F (y) + Ïƒky âˆ’b
xk2 for some
âˆ—
given point b
x, and let x be the minimizer of G(y). If we
minimize G(y) and obtain a point x satisfying
G(x) âˆ’ G(xâˆ— ) â‰¤ Î´ 2 Ïƒ ,

1
then for every Î· âˆˆ 0, max{L,4Ïƒ}
we have the gradient
mapping

kGÎ· (x)k2 â‰¤ 12Ïƒ 2 kxâˆ— âˆ’ b
xk2 + O Î´ 2 .
Notice that when Ïˆ(x) â‰¡ 0 this lemma is trivial, and can
be found for instance in (Carmon et al., 2016). The main
9
The idea for this second kind of retraction, and the idea of
having the updates on a sequence zt but computing gradients at
points xt , is largely motivated by our recent work on the Katyusha
momentum and the Katyusha acceleration (Allen-Zhu, 2017).

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Algorithm 1 Natasha(xâˆ… , p, T 0 , Î±)
Input: starting vector xâˆ… , sub-epoch count p âˆˆ [n], epoch count T 0 , learning rate Î± > 0.
Output: vector xout .
1: b
x â† xâˆ… ; m â† n/p; X â† [];
2: for k â† 1 to T 0 do
 T 0 full epochs
e â†b
3:
x
x; Âµ â† âˆ‡f (e
x);
4:
for s â† 0 to p âˆ’ 1 do
 p sub-epochs in each epoch
5:
x0 â† b
x; X â† [X, b
x];
6:
for t â† 0 to m âˆ’ 1 do
 m iterations in each sub-epoch
7:
i â† a random choice from {1, Â· Â· Â· , n}.

e â† âˆ‡fi (xt ) âˆ’ âˆ‡fi (e
e = âˆ‡ f (x) + Ïƒkx âˆ’ b
8:
âˆ‡
x) + Âµ + 2Ïƒ(xt âˆ’ b
x)
 Ei [âˆ‡]
xk2 x
t
	

1
e yi
ky âˆ’ xt k2 + hâˆ‡,
9:
xt+1 = arg minyâˆˆRd Ïˆ(y) + 2Î±
10:
end for
b
11:
x â† a random choice from {x0 , x1 , . . . , xmâˆ’1 };
 for practitioners, choose the average
12:
end for
13: end for
14: b
x â† a random vector in X;
 for practitioners, choose the last
15: xout â† an approximate minimizer of G(y) := F (y) + Ïƒky âˆ’ b
xk2 using SVRG.
16: return xout .
 it suffices to run SVRG for O(n log 1Îµ ) iterations.

technical difficulty arises in order to deal with Ïˆ(x) 6= 0.
The proof is included in the full version.

5

Base Method: Analysis for One Full Epoch

In this section, we consider problem (1.1) where each fi (x)
is L-smooth and F (x) is Ïƒ-strongly nonconvex. We use
our base method Natasha to minimize F (x), and analyze
its behavior for one full epoch in this section. We assume
Ïƒ â‰¤ L without loss of generality, because any L-smooth
function is also L-strongly nonconvex.
Notations. We introduce the following notations for analysis purpose only.
â€¢ Let b
xs be the vector b
x at the beginning of sub-epoch s.
â€¢ Let xst be the vector xt in sub-epoch s.
â€¢ Let ist be the index i âˆˆ [n] in sub-epoch s at iteration t.
â€¢ Let f s (x) := f (x) + Ïƒkx âˆ’ b
xs k2 , F s (x) := F (x) +
s 2
s
b
Ïƒkx âˆ’ x k , and xâˆ— := arg minx {F s (x)}.
e s (xs ) := âˆ‡fi (xs )âˆ’âˆ‡fi (e
â€¢ Let âˆ‡f
x)+âˆ‡f (e
x)+2Ïƒ(xt âˆ’
t
t
b
x) where i = ist .
e (xst ) := âˆ‡fi (xst ) âˆ’ âˆ‡fi (e
x) + âˆ‡f (e
x) where
â€¢ Let âˆ‡f
i = ist .
We obviously have that f s (x) and F s (x) are Ïƒ-strongly
convex, and f s (x) is (L + 2Ïƒ)-smooth.
5.1

Variance Upper Bound

The following lemma gives an upper bound on the variance
e s (xst ):
of the gradient estimator âˆ‡f


e s (xst ) âˆ’ âˆ‡f s (xst )k2 â‰¤
Lemma 5.1. We have Eist kâˆ‡f
P
sâˆ’1
pL2 kxst âˆ’ b
xs k2 + pL2 k=0 kb
xk âˆ’ b
xk+1 k2 .

Proof. We have




e s (xst ) âˆ’ âˆ‡f s (xst )k2 = Eis kâˆ‡f
e (xst ) âˆ’ âˆ‡f (xst )k2
Eist kâˆ‡f
t


2 
= EiâˆˆR [n]  âˆ‡fi (xst ) âˆ’ âˆ‡fi (e
x) âˆ’ âˆ‡f (xst ) âˆ’ âˆ‡f (e
x)) 
Â¬
2 

â‰¤ Eiâˆˆ [n] âˆ‡fi (xst ) âˆ’ âˆ‡fi (e
x )
R

Â­

2 

â‰¤ pEiâˆˆR [n] âˆ‡fi (xst ) âˆ’ âˆ‡fi (b
xs )
2 

Psâˆ’1
+ p k=0
EiâˆˆR [n] âˆ‡fi (b
xk ) âˆ’ âˆ‡fi (b
xk+1 )
Â®
Psâˆ’1 k
â‰¤ pL2 kxst âˆ’ b
xs k2 + pL2 k=0
kb
x âˆ’b
xk+1 k2 .

Above, inequality Â¬ is because for any random vector Î¶ âˆˆ
Rd , it holds that EkÎ¶ âˆ’ EÎ¶k2 = EkÎ¶k2 âˆ’ kEÎ¶k2 ; inequality
e and for any p vectors a1 , a2 , . . . , ap âˆˆ
Â­ is because b
x0 = x
d
R , it holds that ka1 +Â· Â· Â·+ap k2 â‰¤ pka1 k2 +Â· Â· Â·+pkap k2 ;
and inequality Â® is because each fi (Â·) is L-smooth.

5.2

Analysis for One Sub-Epoch

The following inequality is classically known as the â€œregret inequalityâ€ for mirror descent (Allen-Zhu & Orecchia,
2017), and its proof is classical (see full version):
e s (xst ), xs âˆ’ ui + Ïˆ(xs ) âˆ’ Ïˆ(u) â‰¤
Fact 5.2. hâˆ‡f
kxst âˆ’uk2
2Î±

âˆ’

kxst+1 âˆ’uk2
2Î±

t+1
kxs âˆ’xs k2
âˆ’ t+12Î± t

t+1

for every u âˆˆ Rd .

The following lemma is our main contribution for the base
method Natasha.
Lemma 5.3. As long as Î± â‰¤
h
i
E F s (b
xs+1 ) âˆ’ F s (xsâˆ— )

1
2L+4Ïƒ ,

we have

s
i
h F s (b
X
xs ) âˆ’ F s (xsâˆ— )
â‰¤E
+Î±pL2
kb
xk âˆ’b
xk+1 k2
.
ÏƒÎ±m/2
k=0

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Proof. We first compute that
F s (xst+1 ) âˆ’ F s (u) = f s (xst+1 ) âˆ’ f s (u) + Ïˆ(xst+1 ) âˆ’ Ïˆ(u)
Â¬

â‰¤ f s (xst ) + hâˆ‡f s (xst ), xst+1 âˆ’ xst i +

Dividing both sides by m and rearranging the terms (using
1
2ÏƒÎ± â‰¥ 1), we have

L + 2Ïƒ s
kxt âˆ’ xst+1 k2
2

h
i
E F s (b
xs+1 ) âˆ’ F s (xsâˆ— )

âˆ’ f s (u) + Ïˆ(xst+1 ) âˆ’ Ïˆ(u)
Â­

L + 2Ïƒ s
kxt âˆ’ xst+1 k2
2
+ hâˆ‡f s (xst ), xst âˆ’ ui + Ïˆ(xst+1 ) âˆ’ Ïˆ(u) .

â‰¤E

â‰¤ hâˆ‡f s (xst ), xst+1 âˆ’ xst i +

(5.1)

s

Above, inequality Â¬ uses the fact that f (Â·) is (L + 2Ïƒ)smooth; and inequality Â­ uses the convexity of f s (Â·). Now,
we take expectation with respect to ist on both sides of (5.1),
and derive that:


Eist F s (xst+1 ) âˆ’ F s (u)
h
Â¬
e s (xst ) âˆ’ âˆ‡f s (xst ), xst âˆ’ xst+1 i + hâˆ‡f
e s (xst ), xst+1 âˆ’ ui
â‰¤ Eist hâˆ‡f
i
L + 2Ïƒ s
kxt âˆ’ xst+1 k2 + Ïˆ(xst+1 ) âˆ’ Ïˆ(u)
+
2
h
s
2
Â­
e s (xst ) âˆ’ âˆ‡f s (xst ), xst âˆ’ xst+1 i + kxt âˆ’ uk
â‰¤ Eist hâˆ‡f
2Î±
i
kxst+1 âˆ’ uk2
1
L + 2Ïƒ  s
âˆ’
âˆ’
kxt+1 âˆ’ xst k2
âˆ’
2Î±
2Î±
2
h 
s
2i
s
2
Â®

2
e s (xst ) âˆ’ âˆ‡f s (xst ) + kxt âˆ’ uk âˆ’ kxt+1 âˆ’ uk
â‰¤ Eist Î±âˆ‡f
2Î±
2Î±
sâˆ’1
h
X
Â¯
â‰¤ Eist Î±pL2 kxst âˆ’ b
xs k2 + Î±pL2
kb
xk âˆ’ b
xk+1 k2
k=0

kxs âˆ’ uk2 i
kxs âˆ’ uk2
+ t
âˆ’ t+1
.
2Î±
2Î±

(5.2)

Above, inequality Â¬ is follows from (5.1) together with
e s (xst )] = âˆ‡f s (xst ) implies
the fact that Eist [âˆ‡f


Eist hâˆ‡f s (xst ), xst+1 âˆ’ xst i + hâˆ‡f s (xst ), xst âˆ’ ui


e s (xst )âˆ’âˆ‡f s (xst ), xst âˆ’xst+1 i+hâˆ‡f
e s (xst ), xst+1 âˆ’ui ;
= Eist hâˆ‡f
1
inequality Â­ uses Fact 5.2; inequality Â® uses Î± â‰¤ 2L+4Ïƒ
1
1
together with Youngâ€™s inequality ha, bi â‰¤ 2 kak2 + 2 kbk2 ;
and inequality Â¯ uses Lemma 5.1.
Finally, choosing u = xsâˆ— to be the (unique) minimizer of
F s (Â·) = f s (Â·) + Ïˆ(Â·), and telescoping inequality (5.2) for
t = 0, 1, . . . , m âˆ’ 1, we have

h mâˆ’1
X
i
E
F s (xst ) âˆ’ F s (xsâˆ— )
t=1

h kxs âˆ’ xs k2 mâˆ’1
X
0
âˆ—
â‰¤E
+
Î±pL2 kxst âˆ’ b
xs k2
2Î±
t=0
+ Î±pL2

sâˆ’1
X

kb
xk âˆ’ b
xk+1 k2

i

k=0
s
h F s (b
i
X
xs ) âˆ’ F s (xsâˆ— )
â‰¤E
+ Î±pmL2
kb
xk âˆ’ b
xk+1 k2
.
ÏƒÎ±
k=0

Above, the second inequality uses the fact that b
xs+1 is chos
s
sen from {x0 , . . . , xmâˆ’1 } uniformly at random, as well as
the Ïƒ-strong convexity of F s (Â·).

5.3

h F s (b
xs ) âˆ’ F s (xs )
âˆ—

ÏƒÎ±m/2

+ Î±pL2

s
X

kb
xk âˆ’ b
xk+1 k2

i

.



k=0

Analysis for One Full Epoch

One can telescope Lemma 5.3 for an entire epoch and arrive at the following lemma (see full version):
1
4
Lemma 5.4. If Î± â‰¤ 2L+4Ïƒ
, Î± â‰¥ Ïƒm
and Î± â‰¤ p2ÏƒL2 , we
have
pâˆ’1 h
h
i
X
i
E F s (b
xs ) âˆ’ F s (xsâˆ— ) â‰¤ 2E F (b
x0 ) âˆ’ F (b
xp ) .
s=0

6

Base Method: Final Theorem

We are now ready to state and prove our main convergence
theorem for Natasha:
Theorem 1. Suppose in (1.1), each fi (x) is L-smooth
and F (x) is Ïƒ-strongly nonconvex for Ïƒ â‰¤ L. Then, if

L2
Ïƒ2
1/3
and Î± = Î˜( p2ÏƒL2 ), our base
Ïƒ 2 â‰¤ n, p = Î˜ ( L2 n)
method Natasha outputs a point xout satisfying
 2 1/3 2/3 
E[kGÎ· (xout )k2 ] â‰¤ O (L Ïƒ)T 0 n n
Â· (F (xâˆ… ) âˆ’ F âˆ— ) .

1
. In other words, to obtain
for every Î· âˆˆ 0, max{L,4Ïƒ}
E[kGÎ· (xout )k2 ] â‰¤ Îµ2 , we need gradient complexity


1 (L2 Ïƒ)1/3 n2/3
Â· (F (xâˆ… ) âˆ’ F âˆ— ) .
O n log +
2
Îµ
Îµ
In the above theorem, we have assumed Ïƒ â‰¤ L without
loss of generality because any L-smooth function is also
2
L-strongly nonconvex. Also, we have assumed L
Ïƒ2 â‰¤ n
and if this inequality does not hold, then one should apply
repeatSVRG for a faster running time (see Figure 1).
1/3
Ïƒ2
, m =
Proof of Theorem 1. We choose p = 24L
2n
4
1
n/p, and Î± = Ïƒm
= 6p2ÏƒL2 â‰¤ 2L+4Ïƒ
, so we can apply
Lemma 5.4. If we telescope Lemma 5.4 for the entire algorithm (which has T 0 full epochs), and use the fact that
b
xp of the previous epoch equals b
x0 of the next epoch, we
conclude that if we choose a random epoch and a random
subepoch s, we will have
2
E[F s (b
xs ) âˆ’ F s (xsâˆ— )] â‰¤
(F (xâˆ… ) âˆ’ F âˆ— ) .
pT 0
By the Ïƒ-strong convexity of F s (Â·), we have E[Ïƒkb
xs âˆ’
4
s 2
âˆ…
âˆ—
xâˆ— k ] â‰¤ pT 0 (F (x ) âˆ’ F ).
Now, F s (x) = F (x) + Ïƒkx âˆ’b
xs k2 satisfies the assumption
of G(x) in Lemma 4.1. If we use the SVRG method (see
Theorem 2.3) to minimize the convex function F s (x), we

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

get an output xout satisfying F s (xout ) âˆ’ F s (xsâˆ— ) â‰¤ Îµ2 Ïƒ in

2
1
1
gradient complexity O (n + L
Ïƒ 2 ) log Îµ â‰¤ O(n log Îµ ).
We can therefore apply Lemma 4.1 and conclude that this
output xout satisfies
 Ïƒ 
Â· (F (xâˆ… ) âˆ’ F âˆ— )
E[kGÎ· (xout )k2 ] â‰¤ O
pT 0
 2 1/3 2/3 
Â· (F (xâˆ… ) âˆ’ F âˆ— ) .
= O (L Ïƒ)T 0 n n
In other words, we obtain E[kGÎ· (xout )k2 ] â‰¤ Îµ2 using


2
1/3 2/3
T 0 n = O n + (L Ïƒ)Îµ2 n Â· (F (xâˆ… ) âˆ’ F âˆ— )
computations of the stochastic gradients. Here, the additive
term n is because T 0 â‰¥ 1.

Allen-Zhu, Zeyuan and Li, Yuanzhi. LazySVD: Even
Faster SVD Decomposition Yet Without Agonizing
Pain. In NIPS, 2016.
Allen-Zhu, Zeyuan and Li, Yuanzhi. Doubly Accelerated
Methods for Faster CCA and Generalized Eigendecomposition. In Proceedings of the 34th International Conference on Machine Learning, ICML â€™17, 2017a.
Allen-Zhu, Zeyuan and Li, Yuanzhi. Follow the Compressed Leader: Faster Online Learning of Eigenvectors
and Faster MMWU. In Proceedings of the 34th International Conference on Machine Learning, ICML â€™17,
2017b.

Finally, adding this with O(n log 1Îµ ), the gradient complexity for the application of SVRG in the last line of Natasha,
we finish the proof of the total gradient complexity.


Allen-Zhu, Zeyuan and Orecchia, Lorenzo. Linear Coupling: An Ultimate Unification of Gradient and Mirror
Descent. In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS â€™17, 2017.

7

Allen-Zhu, Zeyuan and Yuan, Yang. Improved SVRG
for Non-Strongly-Convex or Sum-of-Non-Convex Objectives. In ICML, 2016.

Full Method: Final Theorem

We analyze and state the main theorems for our full method
Natashafull in the full version of this paper.

8

Conclusion

Stochastic gradient descent and gradient descent (including
alternating minimization) have become the canonical methods for solving non-convex machine learning tasks. However, can we design new non-convex methods to run even
faster than SGD or GD?
This present paper tries to tackle this general question, by
providing a new Natasha method which is intrinsically different from GD or SGD. It runs faster than GD and SVRGbased methods at least in theory. We hope that this could
be a non-negligible step towards our better understanding
of non-convex optimization.
Finally, our results give rise to an interesting dichotomy in
the best-known complexity of first-order non-convex optiâˆš
mization: the complexity scales
with n3/4 for Ïƒ < L/ n
âˆš
and with n2/3 for Ïƒ > L/ n. It remains open to investigate whether this dichotomy is intrinsic, or we can design
a more efficient algorithm that outperforms both.

References
Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding Approximate Local Minima for Nonconvex Optimization in Linear Time.
In STOC, 2017.
Allen-Zhu, Zeyuan. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. In STOC, 2017.
Allen-Zhu, Zeyuan and Hazan, Elad. Variance Reduction
for Faster Non-Convex Optimization. In NIPS, 2016.

Carmon, Yair, Duchi, John C., Hinder, Oliver, and Sidford,
Aaron. Accelerated Methods for Non-Convex Optimization. ArXiv e-prints, abs/1611.00756, November 2016.
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.
In NIPS, 2014. URL http://arxiv.org/abs/
1407.0202.
Frostig, Roy, Ge, Rong, Kakade, Sham M., and Sidford,
Aaron. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk minimization. In ICML, volume 37, pp. 1â€“28, 2015. URL
http://arxiv.org/abs/1506.07512.
Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Robust shift-and-invert preconditioning: Faster
and more sample efficient algorithms for eigenvector
computation. In ICML, 2016.
Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Escaping from saddle pointsâ€”online stochastic gradient
for tensor decomposition. In Proceedings of the 28th Annual Conference on Learning Theory, COLT 2015, 2015.
Ghadimi, Saeed and Lan, Guanghui.
Accelerated gradient methods for nonconvex nonlinear and stochastic programming.
Mathematical Programming, pp. 1â€“26, feb 2015.
ISSN
0025-5610.
doi:
10.1007/s10107-015-0871-8.
URL
http://arxiv.org/abs/1310.
3787http://link.springer.com/10.1007/
s10107-015-0871-8.

Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance reduction.
In Advances in Neural Information Processing Systems,
NIPS 2013, pp. 315â€“323, 2013.
Lee, Jason D., Simchowitz, Max, Jordan, Michael I., and
Recht, Benjamin. Gradient descent only converges to
minimizers. In Proceedings of the 29th Conference on
Learning Theory, COLT 2016, New York, USA, June 2326, 2016, pp. 1246â€“1257, 2016.
Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid.
A Universal Catalyst for First-Order Optimization.
In NIPS, 2015. URL http://arxiv.org/pdf/
1506.02186v1.pdf.
Nesterov, Yurii. Accelerating the cubic regularization of
newtonâ€™s method on convex problems. Mathematical
Programming, 112(1):159â€“181, 2008.
Reddi, Sashank J., Hefny, Ahmed, Sra, Suvrit, Poczos,
Barnabas, and Smola, Alex. Stochastic variance reduction for nonconvex optimization. ArXiv e-prints,
abs/1603.06160, March 2016.
Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, pp. 1â€“45, 2013.
URL http://arxiv.org/abs/1309.2388. Preliminary version appeared in NIPS 2012.
Shalev-Shwartz, Shai. SDCA without Duality, Regularization, and Individual Convexity. In ICML, 2016.
Zhang, Lijun, Mahdavi, Mehrdad, and Jin, Rong. Linear
convergence with condition number independent access
of full gradients. In Advances in Neural Information Processing Systems, pp. 980â€“988, 2013.

