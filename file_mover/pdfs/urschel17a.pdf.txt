Learning Determinantal Point Processes with Moments and Cycles

John Urschel 1 Victor-Emmanuel Brunel 1 Ankur Moitra 1 Philippe Rigollet 1

Abstract
Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive
behavior, and lend themselves naturally to many
tasks in machine learning where returning a diverse set of objects is important. While there are
fast algorithms for sampling, marginalization and
conditioning, much less is known about learning the parameters of a DPP. Our contribution
is twofold: (i) we establish the optimal sample complexity achievable in this problem and
show that it is governed by a natural parameter,
which we call the cycle sparsity; (ii) we propose
a provably fast combinatorial algorithm that implements the method of moments efficiently and
achieves optimal sample complexity. Finally, we
give experimental results that confirm our theoretical findings.

1. Introduction
Determinantal Point Processes (DPPs) are a family of probabilistic models that arose from the study of quantum mechanics (Macchi, 1975) and random matrix theory (Dyson,
1962). Following the seminal work of Kulesza and Taskar
(Kulesza & Taskar, 2012), discrete DPPs have found numerous applications in machine learning, including in document and timeline summarization (Lin & Bilmes, 2012;
Yao et al., 2016), image search (Kulesza & Taskar, 2011;
Affandi et al., 2014) and segmentation (Lee et al., 2016),
audio signal processing (Xu & Ou, 2016), bioinformatics (Batmanghelich et al., 2014) and neuroscience (Snoek
et al., 2013). What makes such models appealing is that
they exhibit repulsive behavior and lend themselves naturally to tasks where returning a diverse set of objects is
important.
One way to define a DPP is through an N × N symmetric positive semidefinite matrix K, called a kernel, whose
1
Department of Mathematics, MIT, USA. Correspondence to:
John Urschel <urschel@mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

eigenvalues are bounded in the range [0, 1]. Then the DPP
associated with K, which we denote by DPP(K), is the
distribution on Y ⊆ [N ] = {1, . . . , N } that satisfies, for
any J ⊆ [N ],
P[J ⊆ Y ] = det(KJ ),
where KJ is the principal submatrix of K indexed by the
set J. The graph induced by K is the graph G = ([N ], E)
on the vertex set [N ] that connects i, j ∈ [N ] if and only if
Ki,j 6= 0.
There are fast algorithms for sampling (or approximately
sampling) from DPP(K) (Deshpande & Rademacher,
2010; Rebeschini & Karbasi, 2015; Li et al., 2016b;a).
Marginalizing the distribution on a subset I ⊆ [N ] and conditioning on the event that J ⊆ Y both result in new DPPs
and closed form expressions for their kernels are known
(Borodin & Rains, 2005).
There has been much less work on the problem of learning
the parameters of a DPP. A variety of heuristics have been
proposed, including Expectation-Maximization (Gillenwater et al., 2014), MCMC (Affandi et al., 2014), and fixed
point algorithms (Mariet & Sra, 2015). All of these attempt
to solve a nonconvex optimization problem, and no guarantees on their statistical performance are known. Recently,
Brunel et al. (Brunel et al., 2017) studied the rate of estimation achieved by the maximum likelihood estimator, but
the question of efficient computation remains open.
Apart from positive results on sampling, marginalization
and conditioning, most provable results about DPPs are actually negative. It is conjectured that the maximum likelihood estimator is NP-hard to compute (Kulesza, 2012).
Actually, approximating the mode of size k of a DPP to
within a ck factor is known to be NP-hard for some c > 1
(Çivril & Magdon-Ismail, 2009; Summa et al., 2015). The
best known algorithms currently obtain a ek +o(k) approximation factor (Nikolov, 2015; Nikolov & Singh, 2016).
In this work, we bypass the difficulties associated with
maximum likelihood estimation by using the method of moments to achieve optimal sample complexity. We introduce
a parameter `, which we call the cycle sparsity of the graph
induced by the kernel K, which governs the number of moments that need to be considered and, thus, the sample complexity. Moreover, we use a refined version of Horton’s al-

Moments, Cycles and Learning DPPs

gorithm (Horton, 1987; Amaldi et al., 2010) to implement
the method of moments in polynomial time.
The cycle sparsity of a graph is the smallest integer ` so
that the cycles of length at most ` yield a basis for the cycle space of the graph. Even though there are in general
exponentially many cycles in a graph to consider, Horton’s
algorithm constructs a minimum weight cycle basis and,
in doing so, also reveals the parameter ` together with a
collection of at most ` induced cycles spanning the cycle
space.
We use such cycles in order to construct our method of moments estimator. For any fixed ` ≥ 2, our overall algorithm
has sample complexity
n=O

 C
log N 
2`
+ 2 2
α
α ε

for some constant C > 1 and runs in time polynomial in
n and N , and learns the parameters up to an additive ε
with high probability. The (C/α)2` term corresponds to
the number of samples needed to recover the signs of the
entries in K. We complement this result with a minimax
lower bound (Theorem 2) to show that this sample complexity is in fact near optimal. In particular, we show that
there is an infinite family of graphs with cycle sparsity `
(namely length ` cycles) on which any algorithm requires
at least (C 0 α)−2` samples to recover the signs of the entries
of K for some constant C 0 > 1. Finally, we show experimental results that confirm many quantitative aspects of our
theoretical predictions. Together, our upper bounds, lower
bounds, and experiments present a nuanced understanding
of which DPPs can be learned provably and efficiently.

2. Estimation of the Kernel
2.1. Model and definitions
Let Y1 , . . . , Yn be n independent copies of Y ∼ DPP(K),
for some unknown kernel K such that 0  K  IN .
It is well known that K is identified by DPP(K) only
up to flips of the signs of its rows and columns: If K 0
is another symmetric matrix with 0  K 0  IN , then
DPP(K 0 )=DPP(K) if and only if K 0 = DKD for some
D ∈ DN , where DN denotes the class of all N × N diagonal matrices with only 1 and −1 on their diagonals
(Kulesza, 2012, Theorem 4.1). We call such a transform
a DN -similarity of K.
In view of this equivalence class, we define the following
pseudo-distance between kernels K and K 0 :
ρ(K, K 0 ) = inf |DKD − K 0 |∞ ,
D∈DN

where for any matrix K, |K|∞ = maxi,j∈[N ] |Ki,j | denotes the entrywise sup-norm.

For any S ⊂ [N ], we write ∆S = det(KS ), where KS
denotes the |S| × |S| submatrix of K obtained by keeping
rows and colums with indices in S. Note that for 1 ≤ i 6=
j ≤ N , we have the following relations:
Ki,i = P[i ∈ Y ],
∆{i,j} = P[{i, j} ⊆ Y ],
q
and |Ki,j | = Ki,i Kj,j − ∆{i,j} . Therefore, the principal minors of size one and two of K determine K up to the
sign of its off-diagonal entries. In fact, for any K, there
exists an ` depending only on the graph GK induced by K,
such that K can be recovered up to a DN -similarity with
only the knowledge of its principal minors of size at most
`. We will show that this ` is exactly the cycle sparsity.
2.2. DPPs and graphs
In this section, we review some of the interplay between
graphs and DPPs that plays a key role in the definition of
our estimator.
We begin by recalling some standard graph theoretic notions. Let G = ([N ], E), |E| = m. A cycle C of G is
any connected subgraph in which each vertex has even degree. Each cycle C is associated with an incidence vector x ∈ GF (2)m such that xe = 1 if e is an edge in
C and xe = 0 otherwise. The cycle space C of G is
the subspace of GF (2)m spanned by the incidence vectors of the cycles in G. The dimension νG of the cycle
space is called cyclomatic number, and it is well known
that νG := m − N + κ(G), where κ(G) denotes the number of connected components of G.
Recall that a simple cycle is a graph where every vertex
has either degree two or zero and the set of vertices with
degree two form a connected set. A cycle basis is a basis
of C ⊂ GF (2)m such that every element is a simple cycle.
It is well known that every cycle space has a cycle basis of
induced cycles.
Definition 1. The cycle sparsity of a graph G is the minimal ` for which G admits a cycle basis of induced cycles of
length at most `, with the convention that ` = 2 whenever
the cycle space is empty. A corresponding cycle basis is
called a shortest maximal cycle basis.
A shortest maximal cycle basis of the cycle space was also
studied for other reasons by (Chickering et al., 1995). We
defer a discussion of computing such a basis to Section 4.
For any subset S ⊆ [N ], denote by GK (S) = (S, E(S))
the subgraph of GK induced by S. A matching of GK (S)
is a subset M ⊆ E(S) such that any two distinct edges in
M are not adjacent in G(S). The set of vertices incident
to some edge in M is denoted by V (M ). We denote by
M(S) the collection of all matchings of GK (S). Then,
if GK (S) is an induced cycle, we can write the principal

Moments, Cycles and Learning DPPs

minor ∆S = det(KS ) as follows:
X
Y
∆S =
(−1)|M |
M ∈M(S)

+ 2 × (−1)

2
Ki,j

{i,j}∈M
|S|+1

Y

Y

Ki,i

i6∈V (M )

Ki,j .

(1)

{i,j}∈E(S)

Others have considered the relationship between the principal minors of K and recovery of DPP(K). There has been
work regarding the symmetric principal minor assignment
problem, namely the problem of computing a matrix given
an oracle that gives any principal minor in constant time
(Rising et al., 2015).
In our setting, we can approximate the principal minors of
K by empirical averages. However the accuracy of our
estimator deteriorates with the size of the principal minor,
and we must therefore estimate the smallest possible principal minors in order to achieve optimal sample complexity.
Here, we prove a new result, namely, that the smallest `
such that all the principal minors of K are uniquely determined by those of size at most ` is exactly the cycle sparsity
of the graph induced by K.
Proposition 1. Let K ∈ RN ×N be a symmetric matrix,
GK be the graph induced by K, and ` ≥ 3 be some integer.
The kernel K is completely determined up to DN -similarity
by its principal minors of size at most ` if and only if the
cycle sparsity of GK is at most `.
Proof. Note first that all the principal minors of K completely determine K up to a DN -similarity (Rising et al.,
2015, Theorem 3.14). Moreover, recall that principal minors of degree at most 2 determine the diagonal entries of
K as well as the magnitude of its off-diagonal entries. In
particular, given these principal minors, one only needs to
recover the signs of the off-diagonal entries of K. Let the
sign of cycle C in K be the product of the signs of the
entries of K corresponding to the edges of C.
Suppose GK has cycle sparsity ` and let (C1 , . . . , Cν ) be a
cycle basis of GK where each Ci , i ∈ [ν] is an induced cycle of length at most `. By (1), the sign of any Ci , i ∈ [ν] is
completely determined by the principal minor ∆S , where
S is the set of vertices of Ci and is such that |S| ≤ `.
Moreover, for i ∈ [ν], let xi ∈ GF (2)m denote the incidence vector of Ci . By definition,
the incidence vector x of
P
any cycle C is given by i∈I xi for some subset I ⊂ [ν].
The sign of C is then given by the product of the signs of
Ci , i ∈ I and thus by corresponding principal minors. In
particular, the signs of all cycles are determined by the principal minors ∆S with |S| ≤ `. In turn, by Theorem 3.12
in (Rising et al., 2015), the signs of all cycles completely
determine K, up to a DN -similarity.
Next, suppose the cycle sparsity of GK is at least ` + 1,

and let C` be the subspace of GF (2)m spanned by the induced cycles of length at most ` in GK . Let x1 , . . . , xν be
a basis of C` made of the incidence column vectors of induced cycles of length at most ` in GK and form the matrix
A ∈ GF (2)m×ν by concatenating the xi ’s. Since C` does
not span the cycle space of GK , ν < νGK ≤ m. Hence,
the rank of A is less than m, so the null space of A> is
non trivial. Let x̄ be the incidence column vector of an induced cycle C̄ that is not in C` , and let h ∈ GL(2)m with
A> h = 0, h 6= 0 and x̄> h = 1. These three conditions are
compatible because C̄ ∈
/ C` . We are now in a position to
0
= Ki,i
define an alternate kernel K 0 as follows: Let Ki,i
0
and |Ki,j | = |Ki,j | for all i, j ∈ [N ]. We define the signs
of the off-diagonal entries of K 0 as follows: For all edges
e = {i, j}, i 6= j, sgn(Ke0 ) = sgn(Ke ) if he = 0 and
sgn(Ke0 ) = − sgn(Ke ) otherwise. We now check that K
and K 0 have the same principal minors of size at most `
but differ on a principal minor of size larger than `. To that
end, let x be the incidence vector of a cycle C in C` so that
x = Aw for some w ∈ GL(2)ν . Thus the sign of C in K
is given by
Y
Y
>
Ke = (−1)x h
Ke0
e : xe =1

e : xe =1

= (−1)

w> A> h

Y
e : xe =1

Ke0 =

Y

Ke0

e : xe =1

because A> h = 0. Therefore, the sign of any C ∈ C` is
the same in K and K 0 . Now, let S ⊆ [N ] with |S| ≤ `, and
let G = GKS = GKS0 be the graph corresponding to KS
(or, equivalently, to KS0 ). For any induced cycle C in G,
C is also an induced cycle in GK and its length is at most
`. Hence, C ∈ C` and the sign of C is the same in K and
K 0 . By (Rising et al., 2015, Theorem 3.12), det(KS ) =
det(KS0 ). Next observe that the sign of C̄ in K is given by
Y
Y
Y
>
Ke = (−1)x̄ h
Ke0 = −
Ke0 .
e : x̄e =1

e : x̄e =1

e : xe =1

Note also that since C̄ is an induced cycle of GK = GK 0 ,
the above quantity is nonzero. Let S̄ be the set of vertices
in C̄. By (1) and the above display, we have det(KS̄ ) 6=
det(KS̄0 ). Together with (Rising et al., 2015, Theorem
3.14), it yields K 6= DK 0 D for all D ∈ DN .
2.3. Definition of the Estimator
Our procedure is based on the previous result and can be
summarized as follows. We first estimate the diagonal entries (i.e., the principal minors of size one) of K by the
method of moments. By the same method, we estimate the
principal minors of size two of K, and we deduce estimates
of the magnitude of the off-diagonal entries. To use these
estimates to deduce an estimate Ĝ of GK , we make the
following assumption on the kernel K.

Moments, Cycles and Learning DPPs

Assumption 1. Fix α ∈ (0, 1). For all 1 ≤ i < j ≤ N ,
either Ki,j = 0, or |Ki,j | ≥ α.
Finally, we find a shortest maximal cycle basis of Ĝ, and
we set the signs of our non-zero off-diagonal entry estimates by using estimators of the principal minors induced
by the elements of the basis, again obtained by the method
of moments.
n
X
ˆS = 1
1S⊆Yp , and define
For S ⊆ [N ], set ∆
n p=1
ˆ {i}
K̂i,i = ∆

and

Define Ĝ = ([N ], Ê), where, for i 6= j, {i, j} ∈ Ê if and
only if B̂i,j ≥ 12 α2 . The graph Ĝ is our estimator of GK .
Let {Ĉ1 , ..., ĈνĜ } be a shortest maximal cycle basis of the
cycle space of Ĝ. Let Ŝi ⊆ [N ] be the subset of vertices of
Ĉi , for 1 ≤ i ≤ νĜ . We define
X
Y
Y
ˆ −
(−1)|M |
B̂i,j
K̂i,i ,
Ĥi = ∆
Ŝi
{i,j}∈M

i6∈V (M )

for 1 ≤ i ≤ νĜ . In light of (1), for large enough n, this
quantity should be close to
Y
Hi = 2 × (−1)|Ŝi |+1
Ki,j .
{i,j}∈E(Ŝi )

We note that this definition is only symbolic in nature, and
computing Ĥi in this fashion is extremely inefficient. Instead, to compute it in practice, we will use the determinant
of an auxiliary matrix, computed via a matrix factorization.
e ∈ RN ×N such that
Namely, let us define the matrix K
e i,i = K̂i,i for 1 ≤ i ≤ N , and K
e i,j = B̂ 1/2 . We have
K
i,j
X
Y
Y
|M |
e =
det K
(−1)
B̂
K̂i,i
i,j
Ŝi
M ∈M

+ 2 × (−1)

{i,j}∈M

i6∈V (M )

{i,j}∈Ê(Ŝi )

so that we may equivalently write
ˆ − det(K
e ) + 2 × (−1)|Ŝi |+1
Ĥi = ∆
Ŝi
Ŝi

Y

ρ(K̂, K) < 4ε/α .

B̂i,j ≤ (Ki,i + α2 /16)(Kj,j + α2 /16) − (∆{i,j} − α2 /16)
2
≤ Ki,j
+ α2 /4

and
B̂i,j ≥ (Ki,i − α2 /16)(Kj,j − α2 /16) − (∆{i,j} + α2 /16)
2
≥ Ki,j
− 3α2 /16,
2
giving |B̂i,j − Ki,j
| < α2 /4. Thus, we can correctly determine whether Ki,j = 0 or |Ki,j | ≥ α, yielding Ĝ = GK .
In particular, the cycle basis Ĉ1 , . . . , ĈνĜ of Ĝ is a cycle
basis of GK . Let 1 ≤ i ≤ νĜ . Denote by t = (α/4)|Si | .
We have




Ĥi − Hi 
h
i
ˆ − ∆ | + |M(Ŝi )| max (1 + 4tx)|Ŝi | − 1
≤ |∆
Ŝi
Ŝi
x∈±1
h
i
≤ (α/4)|Ŝi | + |M(Ŝi )| (1 + 4t)|Ŝi | − 1
$
%!
|
Ŝ
|
i
≤ (α/4)|Ŝi | + T |Ŝi |,
4t T (|Ŝi |, |Ŝi |)
2

≤ (α/4)|Ŝi | + 4t (2

|Ŝi |
2

− 1)(2|Ŝi | − 1)

≤ (α/4)|Ŝi | + t22|Ŝi |
< 2α|Ŝi | ≤ |Hi |,

1/2
B̂i,j ,

Y

|Ŝi |+1

The main result of this subsection is the following lemma
which relates the quality of estimation of K in terms of ρ
to the quality of estimation of the principal minors ∆S .
Lemma 1. Let K satisfy Assumption 1, and let ` be the
ˆ S − ∆S | ≤ ε for all
cycle sparsity of GK . Let ε > 0. If |∆
ˆ
S ⊆ [N ] with |S| ≤ 2 and if |∆S − ∆S | ≤ (α/4)|S| for all
S ⊆ [N ] with 3 ≤ |S| ≤ `, then

2
Proof. We can bound |B̂i,j − Ki,j
|, namely,

ˆ {i,j} ,
B̂i,j = K̂i,i K̂j,j − ∆

2
where K̂i,i and B̂i,j are our estimators of Ki,i and Ki,j
,
respectively.

M ∈M(Ŝi )

2.4. Geometry

1/2

B̂i,j .

{i,j}∈Ê(Ŝi )

Finally, let m̂ = |Ê|. Set the matrix A ∈ GF (2)νĜ ×m̂
with i-th row representing Ĉi in GF (2)m , 1 ≤ i ≤ νĜ ,
b = (b1 , . . . , bνĜ ) ∈ GF (2)νĜ with bi = 12 [sgn(Ĥi ) + 1],
1 ≤ i ≤ νĜ , and let x ∈ GF (2)m be a solution to the linear
system Ax = b if a solution exists, x = 1m otherwise.
We define K̂i,j = 0 if {i, j} ∈
/ Ê and K̂i,j = K̂j,i =
1/2
(2x{i,j} − 1)B̂i,j for all {i, j} ∈ Ê.

where, for positive
p < q, we denote by

Pp integers
q
T (q, p) =
.
Therefore,
we can deteri=1 i
Q
mine the sign of the product {i,j}∈E(Ŝi ) Ki,j for every element in the cycle basis and recover the signs
of the non-zero off-diagonal
entries of Ki,j . Hence,



ρ(K̂, K) = max1≤i,j≤N |K̂i,j | − |Ki,j |. For i = j,




|K̂i,j | − |Ki,j | = |K̂i,i − Ki,i | ≤ ε. For i 6= j with
{i,
 j} ∈  Ê = E, one can easily show that

2 
B̂i,j − Ki,j
 ≤ 4ε, yielding
4ε
4ε
1/2
|B̂i,j − |Ki,j || ≤  1/2
≤ ,
B̂

α
+ |Ki,j |
i,j

which completes the proof.

Moments, Cycles and Learning DPPs

We are now in a position to establish a sufficient sample
size to estimate K within distance ε.
Theorem 1. Let K satisfy Assumption 1, and let ` be the
cycle sparsity of GK . Let ε > 0. For any A > 0, there
exists C > 0 such that
 1
4 2` 
log N ,
n≥C 2 2 +`
α ε
α
yields ρ(K̂, K) ≤ ε with probability at least 1 − N −A .

where |J| denotes the cardinality of J. The inclusionexclusion principle also yields that pη (S) = | det(K η −
IS̄ )| for all S ⊆ [l], where IS̄ stands for the ` × ` diagonal matrix with ones on its entries (i, i) for i ∈
/ S, zeros
elsewhere.
Denote by D(K + kK − ) the Kullback Leibler divergence
between DPP(K + ) and DPP(K − ):


X
p+ (S)
+
−
D(K kK ) =
p+ (S) log
p− (S)
S⊆[`]

Proof. Using the previous lemma, and applying a union
bound,
h
i
i
X h
ˆ S − ∆S | > αε/4
P ρ(K̂, K) > ε ≤
P |∆

≤

X p+ (S)
(p+ (S) − p− (S))
p− (S)

S⊆[`]

≤ 2α`

|S|≤2

i
h
X
ˆ S − ∆S | > (α/4)|S|
+
P |∆
2≤|S|≤`
2 2

≤ 2N 2 e−nα

ε /8

2`

+ 2N `+1 e−2n(α/4) ,
(2)

where we used Hoeffding’s inequality.

3. Information theoretic lower bound
We prove an information-theoretic lower bound that holds
already if GK is an `-cycle. Let D(KkK 0 ) and H(K, K 0 )
denote respectively the Kullback-Leibler divergence and
the Hellinger distance between DPP(K) and DPP(K 0 ).
Lemma 2. For η ∈ {−, +}, let K η be the `×` matrix with
elements given by

1/2 if j = i



α
if j = i ± 1
.
Ki,j =
ηα
if
(i, j) ∈ {(1, `), (`, 1)}



0
otherwise

and

by (3). Using the fact that 0 < α ≤ 1/8 and the Gershgorin
circle theorem, we conclude that the absolute value of all
eigenvalues of K η − IS̄ are between 1/4 and 3/4. Thus we
obtain from (4) the bound D(K + kK − ) ≤ 4(6α)` .
Using the same arguments as above, the Hellinger distance
H(K + , K − ) between DPP(K + ) and DPP(K − ) satisfies
!2
X
p+ (J) − p− (J)
+
−
p
p
H(K , K ) =
p+ (J) + p− (J)
J⊆[`]
≤

which completes the proof.
The sample complexity lower bound now follows from
standard arguments.
Theorem 2. Let 0 < ε ≤ α ≤ 1/8 and 3 ≤ ` ≤ N . There
exists a constant C > 0 such that if

H(K, K 0 ) ≤ (8α2 )` .

.

If Y is sampled from DPP(K η ), we denote by pη (S) =
P[Y = S], for S ⊆ [`]. It follows from the inclusionexclusion principle that for all S ⊆ [`],
X
+
−
p+ (S) − p− (S) =
(−1)|J| (det KS∪J
− det KS∪J
)
J⊆[`]\S

= (−1)`−|S| (det K + − det K − ) = ±2α` ,
(3)

X 4α2`
= (8α2 )`
2 · 4−`

J⊆[`]

n≤C

Proof. It is straightforward to see that
(
2α` if J = [`]
+
−
det(KJ ) − det(KJ ) =
0
else

(4)

S⊆[`]

Then, for any α ≤ 1/8, it holds
D(KkK 0 ) ≤ 4(6α)` ,

X | det(K + − I )|
S̄
,
| det(K − − IS̄ )|

 8`
log(N/`) log N 
+
+
,
α2`
(6α)`
ε2

then the following holds: for any estimator K̂ based on n
samples, there exists a kernel K that satisfies Assumption 1
and such that the cycle sparsity of GK is ` and for which
ρ(K̂, K) ≥ ε with probability at least 1/3.
Proof. Recall the notation of Lemma 2. First consider the
N × N block diagonal matrix K (resp. K 0 ) where its first
block is K + (resp. K − ) and its second block is IN −` . By a
standard argument, the Hellinger distance Hn (K, K 0 ) between the product measures DPP(K)⊗n and DPP(K 0 )⊗n
satisfies
1−

H2 (K, K 0 ) n
α2` n
H2n (K, K 0 )
≥ 1−
,
= 1−
2
2
2 × 8`

Moments, Cycles and Learning DPPs

which yields the first term in the desired lower bound.
Next, by padding with zeros, we can assume that L = N/`
is an integer. Let K (0) be a block diagonal matrix where
each block is K + (using the notation of Lemma 2). For
j = 1, . . . , L, define the N × N block diagonal matrix
K (j) as the matrix obtained from K (0) by replacing its jth
block with K − (again using the notation of Lemma 2).
Since DPP(K (j) ) is the product measure of L lower dimensional DPPs that are each independent of each other,
using Lemma 2 we have D(K (j) kK (0) ) ≤ 4(6α)` . Hence,
by Fano’s lemma (see, e.g., Corollary 2.6 in (Tsybakov,
2009)), the sample complexity to learn the kernel of a DPP
within a distance ε ≤ α is


log(N/`)
Ω
(6α)`
which yields the second term.
The third term follows from considering K0 = (1/2)IN
and letting Kj be obtained from K0 by adding ε to
the jth entry along the diagonal. It is easy to see that
D(Kj kK0 ) ≤ 8ε2 . Hence, a second application of Fano’s
lemma yields that the sample complexity to learn the kernel
of a DPP within a distance ε is Ω( logε2N ).
The third term in the lower bound is the standard parametric
term and is unavoidable in order to estimate the magnitude
of the coefficients of K. The other terms are more interesting. They reveal that the cycle sparsity of GK , namely, `,
plays a key role in the task of recovering the sign pattern of
K. Moreover the theorem shows that the sample complexity of our method of moments estimator is near optimal.

4. Algorithms

Algorithm 1 Compute Estimator K̂
Input: samples Y1 , ..., Yn , parameter α > 0.
ˆ S for all |S| ≤ 2.
Compute ∆
ˆ {i} for 1 ≤ i ≤ N .
Set K̂i,i = ∆
Compute B̂i,j for 1 ≤ i < j ≤ N .
e ∈ RN ×N and Ĝ = ([N ], Ê).
Form K
Compute a shortest maximal cycle basis {v̂1 , ..., v̂νĜ }.
ˆ for 1 ≤ i ≤ ν .
Compute ∆
Ĝ
Ŝi
e for 1 ≤ i ≤ ν .
Compute ĈŜi using det K
Ŝi
Ĝ
Construct A ∈ GF (2)νĜ ×m , b ∈ GF (2)νĜ .
Solve Ax = b using Gaussian elimination.
1/2
Set K̂i,j = K̂j,i = (2x{i,j} − 1)B̂i,j , for all {i, j} ∈ Ê.

Lemma 4. Let A ∈ GF (2)ν×m , b ∈ GF (2)ν . Then Gaussian elimination will find a vector x ∈ GF (2)m such that
Ax = b or conclude that none exists in O(ν 2 m) time.
We give our procedure for computing the estimator K̂ in
Algorithm 1. In the following theorem, we bound the running time of Algorithm 1 and establish an upper bound on
the sample complexity needed to solve the recovery problem as well as the sample complexity needed to compute
an estimate K̂ that is close to K.
Theorem 3. Let K ∈ RN ×N be a symmetric matrix satisfying 0  K  I, and satisfying Assumption 1. Let GK be
the graph induced by K and ` be the cycle sparsity of GK .
Let Y1 , ..., Yn be samples from DPP(K) and δ ∈ (0, 1). If
n>

Lemma 3. Let G = ([N ], E), |E| = m. There is an
algorithm to compute a shortest maximal cycle basis in
O(m2 N/ log N ) time.
In addition, we recall the following standard result regarding the complexity of Gaussian elimination (Golub &
Van Loan, 2012).

(α/4)

2`

,

then with probability at least 1 − δ, Algorithm 1 computes
an estimator K̂ which recovers the signs of K up to a DN similarity and satisfies

4.1. Horton’s algorithm
We first give an algorithm to compute the estimator K̂
defined in Section 2. A well-known algorithm of Horton (Horton, 1987) computes a cycle basis of minimum
total length in time O(m3 N ). Subsequently, the running
time was improved to O(m2 N/ log N ) time (Amaldi et al.,
2010). Also, it is known that a cycle basis of minimum total
length is a shortest maximal cycle basis (Chickering et al.,
1995). Together, these results imply the following.

log(N `+1 /δ)

ρ(K, K̂) <

1
α



8 log(4N `+1 /δ)
n

1/2
(5)

in O(m3 + nN 2 ) time.
Proof. (5) follows directly from (2) in the proof of Theorem 1. That same proof also shows that with probability
at least 1 − δ, the support of GK and the signs of K are
recovered up to a DN -similarity. What remains is to upper bound the worst case run time of Algorithm 1. We
will perform this analysis line by line. Initializing K̂ requires O(N 2 ) operations. Computing ∆S for all subsets
|S| ≤ 2 requires O(nN 2 ) operations. Setting K̂i,i requires
O(N ) operations. Computing B̂i,j for 1 ≤ i < j ≤ N
e requires O(N 2 )
requires O(N 2 ) operations. Forming K
operations. Forming GK requires O(N 2 ) operations. By

Moments, Cycles and Learning DPPs

Lemma 3, computing a shortest maximal cycle basis requires O(mN ) operations. Constructing the subsets Si ,
ˆS
1 ≤ i ≤ νĜ , requires O(mN ) operations. Computing ∆
i
for 1 ≤ i ≤ νĜ requires O(nm) operations. Computing
e i ]) for 1 ≤ i ≤ ν requires O(m`3 )
ĈSi using det(K[S
Ĝ
e i ] is used to
operations, where a factorization of each K[S
3
compute each determinant in O(` ) operations. Constructing A and b requires O(m`) operations. By Lemma 4,
finding a solution x using Gaussian elimination requires
O(m3 ) operations. Setting K̂i,j for all edges {i, j} ∈ E
requires O(m) operations. Put this all together, Algorithm
1 runs in O(m3 + nN 2 ) time.

Algorithm 2 Compute Signs of Edges in Chordal Graph
ˆ S for |S| ≤ 3.
Input: GK = ([N ], E) chordal, ∆
Compute a PEO {v1 , ..., vN }.
Compute the spanning forest G0 = ([N ], E 0 ).
Set all edges in E 0 to have positive sign.
Compute Ĉ{i,j,i∗ } for all {i, j} ∈ E \ E 0 , j < i.
Order edges E \ E 0 = {e1 , ..., eν } such that i > j if
max ei < max ej .
Visit edges in sorted order and for e = {i, j}, j > i, set
sgn({i, j}) = sgn(Ĉ{i,j,i∗ } ) sgn({i, i∗ }) sgn({j, i∗ }).

4.2. Chordal Graphs
Here we show that it is possible to obtain faster algorithms
by exploiting the structure of GK . Specifically, in the case
where GK chordal, we give an O(m) time algorithm to
determine the signs of the off-diagonal entries of the estimator K̂, resulting in an improved overall runtime of
O(m + nN 2 ). Recall that a graph G = ([N ], E) is said
to be chordal if every induced cycle in G is of length three.
Moreover, a graph G = ([N ], E) has a perfect elimination ordering (PEO) if there exists an ordering of the vertex set {v1 , ..., vN } such that, for all i, the graph induced
by {vi } ∪ {vj |{i, j} ∈ E, j > i} is a clique. It is well
known that a graph is chordal if and only if it has a PEO. A
PEO of a chordal graph with m edges can be computed in
O(m) operations using lexicographic breadth-first search
(Rose et al., 1976).
Lemma 5. Let G = ([N ], E), be a chordal graph and
{v1 , ..., vn } be a PEO. Given i, let i∗ := min{j|j >
i, {vi , vj } ∈ E}. Then the graph G0 = ([N ], E 0 ), where
N −κ(G)
E 0 = {{vi , vi∗ }}i=1
, is a spanning forest of G.
Proof. We first show that there are no cycles in G0 . Suppose to the contrary, that there is an induced cycle C of
length k on the vertices {vj1 , ..., vjk }. Let v be the vertex
of smallest index. Then v is connected to two other vertices
in the cycle of larger index. This is a contradiction to the
construction.
What remains is to show that |E 0 | = N − κ(G). It suffices
to prove the case κ(G) = 1. Suppose to the contrary, that
there exists a vertex vi , i < N , with no neighbors of larger
index. Let P be the shortest path in G from vi to vN . By
connectivity, such a path exists. Let vk be the vertex of
smallest index in the path. However, it has two neighbors
in the path of larger index, which must be adjacent to each
other. Therefore, there is a shorter path.
Now, given the chordal graph GK induced by K and the
estimates of principal minors of size at most three, we provide an algorithm to determine the signs of the edges of

GK , or, equivalently, the off-diagonal entries of K.
Theorem 4. If GK is chordal, Algorithm 2 correctly determines the signs of the edges of GK in O(m) time.
Proof. We will simultaneously perform a count of the operations and a proof of the correctness of the algorithm.
Computing a PEO requires O(m) operations. Computing
the spanning forest requires O(m) operations. The edges
of the spanning tree can be given arbitrary sign, because it
is a cycle-free graph. This assigns a sign to two edges of
each 3-cycle. Computing each Ĉ{i,j,i∗ } requires a constant
number of operations because ` = 3, requiring a total of
O(m − N ) operations. Ordering the edges requires O(m)
operations. Setting the signs of each remaining edge requires O(m) operations.
Therefore, when GK is chordal, the overall complexity
required by our algorithm to compute K̂ is reduced to
O(m + nN 2 ).

5. Experiments
Here we present experiments to supplement the theoretical
results of the paper. We test our algorithm on two types
of random matrices. First, we consider the matrix K ∈
RN ×N corresponding to the cycle on N vertices,
K=

1
1
I + A,
2
4

where A is symmetric and has non-zero entries only on the
edges of the cycle, either +1 or −1, each with probability
1/2. By the Gershgorin circle theorem, 0  K  I. Next,
we consider the matrix K ∈ RN ×N corresponding to the
clique on N vertices,
K=

1
1
I + √ A,
2
4 N

Moments, Cycles and Learning DPPs

where A is symmetric and has all entries either +1 √
or −1,
each with
probability
1/2.
It
is
well
known
that
−2
N 
√
A  2 N with high probability, implying 0  K  I.
For both cases and for a range of values of matrix dimension N and samples n, we run our algorithm on 50 randomly generated instances. We record the proportion of
trials where we recover the graph induced by K, and the
proportion of the trials where we recover both the graph
and correctly determine the signs of the entries.
In Figure 1, the shade of each box represents the proportion of trials where recovery was successful for a given pair
N, n. A completely white box corresponds to zero success
rate, black to a perfect success rate.
The plots corresponding to the cycle and the clique are
telling. We note that for the clique, recovering the sparsity pattern and recovering the signs of the off-diagonal entries come hand-in-hand. However, for the cycle, there is
a noticeable gap between the number of samples required
to recover the sparsity pattern and the number of samples
required to recover the signs of the off-diagonal entries.
This empirically confirms the central role that cycle sparsity plays in parameter estimation, and further corroborates
our theoretical results.

(a) graph recovery, cycle

(b) graph and sign recovery, cycle

6. Conclusion and open questions
In this paper, we gave the first provable guarantees for
learning the parameters of a DPP. Our upper and lower
bounds reveal the key role played by the parameter `, which
is the cycle sparsity of graph induced by the kernel of the
DPP. Our estimator does not need to know ` beforehand,
but can adapt to the instance. Moreover, our procedure
outputs an estimate of `, which could potentially be used
for further inference questions such as testing and confidence intervals. An interesting open question is whether
on a graph by graph basis, the parameter ` exactly determines the optimal sample complexity. Moreover when the
number of samples is too small, can we exactly characterize
which signs can be learned correctly and which cannot (up
to a similarity transformation by D)? Such results would
lend new theoretical insights into the output of algorithms
for learning DPPs, and which individual parameters in the
estimate we can be confident about and which we cannot.

(c) graph recovery, clique

(d) graph and sign recovery, clique

Acknowledgements.
A.M. is supported in part by
NSF CAREER Award CCF-1453261, NSF Large CCF1565235, a David and Lucile Packard Fellowship and an
Alfred P. Sloan Fellowship. P.R. is supported in part
by NSF CAREER DMS-1541099, NSF DMS-1541100,
DARPA W911NF-16-1-0551, ONR N00014-17-1-2147
and a grant from the MIT NEC Corporation.

Figure 1: Plots of the proportion of successive graph recovery, and graph and sign recovery, for random matrices with
cycle and clique graph structure, respectively. The darker
the box, the higher the proportion of trials that were recovered successfully.

Moments, Cycles and Learning DPPs

References
Affandi, Raja Hafiz, Fox, Emily B., Adams, Ryan P., and
Taskar, Benjamin. Learning the parameters of determinantal point process kernels. In Proceedings of the 31th
International Conference on Machine Learning, ICML
2014, Beijing, China, 21-26 June 2014, pp. 1224–1232,
2014.
Amaldi, Edoardo, Iuliano, Claudio, and Rizzi, Romeo. Efficient deterministic algorithms for finding a minimum
cycle basis in undirected graphs. In International Conference on Integer Programming and Combinatorial Optimization, pp. 397–410. Springer, 2010.
Batmanghelich, Nematollah Kayhan, Quon, Gerald,
Kulesza, Alex, Kellis, Manolis, Golland, Polina, and
Bornn, Luke. Diversifying sparsity using variational determinantal point processes. ArXiv: 1411.6307, 2014.
Borodin, Alexei and Rains, Eric M. Eynard–mehta theorem, schur process, and their pfaffian analogs. Journal
of statistical physics, 121(3):291–317, 2005.
Brunel, Victor-Emmanuel, Moitra, Ankur, Rigollet,
Philippe, and Urschel, John.
Maximum likelihood estimation of determinantal point processes.
arXiv:1701.06501, 2017.
Chickering, David M., Geiger, Dan, and Heckerman,
David. On finding a cycle basis with a shortest maximal cycle. Information Processing Letters, 54(1):55 –
58, 1995.
Çivril, Ali and Magdon-Ismail, Malik. On selecting a maximum volume sub-matrix of a matrix and related problems. Theoretical Computer Science, 410(47-49):4801–
4811, 2009.
Deshpande, Amit and Rademacher, Luis. Efficient volume
sampling for row/column subset selection. In Foundations of Computer Science (FOCS), 2010 51st Annual
IEEE Symposium on, pp. 329–338. IEEE, 2010.
Dyson, Freeman J. Statistical theory of the energy levels
of complex systems. III. J. Mathematical Phys., 3:166–
175, 1962. ISSN 0022-2488.
Gillenwater, Jennifer A, Kulesza, Alex, Fox, Emily, and
Taskar, Ben. Expectation-maximization for learning determinantal point processes. In NIPS, 2014.
Golub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHU Press, 2012.
Horton, Joseph Douglas. A polynomial-time algorithm to
find the shortest cycle basis of a graph. SIAM Journal on
Computing, 16(2):358–366, 1987.

Kulesza, A. Learning with determinantal point processes.
PhD thesis, University of Pennsylvania, 2012.
Kulesza, Alex and Taskar, Ben. k-DPPs: Fixed-size determinantal point processes. In Proceedings of the 28th
International Conference on Machine Learning, ICML
2011, Bellevue, Washington, USA, June 28 - July 2,
2011, pp. 1193–1200, 2011.
Kulesza, Alex and Taskar, Ben. Determinantal Point
Processes for Machine Learning. Now Publishers
Inc., Hanover, MA, USA, 2012. ISBN 1601986289,
9781601986283.
Lee, Donghoon, Cha, Geonho, Yang, Ming-Hsuan, and Oh,
Songhwai. Individualness and determinantal point processes for pedestrian detection. In Computer Vision ECCV 2016 - 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings,
Part VI, pp. 330–346, 2016.
Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Fast sampling for strongly rayleigh measures with application to
determinantal point processes. 1607.03559, 2016a.
Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Fast dpp
sampling for nystrom with application to kernel methods. International Conference on Machine Learning
(ICML), 2016b.
Lin, Hui and Bilmes, Jeff A. Learning mixtures of submodular shells with application to document summarization.
In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, Catalina Island, CA,
USA, August 14-18, 2012, pp. 479–490, 2012.
Macchi, Odile. The coincidence approach to stochastic
point processes. Advances in Appl. Probability, 7:83–
122, 1975. ISSN 0001-8678.
Mariet, Zelda and Sra, Suvrit. Fixed-point algorithms for
learning determinantal point processes. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pp. 2389–2397, 2015.
Nikolov, Aleksandar. Randomized rounding for the largest
simplex problem. In Proceedings of the Forty-Seventh
Annual ACM on Symposium on Theory of Computing,
pp. 861–870. ACM, 2015.
Nikolov, Aleksandar and Singh, Mohit. Maximizing determinants under partition constraints. In STOC, pp. 192–
201, 2016.
Rebeschini, Patrick and Karbasi, Amin. Fast mixing for
discrete point processes. In COLT, pp. 1480–1500, 2015.

Moments, Cycles and Learning DPPs

Rising, Justin, Kulesza, Alex, and Taskar, Ben. An efficient
algorithm for the symmetric principal minor assignment
problem. Linear Algebra and its Applications, 473:126
– 144, 2015.
Rose, Donald J, Tarjan, R Endre, and Lueker, George S. Algorithmic aspects of vertex elimination on graphs. SIAM
Journal on computing, 5(2):266–283, 1976.
Snoek, Jasper, Zemel, Richard S., and Adams,
Ryan Prescott.
A determinantal point process latent variable model for inhibition in neural spiking data.
In Advances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a meeting
held December 5-8, 2013, Lake Tahoe, Nevada, United
States., pp. 1932–1940, 2013.
Summa, Marco Di, Eisenbrand, Friedrich, Faenza, Yuri,
and Moldenhauer, Carsten. On largest volume simplices
and sub-determinants. In Proceedings of the TwentySixth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 315–323. Society for Industrial and Applied
Mathematics, 2015.
Tsybakov, Alexandre B. Introduction to nonparametric estimation. Springer Series in Statistics. Springer, New
York, 2009.
Xu, Haotian and Ou, Haotian. Scalable discovery of audio fingerprint motifs in broadcast streams with determinantal point process based motif clustering. IEEE/ACM
Trans. Audio, Speech & Language Processing, 24(5):
978–989, 2016.
Yao, Jin-ge, Fan, Feifan, Zhao, Wayne Xin, Wan, Xiaojun,
Chang, Edward Y., and Xiao, Jianguo. Tweet timeline
generation with determinantal point processes. In Proceedings of the Thirtieth AAAI Conference on Artificial
Intelligence, February 12-17, 2016, Phoenix, Arizona,
USA., pp. 3080–3086, 2016.

