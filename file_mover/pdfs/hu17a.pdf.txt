Dissipativity Theory for Nesterov’s Accelerated Method

Bin Hu 1 Laurent Lessard 1

Abstract
In this paper, we adapt the control theoretic concept of dissipativity theory to provide a natural
understanding of Nesterov’s accelerated method.
Our theory ties rigorous convergence rate analysis to the physically intuitive notion of energy
dissipation. Moreover, dissipativity allows one to
efficiently construct Lyapunov functions (either
numerically or analytically) by solving a small
semidefinite program. Using novel supply rate
functions, we show how to recover known rate
bounds for Nesterov’s method and we generalize
the approach to certify both linear and sublinear
rates in a variety of settings. Finally, we link the
continuous-time version of dissipativity to recent
works on algorithm analysis that use discretizations of ordinary differential equations.

1. Introduction
Nesterov’s accelerated method (Nesterov, 2003) has garnered attention and interest in the machine learning community because of its fast global convergence rate guarantees. The original convergence rate proofs of Nesterov’s
accelerated method are derived using the method of estimate sequences, which has proven difficult to interpret.
This observation motivated a sequence of recent works
on new analysis and interpretations of Nesterov’s accelerated method (Bubeck et al., 2015; Lessard et al., 2016; Su
et al., 2016; Drusvyatskiy et al., 2016; Flammarion & Bach,
2015; Wibisono et al., 2016; Wilson et al., 2016).
Many of these recent papers rely on Lyapunov-based stability arguments. Lyapunov theory is an analogue to the
principle of minimum energy and brings a physical intuition to convergence behaviors. When applying such proof
techniques, one must construct a Lyapunov function, which
is a nonnegative function of the algorithm’s state (an “internal energy”) that decreases along all admissible trajec1
University of Wisconsin–Madison, Madison, WI 53706,
USA. Correspondence to: Bin Hu <bhu38@wisc.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tories. Once a Lyapunov function is found, one can relate
the rate of decrease of this internal energy to the rate of
convergence of the algorithm. The main challenge in applying Lyapunov’s method is finding a suitable Lyapunov
function.
There are two main approaches for Lyapunov function constructions. The first approach adopts the integral quadratic
constraint (IQC) framework (Megretski & Rantzer, 1997)
from control theory and formulates a linear matrix equality (LMI) whose feasibility implies the linear convergence
of the algorithm (Lessard et al., 2016). Despite the generality of the IQC approach and the small size of the associated LMI, one must typically resort to numerical simulations to solve the LMI. The second approach seeks an
ordinary differential equation (ODE) that can be appropriately discretized to yield the algorithm of interest. One can
then gain intuition about the trajectories of the algorithm
by examining trajectories of the continuous-time ODE (Su
et al., 2016; Wibisono et al., 2016; Wilson et al., 2016).
The work of Wilson et al. (2016) also establishes a general equivalence between Lyapunov functions and estimate
sequence proofs.
In this paper, we bridge the IQC approach (Lessard et al.,
2016) and the discretization approach (Wilson et al., 2016)
by using dissipativity theory (Willems, 1972a;b). The term
“dissipativity” is borrowed from the notion of energy dissipation in physics and the theory provides a general approach for the intuitive understanding and construction
of Lyapunov functions. Dissipativity for quadratic Lyapunov functions in particular (Willems, 1972b) has seen
widespread use in controls. In the sequel, we tailor dissipativity theory to the automated construction of Lyapunov
functions, which are not necessarily quadratic, for the analysis of optimization algorithms. Our dissipation inequality
leads to an LMI condition that is simpler than the one in
Lessard et al. (2016) and hence more amenable to being
solved analytically. When specialized to Nesterov’s accelerated method, our LMI recovers the Lyapunov function
proposed in Wilson et al. (2016). Finally, we extend our
LMI-based approach to the sublinear convergence analysis of Nesterov’s accelerated method in both discrete and
continuous time domains. This complements the original
LMI-based approach in Lessard et al. (2016), which mainly
handles linear convergence rate analyses.

Dissipativity Theory for Nesterov’s Accelerated Method

An LMI-based approach for sublinear rate analysis similar to ours was independently and simultaneously proposed
by Fazlyab et al. (2017). While this work and the present
work both draw connections to the continuous-time results
mentioned above, different algorithms and function classes
are emphasized. For example, Fazlyab et al. (2017) develops LMIs for gradient descent and proximal/projectionbased variants with convex/quasi-convex objective functions. In contrast, the present work develops LMIs tailored to the analysis of discrete-time accelerated methods
and Nesterov’s method in particular.

2. Preliminaries

for all k. The function V is called a storage function, which
quantifies the energy stored in the state ξ. In addition, (2)
is called the dissipation inequality.
The dissipation inequality (2) states that the change of the
internal energy stored in ξk is equal to the difference between the supplied energy and the dissipated energy. Since
there will always be some energy dissipating from the system, the change in the stored energy (which is exactly
V (ξk+1 ) − V (ξk )) is always bounded above by the energy supplied to the system (which is exactly S(ξk , wk )).
A variant of (2) known as the exponential dissipation inequality states that for some 0 ≤ ρ < 1, we have
V (ξk+1 ) − ρ2 V (ξk ) ≤ S(ξk , wk ),

2.1. Notation

(3)

2

Let R and R+ denote the real and nonnegative real numbers, respectively. Let Ip and 0p denote the p × p identity
and zero matrices, respectively. The Kronecker product of
two matrices is denoted A ⊗ B and satisfies the properties
(A⊗B)T = AT ⊗B T and (A⊗B)(C⊗D) = (AC)⊗(BD)
when the matrices have compatible dimensions. Matrix
inequalities hold in the semidefinite sense unless otherwise indicated. A differentiable function f : Rp → R
is m-strongly convex if f (x) ≥ f (y) + ∇f (y)T (x −
2
p
y) + m
2 kx − yk for all x, y ∈ R and is L-smooth if
k∇f (x)−∇f (y)k ≤ Lkx−yk for all x, y ∈ Rp . Note that
f is convex if f is 0-strongly convex. We use x? to denote
a point satisfying ∇f (x? ) = 0. When f is L-smooth and
m-strongly convex, x? is unique.
2.2. Classical Dissipativity Theory
Consider a linear dynamical system governed by the statespace model
ξk+1 = Aξk + Bwk .

(1)

Here, ξk ∈ Rnξ is the state, wk ∈ Rnw is the input, A ∈
Rnξ ×nξ is the state transition matrix, and B ∈ Rnξ ×nw
is the input matrix. The input wk can be physically interpreted as a driving force. Classical dissipativity theory
describes how the internal energy stored in the state ξk
evolves with time k as one applies the input wk to drive
the system. A key concept in dissipativity theory is the
supply rate, which characterizes the energy change in ξk
due to the driving force wk . The supply rate is a function
S : Rnξ × Rnw → R that maps any state/input pair (ξ, w)
to a scalar measuring the amount of energy delivered from
w to state ξ. Now we introduce the notion of dissipativity.
Definition 1 The dynamical system (1) is dissipative with
respect to the supply rate S if there exists a function V :
Rnξ → R+ such that V (ξ) ≥ 0 for all ξ ∈ Rnξ and
V (ξk+1 ) − V (ξk ) ≤ S(ξk , wk )

(2)

which states that at least a fraction (1 − ρ ) of the internal
energy will dissipate at every step.
The dissipation inequality (3) provides a direct way to construct a Lyapunov function based on the storage function.
It is often the case that we have prior knowledge about how
the driving force wk is related to the state ξk . Thus, we may
know additional information about the supply rate function
S(ξk , wk ). For example, if S(ξk , wk ) ≤ 0 for all k then (3)
directly implies that V (ξk+1 ) ≤ ρ2 V (ξk ), and the storage
function V can serve as a Lyapunov function. The condition S(ξk , wk ) ≤ 0 means that the driving force wk does
not inject any energy into the system and may even extract
energy out of the system. Then, the internal energy will
decrease no slower than the linear rate ρ2 and approach a
minimum value at equilibrium.
An advantage of dissipativity theory is that for any
quadratic supply rate, one can automatically construct the
dissipation inequality using semidefinite programming. We
now state a standard result from the controls literature.
Theorem 2 Consider the following quadratic supply rate
with X ∈ R(nξ +nw )×(nξ +nw ) and X = X T .
 T  
ξ
ξ
S(ξ, w) :=
X
.
(4)
w
w
If there exists a matrix P ∈ Rnξ ×nξ with P ≥ 0 such that
 T

A P A − ρ2 P AT P B
− X ≤ 0,
(5)
BTP A
BTP B
then the dissipation inequality (3) holds for all trajectories
of (1) with V (ξ) := ξ T P ξ.
Proof. Based on the state-space model (1), we have
T
V (ξk+1 ) = ξk+1
P ξk+1

= (Aξk + Bwk )T P (Aξk + Bwk )
 T  T
 
ξk
A P A AT P B ξk
=
.
wk
B T P A B T P B wk

Dissipativity Theory for Nesterov’s Accelerated Method



Hence we can left and right multiply (5) by ξkT wkT and
 T
T
ξk wkT , and directly obtain the desired conclusion.
The left-hand side of (5) is linear in P , so (5) is a linear
matrix inequality (LMI) for any fixed A, B, X, ρ. The set
of P such that (5) holds is therefore a convex set and can be
efficiently searched using interior point methods, for example. To apply the dissipativity theory for linear convergence
rate analysis, one typically follows two steps.
1. Choose a proper quadratic supply rate function S satisfying certain desired properties, e.g. S(ξk , wk ) ≤ 0.
2. Solve the LMI (5) to obtain a storage function V ,
which is then used to construct a Lyapunov function.
In step 2, the LMI obtained is typically very small, e.g. 2×2
or 3 × 3, so we can often solve the LMI analytically. For
illustrative purposes, we rephrase the existing LMI analysis
of the gradient descent method (Lessard et al., 2016, §4.4)
using the notion of dissipativity.

Hence for any 0 ≤ ρ < 1, we have pkxk+1 − x? k2 ≤
ρ2 pkxk − x? k2 if there exists p ≥ 0 such that

(1 − ρ2 )p
−αp

 
−αp
−2mL
+
α2 p
m+L


m+L
≤0
−2

(8)

The LMI (8) is simple and can be analytically solved to
recover the existing rate results for the gradient descent
method. For example, we can choose (α, ρ, p) to be
2
L−m 1
2
2
( L1 , 1 − m
L , L ) or ( L+m , L+m , 2 (L + m) ) to immediately recover the standard rate results in Polyak (1987).
Based on the example above, it is evident that choosing a
proper supply rate is critical for the construction of a Lyapunov function. The supply rate (7) turns out to be inadequate for the analysis of Nesterov’s accelerated method.
For Nesterov’s accelerated method, the dependence between the internal energy and the driving force is more
complicated due to the presence of momentum terms. We
will next develop a new supply rate that captures this complicated dependence. We will also make use of this new
supply rate to recover the standard linear rate results for
Nesterov’s accelerated method.

2.3. Example: Dissipativity for Gradient Descent
There is an intrinsic connection between dissipativity theory and the IQC approach (Megretski et al., 2010; Seiler,
2015). The IQC analysis of the gradient descent method
in Lessard et al. (2016) may be reframed using dissipativity theory. Then, the pointwise IQC (Lessard et al., 2016,
Lemma 6) amounts to using a quadratic supply rate S with
S ≤ 0. Specifically, assume f is L-smooth and m-strongly
convex, and consider the gradient descent method

S(ξk , wk ) =

ξk
wk

2mLIp
−(m + L)Ip

−(m + L)Ip
2Ip

Suppose f is L-smooth and m-strongly convex with m >
0. Let x? be the unique point satisfying ∇f (x? ) = 0. Now
we consider Nesterov’s accelerated method, which uses the
following iteration rule to find x? :

(6)

We have xk+1 − x? = xk − x? − α∇f (xk ), where x? is the
unique point satisfying ∇f (x? ) = 0. Define ξk := xk − x?
and wk := ∇f (xk ). Then the gradient descent method
is modeled by (1) with A := Ip and B := −αIp . Since
wk = ∇f (ξk + x? ), we can define the following quadratic
supply rate
T 

3.1. Dissipativity for Nesterov’s Method

xk+1 = yk − α∇f (yk ),

xk+1 = xk − α∇f (xk ).



3. Dissipativity for Accelerated Linear Rates





ξk
wk
(7)

By co-coercivity, we have S(ξk , wk ) ≤ 0 for all k. This
just restates Lessard et al. (2016, Lemma 6). Then, we can
directly apply Theorem 2 to construct the dissipation inequality. We can parameterize P = p ⊗ Ip and define the
storage function as V (ξk ) = pkξk k2 = pkxk − x? k2 . The
LMI (5) becomes

 

(1 − ρ2 )p −αp
−2mL m + L
+
⊗ Ip ≤ 0.
−αp
α2 p
m+L
−2

(9a)

yk = (1 + β)xk − βxk−1 .
We can rewrite (9) as




xk+1 − x?
xk − x?
=A
+ Bwk
xk − x?
xk−1 − x?

(9b)

(10)

where wk := ∇f (yk ) = ∇f ((1 + β)xk − βxk−1 ). Also,
A := Ã ⊗ Ip , B := B̃ ⊗ Ip , and Ã, B̃ are defined by
Ã :=


1+β
1


−β
,
0

B̃ :=

 
−α
.
0

(11)

Hence, Nesterov’s accelerated method (9) is in the form of

T
(1) with ξk = (xk − x? )T (xk−1 − x? )T .
Nesterov’s accelerated method can improve the convergence rate since the input wk depends on both xk and
xk−1 , and drives the state in a specific direction, i.e. along
(1+β)xk −βxk−1 . This leads to a supply rate that extracts
energy out of the system significantly faster than with gradient descent. This is formally stated in the next lemma.

Dissipativity Theory for Nesterov’s Accelerated Method

Lemma 3 Let f be L-smooth and m-strongly convex with
m > 0. Let x? be the unique point satisfying ∇f (x? ) = 0.
Consider Nesterov’s method (9) or equivalently (10). The
following inequalities hold for all trajectories.
T


xk − x?
xk − x?
xk−1 − x?  X1 xk−1 − x?  ≤ f (xk ) − f (xk+1 )
∇f (yk )
∇f (yk )

T


xk − x?
xk − x?
xk−1 − x?  X2 xk−1 − x?  ≤ f (x? ) − f (xk+1 )
∇f (yk )
∇f (yk )


where Xi = X̃i ⊗ Ip for i = 1, 2, and X̃i are defined by
 2

β m −β 2 m
−β
1 2

−β m β 2 m
β
(12)
X̃1 :=
2
−β
β
α(2 − Lα)


(1 + β)2 m −β(1 + β)m −(1 + β)
1
 (13)
−β(1 + β)m
β2m
β
X̃2 :=
2
−(1 + β)
β
α(2 − Lα)
Given any 0 ≤ ρ ≤ 1, one can define the supply rate as
(4) with a particular choice of X := ρ2 X1 + (1 − ρ2 )X2 .
Then this supply rate satisfies the condition
S(ξk , wk ) ≤ ρ2 (f (xk ) − f (x? ))
− (f (xk+1 ) − f (x? )). (14)
Proof. The proof is similar to the proof of (3.23)–(3.24)
in Bubeck (2015), but Bubeck (2015, Lemma 3.6) must be
modified to account for the strong convexity of f . See the
supplementary material for a detailed proof.

then set P := P̃ ⊗ Ip and define the Lyapunov function

Vk :=

xk − x?
xk−1 − x?

T


P


xk − x?
+ f (xk ) − f (x? ),
xk−1 − x?
(16)

which satisfies Vk+1 ≤ ρ2 Vk for all k. Moreover, we have
f (xk ) − f (x? ) ≤ ρ2k V0 for Nesterov’s method.
Proof. Take the Kronecker product of (15) and Ip , and
hence (5) holds with A := Ã ⊗ Ip , B := B̃ ⊗ Ip , and
X := X̃⊗Ip . Let the supply rate S be defined by (4). Then,
define the quadratic storage function V (ξk ) := ξkT P ξk
and apply Theorem 2 to show V (ξk+1 ) − ρ2 V (ξk ) ≤
S(ξk , wk ). Based on the supply rate condition (14), we can
define the Lyapunov function Vk := V (ξk )+f (xk )−f (x? )
and show Vk+1 ≤ ρ2 Vk . Finally, since P ≥ 0, we have
f (xk ) − f (x? ) ≤ ρ2k V0 .
We can immediately recover the proposed Lyapunov function in Wilson et al. (2016, Theorem 6) by setting P̃ to


q
L
hq
q
q i
2


L
m
L .
q 
(17)
P̃ = q
−
2
2
2
m
L
−
2
2
L
Clearly P̃ ≥ 0. Now define κ := m
. Given α = L1 ,
√
pm
κ−1
2
β = √κ+1 , and ρ = 1 − L , it is straightforward to
verify that the left side of the LMI (5) is equal to


√
−1 1 0
m( κ − 1)3 
√
1 −1 0 ,
2(κ + κ)
0
0 0

The supply rate (14) captures how the driving force wk is
impacting the future state xk+1 . The physical interpretation is that there is some amount of hidden energy in the
system that takes the form of f (xk ) − f (x? ). The supply
rate condition (14) describes how the driving force wk is
coupled with the hidden energy in the future. It says the
delivered energy is bounded by a weighted decrease of the
hidden energy. Based on this supply rate, one can search
Lyapunov function using the following theorem.

which is clearly negative semidefinite. Hence we can
immediately construct a Lyapunov
p function using (16) to
prove the linear rate ρ2 = 1 − m
L.

Theorem 4 Let f be L-smooth and m-strongly convex
with m > 0. Let x? be the unique point satisfying
∇f (x? ) = 0. Consider Nesterov’s accelerated method (9).
For any rate 0 ≤ ρ < 1, set X̃ := ρ2 X̃1 +(1−ρ2 )X̃2 where
X̃1 and X̃2 are defined in (12)–(13). In addition, let Ã, B̃
be defined by (11). If there exists a matrix P̃ ∈ R2×2 with
P̃ ≥ 0 such that
 T

Ã P̃ Ã − ρ2 P̃ ÃT P̃ B̃
− X̃ ≤ 0
(15)
B̃ T P̃ Ã
B̃ T P̃ B̃

3.2. Dissipativity Theory for More General Methods

Searching for analytic certificates such as (17) can either
be carried out by directly analyzing the LMI, or by using
numerical solutions to guide the search. For example, numerically solving (15) for any fixed L and m directly yields
(17), which makes finding the analytical expression easy.

We demonstrate the generality of the dissipativity theory
on a more general variant of Nesterov’s method. Consider
a modified accelerated method
xk+1 = (1 + β)xk − βxk−1 − α∇f (yk ),
yk = (1 + η)xk − ηxk−1 .

(18a)
(18b)

When β = η, we recover Nesterov’s accelerated method.
When η = 0, we recover the Heavy-ball method of Polyak

Dissipativity Theory for Nesterov’s Accelerated Method

(1987). We can rewrite (18) in state-space form (10) where
wk := ∇f (yk ) = ∇f ((1 + η)xk − ηxk−1 ), A := Ã ⊗ Ip ,
B := B̃ ⊗ Ip , and Ã, B̃ are defined by


 
1 + β −β
−α
Ã :=
, B̃ :=
.
1
0
0

improved rate bounds. The same is true of supply rates.
For example, we could include λ1 , λ2 ≥ 0 as decision variables and search for a dissipation inequality with supply
rate λ1 S1 + λ2 S2 where e.g. S1 is (7) and S2 is (21).

Lemma 5 Let f be L-smooth and m-strongly convex with
m > 0. Let x? be the unique point satisfying ∇f (x? ) = 0.
Consider the general accelerated method (18). Define the

T
state ξk := (xk − x? )T (xk−1 − x? )T and the input
wk := ∇f (yk ) = ∇f ((1 + η)xk − ηxk−1 ). Then the
following inequalities hold for all trajectories.
 T
 
ξk
ξ
(X1 + X2 ) k ≤ f (xk ) − f (xk+1 )
(19)
wk
wk
 T
 
ξk
ξ
(X1 + X3 ) k ≤ f (x? ) − f (xk+1 )
(20)
wk
wk

The LMI approach in (Lessard et al., 2016) is tailored for
the analysis of linear convergence rates for algorithms that
are time-invariant (the A and B matrices in (1) do not
change with k). We now show that dissipativity theory can
be used to analyze the sublinear rates O(1/k) and O(1/k 2 )
via slight modifications of the dissipation inequality.

with Xi := X̃i ⊗ Ip for i = 1, 2, 3, and X̃i are defined by


−Lδ 2
Lδ 2
−(1 − Lα)δ
1
Lδ 2
−Lδ 2
(1 − Lα)δ 
X̃1 :=
2
−(1 − Lα)δ (1 − Lα)δ α(2 − Lα)
 2

η m −η 2 m −η
1 2
−η m η 2 m
η 
X̃2 :=
2
−η
η
0


2
(1 + η) m −η(1 + η)m −(1 + η)
1

−η(1 + η)m
η2 m
η
X̃3 :=
2
−(1 + η)
η
0
with δ := β − η. In addition, one can define the supply
rate as (4) with X := X1 + ρ2 X2 + (1 − ρ2 )X3 . Then for
all trajectories (ξk , wk ) of the general accelerated method
(18), this supply rate satisfies the inequality
2

S(ξk , wk ) ≤ ρ (f (xk ) − f (x? ))
− (f (xk+1 ) − f (x? )). (21)
Proof. A detailed proof is presented in the supplementary
material. One mainly needs to modify the proof by taking
the difference between β and η into accounts.
Based the supply rate (21), we can immediately modify
Theorem 4 to handle the more general algorithm (18).
Although we do not have general analytical formulas for
the convergence rate of (18), preliminary numerical results
suggest that there
p are a family of (α, β, η) leading to the
rate ρ2 = 1 − m
L , and the required value of P̃ is quite
different from (17). This indicates that our proposed LMI
approach could go beyond the Lyapunov function (17).
Remark 6 It is noted in Lessard et al. (2016, §3.2) that
searching over combinations of multiple IQCs may yield

4. Dissipativity for Sublinear Rates

4.1. Dissipativity for O(1/k) rates
The O(1/k) modification, which we present first, is very
similar to the linear rate result.
Theorem 7 Suppose f has a finite minimum f? . Consider
the LTI system (1) with a supply rate satisfying
S(ξk , wk ) ≤ −(f (zk ) − f? )

(22)

for some sequence {zk }. If there exists a nonnegative
storage function V such that the dissipation inequality (2)
holds over all trajectories of (ξk , wk ), then the following
inequality holds over all trajectories as well.
T
X

(f (zk ) − f? ) ≤ V (ξ0 ).

(23)

k=0

In addition, we have the sublinear convergence rate
min (f (zk ) − f? ) ≤

k:k≤T

V (ξ0 )
.
T +1

(24)

If f (zk+1 ) ≤ f (zk ) for all k, then (24) implies that
(ξ0 )
f (zk ) − f? ≤ Vk+1
for all k.
Proof. By the supply rate condition (22) and the dissipation
inequality (2), we immediately get
V (ξk+1 ) − V (ξk ) + f (zk ) − f? ≤ 0.
Summing the above inequality from k = 0 to T and using
V ≥ 0 yields the desired result.
To address the sublinear rate analysis, the critical step is
to choose an appropriate supply rate. If f is L-smooth
and convex, this is easily done. Consider the gradient
method (6) and define the quantities ξk := xk − x? ,
A := Ip , and B := −αIp as in Section 2.3. Since f is
L-smooth and convex, define the quadratic supply rate
 
 T 
ξk
0p
− 12 Ip ξk
S(ξk , wk ) :=
,
1
wk
− 21 Ip 2L
Ip wk

Dissipativity Theory for Nesterov’s Accelerated Method

which satisfies S(ξk , wk ) ≤ f? − f (xk ) for all k (cocoercivity). Then we can directly apply the LMI (5) with
ρ = 1 to construct the dissipation inequality. Setting
P = p ⊗ Ip and defining the storage function as V (ξk ) :=
pkξk k2 = pkxk − x? k2 , the LMI (5) becomes


 
1
0
−αp
0
2
⊗ Ip ≤ 0,
+
1
1
−αp α2 p
− 2L
2
which is equivalent to

0
−αp +

1
2

−αp + 12
1
α2 p − 2L


≤ 0.

(25)

Due to the (1, 1) entry being zero, (25) holds if and only if
(
(
1
−αp + 12 = 0
p = 2α
=⇒
1
α ≤ L1
≤0
α2 p − 2L
We can choose α =

1
L

and the bound (24) becomes

min(f (xk ) − f? ) ≤
k≤T

Lkx0 − x? k2
.
2(T + 1)

Since gradient descent has monotonically nonincreasing iterates, that is f (xk+1 ) ≤ f (xk ) for all k, we immediately
recover the standard O(1/k) rate result.
4.2. Dissipativity for O(1/k 2 ) rates
Certifying a O(1/k) rate for the gradient method required
solving a single LMI (25). However, this is not the case
for the O(1/k 2 ) rate analysis of Nesterov’s accelerated
method. Nesterov’s algorithm has parameters that depend
on k so the analysis is more involved. We will begin with
the general case and then specialize to Nesterov’s algorithm. Consider the dynamical system
ξk+1 = Ak ξk + Bk wk

(26)

The state matrix Ak and input matrix Bk change with the
time step k, and hence (26) is referred to as a “linear timevarying” (LTV) system. The analysis of LTV systems typically requires a time-dependent supply rate such as
 T
 
ξk
ξ
Sk (ξk , wk ) :=
Xk k
(27)
wk
wk
If there exists a sequence {Pk } with Pk ≥ 0 such that
"
#
AT
AT
k Pk+1 Ak − Pk
k Pk+1 Bk
− Xk ≤ 0
(28)
BkT Pk+1 Ak
BkT Pk+1 Bk
for all k, then we have Vk+1 (ξk+1 ) − Vk (ξk ) ≤ Sk (ξk , wk )
with the time-dependent storage function defined as
Vk (ξk ) := ξkT Pk ξk . This is a standard approach for dissipation inequality constructions of LTV systems and can

be proved using the same proof technique in Theorem 2.
Note that we need (28) to simultaneously hold for all k.
This leads to an infinite number of LMIs in general.
Now we consider Nesterov’s accelerated method for a convex L-smooth objective function f (Nesterov, 2003).
xk+1 = yk − αk ∇f (yk ),
yk = (1 + βk )xk − βk xk−1 .

(29a)
(29b)

It is known that (29) achieves a rate of O(1/k 2 ) when
αk := 1/L and βk is defined recursively as follows.
p
1 + 1 + 4ζk2
ζk−1 − 1
ζ−1 = 0, ζk+1 =
, βk =
.
2
ζk
2
The sequence {ζk } satisfies ζk2 − ζk = ζk−1
. We now
present a dissipativity theory for the sublinear rate analysis
of Nesterov’s accelerated method. Rewrite (29) as




xk+1 − x?
xk − x?
= Ak
+ Bk wk
(30)
xk − x?
xk−1 − x?

where wk := ∇f (yk ) = ∇f ((1 + βk )xk − βk xk−1 ),
Ak := Ãk ⊗ Ip , Bk := B̃k ⊗ Ip , and Ãk , B̃k are given by




1 + βk −βk
−αk
Ãk :=
, B̃k :=
.
1
0
0
Hence, Nesterov’s accelerated method (29) is in the form

T
of (26) with ξk := (xk − x? )T (xk−1 − x? )T . The
O(1/k 2 ) rate analysis of Nesterov’s method (29) requires
the following time-dependent supply rate.
Lemma 8 Let f be L-smooth and convex. Let x? be a
point satisfying ∇f (x? ) = 0. In addition, set f? := f (x? ).
Consider Nesterov’s method (29) or equivalently (30). The
following inequalities hold for all trajectories and for all k.

T


xk − x?
xk − x?
xk−1 − x?  Mk xk−1 − x?  ≤ f (xk ) − f (xk+1 )
∇f (yk )
∇f (yk )

T


xk − x?
xk − x?
xk−1 − x?  Nk xk−1 − x?  ≤ f (x? ) − f (xk+1 )
∇f (yk )
∇f (yk )
where Mk := M̃k ⊗ Ip , Nk := Ñk ⊗ Ip , and M̃k , Ñk are
defined by


0
0
− 21 βk


1
0
(31)
M̃k :=  0
2 βk  ,
1
1
1
− βk 2 βk
2L
 2

0
0
− 21 (1 + βk )


1
0
0
Ñk := 
(32)
.
2 βk
1
− 12 (1 + βk ) 12 βk
2L

Dissipativity Theory for Nesterov’s Accelerated Method

Given any nondecreasing sequence {µk }, one can define
the supply rate as (27) with the particular choice Xk :=
µk Mk + (µk+1 − µk )Nk for all k. Then this supply rate
satisfies the condition

m is incorporated into the formulas of Mk , Nk . By setting
µk := ρ−2k and Pk := ρ−2k P , then the LMI (34) is the
same for all k and we recover (5). This illustrates how the
infinite number of LMIs (34) can collapse to a single LMI
under special circumstances.

S(ξk , wk ) ≤ µk (f (xk ) − f? )
− µk+1 (f (xk+1 ) − f? ). (33)
Proof. The proof is very similar to the proof of Lemma 3
with an extra condition m = 0. A detailed proof is presented in the supplementary material.
Theorem 9 Consider the LTV dynamical system (26). If
there exist matrices {Pk } with Pk ≥ 0 and a nondecreasing
sequence of nonnegative scalars {µk } such that
"

AT
k Pk+1 Ak − Pk
BkT Pk+1 Ak

AT
k Pk+1 Bk
BkT Pk+1 Bk

#

− µk Mk − (µk+1 − µk )Nk ≤ 0

(34)

then we have Vk+1 (ξk+1 ) − Vk (ξk ) ≤ Sk (ξk , wk ) with the
storage function Vk (ξk ) := ξkT Pk ξk and the supply rate
(27) using Xk := µk Mk + (µk+1 − µk )Nk for all k. In
addition, if this supply rate satisfies (33), we have
f (xk ) − f? ≤

µ0 (f (x0 ) − f? ) + V0 (ξ0 )
µk

(35)

Proof. Based on the state-space model (26), we can left



T
and right multiply (34) by ξkT wkT and ξkT wkT , and
directly obtain the dissipation inequality. Combining this
dissipation inequality with (33), we can show
Vk+1 (ξk+1 ) + µk+1 (fk+1 − f? ) ≤ Vk (ξk ) + µk (fk − f? ).
Summing the above inequality as in the proof of Theorem 7
and using the fact that Pk ≥ 0 for all k yields the result.
We are now ready to show the O(1/k 2 ) rate result for
2
Nesterov’s
 accelerated
 method. Set µk := (ζk−1 ) and


ζk−1
ζk−1 1 − ζk−1 . Note that Pk ≥ 0
Pk := L2
1 − ζk−1
and µk+1 − µk = ζk . It is straightforward to verify that this
choice of {Pk , µk } makes the left side of (34) the zero matrix and hence (35) holds. Using the fact that ζk−1 ≥ k/2
(easily proved by induction), we have µk ≥ k 2 /4 and the
O(1/k 2 ) rate for Nesterov’s method follows.
Remark 10 Theorem 9 is quite general. The infinite family of LMIs (34) can also be applied for linear rate analysis and collapses down to the single LMI (5) in that case.
To apply (34) to linear rate analysis, one needs to slightly
modify (31)–(32) such that the strong convexity parameter

5. Continuous-time Dissipation Inequality
Finally, we briefly discuss dissipativity theory for the
continuous-time ODEs used in optimization research. Note
that dissipativity theory was first introduced in Willems
(1972a;b) in the context of continuous-time systems. We
denote continuous-time variables in upper case. Consider a
continuous-time state-space model
Λ̇(t) = A(t)Λ(t) + B(t)W (t)

(36)

where Λ(t) is the state, W (t) is the input, and Λ̇(t) denotes
the time derivative of Λ(t). In continuous-time, the supply
rate is a function S : RnΛ × RnW × R+ → R that assigns
a scalar to each possible state and input pair. Here, we
allow S to also depend on time t ∈ R+ . To simplify our
exposition, we will omit the explicit time dependence (t)
from our notation.
Definition 11 The dynamical system (36) is dissipative
with respect to the supply rate S if there exists a function V : RnΛ × R+ → R+ such that V (Λ, t) ≥ 0 for
all Λ ∈ RnΛ and t ≥ 0 and
V̇ (Λ, t) ≤ S(Λ, W, t)

(37)

for every trajectory of (36). Here, V̇ denotes the Lie
derivative (or total derivative); it accounts for Λ’s dependence on t. The function V is called a storage function,
and (37) is a (continuous-time) dissipation inequality.
For any given quadratic supply rate, one can automatically
construct the continuous-time dissipation inequality using
semidefinite programs. The following result is standard in
the controls literature.
Theorem 12 Suppose X(t) ∈ R(nΛ +nW )×(nΛ +nW ) and
X(t)T = X(t) for all t. Consider the quadratic supply rate

S(Λ, W, t) :=

Λ
W

T


X

Λ
W


for all t.

(38)

If there exists a family of matrices P (t) ∈ RnΛ ×nΛ with
P (t) ≥ 0 such that
 T

A P + P A + Ṗ P B
− X ≤ 0 for all t.
(39)
BTP
0
Then we have V̇ (Λ, t) ≤ S(Λ, W, t) with the storage function defined as V (Λ, t) := ΛT P Λ.

Dissipativity Theory for Nesterov’s Accelerated Method

Proof. Based on the state-space model (36), we can apply
the product rule for total derivatives and obtain
V̇ (Λ, t) = Λ̇T P Λ + ΛT P Λ̇ + ΛT Ṗ Λ
 T  T
 
Λ
A P + P A + Ṗ P B Λ
=
.
W
W
BTP
0


Hence we can left and right multiply (39) by ΛT W T

T
and ΛT W T and obtain the desired conclusion.
The algebraic structure of the LMI (39) is simpler than that
of its discrete-time counterpart (5) because for given P , the
continuous-time LMI is linear in A, B rather than being
quadratic. This may explain why continuous-time ODEs
are sometimes more amenable to analytic approaches than
their discretized counterparts.
We demonstrate the utility of (39) on the continuous-time
limit of Nesterov’s accelerated method in Su et al. (2016):
3
(40)
Ÿ + Ẏ + ∇f (Y ) = 0,
t
T

,
which we rewrite as (36) with Λ := Ẏ T Y T − xT
?
W := ∇f (Y ), x? is a point satisfying ∇f (x? ) = 0, and
A, B are defined by
 3



−Ip
− t Ip 0p
A(t) :=
, B(t) :=
.
0p
Ip
0p
Suppose f is convex and set f? := f (x? ). Su et al. (2016,
Theorem 3) constructs the Lyapunov function V(Y, t) :=
t2 (f (Y )−f? )+2kY + 2t Ẏ − x? k2 to show that V̇ ≤ 0 and
then directly demonstrate a O(1/t2 ) rate for the ODE (40).
To illustrate the power of the dissipation inequality, we use
the LMI (39) to recover this Lyapunov function. Denote
G(Y, t) := t2 (f (Y ) − f? ). Note that convexity implies
f (Y ) − f? ≤ ∇f (Y )T (Y − x? ), which we rewrite as
T
0p
Ẏ
2t(f (Y ) − f? ) ≤ Y − x?  0p
0p
W




0p
Ẏ
tIp Y − x? .
0p
W

0p
0p
tIp

Since Ġ(Y, t) = 2t(f (Y ) − f? ) + t2 ∇f (Y )T Ẏ , we have


T 

0p
Ẏ
Ġ ≤ Y − x?   0p
t2
W
2 Ip

0p
0p
tIp

2

t
2





Ip
Ẏ
tIp  Y − x?  .
W
0p

Now choose the supply rate S as (38) with X(t) given by


0p
X(t) := −  0p
t2
2 Ip

0p
0p
tIp

t2
2 Ip
tIp  .



0p

Clearly S(Λ, W, t) ≤ −Ġ(Y, t). Now we can choose

T  t

Ip . Substituting P and X
P (t) := 2 2t Ip Ip
2 Ip
into (39), the left side of (39) becomes identically zero.
Therefore, V̇ (Λ, t) ≤ S(Λ, W, t) ≤ −Ġ(Y, t) with the
storage function V (Λ, t) := ΛT P Λ. By defining the Lyapunov function V(Y, t) := V (Y, t) + G(Y, t), we immediately obtain V̇ ≤ 0 and also recover the same Lyapunov
function used in Su et al. (2016).

Remark 13 As in the discrete-time case, the infinite family of LMIs (39) can also be reduced to a single LMI for
the linear rate analysis of continuous-time ODEs. For further discussion on the topic of continuous-time exponential
dissipation inequalities, see Hu & Seiler (2016).

6. Conclusion and Future Work
In this paper, we developed new notions of dissipativity
theory for understanding of Nesterov’s accelerated method.
Our approach enjoys advantages of both the IQC framework (Lessard et al., 2016) and the discretization approach
(Wilson et al., 2016) in the sense that our proposed LMI
condition is simple enough for analytical rate analysis of
Nesterov’s method and can also be easily generalized to
more complicated algorithms. Our approach also gives an
intuitive interpretation of the convergence behavior of Nesterov’s method using an energy dissipation perspective.
One potential application of our dissipativity theory is for
the design of accelerated methods that are robust to gradient noise. This is similar to the algorithm design work in
Lessard et al. (2016, §6). However, compared with the IQC
approach in Lessard et al. (2016), our dissipativity theory
leads to smaller LMIs. This can be beneficial since smaller
LMIs are generally easier to solve analytically. In addition,
the IQC approach in Lessard et al. (2016) is only applicable
to strongly-convex objective functions while our dissipativity theory may facilitate the design of robust algorithm
for weakly-convex objective functions. The dissipativity
framework may also lead to the design of adaptive or timevarying algorithms.

Acknowledgements
Both authors would like to thank the anonymous reviewers
for helpful suggestions that improved the clarity and quality of the final manuscript.
This material is based upon work supported by the National Science Foundation under Grant No. 1656951. Both
authors also acknowledge support from the Wisconsin Institute for Discovery, the College of Engineering, and the
Department of Electrical and Computer Engineering at the
University of Wisconsin–Madison.

Dissipativity Theory for Nesterov’s Accelerated Method

References
Bubeck, S. Convex optimization: Algorithms and comR in Machine Learning,
plexity. Foundations and Trends
8(3-4):231–357, 2015.
Bubeck, S., Lee, Y., and Singh, M. A geometric alternative to Nesterov’s accelerated gradient descent. arXiv
preprint arXiv:1506.08187, 2015.
Drusvyatskiy, D., Fazel, M., and Roy, S. An optimal
first order method based on optimal quadratic averaging.
arXiv preprint arXiv:1604.06543, 2016.
Fazlyab, M., Ribeiro, A., Morari, M., and Preciado,
V. M. Analysis of optimization algorithms via integral quadratic constraints: Nonstrongly convex problems. arXiv preprint arXiv:1705.03615, 2017.
Flammarion, N. and Bach, F. From averaging to acceleration, there is only a step-size. In COLT, pp. 658–695,
2015.
Hu, B. and Seiler, P. Exponential decay rate conditions
for uncertain linear systems using integral quadratic constraints. IEEE Transactions on Automatic Control, 61
(11):3561–3567, 2016.
Lessard, L., Recht, B., and Packard, A. Analysis and design
of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95,
2016.
Megretski, A. and Rantzer, A. System analysis via integral
quadratic constraints. IEEE Transactions on Automatic
Control, 42:819–830, 1997.
Megretski, A., Jönsson, U., Kao, C. Y., and Rantzer,
A. Control Systems Handbook, chapter 41: Integral
Quadratic Constraints. CRC Press, 2010.
Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers,
2003.
Polyak, B. T. Introduction to optimization. Optimization
Software, 1987.
Seiler, P. Stability analysis with dissipation inequalities and
integral quadratic constraints. IEEE Transactions on Automatic Control, 60(6):1704–1709, 2015.
Su, W., Boyd, S., and Candès, E. A differential equation for
modeling Nesterov’s accelerated gradient method: Theory and insights. Journal of Machine Learning Research,
17(153):1–43, 2016.

Wibisono, A., Wilson, A., and Jordan, M. A variational perspective on accelerated methods in optimization. Proceedings of the National Academy of Sciences,
pp. 201614734, 2016.
Willems, J.C. Dissipative dynamical systems Part I: General theory. Archive for Rational Mech. and Analysis, 45
(5):321–351, 1972a.
Willems, J.C. Dissipative dynamical systems Part II: Linear
systems with quadratic supply rates. Archive for Rational Mech. and Analysis, 45(5):352–393, 1972b.
Wilson, A., Recht, B., and Jordan, M. A Lyapunov analysis
of momentum methods in optimization. arXiv preprint
arXiv:1611.02635, 2016.

