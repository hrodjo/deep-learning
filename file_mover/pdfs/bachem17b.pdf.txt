Distributed and Provably Good Seedings for k-Means in Constant Rounds

Olivier Bachem 1 Mario Lucic 1 Andreas Krause 1

Abstract
The k-means++ algorithm is the state of the art
algorithm to solve k-Means clustering problems
as the computed clusterings are O(log k) competitive in expectation. However, its seeding step
requires k inherently sequential passes through
the full data set making it hard to scale to massive data sets. The standard remedy is to use the
k-meansk algorithm which reduces the number
of sequential rounds and is thus suitable for a distributed setting. In this paper, we provide a novel
analysis of the k-meansk algorithm that bounds
the expected solution quality for any number of
rounds and oversampling factors greater than k,
the two parameters one needs to choose in practice. In particular, we show that k-meansk provides provably good clusterings even for a small,
constant number of iterations. This theoretical
finding explains the common observation that
k-meansk performs extremely well in practice
even if the number of rounds is low. We further
provide a hard instance that shows that an additive error term as encountered in our analysis is
inevitable if less than k − 1 rounds are employed.

1. Introduction
Over the last several years, the world has witnessed the
emergence of data sets of an unprecedented scale across
different scientific disciplines. This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data. In this
paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.
Seeding — the task of finding initial cluster centers —
is critical to finding good clusterings for k-Means. In
fact, the seeding step of the state of the art algorithm
k-means++ (Arthur & Vassilvitskii, 2007) provides the
1

Department of Computer Science, ETH Zurich. Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

theoretical guarantee on the solution quality while the subsequent refinement using Lloyd’s algorithm (Lloyd, 1982)
only guarantees that the quality does not deteriorate. While
the k-means++ seeding step guarantees a solution that is
O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through
the data set. This makes it unsuitable for the massive data
setting where the data set is distributed across machines and
computation has to occur in parallel.
As a remedy, Bahmani et al. (2012) propose the
k-meansk algorithm which produces seedings for kMeans with a reduced number of sequential iterations.
Whereas k-means++ only samples a single cluster center in each of k rounds, k-meansk samples in expectation
` points in each of t iterations. Provided t is small enough,
this makes k-meansk suitable for a distributed setting as
the number of synchronizations is reduced.
Our contributions. We provide a novel analysis of
k-meansk that bounds the expected solution quality for
any number of rounds t and any oversampling factor ` ≥ k,
the two parameters that need to be chosen in practice. Our
bound on the expected quantization error includes both a
“traditional” multiplicative error term based on the optimal solution as well as a scale-invariant additive error term
based on the variance of the data. The key insight is that

k t
if t or ` is
this additive error term vanishes at a rate of e`
increased. This shows that k-meansk provides provably
good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon
that k-meansk works very well even for small t.
We further provide a hard instance on which k-meansk
provably incurs an additive error based on the variance of
the data and for which an exclusively multiplicative error
guarantee cannot be achieved. This implies that an additive
error term such as the one in our analysis is in fact necessary if less than k − 1 rounds are employed.

2. Background & related work
k-Means clustering. Let X denote a set of points in Rd .
The k-Means clustering problem is to find a set C of k
cluster centers in Rd that minimizes the quantization error
X
X
φX (C) =
d(x, C)2 =
minkx − qk22 .
x∈X

x∈X

q∈C

Distributed and Provably Good Seedings for k-Means in Constant Rounds

Algorithm 1 k-means++ seeding
Require: weighted data set (X , w), number of clusters k
1: C ← sample single x ∈ X with probability P 0 wx w 0
x
x ∈X
2: for i = 2, . . . , k do
d(x,C)2
3:
Sample x ∈ X with probability P 0 wx w
0
2
x0 d(x ,C)
x ∈X
4:
C ← C ∪ {x}
5: Return C
We denote the optimal quantization error by φOPT (X )
while the variance of the data is defined as Var(X ) =
φX ({µ(X )}) where µ(X ) is the mean of X .
k-means++ seeding. Given a data set X and any set of
cluster centers C ⊂ X , the D2 -sampling strategy selects a
new center by sampling each point x ∈ X with probability
d(x, C)2
.
0
0 2
x0 ∈X d(x , C )

p(x) = P

The seeding step of k-means++ (Arthur & Vassilvitskii,
2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k − 1 cluster centers using D2 sampling whereby C is always the set of previously
sampled centers. Arthur & Vassilvitskii (2007) show that
the solution quality φk-means++ of k-means++ seeding is
bounded in expectation by
E[φk-means++ ] ≤ 8 (log2 k + 2) φOPT (X ).
The computational complexity of k-means++ seeding is
O(nkd) where n is the number of data points and d the dimensionality. Unfortunately, the iterations in k-means++
seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data. This makes
the algorithm unsuitable for the distributed setting.
k-meansk seeding. As a remedy, Bahmani et al. (2012)
propose the algorithm k-meansk which aims to reduce
the number of sequential iterations. The key component
of k-meansk is detailed in Algorithm 2 in what we call
k-meansk overseeding: First, a data point is sampled as
the first cluster center uniformly at random. Then, in each
of t sequential rounds, each data pointx ∈ X is inde2
pendently sampled with probability min 1, ` φd(x,C)
and
X (C)
added to the set of sampled centers C at the end of the
round. The parameter ` ≥ 1 is called the oversampling factor and determines the expected number of sampled points
in each iteration.
At the end of Algorithm 2, one obtains an oversampled
solution with t` cluster centers in expectation. The full
k-meansk seeding algorithm as detailed in Algorithm 3
reduces such a solution to k centers as follows: First, each
of the centers in the oversampled solution is weighted by
the number of data points which are closer to it than the

Algorithm 2 k-meansk overseeding
Require: data set X , # rounds t, oversampling factor `
1: C ← sample a point uniformly at random from X
2: for i = 1, 2, . . . , t do
3:
C0 ← ∅
4:
for x ∈ X do


2
5:
Add x to C 0 with probability min 1, ` φd(x,C)
X (C)
6:
C ← C ∪ C0
7: Return C

Algorithm 3 k-meansk seeding
Require: data set X , # rounds t, oversampling factor `
1: B ← Result of Algorithm 2 applied to (X , t, `)
2: for c ∈ B do
3:
Xc ← points x ∈ X whose closest center in B is c
(ties broken arbitrarily but consistently)
4:
wc ← |Xc |
5: C ← Result of Algorithm 1 applied to (B, w)
6: Return C
other centers. Then, k-means++ seeding is run on the
weighted oversampled solution to produce a set of k final centers. The total computational complexity of Algorithm 3 is O(nt`d) in expectation.
The key intuition behind k-meansk is that, if we choose
a large oversampling factor `, the number of rounds t can
be small — certainly much smaller than k, preferably even
constant. The step in lines 4 and 5 in Algorithm 2 can be
distributed over several machines and after each round the
set C can be synchronized. Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently
run in a distributed setting.1
Other related work. Celebi et al. (2013) provide an
overview over different seeding methods for k-Means. D2 sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and
Arthur & Vassilvitskii (2007). This research direction has
led to polynomial time approximation schemes based on
D2 -sampling (Jaiswal et al., 2014; 2015), constant factor
approximations based on sampling more than k centers
(Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Röglin, 2011). Recently, algorithms to approximate
k-means++ seeding based on Markov Chain Monte Carlo
have been proposed by Bachem et al. (2016b;a). Finally,
k-means++ has been used to construct coresets — small
data set summaries — for k-Means clustering (Lucic et al.,
2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic
et al., 2017).
1
A popular choice is the MLLib library of Apache Spark
(Meng et al., 2016) which uses k-meansk by default.

Distributed and Provably Good Seedings for k-Means in Constant Rounds

3. Intuition and key results
In this section, we provide the intuition and the main results
behind our novel analysis of k-meansk and defer the formal statements and the formal proofs to Section 4.
3.1. Solution quality of k-meansk
Solution quality of Algorithm 2. We first consider Algorithm 2 as it largely determines the final solution quality.
Algorithm 3 with its use of k-means++ to obtain the final
k cluster centers, only adds an additional O(log k) factor
as shown in Theorem 1. Our key result is Lemma 4 (see
Section 4) which guarantees that, for ` ≥ k, the expected
error of solutions computed by Algorithm 2 is at most
 t
k
Var(X ) + 26φOPT (X ). (1)
E[φX (C)] ≤ 2
e`
The first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error φOPT (X ). It is scale-invariant since
both the variance and the quantization error are scaled by
λ2 if we scale the data set X by λ > 0. The second term is a
“traditional” multiplicative error term based on the optimal
quantization error.
Given a fixed oversampling factor `, the additive error term
decreases exponentially if the number of rounds t is increased. Similarly, for a fixed number
 of rounds t, it decreases polynomially at a rate O `1t if the over sampling
factor ` is increased. This result implies that even for a constant number of rounds one may obtain good clusterings by
increasing the oversampling factor `. This explains the empirical observation that often even a low number of rounds
t is sufficient and that increasing ` increases the solution
quality (Bahmani et al., 2012). The practical implications
of this result are non-trivial: Even for the choice of t = 5
and ` = 5k one retains at most 0.0004% of the variance as
an additive error. Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error
term (Bachem et al., 2017).
Comparison to previous result. Bahmani et al. (2012)
show the following result: Let C be the set returned by Algorithm 2 with t rounds.
For α =
`
exp −(1 − e−`/(2k) ) ≈ e− 2k , Corollary 3 of Bahmani
et al. (2012) bounds the expected quality of C by
t

16
1+α
ψ+
φOPT (X ), (2)
E[φX (C)] ≤
2
1−α
where ψ denotes the quantization error of X based on the
first, uniformly sampled center in k-meansk. The key difference compared to our result is as follows: First, even as
we increase `, the factor α is always non-negative. Hence,
regardless of the choice of `, the additive ψ term is reduced

by at most 12 per round.2 This means that, given the analysis in Bahmani et al. (2012), one would always obtain a
constant additive error for a constant number of rounds t,
even as ` is increased.
Guarantee for Algorithm 3. Our main result — Theorem 1 — bounds the expected quality of solutions produced
by Algorithm 3. As in Bahmani et al. (2012), one loses another factor of O(ln k) compared to (1) due to Algorithm 3.
Theorem 1. Let k ∈ N, t ∈ N and ` ≥ k. Let X be a data
set in Rd and C be the set returned by Algorithm 3. Then,
!
 t
k
ln k Var(X )+O(ln k)φOPT (X ).
E[φX (C)] ≤ O
e`
3.2. A hard instance for k-meansk
We consider the case t < k − 1 which captures the scenario
where k-meansk is useful in practice as for t ≥ k one
may simply use k-means++ instead.
Theorem 2. For any β > 0, k ∈ N, t < k − 1 and ` ≥ 1,
there exists a data set X of size 2(t + 1) such that
E[φX (C)] ≥

1
−t
(4`t) Var(X ),
4

where C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `. Furthermore,
Var(X ) > 0,

φOPT (X ) = 0 and n∆2 ≤ β

where ∆ is the largest distance between any points in X .
Theorem 2 shows that there exists a data set on which
k-meansk provably incurs a non-negligible error even if
the optimal quantization error is zero. This implies that
k-meansk with t < k − 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets. We thus argue that an additive error bound
such as the one in Theorem 1 is required. We note that the
upper bound in (1) and the lower bound in Theorem 2 exhibit the same `1t dependence on the oversampling factor `
for a given number of rounds t.
Furthermore, Theorem 2 implies that, for general data
sets, k-meansk cannot achieve the multiplicative error
of O(log k) in expectation as claimed by Bahmani et al.
(2012).3 In particular, if the optimal quantization error is
2

Note that E[ψ] ≤ 2 Var(X ) (Arthur & Vassilvitskii, 2007).
To see this, let ψ = φX (c1 ) be the quantization error of the
first sampled center in Algorithm 2 and choose β small enough
such that the choice of t ∈ O(log ψ) leads to t < k − 1. For
X in Theorem 2, φOPT (X ) = 0 which implies that the desired
multiplicative guarantee would require E[φX (C)] = 0. However,
the non-negligible, additive error in Theorem 2 and Var(X ) > 0
implies that E[φX (C)] > 0.
3

Distributed and Provably Good Seedings for k-Means in Constant Rounds

zero, then k-meansk would need to return a solution with
quantization error zero. While we are guaranteed to remove
a constant fraction of the error in each round, the number
of required iterations may be unbounded.

4. Theoretical analysis
Proof of Theorem 1. The proof is divided into four
steps: First, we relate k-meansk-style oversampling to
k-means++-style D2 -sampling in Lemmas 1 and 2. Second, we analyze a single iteration of Algorithm 2 in
Lemma 3. Third, we bound the expected solution quality
of Algorithm 2 in Lemma 4. Finally, we use this to bound
the expected solution quality of Algorithm 3 in Theorem 1.
Lemma 1. Let A be a finite set and let f : 2A → R be
a set function that is non-negative and monotonically decreasing, i.e., f (V ) ≥ f (U ) ≥ 0, for all V ⊆ U .
Let P be a probability distribution where, for each a ∈ A,
Ea denotes an independent event that occurs with probability qa ∈ [0, 1]. Let C be the set of elements a ∈ A for
which the event Ea occurs.
Let Q be the probability distribution on
P A where a single
a ∈ A is sampled with probability qa / a∈A qa .
Then, with ∅ denoting the empty set, we have that
EP [f (C)] ≤ EQ [f ({a})] + e−

P

a∈A

qa

f (∅).

Proof. To prove the claim, we first construct a series of
sub-events of the events {Ea }a∈A and then use them to
recursively bound EP [f (C)].
Let m ∈ N. For each a ∈ A, let ia be an independent random variable drawn uniformly at random from
{1, 2, . . . , m}. For each a ∈ A and i = 1, 2, . . . , m, let
Fai be an independent event that occurs with probability

qa i−1
.
P[Fai ] = 1 −
m
For each a ∈ A and i = 1, 2, . . . , m, denote by Eai the
event that occurs if i = ia and both Ea and Fai occur. By
design, all these events are independent and thus
P[Eai ] = P[Ea ]P[Fai ]P[ia = i] =

qa 
qa i−1
1−
(3)
m
m

for each a ∈ A and i = 1, 2, . . . , m. Furthermore, for any
a, a0 ∈ A with a 6= a0 and any i, i0 ∈ {1, 2, . . . , m}, the
events Eai and Ea0 i0 are independent.
For i = 1, 2, . . . , m let Gi be the event that none of the
events {Eai0 }a∈A,i0 ≤i occur, i.e.,
Gi =

\ \
i0 ≤i a∈A

Eai0

where A denotes the complement of A. For convenience,
let G0 be the event that occurs with probability one.
Let (a1 , a2 , . . . , a|A| ) be any enumeration of A. For i =
1, 2, . . . , m and j = 1, 2, . . . , |A|+1, define the event
\
Gi,j = Gi−1 ∩
Eaj0 i .
0<j 0 <j

We note that by definition Gi,1 = Gi−1 and Gi,|A|+1 = Gi
for i = 1, 2, . . . , m.
For i = 1, 2, . . . , m and j = 1, 2, . . . , |A|, we have

 

E[f (C)|Gi,j ] =P Eaj i | Gi,j E f (C) | Eaj i ∩ Gij
i
h
+ P Eaj0 i | Gij E[f (C) | Gi,j+1 ].
(4)
We now bound the individual terms. The event Gi,j implies that the events {Eaj i0 }i0 <i did not occur. Furthermore, Eaj i is independent of the events {Eaj0 i0 }i0 =1,2,...,m
for j 0 6= j. Hence, we have
"
#
\


P Eaj i | Gi,j = P Eaj i | G0 ∩
Eaj i0
i0 <i



P Eaj i

= 
T
P G0 ∩ i0 <i Eaj i0


P Eaj i
S

=
1 − P i0 <i Eaj i0


P Eaj i

,
P
=
1 − i0 <i P Eaj i0

(5)

where the last equality follows since the events {Eaj i0 }i0 <i


P
are disjoint. Using (3), we observe that i0 <i P Eaj i0 is
a sum of a finite geometric series and we have
0
X 
 X qa 
qa i −1
1−
P Eaj i0 =
m
m
i0 <i
i0 <i

q i−1
qa 1 − 1 − ma

=
m 1 − 1 − qma

qa i−1
=1− 1−
.
m

Together with (3) and (5), this implies

qa
qa i−1


qa
m 1− m
P Eaj i | Gi,j =
= .

qa i−1
m
1− m

(6)

The event Eaj i implies that C contains aj . Hence, since f
is monotonically decreasing, we have


E f (C) | Eaj i ∩ Gij ≤ f ({aj }).

Distributed and Provably Good Seedings for k-Means in Constant Rounds

If EQ [f ({a})] = 0, the contradiction follows directly
from (7). Otherwise, EQ [f ({a})] > 0 implies that there


qa
qa
E[f (C)|Gi,j ] ≤ j f ({aj })+ 1 − j E[f (C) | Gi,j+1 ]. exists an  > 0 such that
m
m
Using (4) and (6), this implies

Applying this result iteratively for j = 1, 2, . . . , |A| implies




|A|
X
Y
q
qaj
a 0

E[f (C)|Gi,1 ] =
1 − j  f ({aj })
m
m
j=1
j 0 <j


|A| 
Y


qaj 
 E f (C) | Gi,|A|+1 .
+
1−
m
j=1
Note that 0 ≤ 1 − qma ≤ 1 for all a ∈ A and that f is
non-negative. This implies that for i = 1, 2, . . . , m
X qa


f ({a}) + c · E f (C)|Gi,|A|+1
E[f (C)|Gi,1 ] ≤
m

EP [f (C)] > (1 + )EQ [f ({a})] + e−

P

a∈A

qa

f (∅). (8)

By definition, we have
c=

Y
a∈A

1−

X qa
qa 
=1−
+o
m
m



a∈A

1
m


.

Thus, there exists a m ∈ N sufficiently large such that


X qa
1
1 X qa
c=1−
+o
≤1−
.
m
m
1+
m
a∈A

a∈A

Together with (7), this implies

a∈A

where
c=

Y
a∈A

P
1 +  X qa
f ({a}) + e− a∈A qa · f (∅)
E[f (C)] ≤ P
qa
m
a∈A m

qa 
1−
.
m

a∈A

= (1 + )EQ [f ({a})] + e−

Since Gi,1 = Gi−1 and Gi,|A|+1 = Gi , we have for i =
1, 2, . . . , m
X qa
f ({a}) + c · E[f (C)|Gi ].
E[f (C)|Gi−1 ] ≤
m
a∈A

Applying this result iteratively, we obtain
!
m
X
X qa
i−1
E[f (C)] ≤
c
f ({a}) + cm · f (∅).
m
i=1
a∈A

ci−1 ≤

i=1

∞
X

ci−1 =

i=1

1
.
1−c

c


X
qa m
qa 
=
1−
= exp m
log 1 −
m
m
a∈A
a∈A
!
X qa
P
≤ exp −m
= e− a∈A qa .
m
Y

!

a∈A

This implies that
E[f (C)] ≤

qa

f (∅).

which is a contradiction to (8) and thus proves the claim.

Lemma 2 extends Lemma 1 to k-meansk-style sampling
probabilities of the form qa = min (1, `pa ).
Lemma 2. Let ` ≥ 1. Let A be a finite set and let f : 2A →
R be a set function that is non-negative and monotonically
decreasing, i.e., f (V ) ≥ f (U
P) ≥ 0, for all V ⊆ U . For
each a ∈ A, let pa ≥ 0 and a∈A pa ≤ 1.

Let Q be the probability distribution on
P A where a single
a ∈ A is sampled with probability pa / a∈A pa .

For x ∈ [−1, 0] it holds that log(1 + x) ≤ x and hence
m

a∈A

Let P be the probability distribution where, for each a ∈ A,
Ea denotes an independent event that occurs with probability qa = min (1, `pa ). Let C be the set of elements a ∈ A
for which the event Ea occurs.

Since 0 ≤ c ≤ 1, we have
m
X

P

P
1 X qa
f ({a}) + e− a∈A qa · f (∅). (7)
1−c
m

Then, with ∅ denoting the empty set, we have that
EP [f (C)] ≤ 2EQ [f ({a})] + e−`

P

a∈A

pa

Proof. Let A1 be the set of elements a ∈ A such that `pa ≤
1 and A2 the set of elements a ∈ A such that `pa > 1. By
definition, every element in A2 is sampled almost surely,
i.e., A2 ⊆ C. This implies that almost surely
f (C) = f (A2 ∪ (C ∩ A1 )) .

a∈A

We show the main claim by contradiction. Assume that
EP [f (C)] > EQ [f ({a})] + e−

P

a∈A

qa

f (∅).

f (∅).

If |A1 |= 0, the result follows trivially since
EP [f (C)] = f (A2 ) = EQ [f ({a})].

(9)

Distributed and Provably Good Seedings for k-Means in Constant Rounds

Similarly, if |A2 |= 0, the result follows directly from
Lemma 1 with qa = `pa . For the remainder of the proof,
we may thus assume that both A1 and A2 are non-empty.
For a ∈ A1 , let qa = `pa and define the non-negative and
monotonically decreasing function
g(C) = f (A2 ∪ C) .
P
P
Let p1 = a∈A1 pa and p2 = a∈A2 pa . Lemma 1 applied to A1 , qa and g implies that
EP [f (C)] = E[g(C)] ≤

X pa
g({a}) + e−`p1 g(∅).
p1
a∈A1
(10)

Let

d = 1 − e−`p2 e−`p1

Similarly, we have
p2
p1
−
d + e−`p1
p1 + p2
p1 + p2
p2
p2
=
+d
− d + e−`p1
p1 + p2
p1 + p2
p2
+ e−`(p1 +p2 ) .
= (1 + d)
p1 + p2

α + e−`p1 =

Since g(∅) ≤ f (∅), it follows that

p2
α + e−`p1 g(∅) ≤ (1+d)
g(∅)+e−`(p1 +p2 ) f (∅).
p1 + p2
(13)
Since g(∅) = f (A2 ) and thus g(∅) ≤ f ({a}) for all a ∈
A2 , we have
X
X
pa f ({a}).
(14)
pa g(∅) ≤
p2 g(∅) =
a∈A2

and define
α=

p2
p1
−
d.
p1 + p2
p1 + p2

By design, α ≤ 1. Furthermore

a∈A2

Combining (11), (12), (13), and (14) leads to
EP [f (C)] ≤ (1 + d)EQ [f ({a})] + e−`

P

a∈A

pa

f (∅).

Since p1 ≥ 0, we have
`p1 ≥ log `p1 .
Since A2 is nonempty and pa ≥
that p2 ≥ 1` . This implies

1
`

e`p1 ≥ `p1 ≥
Since 0 ≤ 1 − e

−`p2



for all a ∈ A2 , it follows

which proves the main claim.

p1
.
p2

Lemma 3 bounds the solution quality after each iteration of
Algorithm 2 based on the solution before the iteration.

≤ 1, we have


p2 ≥ p1 e−`p1 ≥ p1 1 − e−`p2 e−`p1 = p1 d.
Hence,
α=


p1
p2
1 − e−`p2 e−`p1 ≥ 0.
−
p1 + p2
p1 + p2

Since α ∈ [0, 1] and g({a}) ≤ g(∅) for any a ∈ A1 , we
may write (10), i.e.,
EP [f (C)] ≤ (1 − α)

X pa

g({a}) + α + e−`p1 g(∅).
p1
a∈A1
(11)

By definition, we have
1−α=1−

p2
p1
p1
+
d=
(1 + d).
p1 + p2
p1 + p2
p1 + p2

Since g({a}) ≤ f ({a}), we thus have
(1 − α)


1 + d = 1 + 1 − e−`p2 e−`p1 ≤ 2

X pa
X
pa
g({a}) ≤ (1 + d)
f ({a}).
p1
p1 + p2
a∈A1
a∈A1
(12)

Lemma 3. Let k ∈ N and ` ≥ 1. Let X be a data set in Rd
and denote by φOPT (X ) the optimal k-Means clustering
cost. Let C denote the set of cluster centers at the beginning
of an iteration in Algorithm 2 and C 0 the random set added
in the iteration. Then, it holds that
 
k
E[φX (C ∪ C 0 )] ≤
φX (C) + 16φOPT (X ).
e`
Proof. The proof relies on applying Lemma 2 to each cluster of the optimal solution. Let OPT denote any clustering
achieving the minimal cost φOPT (X ) on X . We assign all
the points x ∈ X to their closest cluster center in OPT
with ties broken arbitrarily but consistently. For c ∈ OPT
we denote by Xc the subset of X assigned to c. For each
c ∈ OPT, let
Cc0 = C 0 ∩ Xc .
By definition, a ∈ Xc is included in Cc0 with probability


` d(a, C)2
P
.
qa = min 1,
0
2
a0 ∈X d(a , C)
For each c ∈ OPT, we define the monotonically decreasing function fc : 2Xc → R≥0 to be
fc (Cc0 ) = φXc (C ∪ Cc0 ).

Distributed and Provably Good Seedings for k-Means in Constant Rounds

For each c ∈ OPT, Lemma 2 applied to Xc , Cc0 and fc
implies
E[fc (Cc0 )] ≤2

d(a, C)2
f ({a})
0
2 c
a0 ∈Xc d(a , C)

X

P
a∈Xc
P

+e

−` P a∈Xc
a0 ∈X

d(a,C)2
d(a0 ,C )2

(15)

Lemma 4. Let k ∈ N, t ∈ N and ` ≥ k. Let X be a data
set in Rd and C be the random set returned by Algorithm 2.
Then,
 t
k
E[φX (C)] ≤ 2
Var(X ) + 26φOPT (X ).
e`

fc (∅).

Since fc ({a}) = φXc (C ∪ {a}), the first term is equivalent
to sampling a single element from Xc using D2 sampling.
Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we
have for all c ∈ OPT

Proof. The algorithm starts with a uniformly sampled initial cluster center c1 . We iteratively apply Lemma 3 for
each of the t rounds to obtain

E[φX (C)] ≤

2

d(a, C)
fc ({a}). ≤ 8φOPT (Xc ).
d(a0 , C)2
c

X
P

P

e
where

E[φX ({c1 })] + 16st φOPT (X ) (19)

where
st =

For each c ∈ OPT, we further have
−` P a∈Xc

t

(16)

a0 ∈X

a∈Xc

k
e`

For ` ≥ k, we have 0 ≤

fc (∅) = e−`uc uc φX (C).

P
d(a, C)2
φXc (C)
=
uc = P a∈Xc
.
0
2
φX (C)
a0 ∈X d(a , C)

st ≤

⇐⇒

`uc ≤

k
e`

.

≤ 1/e and hence

t
∞
X
X
1
1
1
≤
=
.
i−1
i
e
e
1 − 1/e
i=1
i=0

(20)

By Lemma 3.2 of Arthur & Vassilvitskii (2007), we have
that E[φX ({c1 })] ≤ 2 Var(X ). Together with (19), (20)
and 16/(1 − 1/e) ≈ 25.31 < 26, this implies the required
result.

We have that
log `uc ≤ `uc − 1

e`

i=1

d(a,C)2

d a0 ,C )2
a0 ∈X (

i−1
t 
X
k

e`uc
e

which implies
e

−`uc

1
uc φX (C) ≤ φX (C).
e`

(17)

Combining (15), (16) and (17), we obtain
E[fc (Cc0 )] ≤16φOPT (Xc ) +

1
φX (C).
e`

(18)

With Lemma 4, we are further able to bound the solution
quality of Algorithm 3 and prove Theorem 1.
Proof of Theorem 1. Let B be the set returned by Algorithm 2. For any x ∈ X , let bx denote its closest point
in B with ties broken arbitrarily. By the triangle inequality
2
and since (|a|+|b|) ≤ 2a2 + 2b2 , for any x ∈ X
d(x, C)2 ≤ 2 d(x, bx )2 + 2 d(bx , C)2

Since
X

0

E[φX (C ∪ C )] ≤

E[fc (Cc0 )]

c∈OPT

and hence

and
X

φX (OPT) =

φXc (OPT),

c∈OPT

X



k
e`

d(x, C)2

x∈X

≤2

we thus have
E[φX (C ∪ C 0 )] ≤

E[φX (C)] =

X

d(x, bx )2 + 2

x∈X


φX (C) + 16φOPT (X )

= 2φX (B) + 2

X

d(bx , C)2

(21)

x∈X

X

wx d(x, C)2 .

x∈B

which concludes the proof.
An iterated application of Lemma 3 allows us to bound the
solution quality of Algorithm 2 in Lemma 4.

Let OPTX be the optimal k-Means clustering solution on
X and OPT(B,w) the optimal solution on the weighted set
(B, w). By Theorem 1.1 of Arthur & Vassilvitskii (2007),

Distributed and Provably Good Seedings for k-Means in Constant Rounds

k-means++ produces an α = 8(log2 k + 2) approximation to the optimal solution. This implies that
X
X

wx d(x, C)2 ≤ α
wx d x, OPT(B,w) 2
x∈B

x∈B

≤α

X

wx d(x, OPTX )2

(22)

x∈B

=α

X

d(bx , OPTX )2 .

BySthe union bound, the probability that any of the points
in j>i {xj } are sampled is bounded by

x∈X
2

By the triangle inequality and since (|a|+|b|) ≤ 2a2 +2b2 ,
it holds for any x ∈ X that

X ` d(xj , 0)2
j>i

d(bx , OPTX )2 ≤ 2 d(x, bx )2 + 2 d(x, OPTX )2
and hence
X
d(bx , OPTX )2 ≤ 2φX (B) + 2φOPT (X ).

Consider a single iteration of Algorithm 2 where C = Ci .
In this case, all points in Xj with j < i are added to C 0
with probability zero and for j > i each point xj is added
to C 0 with probability
!
` d(xj , 0)2
` d(xj , 0)2
=
.
min 1, P
1−i
2
0
β 0 (4`t)
j 0 ≥i d(xj , 0)

(23)

x∈X

1−i
β 0 (4`t)

≤

E[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT (X ).
Finally, by Lemma 4, we have
k
E[φX (C)] ≤(32 log2 k + 68)
e`

t
Var(X )

+ (432 log2 k + 916)φOPT (X ).
Proof of Theorem 2. For this proof, we explicitly construct a data set: Let β 0 > 0 and consider points in onedimensional Euclidean space. For i = 1, 2, . . . , t, set
q
1−i
−i
xi = β 0 (4`t)
− β 0 (4`t)
as well as
xt+1

q
−t
= β 0 (4`t) .

Let the data set X consist of the t+1 points {xi }t=1,2,...,t+1
as well as t + 1 points at the origin. Since t < k − 1,
the optimal k-Means clustering solution consists of t + 2
points placed at each of the {xi }i=1,2,...t+1 and at 0. By
design, this solution has a quantization error of zero and the
variance is nonzero, i.e., φOPT (X ) = 0 and Var(X ) > 0
as claimed.
β
Choose β 0 = 2(t+1)
. The maximal distance ∆ between
any two points in X is bounded by ∆ = d(0, x1 )2 ≤ β 0 .
Since n = 2(t + 1), this implies ψ ≤ n∆2 ≤ β as claimed.

For i = 1, 2, . . . , t, let Ci consist of 0 and all xj with j < i.
By design, we have d(0, Ci )2 = 0 as well as d(xj , Ci )2 =
0 for j < i. For j ≥ i, we have d(xj , Ci )2 = d(xj , 0)2 .
For any i = 1, 2, . . . , t + 1, we thus have
X
1−i
d(xj , 0)2 = β 0 (4`t) .
j≥i

1
.
4t

The point xi is not sampled with probability at most
!


1
` d(xi , 0)2
= 1 − min 1, ` −
1 − min 1, P
2
0
4t
j 0 ≥i d(xj , 0)

Combining (21), (22) and (23), we obtain



=

1
.
4t

By the union bound, a single iteration of Algorithm 2 with
C = Ci hence samples exactly
the set C 0 = {xi } with

1
probability at least 1 − 2t .
In Algorithm 2, the first center is sampled uniformly at random from X . Since half of the elements in X are placed at
0, with probability at least 12 , the first center is at 0 or equiv
1 t
≥ 12 , we then
alently C = C1 . With probability 1 − 2t
sample exactly the points x1 , x2 , . . . , xt in the t subsequent
iterations. Hence, with probability at least 41 , the solution
produced by Algorithm 2 consists of 0 and all xi except
xt+1 . Since xt+1 is closest to 0, this implies
E[φX (C)] ≥

1
1
−t
d(xt+1 , 0)2 = β 0 (4`t) .
4
4

(24)

The variance of X is bounded by a single point at 0, i.e.,
X
Var(X ) ≤ φX ({0}) =
d(xj , 0)2 = β 0 .
j≥1

Together with (24), we have that
E[φX (C)] ≥

1
−t
(4`t) Var(X ).
4

The same result extends to the output of Algorithm 3 as it
always picks a subset of the output of Algorithm 2.

Acknowledgements
This research was partially supported by SNSF NRP 75,
ERC StG 307036, a Google Ph.D. Fellowship and an IBM
Ph.D. Fellowship. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.

Distributed and Provably Good Seedings for k-Means in Constant Rounds

References
Ackermann, Marcel R, Märtens, Marcus, Raupach,
Christoph, Swierkot, Kamil, Lammersen, Christiane,
and Sohler, Christian. StreamKM++: A clustering algorithm for data streams. Journal of Experimental Algorithmics (JEA), 17:2–4, 2012.
Aggarwal, Ankit, Deshpande, Amit, and Kannan, Ravi.
Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 15–28. Springer,
2009.
Ailon, Nir, Jaiswal, Ragesh, and Monteleoni, Claire.
Streaming k-means approximation. In Advances in Neural Information Processing Systems, pp. 10–18, 2009.
Arthur, David and Vassilvitskii, Sergei. k-means++: The
advantages of careful seeding. In Symposium on Discrete
Algorithms (SODA), pp. 1027–1035. SIAM, 2007.
Bachem, Olivier, Lucic, Mario, and Krause, Andreas.
Coresets for nonparametric estimation - the case of DPmeans. In International Conference on Machine Learning (ICML), 2015.
Bachem, Olivier, Lucic, Mario, Hassani, Hamed, and
Krause, Andreas. Fast and provably good seedings for
k-means. In Advances in Neural Information Processing
Systems (NIPS), pp. 55–63, 2016a.
Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and
Krause, Andreas. Approximate k-means++ in sublinear
time. In Conference on Artificial Intelligence (AAAI),
February 2016b.
Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and
Krause, Andreas. Uniform deviation bounds for kmeans clustering. In To appear in International Conference on Machine Learning (ICML), 2017.
Bahmani, Bahman, Moseley, Benjamin, Vattani, Andrea,
Kumar, Ravi, and Vassilvitskii, Sergei. Scalable KMeans++. Very Large Data Bases (VLDB), 5(7):622–
633, 2012.
Brunsch, Tobias and Röglin, Heiko. A bad instance for kmeans++. In International Conference on Theory and
Applications of Models of Computation, pp. 344–352.
Springer, 2011.
Celebi, M Emre, Kingravi, Hassan A, and Vela, Patricio A.
A comparative study of efficient initialization methods
for the k-means clustering algorithm. Expert Systems
with Applications, 40(1):200–210, 2013.

Fichtenberger, Hendrik, Gillé, Marc, Schmidt, Melanie,
Schwiegelshohn, Chris, and Sohler, Christian. Bico:
Birch meets coresets for k-means clustering. In European Symposium on Algorithms, pp. 481–492. Springer,
2013.
Jaiswal, Ragesh, Kumar, Amit, and Sen, Sandeep. A
simple D2 -sampling based PTAS for k-means and other
clustering problems. Algorithmica, 70(1):22–46, 2014.
Jaiswal, Ragesh, Kumar, Mehul, and Yadav, Pulkit. Improved analysis of D2 -sampling based PTAS for k-means
and other clustering problems. Information Processing
Letters, 115(2):100–103, 2015.
Lloyd, Stuart. Least squares quantization in PCM. IEEE
Transactions on Information Theory, 28(2):129–137,
1982.
Lucic, Mario, Bachem, Olivier, and Krause, Andreas.
Strong coresets for hard and soft Bregman clustering
with applications to exponential family mixtures. In
International Conference on Artificial Intelligence and
Statistics (AISTATS), May 2016.
Lucic, Mario, Faulkner, Matthew, Krause, Andreas, and
Feldman, Dan. Training mixture models at scale via
coresets. To appear in Journal of Machine Learning Research (JMLR), 2017.
Meng, Xiangrui, Bradley, Joseph, Yuvaz, B, Sparks, Evan,
Venkataraman, Shivaram, Liu, Davies, Freeman, Jeremy,
Tsai, D, Amde, Manish, Owen, Sean, et al. Mllib: Machine learning in Apache Spark. Journal of Machine
Learning Research (JMLR), 17(34):1–7, 2016.
Ostrovsky, Rafail, Rabani, Yuval, Schulman, Leonard J,
and Swamy, Chaitanya. The effectiveness of Lloyd-type
methods for the k-means problem. In Symposium on
Foundations of Computer Science (FOCS), pp. 165–176.
IEEE, 2006.

