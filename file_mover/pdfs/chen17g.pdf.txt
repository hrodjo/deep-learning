Toward Efficient and Accurate Covariance Matrix
Estimation on Compressed Data

Xixian Chen 1 2 Michael R. Lyu 1 2 Irwin King 1 2

Abstract
Estimating covariance matrices is a fundamental technique in various domains, most notably
in machine learning and signal processing. To
tackle the challenges of extensive communication costs, large storage capacity requirements,
and high processing time complexity when handling massive high-dimensional and distributed
data, we propose an efficient and accurate covariance matrix estimation method via data compression. In contrast to previous data-oblivious
compression schemes, we leverage a data-aware
weighted sampling method to construct lowdimensional data for such estimation. We rigorously prove that our proposed estimator is unbiased and requires smaller data to achieve the
same accuracy with specially designed sampling
distributions. Besides, we depict that the computational procedures in our algorithm are efficient.
All achievements imply an improved tradeoff between the estimation accuracy and computational
costs. Finally, the extensive experiments on synthetic and real-world datasets validate the superior property of our method and illustrate that it
significantly outperforms the state-of-the-art algorithms.

1. Introduction
Covariance matrices play a fundamental role in machine
learning and statistics owing to their capability to retain the
second-order information of data samples (Feller, 1966).
For example, Principal Component Analysis (PCA) along
1

Shenzhen Research Institute, The Chinese Univer2
sity of Hong Kong, Shenzhen, China.
Department of
Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong.
Correspondence to: Xixian Chen <xxchen@cse.cuhk.edu.hk>,
Michael R. Lyu <lyu@cse.cuhk.edu.hk>, Irwin King
<king@cse.cuhk.edu.hk>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

with its extensions (Zou et al., 2006), Linear Discriminant Analysis (LDA), and Quadratic Discriminant Analysis
(QDA) (Anzai, 2012) are powerful for dimension reduction
and denoising, which require the estimation of a covariance
matrix from a given collection of data points. Other prominent examples include Generalized Least Squares (GLS)
regression that requires the estimation of the noise covariance matrix (Kariya & Kurata, 2004), Independent Component Analysis (ICA) that relies on pre-whitening based on
the covariance matrix (Hyvärinen et al., 2004), and Generalized Method of Moments (GMM) (Hansen, 1982) that
improves the effectiveness by a precise covariance matrix.
Many practical applications also rely on covariance matrix directly (Bartz, 2016). In biology, gene relevance networks and gene association networks are straightforwardly
inferred from the covariance matrix (Butte et al., 2000;
Schäfer & Strimmer, 2005). In modern wireless communications, protocols optimize the bandwidth based on covariance estimates (Tulino & Verdú, 2004). In array signal processing, the capon beamformer linearly combines
the sensors to minimize the noise in the signal, which is
closely related to the portfolio optimization on covariance
matrices (Abrahamsson et al., 2007). For policy learning
in the field of robotics, it requires reliable estimates of the
covariance matrix between policy parameters (Deisenroth
et al., 2013).
Calculation of a covariance matrix usually requires enormous computational resources in the form of communication and storage because large and high-dimensional data
are now routinely gathered at an exploding rate from many
distributed remote sites, such as sensor networks, surveillance, and distributed databases (Haupt et al., 2008; Shi
et al., 2014; Ha & Barber, 2015). In particular, high communication cost of transmitting the distributed data from
the remote sites to the fusion center (i.e., a destination to
conduct complex data analysis tasks) will require tremendous bandwidth and power consumption (Srisooksai et al.,
2012; Abbasi-Daresari & Abouei, 2016). Formally, given
a data matrix X ∈ Rd×n with d features and n instances
collected from the remote sites, the covariance matrix is
computed in the
by C , n1 XXT − x̄x̄T ,
Pnfusion center
1
d
where x̄ = n i=1 xi ∈ R (Feller, 1966). For simplicity

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

of discussion, we temporarily assume the empirical mean is
zero, i.e., x̄ = 0. The covariance matrix can be written as
C = n1 XXT consequently (Azizyan et al., 2015). Then, it
takes O(nd) communication burden to transmit data from
numerous remote sites to the fusion center to form the full
data set X, O(nd) storage in total to store X in remote
sites, and O(nd+d2 ) storage with O(nd2 ) time to calculate
C in the fusion center. When n, d  1, the overall cost is
prohibitively expensive for practical scenarios like wireless
sensors which have narrow transmission bandwidth, limited storage, and low power supply.
To tackle such computational challenges, compressed data
can be leveraged to estimate the covariance matrix, which
essentially has roots in compressed sensing. One solution
is to process each data point by multiplying it with a single projection matrix S ∈ Rd×m whose entry follows the
1
) (Mahoney, 2011). Thus,
Gaussian distribution N (0, m
T
storing S X and the estimated covariance matrix requires
O(mn + d2 ) space in total, sending ST X to the fusion center incurs a O(mn) communication cost, and calculating
ST X and the covariance matrix estimator n1 SST XXT SST
takes O(mdn + m2 n + m2 d + md2 ) time. This method
substantially reduces all computational costs if m  n, d.
Note that synchronizing only a seed between remote sites
and the fusion center allows pseudo-random number generators to reconstruct an identical S, which avoids sending
S directly and imposes a negligible computational burden.
However, the example solution has two critical drawbacks.
The first is that the operations on the Gaussian matrix is
inefficient. One could use a sparse projection matrix (Li
et al., 2006), structured matrix (Ailon & Chazelle, 2009) or
sampling matrix (Drineas et al., 2006b) to achieve a better
tradeoff between computational cost and estimation precision. The second problem is that applying a single projection matrix to all data points cannot consistently estimate
the covariance matrix, i.e., the estimator cannot converge
to the actual covariance matrix even if the sample size n
grows to infinity with d fixed. This issue is demonstrated
both theoretically and empirically in (Azizyan et al., 2015)
and also briefly described in (Gleichman & Eldar, 2011;
Anaraki & Hughes, 2014; Anaraki & Becker, 2017).
In this paper, we thus adopt n distinct projection matrices
for n data vectors (Azizyan et al., 2015; Anaraki & Hughes,
2014; Anaraki & Becker, 2017; Anaraki, 2016) to achieve
consistent covariance matrix estimation, and construct a
specific sampling matrix to increase both its efficiency and
accuracy. On the whole, we do not make statistical assumptions on the distributed data X ∈ Rd×n with n, d  1,
nor do we impose structural assumptions on the covariance
matrix C such as being low-rank or sparse. Our goal is to
compress data and recover C efficiently and accurately, and
the contributions in our work are summarized as follows:

• First, in contrast to all existing methods (Azizyan
et al., 2015; Anaraki & Hughes, 2014; Anaraki &
Becker, 2017; Anaraki, 2016) that are based on dataoblivious projection matrices, we propose to estimate
the covariance matrix based on the data compressed
by a weighted sampling scheme. This strategy is dataaware with a capacity to explore the most important
entries. Hence, we require considerably fewer entries
to achieve an equal estimation accuracy.
• Second, we provide error analysis for the derived unbiased covariance estimator, which rigorously demonstrates that our method can compress data to a much
smaller volume than other methods. The proofs also
indicate our probability distribution is specifically designed to render a covariance matrix estimation based
on the compressed data as accurate as possible.
• Third, we specify our method by an efficient algorithm
whose computational complexity is superior to other
methods. By additionally considering the best tradeoff
between the estimation accuracy and the compression
ratio, our algorithm ultimately incurs a significantly
lower computational cost than the other methods.
• Finally, we validate our method on both synthetic and
real-world datasets, which demonstrates a better performance than the other methods.
The remainder of this paper is organized as follows. In Section 2, we review the prior work. In Section 3, we present
our method along with theoretical analysis and emphasize
its achievements. In Section 4, we provide extensive empirical results, and in Section 5 we conclude the whole work.

2. Related Work
There have been several investigations of ways to achieve
accurate covariance matrix estimation from the lowdimensional compressed observations constructed by applying a distinct projection matrix {Si }ni=1 ∈ Rd×m
The work
to each data vector {xi }ni=1 ∈ Rd .
of (Qi & Hughes, 2012) adopts a Gaussian matrix
to compress data via STi xi , and recovers them by
Si (STi Si )−1 (STi xi ). Because Si (STi Si )−1 STi is a strictly
m-dimensional orthogonal projection drawn uniformly
at random, it can capture the information of all entries in P
each data vector uniformly and substantively.
n
Then, n1 i=1 Si (STi Si )−1 STi xi xTi Si (STi Si )−1 STi up to
a known scaling factor is expected to constitute accurate
and consistent covariance matrix estimation. This estimator can be modified to an unbiased one, and its error analysis is thoroughly provided in (Azizyan et al., 2015). However, a Gaussian matrix is dense and unstructured, which
imposes an extra computational burden. Also, many matrix inversions take a considerable amount of time, and

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

the whole square matrixPhas to be loaded into the memn
ory. Biased estimator n1 i=1 Si STi xi xTi Si STi is thus proposed in (Anaraki & Hughes, 2014) to improve the efficiency by avoiding matrix inversions and assigning Si
to be a sparse matrix. This method is less accurate because Si STi approximates only an m-dimensional random
orthogonal projection. Its another disadvantage is that the
result only holds for data samples under statistical assumptions. Based on (Anaraki & Hughes, 2014), another study
proposes an unbiased estimator (Anaraki, 2016), but it still
adopts an unstructured sparse matrix that is insufficiently
computation-efficient and fails to provide the error bounds
to characterize the estimation error versus the compression
ratio. Recently, sampling matrices Si ∈ Rd×m constructed
via uniform sampling without replacement have been employed (Anaraki & Becker, 2017). This approach is efficient, but it only results in poor accuracy if data are compressed directly by STi xi ∈ Rd because Si STi is an mdimensional orthogonal projection drawn only from d deterministic orthogonal spaces/coordinates, and the d − m
entries of each vector are removed. To avoid sacrificing much accuracy, use of the computationally efficient
Hadamard matrix (Tropp, 2011) before sampling has also
been proposed in (Anaraki & Becker, 2017). It flattens out
whole entries, particularly those with large magnitudes, to
all coordinates to ensure that poor uniform sampling with a
small sampling size still obtains some information among
all entries. However, the Hadamard matrix involves deterministic orthogonal projection and is unable to capture
the information uniformly in all coordinates of each vector,
which results in the need for numerous samples to achieve
sufficient accuracy. (Anaraki & Becker, 2017) constitutes
the current state of the art in the tradeoff between the estimation accuracy and computational efficiency. Throughout the paper, we group the foregoing representative methods into Gauss-Inverse (Azizyan et al., 2015; Qi & Hughes,
2012), Sparse (Anaraki & Hughes, 2014; Anaraki, 2016),
and UniSample-HD (Anaraki & Becker, 2017), and the unbiased estimators produced by these methods are adopted
in the subsequent theoretical and empirical comparisons.
A number of other methods have been proposed to recover
covariance matrix from compressed data (Chen et al., 2013;
Bioucas-Dias et al., 2014; Dasarathy et al., 2015; Cai et al.,
2015). These methods are only applicable to low-rank,
sparse, or statistically-assumed covariance matrices.
Interesting work has also been done in the area of low-rank
matrix approximation via randomized techniques. In addition to simply embedding the data X into space spanned
by a single random projection matrix S, a representative
study (Halko et al., 2011) improves approximation accuracy by replacing the random projection matrix S with a
low-dimensional data-aware matrix XS0 , where S0 is a random projection matrix. However, X has to be low-rank,

and computing XS0 requires one extra pass through all entries in X. It is not suitable for our settings, where we do
not impose structural assumptions on the covariance matrix, nor do we fully observe all data. Moreover, (Azizyan
et al., 2015) demonstrates both theoretically and empirically that a single projection matrix for all data points cannot consistently and accurately estimate the covariance matrix. The problem also exist in (Wu et al., 2016; Mroueh
et al., 2016) aiming for a fast approximation of matrix products in a single pass, which only results in an inconsistent
covariance matrix estimation and suits the low-rank case.
Among randomized techniques, it is also worth briefly
discussing sampling approaches in matrix approximation.
Literature in (Drineas et al., 2006a; Papailiopoulos et al.,
2014; Woodruff, 2014; Holodnak & Ipsen, 2015) proposes
to leverage column sampling in which the sampling probabilities in the sampling matrix are either the column norms
or leverage scores. Other work (Woodruff, 2014; Achlioptas & Mcsherry, 2007; Achlioptas et al., 2013) performs
element-wise sampling on the entire matrix based on the
relative magnitudes over all data entries. These researches
employ different sampling distributions to sample entries
in a matrix. However, they have to observe all data fully
to calculate the sampling distributions, which also requires
one or more extra passes. In addition, their sampling probabilities are designed for matrix approximation, which cannot be trivially extended to covariance matrix estimation
because the exact covariance matrix in our setting cannot be
calculated in advance. Note that although the uniform sampling in matrix approximation is a simple one-pass algorithm, it performs poorly on many problems because usually there exists structural non-uniformity in the data which
has been verified in (Anaraki & Becker, 2017).

3. Our Approach
In this section, we first introduce the definition and background to our overall work. We then justify and present our
method of data compression and covariance matrix estimation, followed by the primary results and analysis.
3.1. Preliminaries
Let [k] denote a set of integers {1, 2, . . . , k}. Given a matrix X ∈ Rd×n , for j ∈ [d], i ∈ [n], we let xi ∈ Rd denote
the i-th column of X, and xji denote the (j, i)-th element of
X or j-th element of xi . Let {Xt }kt=1 denote the set of matrices {X1 , X2 , . . . , Xk }, and xji,t denote the (j, i)-th element of Xt . Let XT denote the transpose of X, and Tr(X)
denote its trace. Let |x| denote the absolute value of x. Let
kXk2 and kXkF denote the spectral norm and Frobenius
Pd
norm of X, respectively. Let kxkq = ( j=1 |xj |q )1/q for
q ≥ 1 be the `q -norm of x ∈ Rd . Let D(x) be a square

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

diagonal matrix with the elements of vector x on the main
diagonal, and D(X) also be a square diagonal matrix whose
main diagonal has only the main diagonal elements of X.
3.2. Method and Algorithm
As discussed previously, Gauss-Inverse (Azizyan et al.,
2015; Qi & Hughes, 2012) and Sparse (Anaraki & Hughes,
2014; Anaraki, 2016) suffer from deficiencies in either
computational efficiency or estimation accuracy, whereas
UniSample-HD (Anaraki & Becker, 2017) is less accurate
but offers a good tradeoff between estimation accuracy and
computational efficiency. We thus propose the adoption of
weighted sampling matrices {Si }ni=1 ∈ Rd×m to compress
data via STi xi and then back-project the compressed data
into the original space via Si STi xi . The recovered data
is then used for covariance matrix estimation as shown in
Eq. (1). Hence, a high computational efficiency is maintained. Although Si removes at least d−m entries from the
i-th vector, the remainders can be the most informative and
are retained. With the carefully designed sampling probabilities, our unbiased estimator Ce performs as accurately
as or more accurately than its counterparts asymptotically
in terms of matrix spectral norm kCe − Ck2 . Note we
have not quantified the error in any other entry-wise norm
(e.g., the Frobenius norm) that could be uninformative on
the quality of the approximate invariant subspace and unstable regarding the additive random error (Anaraki, 2016;
Achlioptas et al., 2013; Gittens, 2011).
Algorithm 1 The proposed algorithm.
Input:
Data X ∈ Rd×n , sampling size m, and 0 < α < 1.
Output:
Estimated covariance matrix Ce ∈ Rd×d .
1: Initialize Y ∈ Rm×n , T ∈ Rm×n , v ∈ Rn , and w ∈
Rn with 0.
2: for all i ∈ [n] do
Pd
3:
Load xi into memory, let vi = kxi k1 = k=1 |xki |
Pd
and wi = kxi k22 = k=1 x2ki
4:
for all j ∈ [m] do
5:
Pick tji ∈ [d] with pki ≡ P(tji = k) = α |xvkii | +
x2

6:
7:
8:
9:
10:

(1 − α) wkii , and let yji = xtji i
Pass the compressed data Y, sampling indices T, v,
w, and α to the fusion center.
for all i ∈ [n] do
Initialize Si ∈ Rd×m and P ∈ Rd×n with 0
for all j ∈ [m] do
Let ptji i = α
1
√mp

|yji |
vi

y2

+ (1 − α) wjii , and stji j,i =

tji i

11: Compute Ce as defined in Eq. (1) by using {Si }n
i=1 ,

T, P, and Y.

We here summarize our method in Algorithm 1. In a nutshell, we employ a weighted sampling that is able to explore the most important entries to reduce estimation error
kCe − Ck2 . Steps 1 to 5 in our proposed algorithm show
how to compress distributed data in many remote sites.
In step 5, each entry is retained with probability proportional to the combination of its relative absolute value and
square value, and such sampling probability is designed to
make kCe − Ck2 as small as possible. Step 6 shows the
communication procedure, and steps 7 to 11 reveal how to
construct an unbiased covariance matrix estimator in the
fusion center from compressed data. In many computing
cases, it is possible to manipulate vectors of length O(d)
in memory, and thus when compressing data via weighted
sampling, only one pass is required to move data from the
external space to memory. Hence, our algorithm is also
applicable to streaming data. For a covariance matrix decalculate
fined asPC = n1 XXT − x̄x̄T , we can exactlyP
n
g
x̄ = n1 i=1 xi in the fusion center via x̄ = n1 j=1 uj ,
where {xi }ni=1 are from g  n remote sites, and uj ∈ Rd
is the summation of all data vectors in the j-th remote site
before being compressed. Doing so makes no deviation on
the following error analysis and imposes only a negligible
computational burden.
3.3. Primary Provable Results
In this part, we introduce the proposed covariance matrix
estimator. In Algorithm 1, we employ Y, T, v, and w
to calculate {Si }ni=1 . It can be verified that using only
{Si }ni=1 and Y is able to obtain {STi xi }ni=1 . Thus, we
describe our estimator via {Si }ni=1 and {STi xi }ni=1 in the
following theorem, which shows our estimator is unbiased.
Theorem 1. Assume X ∈ Rd×n and the sampling size
2 ≤ m < d. Sample m entries from each xi ∈ Rd with replacement by running Algorithm 1. Let {pki }dk=1 and Si ∈
Rd×m denote the sampling probabilities and sampling matrix, respectively. Then, the unbiased
Pn estimator for the target covariance matrix C = n1 i=1 xi xTi = n1 XXT can
be recovered as
b1 − C
b 2,
Ce = C
(1)
P
n
m
T
T
T
b1 =
where E [Ce ] = C, C
i=1 Si Si xi xi Si Si ,
nm−n
P
n
m
b2 =
and C
D(Si ST xi xT Si ST )D(bi ) with
bki =

nm−n
1
1+(m−1)pki .

i=1

i

i

i

Note that at most m entries in each bi have to be calculated
because each Si STi xi xTi Si STi has at most m non-zero entries in its diagonal. Now, having achieved the above unbiased estimator Ce , we analyze its properties. We precisely
upper bound the estimation error for the original estimator
C in the matrix spectral norm.
Theorem 2. Given X ∈ Rd×n and the sampling size 2 ≤
m < d, let C and Ce be defined as in Theorem 1. If the

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data
x2

|xki |
+(1−α) kxki
sampling probabilities satisfy pki = α kx
2
i k1
i k2
with 0 < α < 1 for all k ∈ [d] and i ∈ [n], then with
probability at least 1 − η − δ,
r
2d 2R
2d
+ 2σ 2 log( ), (2)
kCe − Ck2 ≤ log( )
δ 3
δ
h
i
2
2
7kxi k2
2 2nd 14kxi k1
where R = maxi∈[n]
+
log
(
)
, and
2
n
η
nmα
h
4
2
2
4
P
8kx
k
4kx
k
kx
k
9kx
k
n
i 2
i 1
i 2
i 2
σ 2 = i=1 n2 m2 (1−α)
2 + n2 m3 α2 (1−α) + n2 m(1−α)
i
2
2
2
2
Pn kx k x x
2kx k2 kxi k1
+ k i=1 ni 21mαi i k2 .
+ n2 mi2 α(1−α)

A large R and σ 2 work against the accuracy of Ce . Accordingly, our sampling probabilities are designed to make
R and σ 2 as small as possible to improve the accuracy. In
the proof of Theorem 2, we also show that the selection of
q
q = 1, 2 in Pd|xki|x| |q used for constructing the sampling
k=1

ki

x2

|xki |
+ (1 − α) kxki
probability pki = α kx
2 is necessary and
i k1
i k2
sufficient to make the error bound considerably tight.

Furthermore, α balances the performance by `1 -norm
x2
|xki |
based sampling kx
and `2 -norm based sampling kxki
2.
i k1
i k2
`2 sampling penalizes small entries more than `1 sampling.
Hence `2 sampling is more likely to select larger entries to
decrease error. However, as seen from the proof in the appendix, different from `1 sampling, `2 sampling is unstable
and sensitive to small entries, and it can make estimation
error incredibly high if extremely small entries are picked.
Hence, if α varies from 1 to 0, the estimation error will decrease and then increase, which is also empirically verified
in the appendix.
The error bound in Theorem 2 involves many datadependent quantities, whereas our primary interest lies in
studying the tradeoff between the computational efficiency
and estimation accuracy by employing weighted sampling
to compress data and estimate covariance matrix. To clarify, we modify Theorem 2 and make the bound explicitly
dependent on n, d, and m with the constraint 2 ≤ m < d.
Corollary 1. Given X ∈ Rd×n and the sampling size 2 ≤
m < d, let C and Ce be created
√ by Algorithm 1. Define
kxi k1
d, and kxi k2 ≤ τ for all
kxi k2 ≤ ϕ with 1 ≤ ϕ ≤
i ∈ [n]. Then, with probability at least 1 − η − δ we have
r
r

τ 2ϕ 1
1 
2
e
kCe − Ck2 ≤ min{O f +
+τ
,
m
n
nm
r
r


e f + τ ϕ dkCk2 + τ dkCk2 }, (3)
O
m
n
nm
q
2 2
2
ϕ
2
e
where f = τn + τnm
+ τ ϕ kCk
nm , and O(·) hides the
logarithmic factors on η, δ, m, n, d, and α.
The formulation √above explores the fact that 1 ≤
kxi k1 /kxi k2 ≤ d by the Cauchy-Schwarz inequality.

Before proceeding, we make several remarks to make a
comparison with the following representative work: GaussInverse, UniSample-HD, and Sparse. The first two methods
provide error analysis without assuming data distribution,
which is shown in (Azizyan et al., 2015; Anaraki & Becker,
2017) and illustrated in our appendix. In the following remarks, only our method is sensitive to ϕ, and we also em1
kXk2F ≤ kCk2 ≤ maxi∈[n] kxi k22 =
ploy the fact that nd
2
τ to simplify all asymptotic bounds.
√
Remark 1. Eq. (3) with ϕ = d indicates the error bound
for our estimator Ce in the worst case, where the magnitudes of each entry in all of the input data vectors are the
same (i.e., highly uniformly distributed). Even in this
 case,
e τ 2d +
our error bound has a leading term of order min{O
nm
q
q
q   2

2
dkCk2
kCk
τ
d
2
e τ d τd
τ
}, which is the
nm + m
n , O nm + m
n
same as Gauss-Inverse ignoring logarithmic factors. In
contrast, as the magnitudes of the entries in each data vector become uneven, ϕ gets smaller, leading to a tighter error bound than that in Gauss-Inverse. Furthermore, when
most of the entries in each vector xi have very low magnitudes, the summation of these magnitudes will be comparable to a particular constant. This situation is typical because in practice only a limited number of features in each input data dominate the learning performance. Hence, ϕ turns to O(1), and
(3) becomes
q Eq. 
q   2
 2
dkCk
τ
1
τ
2
2
e
e
min{O
,O
}, which is
+τ
+τ
n

nm

n

nm

tighter than
pthe leading term of Gauss-Inverse by a factor
of at least d/m. As explained in the next section, GaussInverse also lacks computational efficiency.
Remark 2. As our target is to compress data to a smaller
m that is not comparable to d in practice, O(d − m)
can be approximately regarded asqO(d). Then, the error
q 

1
e τ 2 d + τ dkCk2 + τ 2 d
of UniSample-HD is O
,
nm

nm

m

nm

which is asymptotically worse than our bound. When n
is sufficiently
large, q
the leading term of its error becomes
 q

dkCk2
τ 2d
1
e
+
O τ
nm
m
nm , which can be weaker than the
p
leading
√ term in our method by a factor of 1 to d/m when
ϕ = d, and at least d/m when ϕ = O(1).
However, if m is sufficiently close to d, which is not meaningful for practical usage, O(d − m) = O(1) will
qhold and
 2
τ d
2
e
the error of UniSample-HD becomes O nm +τ dkCk
nm +
q

τ2
d
m
nm . This bound may slightly outperform ours by a
p
√
factor of d/m = O(1) when ϕ = d, but is still worse
than ours when ϕ = O(1). These results also coincide with
the fact that UniSample-HD adopts uniform sampling without replacement combined with the Hadamard matrix, but
we employ weighted sampling with replacement.
Remark 3. The Sparse method, which employs a sparse

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

matrix for each Si , is not sufficiently accurate as demonstrated in our experiments. Moreover, there is no error analysis available for its unbiased estimator to characterize the
estimation error versus the compression ratio.
Thus far, we have not made statistical nor structural assumptions concerning the input data or covariance matrix
to derive our provable results. However, motivated by (Azizyan et al., 2015), it is also straightforward to extend our
results to the statistical data and a low-rank covariance matrix estimation. The derived results below are polynomially
equivalent to those in Gauss-Inverse (Azizyan et al., 2015).
Corollary 2 shows the (low-rank) covariance matrix estimation on Gaussian data, and Corollary 3 indicates the derived covariance estimator also guarantees the accuracy of
the principal components regarding the subspace learning.
Corollary 2. Given X ∈ Rd×n (2 ≤ d) and an unknown
population covariance matrix Cp ∈ Rd×d with each column vector xi ∈ Rd i.i.d. generated from the Gaussian distribution N (0, Cp ). Let Ce be constructed by Algorithm 1
with the sampling size 2 ≤ m < d. Then, with probability
at least 1 − η − δ − ζ,
r
 d2
d d
kCe − Cp k2
e
+
;
(4)
≤O
kCp k2
nm m n
Additionally, assuming rank(Cp )≤ r, with probability at
least 1 − η − δ − ζ we have
r
r
k[Ce ]r − Cp k2 e  rd
r
d
rd 
+
+
, (5)
≤O
kCp k2
nm m n
nm
where [Ce ]r is the solution to minrank(A)≤r kA − Ce k2 ,
e hides the logarithmic factors on η, δ, ζ, m, n, d,
and O(·)
and α.
Corollary 3. Given X, d, m, Cp and Ce as defined
Q
Pk
Q
T
b =
in Corollary 2. Let k =
i=1 ui ui and
k
Pk
T
k
k
û
û
with
{u
}
and
{û
}
being
the
leading
i
i
i
i
i=1
i=1
i=1
k eigenvectors of Cp and Ce , respectively. Denote by λk
the k-th largest eigenvalue of Cp . Then, with probability at
least 1 − η − δ − ζ,
r
Q
Q
 d2
k b k − k k2
1
d d
e
≤
O
+
, (6)
kCp k2
λk − λk+1
nm m n
e hides the
where the eigengap λk − λk+1 > 0 and O(·)
logarithmic factors on η, δ, ζ, m, n, d, and α.
The proof details of all our theoretical results are relegated to the appendix. We leverage the Matrix Bernstein
inequality (Tropp, 2015) and establish the error bound of
our proposed estimator on an arbitrary sampling probability in order to determine which sampling probability brings
the best estimation accuracy. The employment of the Matrix Bernstein inequality involves controlling the range and

variance of all zero-mean random matrices, whose derivations differ from those in (Azizyan et al., 2015; Anaraki
& Becker, 2017) because of different data compression
schemes. Moreover, to obtain the desired tight bound for
the range and variance, we precisely provide a group of
closed-form equalities or concentration inequalities for various quantities (see our proposed Lemma 1 and Lemma 2
along with their proofs in the appendix).
3.4. Computational Complexity
Recall that we have n data samples in the d-dimensional
space, and let m be the target compressed dimension. Regarding estimating C = n1 XXT , the computational comparisons between our method and the representative baseline methods are presented in Table 1, in which Standard method means that we compute C directly without
data compression. For the definition of covariance matrix
C = n1 XXT − x̄x̄T , extra computational costs (i.e., O(gd)
storage, O(gd) communication cost, and O(nd) time) must
be added to the last four compression methods in the table,
where g  n is the number of the entire remote sites. All
detailed analysis is relegated to the appendix.
TG and TS in Table 1 represent the time taken to
generate the Gaussian matrices and sparse matrices by
fast pseudo-random number generators like Mersenne
Twister (Matsumoto & Nishimura, 1998), which can be
enormous (Anaraki & Becker, 2017) and proportional to
nmd and nd2 , respectively, up to certain small constants.
Hence, our method can be regarded as the most efficient
when d is large. Furthermore, by using the smallest m to
obtain the same estimation accuracy as the other methods,
our approach incurs the least computational burden.

4. Empirical Studies
In this section, we empirically verify the properties of the
proposed method and demonstrate its superiority. We compare its estimation accuracy with that of Gauss-Inverse,
Sparse, and UniSample-HD. We also report the time comparisons.
We run all algorithms on both synthetic and real-world
datasets whose largest dimension is around and below 105 .
Such dimension is not very high in modern data analysis,
but this limitation is due to that reporting the estimation
error by calculating the spectral norm of a covariance matrix with its size larger than 105 × 105 will take intolerable
amount of memory and time. The parameter selection on α
is deferred to the appendix, and we empirically set α = 0.9.
To allow a fair comparison of the time consumptions measured by FLOPS, we implement all algorithms in C++ and
run them in a single thread mode on a standard workstation
with Intel CPU@2.90GHz and 128GB RAM.

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data
Table 1. Computational costs on the storage, communication, and time.

Communication
O(nd)
O(nm)
O(nm)
O(nm)
O(nm)

To clearly examine the performance, we compare all methods on six synthetic datasets: {Xi }3i=1 ∈ R1024×20000 ,
X4 ∈ R1024×200000 , X5 ∈ R2048×200000 , and X6 ∈
R65536×200000 , which are generated based on the generative model (Liberty, 2013). Specifically, given a matrix
X ∈ Rd×n from such model, it is formally defined as
X = UFG, where U ∈ Rd×k defines the signal column
space with UT U = Ik (k ≤ d), the square diagonal matrix
F ∈ Rk×k contains the diagonal entries fii = 1−(i−1)/k
that gives linearly diminishing signal singular values, and
G ∈ Rk×n is the signal coefficient with gij ∼ N (0, 1) that
is the Gaussian distribution. We let k ≈ 0.005d, then setting d = 1024 and n = 20000 completes the creation of
data X1 . For X2 , it is defined as DX, where each entry in
the square diagonal matrix D is defined by dii = 1/βi , and
βi is randomly sampled from the integer set [15]. Regarding X3 , it is constructed in the same way as X1 except that
F now becomes an identity matrix. Next, {Xi }6i=4 follow
the same generation strategy of X2 except for the n and d.
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.1

Error

0.12
0.08

0.2

m/d
X4, d=1024 n=200000
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.1

0.16
0.12
0.08

0.2

0.3

0.4

m/d

0.3
0.2

0

0

0.5

m/d
X5, d=2048 n=200000
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.04
0.1 0.2 0.3 0.4 0.5

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.4

0.1

0

0.1 0.2 0.3 0.4 0.5

0.04
0

0.3

X3, d=1024 n=20000

0.5

0.1

Error

0.16

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.4

0.05
0

X2, d=1024 n=20000

0.5

Error

0.15

X1, d=1024 n=20000

0.1

0.6
0.45

Error

Error

0.2

Error

0.25

0.2

0.3

0.4

0.5

m/d
X6, d=65536 n=200000
Sparse
UniSample-HD
Our method

0.3
0.15

0.1 0.2 0.3 0.4 0.5

m/d

0

0.1 0.2 0.3 0.4 0.5

m/d

Figure 1. Accuracy comparisons of covariance matrix estimation
on synthetic datasets. The estimation error is measured by
kCe − Ck2 /kCk2 with Ce calculated by all compared methods,
and cf = m/d is the compression ratio.

In Figure 1, we plot the relative estimation error averaged
over ten runs with its standard deviation versus the naive
compression ratio cf = m/d. Note that a large cf is not
necessary for practical usage, and our aim is to compress
data to a smaller volume. In Figure 2, we report the running
time taken in both the compressing and recovering stages,
which preliminarily depicts the efficiency of the different
methods and indicates how much power should be spent in
the practical computation.

Generally, our method displays the least error and deviation for all datasets and its error decreases dramatically
with an increase at a small cf. This observation indicates
that our method can achieve sufficient estimation accuracy
by using substantially fewer √
data entries than the other
methods. For X1 (ϕ = 0.81 d), the magnitudes of the
data entries are highly uniformly distributed, and thus our
method can be regarded as uniform sampling with replacement, which may perform slightly worse than UniSampleHD and Gauss-Inverse if cf becomes large enough. After allowing the magnitudes to√vary within a moderately
larger range in X2 (ϕ = 0.55 d), our method considerably outperforms the other three methods. Its improvement
comes from that only our method is sensitive to ϕ and a
smaller ϕ produces a tighter result, as demonstrated by Remarks
p 1 and 2. However, the√error of each method in X3
(τ /pkCk2 = 5.5, ϕ = 0.81 √
d) is larger than that in X1
It is be(τ / kCk2 = 4.3, ϕ = 0.81 d), respectively. p
cause of that almost all methods are sensitive to τ / pkCk2 ,
and the error kCe − Ck2 /kCk2 increases when τ / kCk2
rises. Such phenomenon is demonstrated via dividing numerous error bounds in Remarks 1 and 2 by kCk2 . Our
method also achieves
p the best performance in X4 . Although the ϕ and τ / kCk2 in X4 are approximately equal
with those in X2 , yet the proved error bounds with Remarks 1 and 2 reveal that a larger n in X4 will lead to
smaller estimation errors given the same cf. Finally, our
method also achieves the best accuracy when the dimension
d increases in both X5 and X6 . Besides, taking more data
(i.e., enlarging n) as suggested by X4 can be considered to
reduce the error in X5 and X6 . Note that Gauss-Inverse
has not been run on X6 since it costs enormous time.
103
102
101
100
10-1
10-2
10-3

X2, d=1024 n=20000
Gauss-Inverse
Sparse
UniSample-HD
Our method
Standard

0.1 0.2 0.3 0.4 0.5

m/d

103
102
101
100
10-1
10-2
10-3

X4, d=1024 n=200000
Gauss-Inverse
Sparse
UniSample-HD
Our method
Standard

Rescaled Time

4.1. Experiments on Synthetic Datasets

Time
O(nd2 )
O(nmd + nm2 d + nd2 ) + TG
O(d + nm2 ) + TS
O(nd log d + nm2 )
O(nd + nm log d + nm2 )

Rescaled Time

Storage
O(nd + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )

Rescaled Time

Method
Standard
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.1 0.2 0.3 0.4 0.5

m/d

103
102
101
100
10-1
10-2
10-3

X5, d=2048 n=200000
Gauss-Inverse
Sparse
UniSample-HD
Our method
Standard

0.1 0.2 0.3 0.4 0.5

m/d

Figure 2. Time comparisons of covariance matrix estimation on
synthetic datasets. Rescaled time results from the running time
normalized by that spent in the Standard way of calculating C =
XXT /n without data compression, and it is plotted in log scale.

Turning to Gauss-Inverse, it becomes highly accurate when

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

cf increases but requires much more time than Standard
(see Figure 2) so that its usage might be ruled out in practice. However, Gauss-Inverse remains a good choice when
we are in urgent need of reducing the storage and communication burden. Sparse, which has no error analysis of its
unbiased estimator, generally performs less accurately than
the others but requires less time than Standard. UniSampleHD is efficient while it still consumes more time than our
method. Also, its accuracy is inferior to our method especially when cf is small. In conclusion, our method is capable of compressing data to a very small size while guaranteeing both estimation accuracy and computational efficiency.

16
8
0
4

6

n

8

10

×104

4

6

n
(e), X8

25

8

10

2

20
15
10
5
0
2

4

6

n

4

×104

8

10

×104

6

n
(f), X7

1

8

10

×104

0.8
0.6
0.4
0.2
0
2

4

6

n

8

0.3
0.25
0.2
0.15
0.1
0.05
0

10

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.08
0.06

0.8

0

0.1 0.2 0.3 0.4 0.5
Arcene, d=10000 n=800
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.6
0.4

The observation that the curves in plots (d)-(f) are roughly
flat validates that the error bounds
√ induced by our method
decay rapidly with n in the 1/ n convergence rate, which
coincides with Eqs. (4)-(6). In addition to the fast error
convergence for the low-rank matrix Cp8 , our method can
also obtain an increasingly better estimation accuracy for a
high-rank covariance matrix Cp7 if we enlarge n, which is
displayed in plot (a). Besides, considering the omitted plot
where the eigengap λk −λk+1 of Cp7 decreases with k, the
fact that the errors in plot (c) increase with k also coheres
with Eq. (6). To conclude, our method also adapts well to
the specific settings in Corollaries
√ 2 and 3, and all induced
error bounds indeed satisfy a 1/ n convergence rate.

0.05
0.04

Error

0.3

0.1

0.2

0.03

0.3

0.4

0

0.1

0.1

0.2

0.3

0.4

m/d

0.5

0

0.3

0.4

m/d

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.5

0.5

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.4
0.3
0.2
0.1
0

0.1 0.2 0.3 0.4 0.5

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.05
0.1

0.2

A9a, d=123 n=48842

Mnist, d=780 n=70000

0.2

0.1

Gist1M, d=960 n=1000000

Slice, d=384 n=53500

0.15

0.2

m/d

m/d

Gauss-Inverse
Sparse
UniSample-HD
Our method

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.3

0.1 0.2 0.3 0.4 0.5

0.02
0

0.5

Isolet, d=617 n=7797

0.4

0.1

m/d

0.2

0

Gauss-Inverse
Sparse
UniSample-HD
Our method

0.01

Error

0

Cifar10, d=3072 n=60000

0.02

0.2

As confirmed in Figure 1, a large n benefits the estimation
accuracy. Thus, we study its effect more quantitatively. We
conduct experiments following the settings as defined in
Corollaries 2 and 3, and their results
in Eqs. (4)-(6) clearly
√
show that the errors decay in 1/ n convergence rate if d 
n. We run our method on another two synthetic datasets
{Xt }8t=7 ∈ Rd×n that follow the d-dimensional multivariate normal distribution N (0, Cpt ), where the (i, j)-th element of Cp7 ∈ Rd×d is 0.5|i−j|/50 , and Cp8 ∈ Rd×d is
a low-rank matrix that satisfies minrank(A)≤r kA − Cp7 k2 .
We take d = 1000, r = 5, m/d = {0.02, 0.05, 0.15},
k = {5, 10, 15}, and vary n from 1000 to 100000. In Figure 3, the top three plots report the errors as defined in the
LHS of Eqs.
√ (4)-(6), respectively. Then, dividing such errors by 1/ n obtains the bottom three plots accordingly.

0.04

m/d

×104

Figure 3. Convergence rates of our method for the settings in
Corollaries 2 and 3.

DailySports, d=5625 n=9120

Error

24

2

0
2

Error

10

×104

Rescaled Error

32

8

0.1

0.2

0.3

0.4

0.5

m/d
0.2
0.15

Error

6

n
(d), X7 X8

2

Error

4

4

Error

0

k=5, m/d=0.02
k=10, m/d=0.02
k=15, m/d=0.02
k=5, m/d=0.05
k=10, m/d=0.05
k=15, m/d=0.05
k=5, m/d=0.15
k=10, m/d=0.15
k=15, m/d=0.15

Error

0

(c), X7

In the second set of experiments, we use nine publicly
available real-world datasets (Chang & Lin, 2011; Blake
& Merz, 1998; Amsaleg, 2010), some of which are gathered from many distributed sensors. Their statistics are
displayed in Figure 4. We again compare the estimation
accuracy of the proposed method against the other three
approaches. As can be seen from the figure, our method
consistently exhibits superior accuracy over all cf = m/d,
and its error decreases dramatically when cf grows. The
error of the other three methods also decreases with cf but
is still large at a small cf. Besides, our method enjoys the
least deviation. In summary, these results confirm that our
method can compress data to the lowest volume with the
best accuracy, thereby substantially reducing storage, communication, and processing time cost in practice.

Error

0.2
2

Rescaled Error

0.4

0.2

×10-3

6

Rescaled Error

0.4

8
m/d=0.02
m/d=0.05
m/d=0.15

0.6

Error

Error

0.6

(b), X8

0.8

X7, m/d=0.02
X8, m/d=0.02
X7, m/d=0.05
X8, m/d=0.05
X7, m/d=0.15
X8, m/d=0.15

Error

(a), X7 X8

1
0.8

4.2. Experiments on Real-world Datasets

0.1

UJIIndoorLoc, d=520 n=21048
Gauss-Inverse
Sparse
UniSample-HD
Our method

0.05
0.1 0.2 0.3 0.4 0.5

m/d

0

0.1 0.2 0.3 0.4 0.5

m/d

Figure 4. Accuracy comparisons of covariance matrix estimation
on real-world datasets.

5. Conclusion
In this paper, we describe a weighted sampling method for
accurate and efficient calculation of an unbiased covariance matrix estimator. The analysis demonstrates that our
method can employ a smaller data volume than the other
approaches to achieve an equal accuracy, and is highly efficient regarding the communication, storage, and processing
time. The empirical results of the algorithm’s application to
both synthetic and real-world datasets further support our
analysis and demonstrate its significant improvements over
other state-of-the-art methods.
Compared with the sampling-with-replacement scheme in
this paper, we seek to make more achievements via a
sampling-without-replacement scheme in the future work.
Analyzing the corresponding unbiased estimator will pose
significant technical challenges in this research direction.

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

Acknowledgments
We truly thank Akshay Krishnamurthy for the fruitful discussions and interpretations on (Azizyan et al., 2015). We
also thank Yuxin Su for the help on the experiments. The
work described in this paper was fully supported by the
National Natural Science Foundation of China (Project
No. 61332010), the Research Grants Council of the Hong
Kong Special Administrative Region, China ((No. CUHK
14208815 and No. CUHK 14234416 of the General Research Fund), and 2015 Microsoft Research Asia Collaborative Research Program (Project No. FY16-RESTHEME-005).

References

Azizyan, M., Krishnamurthy, A., and Singh, A. Extreme
compressive sampling for covariance estimation. arXiv
preprint arXiv:1506.00898, 2015.
Bartz, D. Advances in high-dimensional covariance matrix
estimation. 2016.
Bioucas-Dias, J., Cohen, D., and Eldar, Y. C. Covalsa: Covariance estimation from compressive measurements using alternating minimization. In Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European, pp. 999–1003. IEEE, 2014.
Blake, C.L. and Merz, C.J. UCI repository of machine
learning databases, 1998.

Abbasi-Daresari, S. and Abouei, J. Toward cluster-based
weighted compressive data aggregation in wireless sensor networks. Ad Hoc Networks, 36:368–385, 2016.

Butte, A. J., Tamayo, P., Slonim, D., Golub, T. R., and Kohane, I. S. Discovering functional relationships between
rna expression and chemotherapeutic susceptibility using relevance networks. Proceedings of the National
Academy of Sciences, 97(22):12182–12186, 2000.

Abrahamsson, R., Selen, Y., and Stoica, P. Enhanced covariance matrix estimators in adaptive beamforming. In
Acoustics, Speech and Signal Processing, 2007. ICASSP
2007. IEEE International Conference on, volume 2, pp.
II–969. IEEE, 2007.

Cai, T. T., Zhang, A., et al. Rop: Matrix recovery via rankone projections. The Annals of Statistics, 43(1):102–138,
2015.

Achlioptas, D. and Mcsherry, F. Fast computation of lowrank matrix approximations. Proceedings of the annual
ACM symposium on Theory of computing, 54(2):9, 2007.
Achlioptas, D., Karnin, Z. S., and Liberty, E. Near-optimal
entrywise sampling for data matrices. In Advances in
Neural Information Processing Systems, pp. 1565–1573,
2013.
Ailon, N. and Chazelle, B. The fast johnson-lindenstrauss
transform and approximate nearest neighbors. SIAM
Journal on Computing, 39(1):302–322, 2009.
Amsaleg, L. Datasets for approximate nearest neighbor
search, 2010.
Anaraki, F. Estimation of the sample covariance matrix
from compressive measurements. IET Signal Processing, 2016.
Anaraki, F. and Becker, S. Preconditioned data sparsification for big data with applications to pca and k-means.
IEEE Transactions on Information Theory, 2017.
Anaraki, F. and Hughes, S. Memory and computation efficient pca via very sparse random projections. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1341–1349, 2014.
Anzai, Y. Pattern Recognition & Machine Learning. Elsevier, 2012.

Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.
Chen, Y., Chi, Y., and Goldsmith, A. Exact and stable covariance estimation from quadratic sampling via convex
programming. 2013.
Dasarathy, G., Shah, P., Bhaskar, B. N., and Nowak, R. D.
Sketching sparse matrices, covariances, and graphs via
tensor products. Information Theory, IEEE Transactions
on, 61(3):1373–1388, 2015.
Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey
on policy search for robotics. Foundations and Trends in
Robotics, 2(1-2):1–142, 2013.
Drineas, P., Kannan, R., and Mahoney, M. W. Fast monte
carlo algorithms for matrices i: Approximating matrix
multiplication. SIAM Journal on Computing, 36(1):132–
157, 2006a.
Drineas, P., Mahoney, M. W., and Muthukrishnan, S. Subspace sampling and relative-error matrix approximation.
In Approximation, Randomization, and Combinatorial
Optimization. 2006b.
Feller, W. introduction to probability theory and its applications. vol. ii.[an]. 1966.
Gittens, A. The spectral norm error of the naive nystrom
extension. arXiv preprint arXiv:1110.5305, 2011.

Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data

Gleichman, S. and Eldar, Y. C. Blind compressed sensing. Information Theory, IEEE Transactions on, 57(10):
6958–6975, 2011.

Qi, H. and Hughes, S. M. Invariance of principal components under low-dimensional random projection of the
data. In Image Processing. IEEE, 2012.

Ha, W. and Barber, R. F. Robust pca with compressed data.
In Advances in Neural Information Processing Systems,
pp. 1936–1944, 2015.

Schäfer, J. and Strimmer, K. An empirical bayes approach to inferring large-scale gene association networks. Bioinformatics, 21(6):754–764, 2005.

Halko, N., Martinsson, P., and Tropp, J. A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217–288, 2011.

Shi, T., Tang, D., Xu, L., and Moscibroda, T. Correlated
compressive sensing for networked data. In UAI, pp.
722–731, 2014.

Hansen, L. P. Large sample properties of generalized
method of moments estimators. Econometrica: Journal
of the Econometric Society, pp. 1029–1054, 1982.

Srisooksai, T., Keamarungsi, K., Lamsrichan, P., and
Araki, K. Practical data compression in wireless sensor
networks: A survey. Journal of Network and Computer
Applications, 35(1):37–59, 2012.

Haupt, J., Bajwa, W. U., Rabbat, M., and Nowak, R. Compressed sensing for networked data. IEEE Signal Processing Magazine, 25(2):92–101, 2008.

Tropp, J. A. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data
Analysis, 3(01n02):115–126, 2011.

Holodnak, J. T. and Ipsen, I. C. Randomized approximation
of the gram matrix: Exact computation and probabilistic
bounds. SIAM Journal on Matrix Analysis and Applications, 36(1):110–137, 2015.

Tropp, J. A. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1-2):1–230, 2015.

Hyvärinen, A., Karhunen, J., and Oja, E. Independent component analysis, volume 46. John Wiley & Sons, 2004.

Tulino, A. M. and Verdú, S. Random matrix theory and
wireless communications, volume 1. Now Publishers
Inc, 2004.

Kariya, T. and Kurata, H. Generalized least squares. John
Wiley & Sons, 2004.
Li, P., Hastie, T. J., and Church, K. W. Very sparse random
projections. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining. ACM, 2006.
Liberty, E. Simple and deterministic matrix sketching.
In Proceedings of the 19th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pp. 581–588. ACM, 2013.
Mahoney, M. Randomized algorithms for matrices and
data. Foundations and Trends in Machine Learning, 3
(2):123–224, 2011.
Matsumoto, M. and Nishimura, T. Mersenne twister:
a 623-dimensionally equidistributed uniform pseudorandom number generator. ACM Transactions on Modeling and Computer Simulation, 1998.
Mroueh, Y., Marcheret, E., and Goel, V. Co-occuring directions sketching for approximate matrix multiply. arXiv
preprint arXiv:1610.07686, 2016.
Papailiopoulos, D., Kyrillidis, A., and Boutsidis, C. Provable deterministic leverage score sampling. In Proceedings of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 997–1006.
ACM, 2014.

Woodruff, D. P. Sketching as a tool for numerical linear
algebra. arXiv preprint arXiv:1411.4357, 2014.
Wu, S., Bhojanapalli, S., Sanghavi, S., and Dimakis, A.
Single pass pca of matrix products. In Advances In
Neural Information Processing Systems, pp. 2577–2585,
2016.
Zou, H., Hastie, T., and Tibshirani, R. Sparse principal component analysis. Journal of computational and
graphical statistics, 15(2):265–286, 2006.

