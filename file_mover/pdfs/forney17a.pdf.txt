Counterfactual Data-Fusion for Online Reinforcement Learners
Andrew Forney 1 Judea Pearl 1 Elias Bareinboim 2

Abstract
The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers
decision-making settings where unmeasured
variables can influence both the agent‚Äôs decisions and received rewards (Bareinboim et al.,
2015). Recent findings showed that unobserved
confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e.,
experimental data); if UCs are naively averaged
out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we
show how counterfactual-based decision-making
circumvents these problems and leads to a coherent fusion of observational and experimental
data. We then demonstrate this new strategy in
an enhanced Thompson Sampling bandit player,
and support our findings‚Äô efficacy with extensive
simulations.

1. Introduction
Active learning agents are becoming increasingly integrated into complex environments, where a number of heterogeneous sources of information are available for use
(4; 6; 10). These agents have the ability to both intervene in
their environments (choosing actions, receiving feedback
on the quality of their choices, and then modifying future
actions accordingly), as well as observe other agents interacting. With the opportunity to learn from data collected
from different sources other than personal experimentation
come new challenges of ‚Äútransfer‚Äù in learning. In particular, agents should know that actions that are desirable for
populations may not be desirable for all individuals, and as
such, should be wary of how observed behavior generalizes
(i.e., transfers) to them and how these observations should
be combined with the agent‚Äôs own experience.

In this work, we study the conditions under which data collected under heterogeneous conditions (to be defined) can
be combined by an online agent to improve performance in
a reinforcement learning task. This challenge is not without precedent, as recent works have investigated dataset
transportability (when source and target differ structurally),
though in offline domains (3; 4). Others have studied scenarios in which agents learn from expert teachers in the
inverse reinforcement learning problems (1; 8). Recent
work from causal analysis has addressed data-fusion for
interventional quantities in reinforcement learning tasks
(18). However, this work addresses data-fusion in domains
where counterfactual quantities are sought, as for personalized decision-making (2; 17).
Environments for which an agent possesses fully labelled
data and a fully specified model (in which all factors relating contexts, actions, and their associated rewards are
known) are trivial from a learning transfer perspective; in
such scenarios, collected data is homogeneous because all
factors that may introduce bias between samples can be
controlled. Conversely, in this paper, we focus on the challenges that arise due to unobserved confounders (UCs),
namely, unmeasured and unlabelled variables that influence
an agent‚Äôs natural action choice as well as the outcome of
that action. Such factors are particularly subtle when left
uncontrolled due to their invisible nature and emergence of
what is known as confounding bias (12).
Our agent‚Äôs goal is to quickly learn about its environment by consolidating data collected from observing other
agents and data collected through its own experience, so
UCs pose a fundamental challenge: the results from seeing
another agent performing an action are not always qualitatively the same as doing the action itself. As such, throughout this paper, we will differentiate three classes of data
that may be employed by an autonomous agent to inform
its decision-making:

University of California, Los Angeles, California, USA
Purdue University, West Lafayette, Indiana, USA. Correspondence to: Andrew Forney <forns@cs.ucla.edu>.

1. Observational data is gathered through passive examination of the actions and rewards of agents other
than the actor, but for whom the actor is assumed to
be exchangeable.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

2. Experimental data is gathered through randomization (e.g., standard MAB exploration), or from a fixed,
non-reactive policy.

1

2

Counterfactual Data-Fusion for Online Reinforcement Learners

3. Counterfactual data (though traditionally computed
from a fully specified model or under specific empirical conditions) represents the rewards associated with
actions under a particular (or ‚Äúpersonalized‚Äù) instantiation of the UCs.
In the remainder of this work, we demonstrate how these
data types can be fused to facilitate learning in a variant of
the Multi-Armed Bandit problem with Unobserved Confounders (MABUC), first discussed in (2). In traditional
bandit instances (e.g., (13; 9; 7; 14; 5)), an agent is faced
with K ‚àà N, K ‚â• 2 discrete action choices (often called
‚Äúarms‚Äù), each with its own, independent, and initially unknown reward distribution. The agent‚Äôs task is to maximize
cumulative rewards over a series of rounds, which requires
learning about the underlying reward distributions associated with each arm. In the MABUC (formalized shortly),
agents are faced with the same task, except that UCs modify the agent‚Äôs arm-choice predilections and payout rates at
each round, and the dimensionality and functional form of
the UCs are unknown.
Though the data-fusion problem is an ongoing exploration
in the data sciences (4), this paper is the first to study learning techniques in MABUC settings that combine data sampled under heterogeneous data-collection modes. Specifically, our contributions are as follows:
1. Using counterfactual calculus, we formally show that
counterfactual quantities can be estimated by active
agents empirically (Sec. 4).
2. We demonstrate how observational, experimental, and
counterfactual datasets can be combined through a
heuristic for MABUC agents (Sec. 4).
3. We then develop a variant of the Thompson Sampling
algorithm that implements this new heuristic, and run
extensive simulations demonstrating its faster convergence rates compared to the current state-of-the-art
(Sec. 5).1

2. The Greedy Casino Revisited
In this section, we consider an expanded version of the
Greedy Casino problem introduced in (2). In its new floor‚Äôs
configuration, the Greedy Casino is considering four new
themed slot-machines (instead of the two used in the previous version) and wishes to make them as lucrative as
possible. After running a battery of preliminary tests, the
casino executives discover that two traits in particular predict which of the four machines that a gambler is likely
1
Supplemental material. For paper appendices and other resources, visit: https://goo.gl/MYJWbY

Figure 1. Plots of MAB algorithms performance vs. RDC in the
Greedy Casino scenario.

to play: whether or not the machines are all blinking (denoted B ‚àà {0, 1}), and whether or not the gambler is drunk
(denoted D ‚àà {0, 1}). After consulting with a team of psychologists and statisticians, the casino learns that any arbitrary gambler‚Äôs natural machine choice can be modeled by
the structural equation (12): X ‚Üê fX (B, D) = B + 2 ‚àó D
if the four machines are indexed as X ‚àà {0, 1, 2, 3}. The
casino also learns that its patrons have an equal chance of
being drunk or not (i.e., P (D = 1) = 0.5) and decide to
program their new machines to blink half of the time (i.e.,
P (B = 1) = 0.5).
To prevent casinos from exploiting their patrons, a new
gambling law stipulates that all slot machines in the state
must maintain a minimum 30% win rate. Wishing to leverage their new discovery about gamblers‚Äô machine choice
predilections while conscious of this law, the casino implements a reactive payout strategy for their machines, which
are equipped with sensors to determine if their gambler is
drunk or not (assume that the sensors are perfect at making this determination). As such, the machines are programmed with the payout distribution illustrated in Table
1a.
After the launch of the new slot machines, some observant
gamblers note that players appear to be winning only 20%
of the time, and report their suspicions to the state gambling commission. An investigator is then sent to the casino
to determine the merit of these complaints, and begins recruiting random gamblers from the casino floor to play at
randomly selected machines, despite the players‚Äô natural
predilections. Surprisingly, he finds that players in this experiment win 40% of the time, and declares that the casino
has committed no crime. Meanwhile, the casino continues to exploit players‚Äô gambling predilections, paying them
10% less than the law-mandated minimum. Plainly, gamblers are unaware of being manipulated by the UCs B, D,
and of the predatory payout policy that the casino has constructed around them. The collected data is summarized
in Table 1b; the second column (E[y1 |X]) represents the
observations drawn from the casino‚Äôs floor while the third

Counterfactual Data-Fusion for Online Reinforcement Learners

(a)
E[y1 |X, B, D]
X=0
X=1
X=2
X=3
(b)
X=0
X=1
X=2
X=3

D=0
D=1
B=0 B=1 B=0 B=1
*0.20
0.30
0.50
0.60
0.60 *0.20
0.30
0.50
0.50
0.60 *0.20
0.30
0.30
0.50
0.60 *0.20
E[y1 |X] E[y1 |do(X)]
0.20
0.40
0.20
0.40
0.20
0.40
0.20
0.40

Table 1. (a) Payout rates decided by reactive slot machines as a
function of arm choice X, sobriety D, and machine conspicuousness B. Players‚Äô natural arm choices under D, B are indicated by asterisks. (b) Payout rates according to the observational,
E[y1 |X], and experimental E[y1 |do(X)], distributions. Y = y1
represents winning.

(E[y1 |do(X)]) represents the randomized experiment performed by the state investigator (both with large sample
sizes).
In an attempt to find a better gambling strategy, an observant habitueÃÅ decides to run a battery of experiments using
standard MAB algorithms (e.g., -greedy, UCB, Thomson
Sampling) as well as an algorithm following an approach
presented in (2) known as the Regret Decision Criterion
(RDC) (reviewed in the next section). Importantly, the
RDC agent lacks the capability to identify and observe the
UCs. The results of her experiments are depicted in Fig. 1.
She notes, somewhat surprised, that all algorithms which
ignore the influence of the UCs (i.e., all but RDC) perform
equivalently to the randomized experiment conducted by
the investigator. Noting the differences in the payout rates
between the observational and experimental data, she ponders how this can be the case and how she might use these
datasets to improve her gambling strategy and winnings.

3. Background
In this section, we formalize the MABUC problem in the
language of Structural Causal Models (SCMs), which will
allow us to articulate the notions of observational, experimental, and counterfactual distributions as well as formalize the problem of confounding due to the influence of UCs.
Each SCM M is associated with a causal diagram G and
encodes a set of endogenous (or observed) variables V and
exogenous (or unobserved) variables U ; edges in G correspond to functional relationships relative to each endogenous variable Vi ‚àà V , namely, Vi ‚Üê fi (P Ai , Ui ), where
P Ai ‚äÜ V \ Vi and Ui ‚äÜ U ; and a probability distribution
over the exogenous variables P (U = u).

Each M induces: (1) observational distributions P (V =
v), which represent the ‚Äúnatural‚Äù world, without external
interventions; (2) a set of experimental (a.k.a. interventional) distributions P (Y = y|do(X = x)) for X, Y ‚äÜ V ,
which represent the world in which X is forced to the value
x despite any causal influences that would otherwise functionally determine its value in the natural setting; and (3) a
set of counterfactual distributions, defined next (12).2
Definition 3.1. (Counterfactual) (12) Let X and Y be two
subsets of endogenous variables in V . The counterfactual
sentence ‚ÄúY would be y (in situation U = u), had X been
x‚Äù is interpreted as the equality Yx (u) = y, where Yx (u)
encodes the solution for Y in a structural system where for
every Vi ‚àà X, the equation fi is replaced with the constant
x.
Note that the counterfactual expression E[Yx = y|X = x0 ]
is well-defined, even when x 6= x0 , and is read ‚ÄúThe expectation that Y = y had X been x given that X was observed to be x0 ‚Äù. Despite being logically valid statements in
SCMs, counterfactual quantities must be estimated from either a fully specified model, or, in the absence of such, from
data. In offline settings, however, counterfactual quantities
are not empirically estimable (namely, when the antecedent
of the counterfactual contradicts the observed value), except in some special cases ((12), Chs. 7, 9). The reason
is that if we submitted the subject to condition X = x0 ,
we cannot go back in time before exposure and submit the
same subject to a new condition X = x. As is well understood in the causal inference literature, this procedure is
not the same as first exposing a random unit to condition
X = x0 since the ones who initially were inclined to act as
X = x are somehow different than the randomly selected
subject. That said, we will show (in Sec. 4) that online
learning agents possess the means to estimate counterfactuals directly.
In practice, the observational and experimental distributions can be estimated through procedures known as random sampling and random experimentation, respectively.
Confounding bias emerges when UCs are present and can
be seen through the difference between these two distributions, P (Y |do(X = x)) ‚àí P (Y |X = x). The absence
of UCs implies that P (Y |do(X = x)) = P (Y |X = x),
which allows random sampling (instead of a randomized
experiment) to estimate the experimental distribution.
The contrast between observational and experimental data
is mirrored in the distinction between actions (which represent reactive ‚Äúchoices‚Äù resulting from an agents‚Äô environments, beliefs, and other causes) and acts (which represent
deliberate choices resulting from rational decision-making
or interventions that sever the causal influences of the sys2
For a comprehensive review of SCMs, we refer readers to
(12).

Counterfactual Data-Fusion for Online Reinforcement Learners

tem (12)). To tie these concepts to the MABUC problem,
one important tool introduced in (2) is known as the agent‚Äôs
intent.
Definition 3.2. (Intent) Consider a SCM M and an endogenous variable X ‚àà V that is amenable to external interventions and is (naturally) determined by the structural
equation fx (P Ax , Ux ), where P Ax ‚äÜ V represents the
observed parents of X, and Ux ‚äÜ U are the UCs of X.
After realization P Ax = pax and Ux = ux (without any
external intervention), we say that the output of the structural function given the current configuration of all UCs is
the agent‚Äôs intent, I = fx (pax , ux ).
Thus, intent can be seen as an agent‚Äôs chosen action before
its execution, which, in fact, is a proxy for any influencing
UCs.3 To ground these notions, consider again the Greedy
Casino example in which the gamblers‚Äô intents are enacted
on the unperturbed casino floor, but are then averaged over
during the investigator‚Äôs randomized study.
We can now put these observations together and explicitly
define the MABUC problem:
Definition 3.3. (K-Armed Bandits with Unobserved
Confounders) A K-Armed bandit problem (K ‚àà N, K ‚â•
2) with unobserved confounders (MABUC, for short) is defined as a model M with a reward distribution over P (u)
where, for each round 0 < t < T, t ‚àà N:
1. Unobserved confounders: Ut represents the unobserved variable encoding the payout rate and unobserved influences to the propensity to choose arm xt
at round t.
2. Intent: It ‚àà {i1 , ..., ik } represents the agent‚Äôs intended arm choice at round t (prior to its final choice,
Xt ) such that It = fi (paxt , ut ).
3. Policy: œÄt ‚àà {x1 , ..., xk } denotes the agent‚Äôs decision algorithm as a function of its history (discussed
shortly) and current intent, fœÄ (ht , it ).4
4. Choice: Xt ‚àà {x1 , ..., xk } denotes the agent‚Äôs final
arm choice that is ‚Äúpulled‚Äù at round t, xt = fx (œÄt ).
5. Reward: Yt ‚àà {0, 1} represents the Bernoulli reward
(0 for losing, 1 for winning) from choosing arm xt
under UC state ut as decided by yt = fy (xt , ut ).

Figure 2. Model for each round of the MABUC decision process.
Solid nodes denote observed variables and open nodes represent
unobserved variables. The square node indicates a decision point
made by the agent‚Äôs strategy.

The graphical model in Fig. 2 represents a prototypical
MABUC (Def. 3.3). We also add a graphical representation of the agent‚Äôs history Ht , a data structure containing
the agent‚Äôs observations, experiments, and counterfactual
experiences up to time step t. The means by which these
different data-collections can be used in the agent‚Äôs policy
are explored at length in the next section. In summary, at
every round t of MABUC, the unobserved state ut is drawn
from P (u), which then decides it , which is then considered
by the strategy œÄt in concert with the game‚Äôs history ht ; the
strategy makes a final arm choice, which is then pulled, as
represented by xt , and the reward yt is revealed.
Based on this definition, the regret decision criterion can be
stated (2):
Definition 3.4. (Regret Decision Criterion (RDC)) (2)
In a MABUC instance with arm choice X, intent I = i,
and reward Y , agents should choose the action a that maximizes their intent-specific reward, or formally:
argmax E[YX=a |X = i]

(1)

a

In brief, RDC prescribes that the arm X = a that maximizes the expected value of reward Y having conditioned
on the intended arm X = i should be selected, even when
a 6= i.

4. Fusing Datasets
3

Def. 3.2 does not require that all its influencing factors be
measured or acknowledged by the agent. This definition accomodates the fact that an agent‚Äôs decisions can be influenced by
unknown factors, an observation that is not new to the cognitive
sciences (16; 11).
4
Using this representation, the distinction between obs. and
exp. settings is made transparent ‚Äì œÄt copies it in the former, but
ignores it and listens instead to a random device (e.g., a coin toss)
in the latter.

Suppose our agent assumes the role of a gambler in the
Greedy Casino (Sec. 2) and possesses (1) observations
of arm choices and payouts from other players in the
casino, (2) the randomized experimental results from the
state investigator, and (3) the knowledge to use intent in
its decision-making for choosing arms by the Regret Decision Criterion (RDC). In other words, the agent begins

Counterfactual Data-Fusion for Online Reinforcement Learners

the MABUC problem with large samples of observations
(E[Y |X]) and experimental results (E[Y |do(X)]), and will
maximize the counterfactual RDC (E[YX=a = 1|X = i])
because it recognizes the presence of UCs (viz. E[Y |X] 6=
E[Y |do(X)]; see Table 1b). We note that the observational
and experimental data available to our agent contains information about its environment, but cannot simply be incorporated into the counterfactual maximization criteria (viz.
E[Y |X] 6= E[Y |do(X)] 6= E[Yx |x0 ]; see Table 1). So,
the agent can choose to either discard its observations and
experiments, and simply gamble by the tenets of RDC, or
combine them in an informative way. This section explores
the latter option.
Relating the Datasets
Note that the experimental quantity E[Y |do(X = x)] can
be written in counterfactual notation E[YX=x ] = E[Yx ],
which reads as ‚ÄúThe expected value of Y had X been x.‚Äù
Note also that E[Yx ] can be written as a weighted average
of the reward associated with arm x across all intent conditions (by the law of total probabilities), namely:
E[Yx ] = E[Yx |x1 ]P (x1 ) + ... + E[Yx |xK ]P (xK )

(2)

Examining Eq. 2, we see that the equation is composed
of expressions from our agent‚Äôs three datasets (observational, experimental, and counterfactual). By definition,
the LHS of the equation (E[Yx ]) is drawn from the experimental dataset. On the RHS, we have two types of quantities. Expressions of the form E[Yx |x0 ] for which x = x0
are observational by the consistency axiom (12), because
when the hypothesized and observed actions are the same,
the value of y is the same (i.e., E[Yx |x] = E[Y |x]). On
the other hand, expressions of the form E[Yx |x0 ] for which
x 6= x0 are non-trivial counterfactuals, mixing observations
and antecedents occurring in different worlds.
In general, evaluating counterfactuals empirically is not
possible, except for some special cases, such as when the
action X is binary (12). However, RDC asserts that if one
preempts the agent‚Äôs decision process when the intent I = i
is about to become a decision (X), i still encodes information about the UCs Ui (because i = fi (P Ax , Ux )). This
implies that randomizing within intent conditions can lead
to the computation of the counterfactual given by RDC,
which is a special counterfactual also called the effect of
treatment on the treated (ETT) (12).
In order for us to exploit the properties of this equivalence
to improve the performance of RDC agents in the MABUC
setting, we first demonstrate that RDC indeed measures the
counterfactual quantity of the ETT.
Theorem 4.1. The counterfactual ETT is empirically estimable for arbitrary action-choice dimension (i.e., |X| =
k for k ‚â• 2) when agents condition on their intent I = i

Figure 3. Counterfactual reward-history table as a cross of arm
choice and intent.

and estimate the response Y to their final action choice
X = a.
For a proof of Theorem 4.1, see supplementary material,
Appendix A. Because RDC is equivalently an interventional quantity, we have shown that the ETT, a counterfactual expression, can be estimated empirically through
counterfactual-based randomization.5 The main advantages of this, now proven, equivalence are threefold: (1)
the empirical estimation of previously unidentifiable counterfactual quantities presents opportunities for further exploration in causal analysis, (2) the ETT‚Äôs prescription for
integrating experimental and observational data (see Eq.
2) permits a now interventional data-fusion strategy when
such data is available, and (3) data points sampled by the
agent using intent-specific decision-making are counterfactual in nature, and should therefore be added to the
agent‚Äôs counterfactual history. Procedures implementing
RDC-type randomization should thus record intent-specific
arm rewards in a table similar to Fig. 3. A second consequence of recording arm-intent-specific payouts in this
fashion is that observational data may be substituted directly into cells for which the final arm choice and intent
agree (see reference to consistency axiom below Eq. 2).
Strategies for Online Agents
Now that we have illustrated how the different datasets relate to our agent in the MABUC setting, we consider that
the counterfactual expressions in Eq. 2 must be learned
by our agent and are not known at the start of the game.
Because of this finite-sample concern, we propose different learning strategies that exploit the datasets‚Äô relationship
while managing the uncertainty implicit in a MAB learning
scenario.
Strategy 1: Cross-Intent Learning. Consider Eq. 2 once
5
It is understood that the ETT can be computed for binary decisions or when the backdoor criterion holds (12), but it was not
believed to be estimable for arbitrary dimensions prior to RDCrandomization.

Counterfactual Data-Fusion for Online Reinforcement Learners

more. This holds for every arm X = x, which induces a
system of equations as shown in Fig. 3. Consider a single
cell in this system, say E[Yxr |xw ], which we can solve and
rewrite as:
EXInt [Yxr |xw ] = [E[Yxr ] ‚àí

K
X

E[Yxr |xi ]P (xi )]/P (xw )

i6=w

(3)
This form provides a systematic way of learning about arm
payouts across intent conditions, which is desirable because an arm pulled under one intent condition provides
knowledge about the payouts of that arm under other intent
conditions. This can be depicted graphically, as shown by
row B in Fig. 3 ‚Äì information about Yxr flows from intent
conditions xi 6= xw to intent xw (a form of information
leakage, (15)).
Strategy 2: Cross-Arm Learning. Consider any three
arms, xr , xs , xw such that r ‚àà
/ {s, w} and assume we are
interested in estimating the value of E[Yxr |xw ] (our query,
for short). Considering again the equations induced by Eq.
(2), we have,
E[Yxr ] =

K
X

E[Yxr |xi ]P (xi )

(4)

E[Yxs |xi ]P (xi )

(5)

a more robust estimate of the target quantity, this pairwise
comparison can be repeated between the query arm and all
other arms with the same intent, and then pooled together.
This can be seen as information about Yxr |xw flowing from
arm xs 6= xr to xr (under intent xw ) ‚Äì column C in Fig.
2(b).
One such pooling strategy is to take the inverse-varianceweighted average.6 Formally, we can consider a function
E[Yxr |xw ] = hXArm (xr , xw , xs ) such that hXArm performs the empirical evaluation of the RHS of Eq. (7). Ad2
ditionally, let œÉx,i
= V arsamp [Yx |i] indicate the empirical
payout variance for each arm-intent condition (as from the
reward successes and failures captured by the agent in Table 3). To estimate our query from all other arms in the
same intent through inverse-variance weighting, we have
our now complete, second heuristic:

PK
EXArm [Yxr |xw ] =

i6=r

hXArm (xr , xw , xi )/œÉx2i ,xw
(8)
PK
2
i6=r 1/œÉxi ,xw

i

E[Yxs ] =

K
X
i

Note that each of Eqs. (4, 5) share the same intent priors
on our query intent P (xw ), so we can solve for P (xw ) in
both equations using simple algebra, which yields,
PK
E[Yxr ] ‚àí i6=w E[Yxr |xi ]P (xi )
P (xw ) =
E[Yxr |xw ]
(6)
PK
E[Yxs ] ‚àí i6=w E[Yxs |xi ]P (xi )
=
E[Yxs |xw ]
Using Eq. (6) and solving for the query in terms of our
paired arm xs , ‚àÄ r 6= s we have


PK
E[Yxr ] ‚àí i6=w E[Yxr |xi ]P (xi ) E[Yxs |xw ]
E[Yxr |xw ] =
PK
E[Yxs ] ‚àí i6=w E[Yxs |xi ]P (xi )
(7)
Eq. (7) illustrates that any non-diagonal cell from the table
in Fig. 3 can be estimated through pairwise arm comparisons with the same intent. Put differently, Eq. (7) allows
our agent to estimate E[Yxr |xw ] from samples in which
any arm xs 6= xr was pulled under the same intent xw .
In practice, the online nature of the problem can make some
of these pairwise computations noisy due to sampling variability when xr is an infrequently explored arm. To obtain

Strategy 3: The Combined Approach. The payout estimates for a MABUC algorithm using RDC (Fig. 3) can be
estimated from three different sources: (1) Esamp [Yxr |xw ],
the sample estimates collected by the agent during the execution of the algorithm. (2) EXInt [Yxr |xw ], the computed
estimate using cross-intent learning. (3) EXArm [Yxr |xw ],
the computed estimate using cross-arm learning. Naturally,
these three quantities can be combined to obtain a more robust and stable estimate to the target query.
We employ an inverse-variance weighting scheme so as to
leverage these three estimators, and so we must formulate
a metric for the payout variance associated with each strategy‚Äôs computed estimate. To do so, we define an average
variance for each strategy, which is the average over each
2
sample estimate‚Äôs variance (i.e., œÉx,i
) used in the computation. Specifically, for the cross-arm approach (Eq. 8),
we have two summations over sample payout estimates
E[Yxr |xi ], E[Yxs |xi ] ‚àÄi 6= w which involve 2(K ‚àí 1)
terms, plus the numerator‚Äôs E[Yxs |xw ], giving us a total
of 2(K ‚àí 1) + 1 = 2K ‚àí 1 variances to average. The
same is true for the cross-intent apprach (Eq. 3), which involves K‚àí1 sample variances to average. When estimating
6

This strategy follows from the fact that we have Bernoulli rewards for each arm-intent condition, and as the number of samples
increases for these distributions, the variance diminishes, meaning that arm-intent conditions with smaller variances are more reliable than those with larger ones.

Counterfactual Data-Fusion for Online Reinforcement Learners

3. From these UCs and any other observed features in the
environment, the agent‚Äôs heuristics suggest an action
to take, i.e., its intent. With its intent known, the agent
combines the data in its history (in this work, by the
prescription of Strategy 3 above) to better inform its
decision-making.
4. Based on its intent and combined history, the agent
commits to a final action choice.
5. The action‚Äôs response in the environment (i.e., its reward) is observed, and the collected data point is
added to the agent‚Äôs counterfactual dataset (as a consequence of Theorem 4.1).

5. Simulations & Results

Figure 4. Illustrated data-fusion process.

E[Yxr |xw ], we can write the corresponding variances:
2
œÉXArm

K

K

i6=w

i6=w

i
1 h X 2   X 2 
œÉxr ,xi +
œÉxs ,xi + œÉx2s ,xw
=
2K ‚àí 1

2
œÉXInt
=

1
K ‚àí1

K
X

œÉx2r ,xi

i6=w

Finally, to estimate E[Yxr |xw ] using our combined approach, we have:
2
Œ± = Esamp [Yxr |xw ]/œÉx2r ,xw + EXInt [Yxr |xw ]/œÉXInt
2
+ EXArm [Yxr |xw ]/œÉXArm
2
2
Œ≤ = 1/œÉx2r ,xw + 1/œÉXInt
+ 1/œÉXArm

Ecombo [Yxr |xw ] =

Œ±
Œ≤

(9)

To visualize the data-fusion process discussed here, consider the diagram in Figure 4.
1. In this scenario, we consider that our agent has collected large samples of experimental and observational data from its environment (e.g., in the Greedy
Casino, the agent might observe other gamblers to
comprise its observational data and incorporate experimental findings from the state investigator‚Äôs report).
2. Unobserved confounders are realized in the environment, though their labels and values are unknown to
the agent.

In this section, we validate the efficacy of the strategies
discussed previously through simulations. To make a fair
comparison to previous MABUC bandit players, we will
follow the first implementation of RDC that used Thompson Sampling (TS) as its basis, embedding the strategies
described in the previous section within a TS player called
(T S RDC‚àó ). We note that after moving from traditional
to counterfactual-based decision-making we moved from
optimal arm-choice nonconvergence to convergence in the
MABUC setting (e.g., Fig. 1); now, the goal is to accelerate
convergence.
In brief, T S RDC‚àó agents perform the following at each
round: (1) Observe the intent it from the current round‚Äôs
realization of UCs, ut . (2) Sample EÃÇsamp [Yxr |it ] from
each arm‚Äôs (xr ) corresponding intent-specific beta distribution Œ≤(sxr ,it , fxr ,it )7 in which sxr ,it is the number of successes (wins) and fxr ,it is the number of failures (losses).
(3) Compute each arm‚Äôs it -specific score using the combined datasets via Strategy 3 (Eq. 9). (4) Choose the arm,
xa , with the highest score computed in previous step. (5)
Observe result (win / loss) and update EÃÇsamp [Yxa |it ].
Procedure. Simulations were performed on the 4-arm
MABUC problem, with results averaged across N = 1000
Monte Carlo repetitions, each T = 3000 rounds in duration. To illustrate the robustness of each proposed strategy,
we performed simulations spanning across a wide range of
payout parameterizations (see Appendix B for a complete
report of experimental results).
Compared Algorithms. Each simulation compares the
performance of four variants of Thompson Sampling, described below and with the data-sets employed by each indicated in Table 2:
7

The parameters for these distributions are decided by the
agent‚Äôs history (see Figure 2(b)), including contributions from observational data for cells in which action and intent agree.

Counterfactual Data-Fusion for Online Reinforcement Learners

Algorithm
T S RDC‚àó
T S RDC+
T S RDC
TS

Cf. Data
X
X
X

Obs. Data
X
X

Exp. Data
X

X

Table 2. Data-sets employed by the compared TS variants.

1. T S is the traditional Thompson Sampling bandit algorithm that attempts to maximize the interventional
quantity E[y|do(x)], and does not condition on intent.
2. T S RDC is TS player that uses RDC (Def. 3.4), but
employs no additional observational or experimental
data in its play.
3. T S RDC+ is the approach produced by (2), which uses
RDC and employs observational data, but does not incorporate experimental data nor exploit the relationship between data types detailed in the previous section.
4. T S RDC‚àó follows the algorithm described above and
uses the data-fusion strategy described in the previous
section.
Evaluation. Each algorithm‚Äôs performance is evaluated using two standard metrics: (1) the probability of optimal arm
choice and (2) cumulative regret, both as a function of t averaged across all N Monte Carlo simulations. However,
unlike in the traditional MAB literature, we compare each
algorithm‚Äôs choice to the optimal choice of an omniscient
oracle that knows the value of any UCs in any given round
of any MC repetition (indicated as x‚àón,t ). Formally, for all
P
0 < t < T we evaluate (1) as N1 n 1(x‚àón,t = xn,t ) and
P Pt
(2) as N1 n i E[Yx‚àón,i |un,i ] ‚àí yn,i .
Experiment 1: ‚ÄúGreedy Casino.‚Äù The Greedy Casino
parameterization, as described in Table 1, exemplifies the
scenario where all arms are both observationally equivalent (E[Y |x] = E[Y |x0 ], ‚àÄx, x0 ) and experimentally equivalent (E[Y |do(x)] = E[Y |do(x0 )], ‚àÄx, x0 ), but distinguishable within intent conditions (E[Yx |x0 ]). In this reward
parameterization, T S RDC‚àó experienced significantly less
regret (M = 42.23) than its chief competitor, T S RDC+ ,
(M = 65.04), t(1998) = 13.25, p < .001.
Experiment 2: ‚ÄúParadoxical Switching.‚Äù The Paradoxical Switching parameterization (see Appendix B for parameters) exemplifies a curious scenario wherein E[Yx1 ] =
0.5 > E[Yx0 ], ‚àÄx0 6= x1 , but for which x1 is the optimal
arm choice in only one intent condition (I = x1 ). Agents
unempowered by RDC will face a paradox in that the arm
with the highest experimental payout is not always optimal. Again, T S RDC‚àó experienced significantly less re-

Figure 5. Plots of TS variant performances in the Greedy Casino
[Ex1] and Paradoxical Switching [Ex2] scenarios.

gret (M = 36.91) than its chief competitor, T S RDC+ ,
(M = 64.70), t(1998) = 22.43, p < .001.
The accelerated learning enjoyed by RDC+ is not localized to these parameter choices alone. In Appendix B, we
show that T S RDC‚àó consistently experiences significantly
less regret than its competitors across a wide range of reward parametrizations.

6. Conclusion
The present work addresses the challenges faced by online learning agents that gain access to increasingly diverse,
and qualitatively different sources of information, and how
these sources can be meaningfully synthesized to accelerate learning. This data-fusion problem is complicated
by the presence of unobserved confounders (UCs), whose
identities and influences are unknown to the modeler. In response, we present a novel method by which online agents
may combine their observations, experiments, and counterfactual (i.e., personalized) experiences to more quickly
learn about their environments, even in the presence of
UCs. We then illustrate the efficacy of this approach in
the Multi-Armed Bandit problem with Unobserved Confounders (MABUC), and demonstrate how a traditional
Thompson Sampling player may be improved by its application. Simulations demonstrate that our data-fusion approach generalizes across reward parameterizations and results in significantly less regret (in some cases, as much as
half) than other competitive MABUC algorithms.

Counterfactual Data-Fusion for Online Reinforcement Learners

References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning
via inverse reinforcement learning. In Proceedings of
the twenty-first international conference on Machine
learning, page 1. ACM, 2004.
[2] E. Bareinboim, A. Forney, and J. Pearl. Bandits
with unobserved confounders: A causal approach. In
Advances in Neural Information Processing Systems,
pages 1342‚Äì1350, 2015.
[3] E. Bareinboim and J. Pearl. Causal inference by surrogate experiments: z-identifiability. In N. de Freitas
and K. Murphy, editors, Proceedings of the TwentyEighth Conference on Uncertainty in Artificial Intelligence (UAI 2012), pages 113‚Äì120. AUAI Press, 2012.
[4] E. Bareinboim and J. Pearl. Causal inference and
the data-fusion problem. Proceedings of the National
Academy of Sciences, 113(27):7345‚Äì7352, 2016.
[5] S. Bubeck and N. Cesa-Bianchi. Regret analysis
of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5:1‚Äì122, 2012.
[6] N. R. Council et al. Frontiers in massive data analysis. National Academies Press, 2013.
[7] E. Even-Dar, S. Mannor, and Y. Mansour. Action
elimination and stopping conditions for the multiarmed bandit and reinforcement learning problems. J.
Mach. Learn. Res., 7:1079‚Äì1105, Dec. 2006.
[8] M. K. Ho, M. Littman, J. MacGlashan, F. Cushman,
J. Austerweil, and J. L. Austerweil. Showing versus
doing: Teaching by demonstration. In Advances In
Neural Information Processing Systems, pages 3027‚Äì
3035, 2016.
[9] T. L. Lai and H. Robbins. Asymptotically efficient
adaptive allocation rules. Advances in Applied Mathematics, 6(1):4 ‚Äì 22, 1985.
[10] A. Liang, X. Mu, and V. Syrgkanis. Optimal Learning
from Multiple Information Sources. 2017.
[11] A. Murata and T. Nakamura. Basic study on prevention of human error-how cognitive biases distort decision making and lead to crucial accidents. Advances
in Cross-Cultural Decision Making, 5:83, 2014.
[12] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, 2000.
Second ed., 2009.
[13] H. Robbins. Some aspects of the sequential design of
experiments. Bull. Amer. Math. Soc., 58(5):527‚Äì535,
09 1952.

[14] S. L. Scott. A modern bayesian look at the multiarmed bandit. Applied Stochastic Models in Business
and Industry, 26(6):639‚Äì658, 2010.
[15] R. Sen, K. Shanmugam, A. Dimakis, and S. Shakkottai. Identifying Best Interventions through Online Importance Sampling . International Conference on Machine Learning, page to appear, 2017.
[16] A. Tversky and D. Kahneman. Judgment under uncertainty: Heuristics and biases. In Utility, probability, and human decision making, pages 141‚Äì162.
Springer, 1975.
[17] J. Zhang and E. Bareinboim. Markov decision processes with unobserved confounders: A causal approach. Technical Report R-23, Purdue AI Lab, 2016.
[18] J. Zhang and E. Bareinboim. Transfer Learning in
Multi-Armed Bandits: A Causal Approach. International Joint Conference on Artificial Intelligence,
2017.

