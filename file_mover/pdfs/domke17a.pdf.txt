A Divergence Bound for Hybrids of MCMC and Variational Inference and an
Application to Langevin Dynamics and SGVI
Justin Domke 1

Abstract
Two popular classes of methods for approximate inference are Markov chain Monte Carlo
(MCMC) and variational inference. MCMC
tends to be accurate if run for a long enough time,
while variational inference tends to give better
approximations at shorter time horizons. However, the amount of time needed for MCMC to
exceed the performance of variational methods
can be quite high, motivating more fine-grained
tradeoffs. This paper derives a distribution over
variational parameters, designed to minimize a
bound on the divergence between the resulting
marginal distribution and the target, and gives an
example of how to sample from this distribution
in a way that interpolates between the behavior
of existing methods based on Langevin dynamics and stochastic gradient variational inference
(SGVI).

1. Introduction
One of the simplest Markov chain Monte Carlo (MCMC)
methods is Langevin dynamics (Grenander & Miller, 1994;
Robert & Casella, 2004, Sec. 7.8.5) where one repeats gradient steps with injected Gaussian noise. Namely, one iterates
p
✏
z
z + rz log p(z) + ✏⌘,
(1)
2
where ⌘ is sampled from a standard Gaussian distribution
and ✏ is a step-size that may decay over time. To get exact samples, a Metropolis-Hastings rejection step should
be used. However, as the step-size ✏ becomes smaller,
the acceptance probability goes to one, and so this can be
disregarded. In the Bayesian inference setting, z denotes
unknown parameters and p(z) represents a posterior over
1

College of Computing and Information Sciences, University of Massachusetts, Amherst, USA. Correspondence to: Justin
Domke <domke@cs.umass.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

them. Computing p(z) thus requires a full pass over the
dataset. The idea of Stochastic Gradient Langevin Dynamics (Welling & Teh, 2011) is to get an unbiased estimate
of the gradient of log p(z) by approximating the posterior
using a minibatch. Since the gradient term is multiplied by
2
✏, the
p variance of its error has order ✏ , while the variance
of ✏⌘ has order ✏. Thus, with a small step ✏, the injected
noise dominates, meaning one can might expect these to
have a stationary distribution close to the target p(z).
The idea of variational inference is to posit a simple family of distributions qw (z) and then optimize w to minimize
the KL-divergence from qw (z) to the target p(z). (In a
Bayesian setting, this is equivalent to maximizing the evidence lower bound.) If one were to use plain gradient ascent to perform this optimization, this leads to the iterate
of
w

✏
w + rw Eqw (Z) [log p(Z)
2

log qw (Z)],

(2)

where the gradient step ✏ is divided by two for convenience,
and may again decay over time. In practice, doing gradient
ascent like this has two difficulties. First, in the Bayesian
inference setting, computing log p(z) requires a full pass
over the data. This can be approximated without bias using
a minibatch. The second difficulty stems from the expectation of log p(z) with respect to qw (z) in Eq. 2. The gradient
of this expectation can be approximated in different ways
using samples from qw (Z) (Section 3.2). Since this gives
unbiased estimates of the gradient, stochastic optimization
methods, such as stochastic gradient descent, will converge
to a local optima when the step-size becomes small.
Intuitively, both MCMC and variational methods can be
thought of as trying to find a high-probability region of
log p(z), with different strategies to encourage entropy to
get coverage of the distribution. In MCMC, entropy is created by randomness in the Markov chain, while in variational methods the KL-divergence directly measures the
entropy of the variational distribution. It is natural to think
that hybrid methods might employ fractions of these two
strategies. This paper does so by defining a distribution
over the parameters w of the approximating family q(z|w).
This distribution q(w) is designed to minimize a bound on
the KL-divergence of the resulting marginal distribution

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

=0

= .25

= .5

= .75

=1

Figure 1. Inference on toy distributions. Colored contours show the target density p(z). Sampled weights w are drawn equally from
5 ⇥ 105 iterations and pictured as ellipsoids at one standard deviation. Fewer w are shown for smaller to avoid visual clutter.

q(z) to the target p(z). Intuitively, when defining q(w),
there are two ways to encourage entropy, namely to give
higher density to vectors w where q(Z|w) has high entropy
or to encourage entropy over w itself.
Given a distribution q(w, z) and a target distribution p(z),
this paper uses two bounds on the marginal divergence
Dtrue = KL (q(Z)kp(Z)) . First, one can upper-bound by
the conditional divergence D0 = KL (q(Z|W )kp(Z)).
Second, one can “adjoin” some distribution p(w|z) to
p(z), and bound Dtrue by the joint divergence D1 =
KL (q(W, Z)kp(W, Z)). This paper takes a convex combination of these two bounds, i.e. uses the family of bounds
D = (1
)D0 + D1 . For any between 0 and 1, the
distribution q(w) is derived that minimizes this bound for
fixed p and a fixed approximating family q(z|w) (Theorem
3). Then, one can sample from this distribution over w by
(stochastic) Langevin dynamics. This turns out to lead to
the iterate
⇣
✏
w
w + rw Eqw [log p(Z) + (
1) log qw (Z)]
2
⌘ p
+ log r(w) + ✏ ⌘, (3)
where r(w) is related to the adjoining distribution p(w|z)
and may be thought of as a “base measure” 1 over the space
of w. Of course, exactly computing this gradient is intractable in general, but the same strategies used for variational inference can be used to efficiently compute an un-

1
To see why r is necessary, consider a reparameterization of
w. In order for the marginal distribution q(z) to stay the same, the
density of the base measure must be transformed corresponding
to the reparameterization. Put another way, without r(w), the
results of inference would depend on the parameterization of w,
even with an arbitrarily small step-size.

biased estimate.
It can be shown that as ! 0, the solution concentrates
in the optima of the variational optimization, and thus the
sampling algorithm reduces to gradient ascent on the variational objective. Meanwhile, as will be discussed in Sections 2.3 and 5 it is best to make r a function of such that
w with high entropy have less density when approaches
1. When this is true, then as
! 1, only w indexing
highly concentrated distributions over z retain density, and
so sampling from q(w) is essentially just a reparameterization of sampling from p(z), and the algorithm reduces to
Langevin dynamics.
Thus, for the extreme cases of = 0 and = 1, this algorithm reduces to variational inference and Langevin dynamics, respectively. Informally, at these extremes, the algorithm is “fast but approximate” and “slow but accurate”.
In the intermediate range, the algorithm exhibits new behavior with a fine-grained trade-off between speed and accuracy. Thus, this approach gives a smooth continuum of
algorithms with different priorities of speed and accuracy.
1.1. Notation
This paper uses three notational conventions that are
not universal. First, the conditional probability q(z|w)
will alternatively be written as qw (z) when convenient.
Second, A ' B indicates that A and B have the same
expected value or (if A is a constant) that B is an unbiased
estimator of A. Finally, upper and lower-cases indicate
what terms are random variables in a KL-divergence. So,
KL (q(Z|w)kp(Z|w)) = Eq(Z|w) log (q(Z|w)/p(Z|w))
is the divergence between q(z|w) and p(z|w)
for a fixed w, while KL (q(Z|W )kp(Z|W )) =

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Eq(W,Z) log (q(Z|W )/p(Z|W )) is a standard conditional divergence.

2. Information Theoretic Results

2.2. Divergence Bound and its Minimizer
This paper is based on convex combination of D0 and D1 ,
parameterized by some 2 [0, 1], namely
D = (1

(7)

)D0 + D1.

This section derives a few results from an information theoretic viewpoint, without any particular regard for form of
the target distribution, or how one might sample from it.
Proofs are given in the appendix.

Since D0 and D1 are upper-bounds, so is D . The following theorem gives the distribution q(w) to minimize D .

2.1. Preliminaries

Theorem 3. For fixed values of and p(w|z), the distribution q(w) that minimizes D is

Let p(z) be the target distribution. For simplicity, z is
treated here as a continuous variable, although the results
in this section remain true if it is discrete. There is some
fixed set of conditional distributions q(z|w), which may
also be written as qw (z). In principle, one might like to
know how to set q(w) such that the resulting marginal distribution over z, is close to p(z), as measured by the KLdivergence,
Dtrue = KL (q(Z)kp(Z)) = Eq(Z) log

q(Z)
.
p(Z)

(4)

This quantity is difficult to control directly, since the
marginal q(z) typically cannot be evaluated in closed form.
One bound comes from the conditional KL-divergence, as
stated in the following Lemma.
Lemma 1. The divergence from q(z) to p(z) is
KL (q(Z)kp(Z)) = KL (q(Z|W )kp(Z)) Iq [W, Z],
|
{z
}
D0

(5)
where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is the conditional divergence and Iq = KL (q(Z, W )kq(Z)q(W )) denotes mutual information under q.
A second bound comes from adjoining some distribution
p(w|z) to p(z). Then, the following lemma is essentially
just a re-statement of the chain rule for the KL-divergence
(Cover & Thomas, 2006, Thm. 2.5.3).
Lemma 2. The KL-divergence between q(z) and p(z) can
be written as

q ⇤ (w) = exp s(w) A)
Z
A = log
exp s(w)
w

s(w) = log p(w)
1

D1

KL (q(W |Z)kp(W |Z)) .

(6)

Since the mutual information in Eq. 5 and the conditional
divergence on the right of Eq. 6 are both non-negative,
both D0 and D1 are upper-bounds on Dtrue . Note that if
p(w|z) = q(w), then D1 = D0 , and so Lemma 1 follows
from Lemma 2.

KL (q(Z|w)kp(Z|w))

1 KL (q(Z|w)kp(Z)) .

Moreover, at q ⇤ , the objective value is D⇤ =

A.

Since
A is an upper-bound on the KLdivergence, A must be non-positive.
This can
also be seen Rdirectly, since it can be written as
A
=
log w p(w) exp( KL (q(Z|w)kp(Z|w))
1
1 KL (q(Z|w)kp(Z))), and the inner divergences as well as 1 1 are non-negative.
To understand the connection of this bound to variational
inference and MCMC, one can look at behavior where =
0 or where = 1. For the former case, one obtains a result
closely related to the solution to the variational inference
problem.
Remark 4. In the limit where ! 0 the divergence bound
at the optimal q ⇤ becomes
lim D⇤ = inf KL (q(Z|w)kp(Z)) .
w

!0

(9)

Similarly, when ! 0, q ⇤ (w) concentrates at the minimizer(s) of this divergence. (Formally, with multiple global
optima with the same divergence, q(w) will concentrate
equally around all minimizers.)
For the case where

KL(q(Z)kp(Z))) = KL (q(W, Z)kp(W, Z))
|
{z
}

(8)

D1⇤ =

log

Z

= 1 , it is easy to see that

p(w) exp ( KL (q(Z|w)kp(Z|w))) ,
w

(10)
which will be zero if p(w) is only supported on w where the
divergence between q(z|w) and p(z|w) is zero. This may
initially seem like a strange condition, given that p(w) results from both the target distribution p(z) and the adjoined
distribution p(w|z). However, the next section gives more
specifics.

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

2.3. Specific Form for p(w|z)
The results in the previous section were written without
without considering the specific form of p(w|z). This paper
takes the approach of defining
p(w|z) =

r(w)q(z|w)
,
rz

(11)

where Rr(w) is a possibly improper distribution over w and
rz = w r(w)q(z|w) is a normalizer. Note that p(w|z) is
still well-defined as long as the integral defining rz exists.

Here, we assume for convenience that rz is a constant, not
depending on z. Enforcing rz to be constant essentially
means choosing r(w) in such a way that it doesn’t “favor”
any z over any other since if q(w) / r(w) then q(z) is
uniform over z.
Lemma 5. If p(w|z) = r(w)q(z|w)/rz and rz is a constant, then the solution in Thm. 3 holds with
s(w) = log r(w)
+ Eqw (Z) [

1

log rz
1

log p(Z) + (1

) log qw (Z)].

(12)

This gives a distribution over q(w) that can be written in
various equivalent ways, such as
q ⇤ (w) / r(w) exp

1

E(w) + (

=r(w) exp H(w)
=r(w) exp1/

1

1

(13)

KL (qw (Z)kp(Z))

E [log p(Z) + (

qw (Z)

1)H(w)

1) log qw (Z)] ,

where H(w) = Eqw (Z) [log qw (Z)] is the entropy of qw
and E(w) = Eqw (z) [log p(Z)].
Here, r(w) plays a role similar to a base density in an
exponential family. If one simply used r(w) = 1, then
p(w|z) may not be well-defined. Further, without a base
density, the marginal q(z) would depend on the particular
parameterization of w. To see this, take single parameter
w, and imagine re-parameterizing so the negative reals are
“squashed” by a factor of two. The base density must increase by a factor of two for the negative reals to compensate in order to leave the marginal q(z) unchanged under
the reparameterization.
2.4. Latent variables in variational inference
Adjoining p(w|z) to p(z) to define D1 above is strongly
related to auxiliary random variables (Agakov & Barber,
2004) explored in variational inference to increase the representative power of q, e.g. by including latent stochastic
transition operators (Salimans et al., 2015), random Gaussian process mappings (Tran et al., 2016), or hierarchical
variables (Ranganath et al., 2016). Imagine doing variational inference with q✓ (z, w), where now w is auxiliary

and the goal is to set ✓ so that q✓ (z) ⇡ p(z). Since w
must be integrated out, the entropy of z under q is typically intractable, but can be bounded by augmenting p with
some distribution p(w|z) and then optimizing the joint KLdivergence over z and w, similarly to Lemma 2. Note two
differences. First, here the distribution over w is mathematically derived to minimize the bound, while previous
work numerically optimizes ✓ at run-time. Second, previous work also optimizes parameters of the distribution
p(w|z) to further improve the bound, while here it is left
fixed (for each ) for simplicity (Sec. 5). Future work
might explore optimizing p(w|z) at run-time in the current
paper’s framework.

3. Bayesian Inference
This section considers algorithms to sample from the distribution defined by Eq. 13. Probabilistic inference can
be used in various settings, but for concreteness the rest of
this paper will focus on Bayesian inference. To fix notation,
suppose a set of N inputs xi with corresponding outputs y i .
(In a generative setting, xi would be empty.) Then, if z is
a vector of parameters, the posterior distribution over z is,
up to a constant
N
X
log p(z) = log p0 (z) +
log p(y i |xi , z) + C, (14)
i=1

where p0 (z) is the prior, and p(y i |xi , z) is the likelihood
for the i-th datum. The goal of probabilistic inference is to
be able to evaluate expectations with respect to p(z).
3.1. Langevin Dynamics
The goal of MCMC methods is to obtain samples from
the target distribution p(z). Langevin dynamics sample by
an extremely simple process of repeating gradient steps of
log(z) with injected Gaussian noise. Specifically, the iterate is
p
✏
z
z + r log p(z) + ✏⌘,
(15)
2
where ⌘ is sampled from a standard Multivariate Gaussian
distribution and ✏ is a step-size that may decay over time. If
Langevin dynamics are used as a proposal for a MetropolisHastings sampler, it can be shown that correct acceptance
ratio is
exp s(z 0 )

s(z) +
+

1
(z
2

✏
2
krs(z)k
8

✏
2
krs(z 0 )k
8

z 0 ) · (rs(z) + rs(z 0 )) ,

(16)

where s(z) = log p(z) up to a constant factor, and z 0 is
the proposed point from Eq. 15. It is easy to see that as ✏
becomes small, this acceptance ratio will go to one, and so
one can disregard the acceptance step with some bias in the
results determined by the step size.

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Langevin dynamics explore the space of z through a random walk, and thus are likely to be slower than alternatives
such as Hamiltonian Monte Carlo when used as an exact
method (Neal, 2010, Section 5.2). The main practical advantage of Langevin dynamics comes from the case where
the number of data N is large. In that case, one can use a
minibatch of M elements and approximate r log p(z) as
X
N
r log p(y i |xi , z).
r log p(z) ' r log p0 (z)+
M
i2minibatch
(17)
Thus, Stochastic Gradient Langevin Dynamics, avoid a full
pass through the dataset in each iteration, and so can scale
to large datasets. Though Eq. 17 is an unbiased estimate of
the gradient of log p(z) this still adds a bias to the stationary distribution of the the chain. (See (Mandt et al., 2016)
for an analysis.) In the limit of a small step-size, the variance of the noise due to stochastic estimation of the gradient of log p(z) will be of order ✏2 while the variance of the
injected noise is of order ✏, meaning the latter dominates
(Welling & Teh, 2011; Teh et al., 2016).
3.2. Stochastic Gradient Variational Inference

log qw (Z)],

L(w) = ER [log p(zR,w )] + H(w).

(20)

where H(w) = Eqw (Z) log qw (Z) denotes the entropy.
The advantage of Eq. 20 this is that the expectation is not
a function of w, and so if computing the gradient of L, the
gradient moves inside the expectation. Then, provided zr,w
is differentiable with respect to w, an unbiased estimate of
the gradient of L is available as
rw L(w) ' rw log p(zr,w ) + rw H(w).

(21)

If H(w) cannot be integrated in closed-form an alternative
estimator based on
L(w) = ER [log p(zR,w )

log qw (zR,w )]

(22)

is possible, and in some circumstances this can even have
lower variance (Roeder et al., 2016).

The goal of variational inference is to maximize
L(w) = Eqw (Z) [log p(Z)

2014; Rezende et al., 2014; Titsias & Lázaro-Gredilla,
2014), known in the stochastic approximation literature as
a “pathwise” derivative (Kushner & Yin, 2003, Sec. 2.5.1).
Suppose that there is a deterministic mapping zr,w from parameters w and random numbers r (from some fixed standard distribution) such that zr,w ⇠ qw (z). Then, we could
equivalently write

(18)

equivalent to minimizing the KL divergence between
qw (z) and p(z). If it were possible to exactly compute L,
this could be maximized (to a local optima) by a simple
gradient iteration like
✏
w
w + rw L(w).
(19)
2
While in some cases with specific p and q, one can derive exact updates of L (Ghahramani & Beal, 2000) in general one cannot exactly evaluate the expectation over Z in
L. One line of approach to this (Ranganath et al., 2014;
Salimans & Knowles, 2014) is to write the gradient as
rL = Eqw (Z) [(log qw (Z) log p(Z))r log qw (Z)], and
estimate this by drawing samples from qw (z). This experimentally seems to result in gradients with large variance, but this can be reduced by two strategies. Firstly, one
can use control variates, based on either Taylor expansion
(Paisley et al., 2012) or the fact that the expected value of
rw log qw (Z) is zero. Secondly, since log p(z) is often a
sum of terms defined on subsets of variables, one can RaoBlackwellize by integrating out other variables. This approach only needs to be able to compute log p(z)– no other
access, even to the gradient of p, is needed.
3.2.1. R EPARAMETERIZATION T RICK
Another line of approach to variational inference is based
on the “reparameterization trick” (Kingma & Welling,

Computing the gradient of log p(zr,w ) with respect to w
amounts to computing the derivative of log p(z) with reT
spect to z multiplied by the Jacobian dzr,w
/dw. For simple distributions like Gaussians zr,w this is easy to do analytically (Section 5). Alternatively, this can all be done
efficiently by automatic differentiation, (Kucukelbir et al.,
2017) the approach used here.
As with Langevin dynamics, with a large dataset, log p(z)
can be approximated by taking a sum over a minibatch. The
most obious approach to this would be to choose a single
vector r and use it throughout the minibatch. However,
variance can often be reduced by the “local reparameterization trick” (Kingma et al., 2015) in which a different
random vector ri is drawn for each datum in the minibatch.
Intuitively, the motivation is that if i and j are two data in
the minibatch then log p(y i |xi , zr,w ) should be less correlated with log p(y j |xj , zr0 ,w ) when a different random vector r0 is used for the second datum (consider the limiting
case where all data are identical). Then, one can write the
approximation as
rw L(w) '

1
M

X

i2minibatch

⇣

+ N rw log p(y i |xi , zri ,w )

rw log p0 (zri ,w )
⌘
rw log qw (zri ,w ) .

(23)

Note that the approximation on the right-hand side depends
both on the choice of the minibatch and on the random vectors ri , one chosen per element of the minibatch.

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

4. Hybrid Dynamics
An algorithm that interpolates between the methods in the
previous section results from applying Langevin dynamics
to the distribution over w defined by Thm. 3 If the adjoining distribution p(w|z) is chosen as in in Sec. 2.3 with
some r (w) that depends on , then define
L(w) =

log r (w)

+ Eqw (Z) [log p(z) + (

1) log q(z|w)],

(24)

which is times the form of s(w) derived in 5, with the
constant term of log rz dropped. Applying Langevin Dynamics2 to this results in the iteration
p
✏
w
w + rL(w) + ✏ ⌘,
(25)
2
where again ✏ is a step-size that may decrease over time and
⌘ is sampled from a standard Gaussian distribution.
This paper uses a closed-form for the entropy H(w) =
Eqw (Z) log qw (Z), and so uses the gradient estimator
rL(w) ' r log r (w) + (1

+ Eqw (Z) [log p(z) + (

)rH(w)
1) log q(z|w)].

(26)

5. Specifics For Gaussians
The following experiments use the simplest common variational distribution for qw (Z), namely a fully-factorized
Gaussian distribution. To make w unconstrained, let w =
(µ, ⌫) where µ is a vector of mean components and the
standard deviation of the i-th dimension is i = 10⌫i . To
sample from this distribution given a vector r, one can
simply take zr,w = µ + r
where is element-wise
product. P
The entropy of qw (Z) is, up to constant factors
H(w) = i ⌫i ln 10.

It remains to set the base density.
These experiments used
Q
an improper prior r (w) / i N (⌫i |u , 1) that is uniform
over µ with a Gaussian prior on ⌫ with a fixed variance of
1 and a mean u . Since this is uniform over µ, rz is a constant. The value u was calculated to numerically optimize
the divergence bound D⇤ for a one-dimensional standard
Gaussian p(z). Since D⇤ =
A at the solution (Theorem 3), for any given and u , D⇤ can be evaluated by
symbolically integrating out µ and then numerically integrating over ⌫. The values used in these experiments are
below, with linear interpolation used for any other .
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
u -.33 -.472 -.631 -.792 -.953 -1.11 -1.29 -1.49 -1.74 -2.10 -10
2
For some step-size ✏0 , the Langevin dynamics
would imp
mediately be w
w + (✏0 /2)rs(w) + ✏0 ⌘. If we define
✏ = ✏0 , then this results in the given form since ✏0 rs(w) =
(✏/ )rs(w) = ✏rL(w).

Figure 2. Performance of the hybrid algorithm with various values
of on the toy mixture of three Gaussians from Fig. 1.

Theoretically, it’s easy to show that as ! 1, D⇤ is minimized (with a minimum of zero) by u ! 1, meaning that after one iteration, ⌫ = 1, and henceforth,
zr,w = µ. Thus, Eq. 25 is equivalent to Langevin dynamics
as in Eq. 15. However, using u = 10, r(w) (a prior centered on a standard deviation of = 10 10 ) is practically
equivalent and is more useful to guide linear interpolation.
Meanwhile, when ! 0, the algorithm is independent of
r(w), though the optimal value of u ⇡ .33 is correct for
small .
The above choice of r (w) is quite arbitrary. It is possible that other choices could be better, though performance
was surprisingly insensitive in practice. With large datasets
log p(z) dominates log r(w) just as likelihoods dominate
Bayesian priors. Calling on inspiration from recent work
in variational inference, it might also be useful to optimize
p(w|z) at runtime (Section 2.4).

6. Experiments
There does not appear to be a single standard performance measure to evaluate approximate inference algorithms. For Bayesian inference, test-set accuracy or likelihood are sometimes used, but these measures have high
variance due to the dataset, and conflate evaluation of the
inference method with evaluation of the model it is running
on. (An inference method with poor coverage of p(z) can
have higher accuracy than a perfect sampler due to model
mis-specification or dataset peculiarities.)
For a more fine-grained measure of inference performance,
this paper uses the Maximum Mean Discrepancy (MMD)
measure (Gretton et al., 2006; 2012). MMD is essentially the empirical difference of the means of two samples in some feature space. We first draw a sample from
p(z) by exhaustively running a traditional MCMC algorithm (Stan (Team, 2016), based on a variant of Hamil-

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

= 0.0

= 0.2

= 0.4

= 0.6

= 0.8

= 1.0

Stan

Figure 3. Samples on ionosphere after 104 (top row), 105 (middle row) or 106 (bottom row) iterations. One sample z is drawn from
qw (z) at each iteration and projected onto the first two principal components (computed on Stan samples). Each plot uses the step-size
✏ with the lowest MMD at that time horizon. The same sequence of random numbers is for all inference methods. (More in appendix.)

tonian Monte Carlo (Hoffman & Gelman, 2014)) for a
large number of iterations, and compare this to the results
of approximate inference. Since we have large samples,
and want to compute the online performance at all iterations, a finite-dimensional feature space must be used to
avoid the quadratic complexity of computing MMD using an arbitrary kernel. For two-dimensional problems, it
was beneficial to use random Fourier features (Rahimi &
Recht, 2007) to approximate the radial-basis function kernel k(x, y) = exp( kx yk2 ). For higher dimensions,
the original feature space was sufficiently discriminative.
Approximate inference gives a sequence of vectors wt . For
each time-step, a set of 100 samples At are drawn from
q(z|wt ). Then, the MMD between A1 [A2 [· · · At and the
sample from exhaustive MCMC can be computed for all t
in linear time. A disadvantage of this approach is the need
for exhaustive MCMC for comparison, which by definition
eludes the large-scale cases that motivate Langevin dynamics and stochastic variational inference. However this gives
a more fine-grained view of performance.
6.1. Toy Distributions
For a first demonstration, the algorithm was applied to a
set of toy 2-dimensional distributions, with various values
of . A few results are shown in Fig. 1, with more in

the appendix. While the algorithm displays the expected
trade-offs between speed and accuracy for different (Fig.
2), the results fairly uninteresting as all curves cross near
the same time/MMD point where = 0 (variational inference) and = 1 (MCMC) meet. So, for any given amount
of time, either variational inference or MCMC are nearoptimal, and so intermediate do not much expand the
“Pareto frontier” on these problems, although one might
prefer an intermediate since the crossover horizon is not
known in advance.
6.2. Bayesian Logistic Regression
Next, the algorithm was applied to binary logistic regression with several standard datasets, using a minibatch size
of 25, a different random vector r for each element in the
minibatch, and a standard multivariate Laplace distribution
as a prior. Simply picking a single value or schedule for
the step ✏ is unsatisfactory, as the best schedule for a given
problem, length of time, and value of varies. To give
a fair comparison, inference was run for with a range of
constant ✏ varying by factors of two from 23 /N to 2 2 /N .
This spans the range from large enough to often diverge
to below optimal even after 106 iterations. Then, for each
time, the best value of ✏ was retrospectively chosen (with
performance averaged over repetitions before this choice).
It is likely that decaying step-size schedules would perform

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Figure 4. Iterations vs MMD for inference with various values of , averaged over five runs, each using the same random number
sequence for all . For each , iteration and dataset, the graphs show the MMD for the best step-size ✏ (averaged over the 100 runs
before computing the best). Since a finite range of ✏ varying by factors of two is used, this can result in visible “kinks” at iterations
where a different (smaller) ✏ exceeds the old one. From left to right: a1a, australian, ionosphere, and sonar.

better, but this was not pursued since it would reduce transparency of the results.
The mean errors after 100 repetitions are shown in Fig. 4,
while samples are compared to the ground truth for ionosphere in Fig. 3. This confirms the basic ideas of the algorithm, namely that it smoothly interpolates between the
two previous algorithms without introducing any new behavior. The main experimental finding is that there is usually a large range of time horizons for which an intermediate setting of performs better than either of the previous
existing algorithms. This suggests that, say, someone who
has a time budget slightly larger than that needed for variational inference could benefit from running the algorithm
with a small value of for a slightly larger time. This is in
contrast to the toy two-dimensional examples where for almost all time horizons either = 0 or = 1 was optimal.

7. Discussion
This paper has two contributions. First, a distribution over
variational parameters is derived to minimize a bound on
the marginal divergence to a target distribution. Secondly,
this is used to derive an algorithm that interpolates between Langevin dynamics and stochastic variational inference. The Bayesian logistic regression experiments show
that the intermediate values of this algorithm are useful in
the sense that there are several orders of magnitude of time
horizons where an intermediate algorithm performs better
than either Langevin dynamics or variational inference.
Many improvements to stochastic Langevin dynamics and
variational inference have been proposed beyond the simple algorithms used here. For Langevin dynamics, Welling
& Teh (2011) and Li et al. (2016) suggest adding a preconditioner, Patterson & Teh (2013) propose Riemannian
Langevin Dynamics, and Dubey et al. (2016) propose to
make use of the finite sum structure in a Bayesian posterior
p(z) by incorporating techniques for incremental optimization. For stochastic variational inference, Hoffman et al.

(2013) propose to use the natural gradient, Mandt & Blei
(2014) propose using smoothed gradients, and Sakaya &
Klami (2016) propose a strategy of re-using previous gradient computations. It would be interesting to consider using
these ideas with the hybrid algorithm, which could give insight into the relationship between the two different methods.
There is other work based on combining the advantages
of variational inference and MCMC. Stochastic Gradient Fisher Scoring (Ahn et al., 2012) interpolates between Langevin dynamics and a Gaussian approximation
of the target. However, this approximation is based on the
Bayesian central limit theorem and so will not be the same
in general as the optimal variational distribution, even when
a Gaussian is used as the variational family. Another line of
research is that of interpreting the path of a MCMC algorithm as a variational distribution, and then fitting parameters to tighten a variational bound (Salimans et al., 2015;
Rezende & Mohammed, 2015). While motivationally quite
similar, the contribution of this paper is orthogonal, in that
one could conceivably use these same ideas to define the
variational family qw (z) used in this paper. This would result in a somewhat unusual method that runs MCMC over
the parameters of variational family of distributions that are
themselves defined in terms of a (different) MCMC algorithm.

References
Agakov, Felix V. and Barber, David. An auxiliary variational method. In NIPS, 2004. 2.4
Ahn, Sungjin, Balan, Anoop Korattikara, and Welling,
Max. Bayesian posterior sampling via stochastic gradient fisher scoring. In ICML, 2012. 7
Cover, Thomas M. and Thomas, Joy A. Elements of Information Theory. Wiley-Interscience, 2006. 2.1, 8
Dubey, Kumar Avinava, Reddi, Sashank J., Williamson,

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Sinead A., Póczos, Barnabás, Smola, Alexander J., and
Xing, Eric P. Variance reduction in stochastic gradient
langevin dynamics. In NIPS, 2016. 7

Neal, Radford. MCMC using Hamiltonian dynamics, volume Handbook of Markov Chain Monte Carlo, pp. 113–
162. Chapman & Hall / CRC Press, 2010. 3.1

Ghahramani, Zoubin and Beal, Matthew J. Propagation
algorithms for variational bayesian learning. In NIPS,
2000. 3.2

Paisley, John William, Blei, David M., and Jordan,
Michael I. Variational bayesian inference with stochastic
search. In ICML, 2012. 3.2

Grenander, Ulf and Miller, Michael. Representations of
knowledge in complex systems. J. R. Statist. Soc., 56
(4):549–603, 1994. 1

Patterson, Sam and Teh, Yee Whye. Stochastic gradient riemannian langevin dynamics on the probability simplex.
In NIPS, 2013. 7

Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Schölkopf, Bernhard, and Smola, Alexander J. A kernel
method for the two-sample-problem. In NIPS, 2006. 6

Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In NIPS, 2007. 6

Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Schölkopf, Bernhard, and Smola, Alexander J. A kernel
two-sample test. Journal of Machine Learning Research,
13:723–773, 2012. 6
Hoffman, Matthew D. and Gelman, Andrew. The no-u-turn
sampler: adaptively setting path lengths in hamiltonian
monte carlo. Journal of Machine Learning Research, 15
(1):1593–1623, 2014. 6
Hoffman, Matthew D., Blei, David M., Wang, Chong,
and Paisley, John William. Stochastic variational inference. Journal of Machine Learning Research, 14(1):
1303–1347, 2013. 7
Kingma, Diederik P and Welling, Max. Stochastic gradient
vb and the variational auto-encoder. In ICLR, 2014. 3.2.1
Kingma, Diederik P., Salimans, Tim, and Welling, Max.
Variational dropout and the local reparameterization
trick. In NIPS, 2015. 3.2.1
Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, and Blei, David M. Automatic differentiation variational inference. Journal of Machine Learning
Research, 18(14):1–45, 2017. 3.2.1

Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.
Black box variational inference. In AISTATS, 2014. 3.2
Ranganath, Rajesh, Tran, Dustin, and Blei, David M. Hierarchical variational models. In ICML, 2016. 2.4
Rezende, Danilo Jimenez and Mohammed, Shakir. Variational inference with normalizing flows. In ICML, 2015.
7
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and variational inference in deep latent gaussian models. In International
Conference on Machine Learning, 2014. 3.2.1
Robert, Christian and Casella, George. Monte Carlo Statistical Methods. Springer-Verlag, 2 edition, 2004. 1
Roeder, Geoffrey, Wu, Yuhuai, and Duvenaud, David.
Sticking the landing: A simple reduced-variance gradient for advi. NIPS workshop in Advances in Approximate Bayesian Inference, 2016. 3.2.1
Sakaya, Joseph and Klami, Arto. Re-using gradient computations in automatic variational inference. Technical
report, 2016. NIPS workshop on approximate Bayesian
Inference. 7

Kushner, Harold and Yin, George. Stochastic Approximation and Recursive Algorithms and Applications.
Springer-Verlag, 2003. 3.2.1

Salimans, Tim and Knowles, David A. On using control
variates with stochastic approximation for variational
bayes and its connection to stochastic linear regression.
In ICLR, 2014. 3.2

Li, Chunyuan, Chen, Changyou, Carlson, David E., and
Carin, Lawrence. Preconditioned stochastic gradient
langevin dynamics for deep neural networks. In AAAI,
2016. 7

Salimans, Tim, Kingma, Diedrik, and Welling, Max.
Markov chain monte carlo and variational inference:
Bridging the gap. In ICML, 2015. 2.4, 7

Mandt, Stephan and Blei, David M. Smoothed gradients
for stochastic variational inference. In NIPS, 2014. 7
Mandt, Stephan, Hoffman, Matthew D., and Blei, David M.
A variational analysis of stochastic gradient algorithms.
In ICML, 2016. 3.1

Team, Stan Development. Stan Modeling Language Users
Guide and Reference Manual, 2.14.0. edition, 2016. 6
Teh, Yee Whye, Thiery, Alexandre H., and Vollmer, Sebastian J. Consistency and fluctuations for stochastic gradient langevin dynamics. J. Mach. Learn. Res., 17(1):
193–225, 2016. 3.1

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Titsias, Michalis K. and Lázaro-Gredilla, Miguel. Doubly
stochastic variational bayes for non-conjugate inference.
In ICML, 2014. 3.2.1
Tran, Dustin, Ranganath, Rajesh, and Blei, David M. The
variational gaussian process. In ICLR, 2016. 2.4
Welling, Max and Teh, Yee Whye. Bayesian learning via
stochastic gradient langevin dynamics. In ICML, 2011.
1, 3.1, 7

