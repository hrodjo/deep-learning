Dropout Inference in Bayesian Neural Networks with Alpha-divergences

Yingzhen Li 1 Yarin Gal 1 2

Abstract
To obtain uncertainty estimates with real-world
Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used
for machine vision and medical applications,
but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI‚Äôs KL objective, which are able
to avoid VI‚Äôs uncertainty underestimation. But
these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be
changed radically, thus are of limited use for
practitioners. We propose a re-parametrisation
of the alpha-divergence objectives, deriving a
simple inference technique which, together with
dropout, can be easily implemented with existing models by simply changing the loss of the
model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout
networks. We study our model‚Äôs epistemic uncertainty far away from the data using adversarial
images, showing that these can be distinguished
from non-adversarial images by examining our
model‚Äôs uncertainty.

1. Introduction
Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012;
Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer
et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models
(Sennrich et al., 2016). But unlike deep learning models,
Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing
the models‚Äô ignorance about the world, and able to convey
their increased uncertainty on out-of-data examples. This
1
University of Cambridge, UK 2 The Alan Turing Institute,
UK. Correspondence to: Yingzhen Li <yl494@cam.ac.uk>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

information can be used, for example, to identify when a vision model is given an adversarial image (studied below),
or to tackle many problems in AI safety (Amodei et al.,
2016). With model uncertainty at hand, applications as farreaching as safety in self-driving cars can be explored, using models which can propagate their uncertainty up the
decision making pipeline (Gal, 2016). With deterministic
deep learning models this invaluable uncertainty information is often lost.
Bayesian deep learning ‚Äì an approach to combining
Bayesian probability theory together with deep learning ‚Äì
allows us to use state-of-the-art models and at the same
time obtain model uncertainty (Gal, 2016; Gal & Ghahramani, 2016a). Originating in the 90s (Neal, 1995; MacKay,
1992; Denker & LeCun, 1991), Bayesian neural networks
(BNNs) in particular have started gaining in popularity
again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015). BNNs are standard neural networks (NNs) with prior probability distributions placed
over their weights. Given observed data, inference is
then performed to find what are the more likely and less
likely weights to explain the data. But as easy it is to
formulate BNNs, is as difficult to perform inference in
them. Many approximations have been proposed over the
years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011;
Blundell et al., 2015; Hernandez-Lobato & Adams, 2015;
HernaÃÅndez-Lobato et al., 2016), some more practical and
some less practical. A practical approximation for inference in Bayesian neural networks should be able to scale
well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985;
LeCun et al., 1989)). Much more importantly perhaps, it
would be impractical to change existing model architectures that have been well studied, and it is often impractical
to work with complex and cumbersome techniques which
are difficult to explain to non-experts. Many existing approaches to obtain model confidence often do not scale to
complex models or large amounts of data, and require us to
develop new models for existing tasks for which we already
have well performing tools (Gal, 2016).
One possible solution for practical inference in BNNs is
variational inference (VI) (Jordan et al., 1999), a ubiquitous
technique for approximate inference. Dropout variational
distributions in particular (a mixture of two Gaussians with

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

small standard deviations, and with one component fixed at
zero) can be used to obtain a practical inference technique
(Gal & Ghahramani, 2016b). These have been used for machine vision and medical applications (Kendall & Cipolla,
2016; Kendall et al., 2015; Angermueller & Stegle, 2015;
Yang et al., 2016). Dropout variational inference can be
implemented by adding dropout layers (Hinton et al., 2012;
Srivastava et al., 2014) before every weight layer in the NN
model. Inference is then carried out by Monte Carlo (MC)
integration over the variational distribution, in practice implemented by simulating stochastic forward passes through
the model at test time (referred to as MC dropout). Although dropout VI is a practical technique for approximate
inference, it also has some major limitations. Dropout VI
can severely underestimate model uncertainty (Gal, 2016,
Section 3.3.2) ‚Äì a property many VI methods share (Turner
& Sahani, 2011). This can lead to devastating results in applications that must rely on good uncertainty estimates such
as AI safety applications.
Alternative objectives to VI‚Äôs objective are therefore needed.
Black-box Œ±-divergence minimisation
(HernaÃÅndez-Lobato et al., 2016; Li & Turner, 2016; Minka,
2005) is a class of approximate inference methods extending on VI, approximating EP‚Äôs energy function (Minka,
2001) as well as the Hellinger distance (Hellinger, 1909).
These were proposed as a solution to some of the difficulties encountered with VI. However, the main difficulty
with Œ±-divergences is that the divergences are hard to use in
practice. Existing inference techniques only use Gaussian
approximating distributions, with the density over the approximation having to be evaluated explicitly many times.
The objective offers a limited intuitive interpretation which
is difficult to explain to non-experts, and of limited use
for engineers (Gal, 2016, Section 2.2.2). Perhaps more
importantly, current Œ±-divergence inference techniques require existing models and code-bases to be changed radically to perform inference in the Bayesian counterpart to
these models. To implement a complex CNN structure with
the inference and code of (HernaÃÅndez-Lobato et al., 2016),
for example, one would be required to re-implement many
already-implemented software tools.
In this paper we propose a re-parametrisation of the induced Œ±-divergence objectives, and by relying on some
mild assumptions (justified below), derive a simple approximate inference technique which can easily be implemented with existing models. Further, we rely on the
dropout approximate variational distribution and demonstrate how inference can be done in a practical way ‚Äì requiring us to only change the loss of the NN, L(Œ∏), and
to perform multiple stochastic forward passes at training
time. Precisely, given l(¬∑, ¬∑) some standard NN loss such
as cross entropy or the Euclidean loss, and {f œâb k (xn )}K
k=1
a set of K stochastic dropout network outputs on input xn

b k , our proposed objective
with randomly masked weights œâ
is:
h
i
1X
log-sum-exp ‚àíŒ±l(yn , f œâb k (xn )) + L2 (Œ∏)
L(Œ∏) = ‚àí
Œ± n
with Œ± a real number, Œ∏ the set of network weights to
be optimised, and an L2 regulariser over Œ∏. By selecting
Œ± = 1 this objective directly optimises the per-point predictive log-likelihood, while picking Œ± ‚Üí 0 would focus
on increasing the training accuracy, recovering VI.
Specific choices of Œ± will result in improved uncertainty estimates (and accuracy) compared to VI in dropout BNNs,
without slowing convergence time. We demonstrate this
through a myriad of applications, including an assessment
of fully connected NNs in regression and classification, and
an assessment of Bayesian CNNs. Finally, we study the
uncertainty estimates resulting from our approximate inference technique. We show that our models‚Äô uncertainty
increases on adversarial images generated from the MNIST
dataset, suggesting that these lie outside of the training data
distribution. This in practice allows us to tell-apart such
adversarial images from non-adversarial images by examining epistemic model uncertainty.

2. Background
We review background in Bayesian neural networks and
approximate variational inference. In the next section we
discuss Œ±-divergences.
2.1. Bayesian Neural Networks
Given training inputs X = {x1 , . . . , xN } and their corresponding outputs Y = {y1 , . . . , yN }, in parametric
Bayesian regression we would like to infer a distribution
over parameters œâ of a function y = f œâ (x) that could have
generated the outputs. Following the Bayesian approach, to
find parameters that could have generated our data, we put
some prior distribution over the space of parameters p0 (œâ).
This distribution captures our prior belief as to which parameters are likely to have generated our outputs before
observing any data. We further need to define a probability distribution over the outputs given the inputs p(y|x, œâ).
For classification tasks we assume a softmax likelihood,

p y|x, œâ = Softmax (f œâ (x))
or a Gaussian likelihood for regression. Given a dataset
X, Y, we then look for the posterior distribution over the
space of parameters: p(œâ|X, Y). This distribution captures
how likely the function parameters are, given our observed
data. With it we can predict an output for a new input point
x‚àó by integrating
Z
p(y‚àó |x‚àó , X, Y) = p(y‚àó |x‚àó , œâ)p(œâ|X, Y)dœâ. (1)

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

One way to define a distribution over a parametric set of
functions is to place a prior distribution over a neural network‚Äôs weights œâ = {Wi }L
i=1 , resulting in a Bayesian NN
(MacKay, 1992; Neal, 1995). Given weight matrices Wi
and bias vectors bi for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices,
p0 (Wi ) = N (Wi ; 0, I) and often assume a point estimate
for the bias vectors for simplicity.
2.2. Approximate Variational Inference in Bayesian
Neural Networks

and the mean of one of the Gaussians fixed at zero. The uncertainty in the weights induces prediction uncertainty by
marginalising over the approximate posterior using Monte
Carlo integration:
Z
p(y = c|x, X, Y) = p(y = c|x, œâ)p(œâ|X, Y)dœâ
Z
‚âà p(y = c|x, œâ)qŒ∏ (œâ)dœâ
‚âà

K
1 X
b k)
p(y = c|x, œâ
K
k=1

In approximate inference, we are interested in finding the
distribution of weight matrices (parametrising our functions) that have generated our data. This is the posterior
over the weights given our observables X, Y: p(œâ|X, Y),
which is not tractable in general. Existing approaches to
approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber
& Bishop (1998); Graves (2011); Blundell et al. (2015)).
We need to define an approximating variational distribution
qŒ∏ (œâ) (parametrised by variational parameters Œ∏), and then
minimise w.r.t. Œ∏ the KL divergence (Kullback & Leibler,
1951; Kullback, 1959) between the approximating distribution and the full posterior:

KL qŒ∏ (œâ)||p(œâ|X, Y)
Z
‚àù ‚àí qŒ∏ (œâ) log p(Y|X, œâ)dœâ + KL(qŒ∏ (œâ)||p0 (œâ))
=‚àí

N Z
X

qŒ∏ (œâ) log p(yi |f œâ (xi ))dœâ + KL(qŒ∏ (œâ)||p0 (œâ)),

i=1

where A ‚àù B is slightly abused here to denote equality up
to an additive constant (w.r.t. variational parameters Œ∏).
2.3. Dropout Approximate Inference
Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton
et al., 2012; Srivastava et al., 2014)) can be interpreted
as variational Bayesian approximations in a Bayesian NN
with the same network structure (Gal & Ghahramani,
2016b). This is because applying a stochastic regularisation technique is equivalent to multiplying the NN weight
matrices Mi by some random noise i (with a new noise
realisation for each data point). The resulting stochastic
weight matrices Wi = i Mi can be seen as draws from the
approximate posterior over the BNN weights, replacing the
deterministic NN‚Äôs weight matrices Mi . Our set of variational parameters is then the set of matrices Œ∏ = {Mi }L
i=1 .
For example, dropout can be seen as an approximation to
Bayesian NN inference with dropout approximating distributions, where the rows of the matrices Wi distribute according to a mixture of two Gaussians with small variances

b k ‚àº qŒ∏ (œâ), where qŒ∏ (œâ) is the Dropout distribuwith œâ
tion (Gal, 2016). Given its popularity, we concentrate on
the dropout stochastic regularisation technique throughout
the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise (Srivastava et al., 2014) or dropConnect
(Wan et al., 2013)).
Dropout VI is an example of practical approximate inference, but it also underestimates model uncertainty (Gal,
2016, Section 3.3.2). This is because minimising the KL divergence between q(œâ) and p(œâ|X, Y) penalises q(œâ) for
placing probability mass where p(œâ|X, Y) has no mass,
but does not penalise q(œâ) for not placing probability mass
at locations where p(œâ|X, Y) does have mass. We next
discuss Œ±-divergences as an alternative to the VI objective.

3. Black-box Œ±-divergence minimisation
In this section we provide a brief review of the black box alpha (BB-Œ±, HernaÃÅndez-Lobato et al. (2016)) method upon
which the main derivation in this paper is based. Consider
approximating the following distribution:
p(œâ) =

Y
1
p0 (œâ)
fn (œâ).
Z
n

In Bayesian neural networks context, these factors fn (œâ)
represent the likelihood terms p(yn |xn , œâ), Z = p(Y|X),
and the approximation target p(œâ) is the exact posterior
p(œâ|X, Y). Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and
expectation propagation (EP) (Minka, 2001), where these
two algorithms are special cases of power EP (Minka,
2004) that minimises Amari‚Äôs Œ±-divergence (Amari, 1985)
DŒ± [p||q] in a local way:


Z
1
1 ‚àí p(œâ)Œ± q(œâ)1‚àíŒ± dœâ .
DŒ± [p||q] =
Œ±(1 ‚àí Œ±)
We provide details of Œ±-divergences and local approximation methods in the appendix, and in the rest of the paper
we consider three special cases in this rich family:

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

1. Exclusive KL divergence:


q(œâ)
D0 [p||q] = KL[q||p] = Eq log
;
p(œâ)
2. Hellinger distance:
D0.5 [p||q] = 4Hel2 [q||p] = 2

Z p

p(œâ) ‚àí

p

2
q(œâ) dœâ;

3. Inclusive KL divergence:


p(œâ)
.
D1 [p||q] = KL[p||q] = Ep log
q(œâ)
Since Œ± = 0 is used in VI and Œ± = 1.0 is used in EP, in
later sections we will also refer to these alpha settings as
the VI value, Hellinger value, and EP value, respectively.
Power-EP, though providing a generic variational framework, does not scale with big data. It maintains approximating factors attached to every likelihood term fn (œâ),
resulting in space complexity O(N ) for the posterior approximation which is clearly undesirable. The recently proposed stochastic EP (Li et al., 2015) and BB-Œ± (HernaÃÅndezLobato et al., 2016) inference methods reduce this memory
overhead to O(1) by sharing these approximating factors.
Moreover, optimisation in BB-Œ± is done by descending the
so called BB-Œ± energy function, where Monte Carlo (MC)
methods and automatic differentiation are also deployed to
allow fast prototyping.
BB-Œ± has been successfully applied to Bayesian neural
networks for regression, classification (HernaÃÅndez-Lobato
et al., 2016) and model-based reinforcement learning (Depeweg et al., 2016). They all found that using Œ± 6= 0 often
returns better approximations than the VI case. The reasons for the worse results of VI are two fold. From the
perspective of inference, due to the zero-forcing behaviour
of exclusive KL discussed before, VI often fits to a local
mode of the exact posterior and is over-confident in prediction. On hyper-parameter learning point of view, as the
variational lower-bound is used as a (biased) approximation
to the maximum likelihood objective, the learned model
could be biased towards over-simplified cases (Turner &
Sahani, 2011). These problems could potentially be addressed by using Œ±-divergences. For example, inclusive
KL encourages the coverage of the support set (referred
as mass-covering), and when used in local divergence minimisation (Minka, 2005), it can fit an approximation to a
mode of p(œâ) with better estimates of uncertainty. Moreover the BB-Œ± energy provides a better approximation to
the marginal likelihood as well, meaning that the learned
model will be less biased and thus fitting the data distribution better (Li & Turner, 2016). Hellinger distance seems
to provide a good balance between zero-forcing and masscovering, and empirically it has been found to achieve the
best performance.

Given the success of Œ±-divergence methods, it is a natural
idea to extend these algorithms to other classes of approximations such as dropout. However this task is non-trivial.
First, the original formulation of BB-Œ± energy is an ad hoc
adaptation of power-EP energy (see appendix), which applies to exponential family q distributions only. Second,
the energy function offers a limited intuitive interpretation
to non-experts, thus of limited use for practitioners. Third
and most importantly, a naive implementation of BB-Œ± using dropout would bring in a prohibitive computational burden. To see this, we first review the BB-Œ± energy function
in the general case (Li & Turner, 2016) given Œ± 6= 0:
"
!Œ± #
1
fn (œâ)p0 (œâ) N
1X
log Eq
. (2)
LŒ± (q) = ‚àí
1
Œ± n
q(œâ) N
One could verify that this is the same energy function as
presented in (HernaÃÅndez-Lobato et al., 2016) by considering q an exponential family distribution. In practice (2)
might be intractable, hence an MC approximation is introduced:
"
!Œ± #
1
1 X
1X
fn (b
œâ k )p0 (b
œâk ) N
MC
log
LŒ± (q) = ‚àí
1
Œ± n
K
q(b
œâk ) N
k
(3)
b k ‚àº q(œâ). This is a biased approximation as the
with œâ
expectation in (2) is computed before taking the logarithm.
But empirically HernaÃÅndez-Lobato et al. (2016) showed
that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that
the effect of the bias is negligible. When Œ± ‚Üí 0 it returns
the variational free energy (the VI objective)
X
L0 (q) = LVFE (q) = KL[q||p0 ] ‚àí
Eq [log fn (œâ)] , (4)
n

and the corresponding MC approximation LMC
VFE becomes
‚Üí
LMC
an unbiased estimator of LVFE . Also LMC
Œ±
VFE as the
number of samples K ‚Üí 1.
The original paper (HernaÃÅndez-Lobato et al., 2016) proposed a naive implementation which directly evaluates the
b k ‚àº q(œâ). However
MC estimation (3) with samples œâ
as discussed before, dropout implicitly samples different
b ‚àº q for different data points.
masked weight matrices œâ
This indicates that the naive approach, when applied to
dropout approximation, would gather all these samples for
all M datapoints in a mini-batch (i.e. M K sets of neural
network weight matrices in total), which brings prohibitive
cost if the network is wide and deep. Interestingly, the minimisation of the variational free energy (Œ± = 0) with the
dropout approximation can be computed very efficiently.
The main reason for this success is due to the additive structure of the variational free energy: no evaluation of q density is required if the ‚Äúregulariser‚Äù KL[q||p0 ] can be computed/approximated efficiently. In the following section we

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

propose an improved version of BB-Œ± energy to allow applications with dropout and other flexible approximation
structures.

4. A New Reparameterisation of BB-Œ± Energy
We propose a reparamterisation of the BB-Œ± energy to reduce the computational overhead. First we denote qÃÉ(œâ) as
a free-form ‚Äúcavity distribution‚Äù (see appendix), and write
the approximate posterior q as

 Œ±
qÃÉ(œâ) N ‚àíŒ±
1
,
(5)
qÃÉ(œâ)
q(œâ) =
Zq
p0 (œâ)
where we assume Zq < +‚àû is the normalising constant
to ensure q a valid distribution. When Œ±/N ‚Üí 0, the unnormalised density in (5) converges to qÃÉ(œâ) for every œâ,
and Zq ‚Üí 1 by the assumption of Zq < +‚àû (Van Erven & HarremoeÃàs, 2014). Hence q ‚Üí qÃÉ when Œ±/N ‚Üí 0,
and this happens for example when we choose Œ± ‚Üí 0, or
N ‚Üí +‚àû as well as when Œ± grows sub-linearly to N .
Now we rewrite the BB-alpha energy in terms of qÃÉ:

 Œ± !1‚àí NŒ±
Z
1
qÃÉ(œâ) N ‚àíŒ±
1X
log
qÃÉ(œâ)
LŒ± (q) = ‚àí
Œ± n
Zq
p0 (œâ)
Œ±

p0 (œâ) N fn (œâ)Œ± dœâ


1
Œ±
Œ±
=‚àí
qÃÉ(œâ)fn (œâ) ‚àí 1 ‚àí
log Zq
Œ± n
N

 Œ±
Z
Œ±
qÃÉ(œâ) N ‚àíŒ±
N
1‚àí
log qÃÉ(œâ)
dœâ
=
Œ±
N
p0 (œâ)
1X
‚àí
log EqÃÉ [fn (œâ)Œ± ]
Œ± n
1X
N
= RŒ≤ [qÃÉ||p0 ] ‚àí
log EqÃÉ [fn (œâ)Œ± ] , Œ≤ =
,
Œ± n
N ‚àíŒ±
X Z

where RŒ≤ [qÃÉ||p0 ] represents the ReÃÅnyi divergence (ReÃÅnyi
(1961), see appendix) of order Œ≤. Furthermore, provided
RŒ≤ [qÃÉ||p0 ] < +‚àû (which holds when assuming Zq <
+‚àû), we have RŒ≤ [qÃÉ||p0 ] ‚Üí KL[qÃÉ||p0 ] = KL[q||p0 ] as
Œ±
N ‚Üí 0. This means that for a constant Œ± that scales sublinearly with N , in large data settings we can further approximate the BB-Œ± energy as
1X
log Eq [fn (œâ)Œ± ] .
LŒ± (q) ‚âà LÃÉŒ± (q) = KL[q||p0 ] ‚àí
Œ± n
Note that here we also use the fact that now q ‚âà qÃÉ. Critically, the proposed reparameterisation is continuous in Œ±,
and by taking Œ± ‚Üí 0 the variational free-energy (4) is recovered.
Given a loss function l(¬∑, ¬∑), e.g. l2 loss in regression or
cross entropy in classification, we can define the (unnormalised) likelihood term fn (œâ) ‚àù p(yn |xn , œâ) ‚àù

exp[‚àíl(yn , f œâ (xn ))], e.g. see (LeCun et al., 2006)1 .
Swapping fn (œâ) for this last expression, and approximating the expectation over q using Monte Carlo sampling, we
obtain our proposed minimisation objective:
LÃÉMC
(6)
Œ± (q) = KL[q||p0 ] + const
X
1
log-sum-exp[‚àíŒ±l(yn , f œâb k (xn ))]
‚àí
Œ± n
with log-sum-exp being the log-sum-exp operator over K
b k ‚àº q(œâ). This
samples from the approximate posterior œâ
objective function also approximates the marginal likelihood. Therefore, compared to the original formulation (2),
the improved version (6) is considerably simpler (both to
implement and to understand), has a similar form to standard objective functions used in deep learning research, yet
remains an approximate Bayesian inference algorithm.
To gain some intuitive understanding of this objective, we
observe what it reduces to for different Œ± and K settings.
By selecting Œ± = 1 the per-point predictive log-likelihood
log Eq [p(yn |xn , œâ)] is directly optimised. On the other
hand, picking the VI value (Œ± ‚Üí 0) would focus on increasing the training accuracy Eq [log p(yn |xn , œâ)]. The
Hellinger value could be used to achieve a balance between
reducing training error and improving predictive likelihood, which has been found to be desirable (HernaÃÅndezLobato et al., 2016; Depeweg et al., 2016). Lastly, for
K = 1 the log-sum-exp disappears, the Œ±‚Äôs cancel out, and
the original (stochastic) VI objective is recovered.
In summary, our proposal modifies the loss function by
multiplying it by Œ± and then performing log-sum-exp with
a sum over multiple stochastic forward passes sampled
from the BNN approximate posterior. The remaining KLdivergence term (between q and the prior p) can often be
approximated. It can be viewed as a regulariser added to
the objective function, and reduces to L2 -norm regulariser
for certain popular q choices (Gal, 2016).
4.1. Dropout BB-Œ±
We now provide a concrete example where the approximate
distribution is defined by dropout. With dropout VI, MC
samples are used to approximate the expectation w.r.t. q,
which in practice is implemented as performing stochastic
forward passes through the dropout network ‚Äì i.e. given an
input x, the input is fed through the network and a new
dropout mask is sampled and applied at each dropout layer.
This gives a stochastic output ‚Äì a sample from the dropout
network on the input x. A similar approximation is used
in our case as well, where to implement the MC sampling
in eq. (6) we perform multiple stochastic forward passes
1
fn (œâ) does not need to be a normalised density of yn unless
one would like to optimise the associated hyper parameters.

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

through the network.
Recall the neural network f œâ (x) is parameterised by the
variable œâ. In classification, cross entropy is often used as
the loss function l(y, f œâ (x)) = ‚àíyT log pœâ (x), where the
label yn is a one-hot binary vector, and the network output
pœâ (xn ) = Softmax(f œâ (xn )) encodes the probability vector of class assignments. Applying the re-formulated BB-Œ±
energy (6) with a Bayesian equivalent of the network, we
arrive at the objective function

def softmax_cross_ent_with_mc_logits(alpha):
def loss(y_true, mc_logits):
# mc_logits: MC samples of shape MxKxD
mc_log_softmax = mc_logits \
- K.max(mc_logits, axis=2, keepdims=True)
mc_log_softmax = mc_log_softmax - \
logsumexp(mc_log_softmax, 2)
mc_ll = K.sum(y_true*mc_log_softmax,-1)
return -1./alpha * (logsumexp(alpha * \
mc_ll, 1) + K.log(1.0 / K_mc))
return loss

Figure 1. Code snippet for our induced classification loss.
1 X œâb k
1X T
yn log
(p (xn ))Œ±
Œ± n
K
i
k
!
5.1. Regression
X
X
X
1
1
bk
œâ
Œ±
=
p (xn )
+
L2 (Mi ) The first experiment considers Bayesian neural network rel yn ,
Œ± n
K
i
k
gression with approximate posterior induced by dropout.
We use benchmark UCI datasets2 that have been tested
with {pœâb k (xn )}K
k=1 being K stochastic network outputs
in related literature. The model is a single-layer neuon input xn , pi equals to one minus the dropout rate of
ral network with 50 ReLU units for all datasets except
the ith layer, and the L2 regularization terms coming from
for Protein and Year, which use 100 units. We consider
an approximation to the KL-divergence (Gal, 2016). I.e.
Œ± ‚àà {0.0, 0.5, 1.0} in order to examine the effect of masswe raise network probability outputs to the power Œ± and
covering/zero-forcing behaviour in dropout. MC approxiaverage them as an input to the standard cross entropy loss.
mation with K = 10 samples is also deployed to compute
Taking Œ± 6= 1 can be viewed as training the neural network
the energy function. Other initialisation settings are largely
with an adjusted ‚Äúpower‚Äù loss, regularized by an L2 norm.
taken from (Li & Turner, 2016).
Implementing this induced loss with Keras (Chollet, 2015)
We summarise the test negative log-likelihood (LL) and
is as simple as a few lines of Python. A code snippet is
RMSE with standard error (across different random splits,
given in Figure 1, with more details in the appendix.
the lower the better) for selected datasets in Figure 2 and 3,
In regression problems, the loss function is defined as
respectively. The full results are provided in the appendix.
l(y, f œâ (x)) = œÑ2 ||y ‚àí f œâ (x)||22 and the likelihood term can
Although optimal Œ± may vary for different datasets, using
be interpreted as y ‚àº N (y; f œâ (x), œÑ ‚àí1 I). Plugging this
non-VI values has significantly improved the test-LL perinto the energy function returns the following objective
formances, while remaining comparable in test error meth
i
ric. In particular, Œ± = 0.5 produced overall good results
X
1
Œ±œÑ
LÃÉMC
log-sum-exp ‚àí ||yn ‚àí f œâb k (xn )||22
for
both test LL and RMSE, which is consistent with preŒ± (q) = ‚àí
Œ± n
2
vious findings. We also compare with a BNN with a GausX
ND
sian approximation (VI-G) (Li & Turner, 2016), a BNN
log œÑ +
+
pi ||Mi ||22 ,
(7)
2
with HMC, and a sparse Gaussian process model with 50
i
inducing points (Bui et al., 2016). In test-LL metric our
best dropout model out-performs the Gaussian approximawith {f œâb k (xn )}K
being
K
stochastic
forward
passes
on
k=1
tion method on almost all datasets, and for some datasets
input xn . Again, this is reminiscent of the l2 objective in
is on par with HMC which is the current gold standard
standard deep learning, and can be implemented by simfor Bayesian neural works, and with the GP model that is
ply passing the input through the dropout network multiple
known to be superior in regression.
times, collecting the stochastic outputs, and feeding the set
of outputs through our new BB-alpha loss function.
5.2. Classification

LÃÉMC
Œ± (q) =

X

pi ||Mi ||22 ‚àí

5. Experiments
We test the reparameterised BB-Œ± on Bayesian NNs with
the dropout approximation. We assess the proposed inference in regression and classification tasks on standard
benchmarking datasets, comparing different values of Œ±.
This last experiment leads us to propose a technique that
could be used to identify adversarial image attacks. In the
appendix we further provide a study of run time trade-off.

We further experiment with a classification task, comparing
the accuracy of the various Œ± values on the MNIST benchmark (LeCun & Cortes, 1998). We assessed a fully connect
NN with 2 hidden layers and 100 units in each layer. We
used dropout probability 0.5 and Œ± ‚àà {0, 0.5, 1}. Again,
we use K = 10 samples at training time for all Œ± values,
2
http://archive.ics.uci.edu/ml/datasets.
html

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

Figure 2. Negative test-LL results for Bayesian NN regression.

and Ktest = 100 samples at test time. We use weight decay
10‚àí6 , which is equivalent to prior lengthscale l2 = 0.1 (Gal
& Ghahramani, 2016b). We repeat each experiment three
times and plot mean and standard error. Test RMSE as well
as test log likelihood are given in Figure 4. As can be seen,
Hellinger value Œ± = 0.5 gives best test RMSE, with test
log likelihood matching that of the EP value Œ± = 1. The
VI value Œ± = 0 under-performs according to both metrics.
We next assess a convolutional neural network model
(CNN). For this experiment we use the standard CNN example given in (Chollet, 2015) with 32 convolution filters,
100 hidden units at the top layer, and dropout probability
0.5 before each fully-connected layer. Other settings are as
before. Average test accuracy and test log likelihood are
given in Figure 5. In this case, VI value Œ± = 0 seems to
supersede the EP value Œ± = 1, and performs similarly to
the Hellinger value Œ± = 0.5 according to both metrics.
5.3. Detecting Adversarial Examples
The third set of experiments considers adversarial attacks
on dropout-trained Bayesian neural networks. We test the
hypothesis that certain techniques for generating adversarial examples will give images that lie outside of the image

(a) NN test accuracy

(b) NN test log likelihood

Figure 4. MNIST test accuracy and test log likelihood for a fully
connected NN in a classification task.

Figure 3. Test RMSE results for Bayesian NN regression.

manifold, i.e. far from the data distribution (note though
that there exist techniques that will guarantee the images
staying near the data manifold, by minimising the perturbation used to construct the adversarial example). By assessing the BNN uncertainty, we should see increased uncertainty for adversarial images if they indeed lie outside of the
training data distribution. The tested models are fully connected networks with 3 hidden layers of 1000 units trained
using dropout rate 0.5 and different alpha values. These
models are also compared to a benchmark MLP with the
same architecture but trained by maximum likelihood. The
adversarial examples are generated on MNIST test data that
is normalised to be in the range [0, 1]. For the dropout
trained networks we perform MC dropout at test time with
Ktest = 10 MC samples.
The first attack in consideration is the Fast Gradient Sign
(FGS) method (Goodfellow et al., 2014). This is an untargeted attack, which attempts to reduces the maximum
value of the predicted class label probability
xadv = x ‚àí Œ∑ ¬∑ sgn(‚àáx max log p(y|x)).
y

We use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize Œ∑ varied

(a) CNN test accuracy

(b) CNN test log likelihood

Figure 5. MNIST test accuracy and test log likelihood for a convolutional neural network in a classification task.

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

Figure 6. Un-targeted attack: classification accuracy results as a
function of perturbation stepsize. The adversarial examples are
shown for (from top to bottom) MLP and BNN trained with
dropout and Œ± = 0.0, 0.5, 1.0.

between 0.0 and 0.5. The left panel in Figure 6 demonstrates the classification accuracy on adversarial examples,
which shows that the dropout networks, especially the one
trained with Œ± = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN. For
example, for Œ∑ = 0.1 the adversarial samples still visually close to the original class, and the BNN trained with
Œ± = 0.0 achieves an accuracy level almost 3 times higher
than the MLP and around 20% higher than the VI-trained
version. More interestingly, the test data examples and adversarial images can be told-apart by investigating the uncertainty representation of the dropout models. In the right
panel of Figure 6 we depict the predictive entropy computed on the neural network output probability vector, and
show example corresponding adversarial images below the
axis for each corresponding stepsize. Clearly the deterministic NN model produces over-confident predictions on adversarial samples, e.g. it predicts the wrong label very confidently even when the input is still visually close to digit
‚Äú7‚Äù (Œ∑ = 0.2). While dropout models, though producing
wrong labels, are very uncertain about their predictions.
This uncertainty keeps increasing as we move away from
the data manifold. Hence the dropout networks are much
more immunised from noise-corrupted inputs, as they can
be detected using uncertainty estimates in this example.
The second attack we consider is a targeted version of FGS
(Goodfellow et al., 2014; Carlini & Wagner, 2016), which
maximises the predictive probability of a selected class instead. As an example, we fix class 0 as the target and apply
the iterative gradient-base attack to all non-zero digits in
test data. At step t, the adversarial output is computed as
t‚àí1
xtadv = xt‚àí1
adv + Œ∑ ¬∑ sgn(‚àáx log p(ytarget |xadv )),

where the stepsize Œ∑ is fixed at 0.01 in this case. Results are
presented in the left panel of Figure 7, and again dropout

Figure 7. Targeted attack: classification accuracy results (on both
original and target class) as a function of the number of iterative
gradient steps. Note the log scale x-axis in the left panel.

trained models are more robust to this attack compared with
the MLP. Similarly these adversarial examples could be detected by the Bayesian neural networks‚Äô uncertainty, by examining the predictive entropy. By visually inspecting the
generated adversarial examples in the right panel of Figure 7, it is clear that the MLP overconfidently classifies a
digit 7 to class 0. On the other hand, the dropout models
are still fairly uncertain about their predictions even after
40 gradient steps. More interestingly, running this iterative
attack on dropout models produces a smooth interpolation
between different digits, and when the model is confident
on predicting the target class, the corresponding adversarial
images are visually close to digit zero.
These initial results suggest that assessing the epistemic
uncertainty of classification models can be used as a viable technique to identify adversarial examples. We would
note though that we used this experiment to demonstrate
our techniques‚Äô uncertainty estimates, and much more research is needed to solve the difficulties faced with adversarial inputs.

6. Conclusions
We presented a practical extension of the BB-alpha objective which allows us to use the technique with dropout approximating distributions. The technique often supersedes
existing approximate inference techniques (even sparse
Gaussian processes), and is easy to implement. A code
snippet for our induced loss is given in the appendix.
Acknowledgements
We thank Rich Turner, Nicolas Papernot, and the reviewers
for comments. YL thanks the Schlumberger Foundation
FFTF fellowship for supporting her PhD study.

Dropout Inference in Bayesian Neural Networks with Alpha-divergences

References
Amari, Shun-ichi. Differential-Geometrical Methods in Statistic.
Springer, New York, 1985.
Amodei, Dario, Olah, Chris, Steinhardt, Jacob, Christiano, Paul,
Schulman, John, and Mane, Dan. Concrete problems in ai
safety. arXiv preprint arXiv:1606.06565, 2016.
Angermueller, C and Stegle, O. Multi-task deep neural network to
predict CpG methylation profiles from low-coverage sequencing data. In NIPS MLCB workshop, 2015.
Barber, David and Bishop, Christopher M. Ensemble learning in
Bayesian neural networks. NATO ASI SERIES F COMPUTER
AND SYSTEMS SCIENCES, 168:215‚Äì238, 1998.
Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray, and
Wierstra, Daan. Weight uncertainty in neural network. In
ICML, 2015.
Bui, Thang D, HernaÃÅndez-Lobato, Daniel, Li, Yingzhen,
HernaÃÅndez-Lobato, JoseÃÅ Miguel, and Turner, Richard E. Deep
gaussian processes for regression using approximate expectation propagation. In Proceedings of The 33rd International
Conference on Machine Learning (ICML), 2016.
Carlini, Nicholas and Wagner, David. Towards evaluating the robustness of neural networks. arXiv preprint arXiv:1608.04644,
2016.
Chollet, Francois.
Keras.
fchollet/keras, 2015.

https://github.com/

HernaÃÅndez-Lobato, JoseÃÅ Miguel, Li, Yingzhen, HernaÃÅndezLobato, Daniel, Bui, Thang, and Turner, Richard E. Black-box
alpha divergence minimization. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1511‚Äì1520,
2016.
Hinton, Geoffrey E and Van Camp, Drew. Keeping the neural
networks simple by minimizing the description length of the
weights. In COLT, pp. 5‚Äì13. ACM, 1993.
Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan R. Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580, 2012.
Jordan, Michael I, Ghahramani, Zoubin, Jaakkola, Tommi S, and
Saul, Lawrence K. An introduction to variational methods for
graphical models. Machine learning, 37(2):183‚Äì233, 1999.
Kalchbrenner, Nal and Blunsom, Phil.
translation models. In EMNLP, 2013.

Recurrent continuous

Kendall, Alex and Cipolla, Roberto. Modelling uncertainty in
deep learning for camera relocalization. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp.
4762‚Äì4769. IEEE, 2016.
Kendall, Alex, Badrinarayanan, Vijay, and Cipolla, Roberto.
Bayesian segnet: Model uncertainty in deep convolutional
encoder-decoder architectures for scene understanding. arXiv
preprint arXiv:1511.02680, 2015.

Denker, John and LeCun, Yann. Transforming neural-net output
levels to probability distributions. In Advances in Neural Information Processing Systems 3. Citeseer, 1991.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convolutional neural networks.
In Advances in neural information processing systems, pp.
1097‚Äì1105, 2012.

Depeweg, Stefan, HernaÃÅndez-Lobato, JoseÃÅ Miguel, Doshi-Velez,
Finale, and Udluft, Steffen. Learning and policy search in
stochastic dynamical systems with bayesian neural networks.
arXiv preprint arXiv:1605.07127, 2016.

Kullback, Solomon. Information theory and statistics. John Wiley
& Sons, 1959.

Gal, Yarin. Uncertainty in Deep Learning. PhD thesis, University
of Cambridge, 2016.

Kullback, Solomon and Leibler, Richard A. On information and
sufficiency. The annals of mathematical statistics, 22(1):79‚Äì
86, 1951.

Gal, Yarin and Ghahramani, Zoubin. Bayesian convolutional neural networks with Bernoulli approximate variational inference.
ICLR workshop track, 2016a.

LeCun, Yann and Cortes, Corinna. The mnist database of handwritten digits, 1998.

Gal, Yarin and Ghahramani, Zoubin. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning.
ICML, 2016b.

LeCun, Yann, Boser, Bernhard, Denker, John S, Henderson,
Donnie, Howard, Richard E, Hubbard, Wayne, and Jackel,
Lawrence D. Backpropagation applied to handwritten zip code
recognition. Neural Computation, 1(4):541‚Äì551, 1989.

Goodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Explaining and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572, 2014.

LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M, and
Huang, F. A tutorial on energy-based learning. Predicting
structured data, 1:0, 2006.

Graves, Alex. Practical variational inference for neural networks.
In Advances in Neural Information Processing Systems, pp.
2348‚Äì2356, 2011.

Li, Yingzhen and Turner, Richard E. ReÃÅnyi divergence variational
inference. In NIPS, 2016.

Hellinger, Ernst. Neue begruÃàndung der theorie quadratischer formen von unendlichvielen veraÃànderlichen. Journal fuÃàr die reine
und angewandte Mathematik, 136:210‚Äì271, 1909.

Li, Yingzhen, HernaÃÅndez-Lobato, JoseÃÅ Miguel, and Turner,
Richard E. Stochastic expectation propagation. In Advances
in Neural Information Processing Systems (NIPS), 2015.

Hernandez-Lobato, Jose Miguel and Adams, Ryan. Probabilistic
backpropagation for scalable learning of Bayesian neural networks. In ICML, 2015.

MacKay, David JC. A practical Bayesian framework for backpropagation networks. Neural Computation, 4(3):448‚Äì472,
1992.

Dropout Inference in Bayesian Neural Networks with Alpha-divergences
Mikolov, TomaÃÅsÃå, KarafiaÃÅt, Martin, Burget, LukaÃÅsÃå, CÃåernockyÃÄ, Jan,
and Khudanpur, Sanjeev. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association, 2010.
Minka, Tom. Divergence measures and message passing. Technical report, Microsoft Research, 2005.
Minka, T.P. Expectation propagation for approximate Bayesian
inference. In Conference on Uncertainty in Artificial Intelligence (UAI), 2001.
Minka, T.P. Power EP. Technical Report MSR-TR-2004-149,
Microsoft Research, 2004.
Neal, Radford M. Bayesian learning for neural networks. PhD
thesis, University of Toronto, 1995.
Papernot, Nicolas, Goodfellow, Ian, Sheatsley, Ryan, Feinman, Reuben, and McDaniel, Patrick. cleverhans v1.0.0:
an adversarial machine learning library.
arXiv preprint
arXiv:1610.00768, 2016.
ReÃÅnyi, AlfreÃÅd. On measures of entropy and information. Fourth
Berkeley symposium on mathematical statistics and probability, 1, 1961.
Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J.
Learning internal representations by error propagation. Technical report, DTIC Document, 1985.
Sennrich, Rico, Haddow, Barry, and Birch, Alexandra. Edinburgh
neural machine translation systems for wmt 16. In Proceedings
of the First Conference on Machine Translation, pp. 371‚Äì376,
Berlin, Germany, August 2016. Association for Computational
Linguistics.
Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever,
Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to
prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929‚Äì1958, 2014.
Sundermeyer, Martin, SchluÃàter, Ralf, and Ney, Hermann. LSTM
neural networks for language modeling. In INTERSPEECH,
2012.
Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to
sequence learning with neural networks. In NIPS, 2014.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre,
Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with
convolutions. arXiv preprint arXiv:1409.4842, 2014.
Turner, RE and Sahani, M. Two problems with variational expectation maximisation for time-series models. Inference and
Estimation in Probabilistic Time-Series Models, 2011.
Van Erven, Tim and HarremoeÃàs, Peter. ReÃÅnyi divergence
and Kullback-Leibler divergence. Information Theory, IEEE
Transactions on, 60(7):3797‚Äì3820, 2014.
Wan, L, Zeiler, M, Zhang, S, LeCun, Y, and Fergus, R. Regularization of neural networks using dropconnect. In ICML-13,
2013.
Yang, Xiao, Kwitt, Roland, and Niethammer, Marc. Fast predictive image registration. arXiv preprint arXiv:1607.02504,
2016.

