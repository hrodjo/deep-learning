A Unified Maximum Likelihood Approach for Estimating Symmetric
Properties of Discrete Distributions
Jayadev Acharya 1 Hirakendu Das 2 Alon Orlitsky 3 Ananda Theertha Suresh 4

Abstract
Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications.
Recently, researchers applied different estimators and analysis tools to derive asymptotically
sample-optimal approximations for each of these
properties. We show that a single, simple, plug-in
estimator—profile maximum likelihood (PML)–
is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.

1. Introduction
1.1. Symmetric distribution properties
Pk
def
Let = {(p1 , . . . ,pk ) : pi 0, i=1 pi = 1, 1  k  1}
denote the collection of all discrete distributions over finite
or infinite support. A distribution property is a mapping
f : ! R. It is symmetric if it remains unchanged under
relabeling of domain symbols, namely if it is determined
by just the probability multiset {p1 , p2 , . . . ,pk }. Many
important properties are symmetric. For example:
Support size S(p) = |{x : p(x) > 0}|, plays an important
role in population and vocabulary estimation.
P
Support coverage Sm (p) = x (1 (1 p(x))m ), the expected number of elements observed in m samples, arises
in ecological and biological studies, e.g., (Colwell et al.,
2012).
P
1
Shannon entropy H(p) =
x p(x) log p(x) , central to
information theory (Cover & Thomas, 2006), has numerous
*
Equal contribution 1 Cornell University, Ithaca, NY 2 Yahoo
Inc!, Sunnyvale, CA 3 University of California, San Diego
4
Google Research.
Correspondence to: Jayadev Acharya
<acharya@cornell.edu>, Hirakendu Das <hdas@yahooinc.com>, Alon Orlitsky <alon@ucsd.edu>, Ananda Theertha
Suresh <theertha@google.com>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

applications.

P
Distance to uniform kp uk1 = x |p(x) 1/|X ||, where
u is the uniform distribution over the domain X of p. This
distance measure appears in the error of hypothesis testing,
and the uniform distribution is arguably one of the commonest discrete distributions.
1.2. Distribution estimation
Considerable research, over many years, has focused on estimating distribution properties. In the common setting, an
unknown underlying distribution p 2 generates n indedef
pendent samples X n = X1 , , . . . ,Xn , and the objective is
to estimate a given property f (p) as accurately as possible.
Specifically, an estimator for a distribution p over X is a
function fˆ : X n ! R mapping observed samples to a
property estimate. The sample complexity of fˆ is the smallest number of samples it requires to estimate a property f
with accuracy " and confidence probability , for all distributions in a collection P ✓ ,
ˆ

def

C f (f, P, , ") =
n
min n : p(|f (p)

fˆ(X n )|

") 

o
8p 2 P .

The sample complexity of estimating f is the lowest sample complexity of any estimator,
ˆ

C ⇤ (f, P, , ") = min C f (f, P, , ").
fˆ

By taking the median of about log 1 independent estimators, the error rate can be driven down from a constant to
. Therefore, the sample complexity depends on only
through a factor of at most log 1 . For simplicity, we thereˆ
ˆ
fore abbreviate C f (f, P, 1/3, ") by C f (f, P, ").
1.3. Result summary
Recent research has shown that while simple estimators for
the aforementioned properties require sample size n proportional to the support size k, more sophisticated techniques need only a sub-linear sample size n = ⇥(k/ log k).

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

However, each of the problems was approximated via different estimators and analysis techniques, that for some
properties were rather complex.
Motivated by the principle of maximum likelihood, we
show that a single, simple, plug-in estimator—profile maximum likelihood (PML) (Orlitsky et al., 2004b)— is competitive for estimating any symmetric property. Its sample
complexity is at most quadratically worse than that of any
estimator.
Specifically, we show that if a symmetric property can be
estimated using n samples with confidence , then the PML
plug-in estimator can
p estimate it using as many samples
with confidence ·e n . While this increase may seem high,
note that it is sub-exponential. We show that if a property
has an estimator that has a small bounded difference constant (how much the estimator changes when we change
one sample), then the error probability reduces exponentially with n (Please see Section 7.1). Combined, these two
facts imply that for properties with locally-smooth estimators, the PML plug-in estimator is optimal up to a constant:
C PML = ⇥(C ⇤ ). We then show that all the above properties have locally-smooth estimators, hence they can be estimated by the PML plug-in estimator with up to a constant
factor more than the optimal number of samples.
1.4. Outline
The rest of the paper is organized as follows. In Section 2
we describe existing results and those shown in this paper.
In Section 3 we formally define the quantities involved and
state the results. In Section 4 we define profiles and PML.
In Section 5, we outline the new approach. In Section 6,
we demonstrate auxiliary results for maximum likelihood
estimators. In Section 7, we outline how we apply maximum likelihood to support, support coverage, entropy, and
uniformity. In Section 8, we provide the details for support, and support coverage and in the appendix we outline
results for distance to uniformity and entropy.

2. Previous and New Results
2.1. Previous Results
Plug-in estimation is a general approach for estimating distribution properties. It uses the samples X n to find an approximation p̂ of p, and lets f (p̂) estimate f (p).
One of the most common distribution estimators, dating
back to Fisher is maximum likelihood, that for clarity we
call sequence maximum likelihood (SML) (Aldrich, 1997).
To any sample xn it assigns the distribution p that maximizes p(xn ). The SML estimate is exceedingly simple to
def
derive. The multiplicity Nx = Nx (xn ) of symbol x is the
number of times it appears in the sequence xn . The empiri-

cal frequency estimator assigns to each symbol x, the fracdef
tion p̂(x) = Nx /n of times it appears in the sample xn .
For example, if x7 =bananas, empirical frequency would
assign p̂(a) = 3/7, p̂(n) = 2/7, and p̂(b) = p̂(s) = 1/7.
It can be readily shown that SML is exactly the empirical
frequency estimator.
While the SML plug-in estimator performs well in the
limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.
Support size. With finitely many samples, S(p) cannot
be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed. Motivated by databases, where each entry appears at least
once, (Raskhodnikova et al., 2009) considered distributions
whose non-zero probabilities are at least k1 ,
1
k

def

= {p 2

: p(x) 2 {0} [ [1/k, 1]} ,
def

and estimated the normalized support S̃(p) = S(p)/k.
1
1 , ") = ⇥(k log
It can be shown that C SML (S̃(p),
" ).
k
Yet (Valiant & Valiant, 2011a;
Wu
&
Yang,
2015)
showed
⇣
⌘
2 1
k
1 , ") = ⇥
that C ⇤ (S̃(p),
·
log
.
log
k
"
k

Support coverage. Here too we consider the normalized
def
coverage S̃m (p) = Sm (p)/m. (Good & Toulmin,
1956) proposed the Good Toulmin (GT) estimator that
achieves C GT (S̃m (p), , ") = m/2. Recently, (Orlitsky et al., 2016) derived a simple estimator showing that
C ⇤ (S̃m (p), , ") = ⇥( logmm · log 1" ). (Zou et al., 2016)
derived a more complex estimator with similar dependence
on m but worse dependence on ".
Shannon entropy. Since elements with arbitrarily small
probability can contribute to an arbitrarily high entropy,
H(p) cannot be estimated over aribtrary support with
finitely many samples. Therefore researchers are mostly
interested in estimating entropy of distributions with support size at most k.
k

def

= {p 2

: S(p)  k}.

It can be shown that C SML (H(p), k , ") = ⇥( k" ) (Paninski, 2003). Moreover, (Paninski, 2003) showed that
C ⇤ (H(p), k , ") is sublinear in k, (Valiant & Valiant,
2011a) showed that the optimal dependence on k is k/ log k
and (Wu & Yang, 2016; Jiao et al., 2015) obtained the
optimal dependence on⇣ both k,⌘and ", and showed that
C ⇤ (H(p), k , ") = ⇥ logk k · 1" .

Distance to uniform. (Valiant⇣& Valiant,
⌘ 2011b) showed
k
1
⇤
that C (kp uk1 , k , ") = O log k · "2 , and (Jiao et al.,
2016) showed that this bound is tight.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

These results are summarized in Table 1.
Other properties were considered as well. (Bar-Yossef
et al., 2001; Acharya et al., 2015; Caferov et al., 2015;
Obremski & Skorski, 2017) estimated Rényi entropy
and (Bu et al., 2016) estimated KL divergence. (Canonne,
2015) surveyed testing whether distributions have certain
properties, and (Jiao et al., 2014) studied the performance
of SML estimators for several properties. Closest to this
work in terms of approach and techniques are (Acharya
et al., 2011; 2012; 2013a;b; Valiant & Valiant, 2013; Orlitsky & Suresh, 2015) that design algorithms whose sample
complexity is provably close to the best possible regardless
of the domain size.
2.2. Profile Maximum Likelihood
Symmetric distribution properties do not depend on the
symbol labels. They are determined by a simple sufficient
statistic: the number of elements appearing any given number of times. The profile of a sequence X n , denoted '(X n )
is the multiset of the multiplicities of all the symbols appearing in X n . For example, '(a b r a c a d a b r a) =
{1, 1, 2, 2, 5}, as two symbols appearing once, two appearing twice, and one symbol appearing five times, removing the association of the individual symbols with the
multiplicities. Profiles are also referred to as histograms
of histograms (Batu et al., 2000), histogram order statistics (Paninski, 2003), and fingerprints (Valiant & Valiant,
2011a).
Motivated by the principle of maximum likelihood, (Orlitsky et al., 2004b; 2017b) discarded the symbol labels, and
considered the profile maximum likelihood (PML) distribution that maximizes the probability of the observed profile.
A number of PML properties were established. (Orlitsky
et al., 2004b; 2005) proved PML’s existence, consistency,
and some of its properties. (Orlitsky et al., 2004d; 2005;
Orlitsky & Pan, 2009; Pan et al., 2009) described additional properties and derived the PML distributions of several short and simple profiles. (Orlitsky et al., 2017b;c) provide a unified review of several of these results. (Anevski
et al., 2013) contains a combination of previously-known
and new results. A related distribution-estimation approach
is described in (Orlitsky et al., 2004c; 2003).
Several approaches were taken to computing the PML
distribution.
Algebraic computation was considered
in (Acharya et al., 2010). A combination of the EM and
MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to support-size
estimation (Orlitsky et al., 2004a; 2006; Pan, 2012) and a
summary of some of the results appears in (Orlitsky et al.,
2017a). (Vontobel, 2012; 2014) derived the Bethe approximation of these algorithms.

Following the first draft of this work, (Vatedka & Vontobel, 2016) showed that both theoretically and empirically
plug-in estimators obtained from the PML estimate yield
good estimates for symmetric functionals of Markov distributions.
2.3. New Results
We show that replacing the SML plug-in estimator by PML
yields a unified estimator that, like the best results shown
via specialized techniques developed, is optimal.
Theorem 1. There is a unified approach based on PML
distribution that achieves the optimal sample complexity
for the problems of estimating the entropy, support, support coverage, and distance to uniformity.
We prove in Corollary 1 that the PML approach is competitive with respect to any symmetric property. For symmetric properties, these results are perhaps a justification of
Fisher’s thoughts on Maximum Likelihood:
“Of course nobody has been able to prove that maximum
likelihood estimates are best under all circumstances. Maximum
likelihood estimates computed with all the information available
may turn out to be inconsistent. Throwing away a substantial part
of the information may render them consistent.”
R. A. Fisher’s thoughts on Maximum Likelihood (Le Cam, 1979).

To prove these PML guarantees, we establish two results
that are of interest on their own right.
• With n samples, PML estimates any symmetric property of p
p with essentially the same accuracy, and at
most e3 n times the error, of any other estimator. This
follows by combining Theorem 3 with Lemma 1.
• For a large class of symmetric properties, including
all those mentioned above, if there is an estimator that
uses n samples, and has an error probability 1/3, we
design an estimator using O(n) samples, whose error probability is nearly exponential in n. We remark
that this decay is much faster than applying the median
trick. This result follows by combining McDiarmid’s
inequality with Lemma 2.
Combined, these results prove that PML plug-in estimators
are sample-optimal.
We also introduce the notion of -approximate ML distributions, described in Definition 1. These distributions are
more relaxed version of PML, hence may be more easily
computed, yet they provide essentially the same performance guarantees.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Property name
Entropy
Support size
Support coverage
Distance to u

C SML

P

f (p)
H(p)
S̃(p)
S̃m (p)
kp uk1

k
"

k
1
k

k log
m

X

k
"2

1
"

C⇤
k 1
log k "
2
k
log k log
m
log m log
k 1
log k "2

1
"
1
"

PML
Theorem 5 and Section 8.1
Theorem 5 and Section 8.2
Theorem 5 and Section A
Theorem 5 and Section A

Table 1. Estimation complexity for various properties up to a constant factor. For all properties shown, PML achieves the best known
results up to a constant factor. The details of where the optimal sample complexity was derived for each problem is discussed in
Section 2.1.

3. Formal Definitions and Results
In the past, different sophisticated estimators were used for
every property in Table 1. We show that the simple plug-in
estimator that uses any PML approximation p̃, has optimal
performance guarantees for all these properties.
In the next theorem, assume n is at least the optimal sample
complexity of estimating entropy, support, support coverage, and distance to uniformity (given in Table 1) respectively.
p
Theorem 2. For all " > c/n0.2 , any plug-in exp (
n)approximate PML p̃ satisfies,
Entropy
C p̃ (H(p),

k , ")

⇣ C ⇤ (H(p),

k , "),

Support size
C p̃ (S(p)/k,

1
k

, ") ⇣ C ⇤ (S(p)/k,

1
k

, "),

Support coverage
C p̃ (Sm (p)/m,

, ") ⇣ C ⇤ (Sm (p)/m,

uk1 ,

X , ")

⇣ C ⇤ (kp

uk1 ,

n

|



For a distribution p, the probability of a profile ' is defined
as
X
def
p(') =
p(X n ),
X n :'(X n )='

the probability of observing a sequence
with profile
P
Qn '. Under i.i.d. sampling, p(') =
X n :'(X n )='
i=1 p(Xi ).
For example, the probability of observing a sequence with
profile ' = {1, 2} is the probability of observing a sequence with one symbol appearing once, and one symbol
appearing twice. A sequence with a symbol x appearing
twice and y appearing once (e.g., x y x) has probability
p(x)2 p(y). Appropriately normalized, for any p, the probability of the profile {1, 2} is
✓ ◆ X
n
X
Y
3
p({1, 2}) =
p(Xi ) =
p(a)2 p(b),
1
n
i=1
a6=b2X

'(X )={1,2}

(1)

, "),

where the normalization factor is independent of p. The
summation is a monomial symmetric polynomial in the
probability values. See (Pan, 2012) for more examples.

k , ").

4.2. PML Estimation Scheme

Distance to uniformity
C p̃ (kp

Lemma
p 1 ((Hardy & Ramanujan, 1918)). |
exp(3 n).

4. PML: Profile Maximum Likelihood
4.1. Preliminaries
For a sequence X n , recall that the multilplicity Nx is the
number of times x appears in X n . Discarding, the labels,
profile of a sequence (Orlitsky et al., 2004b) is defined below. Let n be all profiles of length-n sequences. Then,
4
= {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}. In particular, a profile of a length-n sequence is an unordered
partition of n. Therefore, | n |, the number of profiles
of length-n sequences is equal to the partition number of
n. Then, by the Hardy-Ramanujan bounds on the partition
number,
For a, b > 0, denote a . b or b & a if for some universal
constant c, a/b  c. Denote a ⇣ b if both a . b and a & b.

Recall that pX n is the distribution maximizing the probability of X n . Similarly, define (Orlitsky et al., 2004b):
def

p' = max p(')
p2P

as the distribution in P that maximizes the probability of
observing a sequence with profile '.
For example, for ' = {1, 2}. For P = k , from (1),
X
p' = arg max
p(a)2 p(b).
p2

k

a6=b

Note that in contrast, SML only maximizes one term of this
expression.
We give two examples from the table in (Orlitsky et al.,
2004b) to distinguish between SML and PML distributions,

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

and also show an instance where PML outputs distributions
with a larger support than those appearing in the sample.
Example 1. Let X = {a, b, . . . , z}. Suppose X n = x y x,
then the SML distribution is (2/3, 1/3). However, the distribution in that maximizes the probability of the profile
'(x y x) = {1, 2} is (1/2, 1/2). Another example, illustrating the power of PML to predict new symbols is X n =
a b a c, with profile '(a b a c) = {1, 1, 2}. The SML distribution is (1/2, 1/4, 1/4), but the PML is a uniform distribution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).
Suppose we want to estimate a symmetric property f (p)
of an unknown distribution p 2 P given n independent
samples. Our high level approach using PML is described
below.
Input: Class of distributions P, symmetric function
f (·), sample X n

estimation the optimal dependence is 1" , whereas (Valiant
& Valiant, 2011a) yields "12 . This is more prominent for
support size and support coverage, which have optimal
dependence of polylog( 1" ), whereas (Valiant & Valiant,
2011a) gives a "12 dependence. Besides, we analyze the
first method proposed for estimating symmetric properties,
designed from the first principles, and show that in fact it
is competitive with the optimal estimators for various problems.

5. Proof Outline
Our arguments have two components. In Section 6 we
prove a general result for the performance of plug-in estimation via maximum likelihood approaches.
Let P be a class of distributions over Z, and f : P ! R be
a function. For z 2 Z, let

1. Compute p' : arg maxp2P p('(X n )).
2. Output f (p' ).
There are a few advantages of this approach (as is true with
any plug-in approach): (i) the computation of PML is agnostic to the function f at hand, (ii) there are no parameters
to be tuned, (iii) techniques such as Poisson sampling or
median tricks are not necessary, (iv) well motivated by the
maximum-likelihood principle.
Comparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a). Our approach is perhaps closest in flavor to the plug-in estimator of (Valiant &
Valiant, 2011a). Indeed, as mentioned in (Valiant, 2012),
their linear-programming estimator is motivated by the
question of estimating the PML. Their result was the first
estimator to provide sample complexity bounds in terms
of the alphabet size, and accuracy the problems of entropy
and support estimation. Before we explain the differences
of the two approaches, we briefly explain their approach.
Define, 'µ (X n ) to be the number of elements that appear
µ times. For example, when X n = a b r a c a d a b r a,
'1 = 2, '2 = 2, and '5 = 1. (Valiant & Valiant, 2011a)
design a linear program that uses SML for high values of
µ, and formulate a linear program to find a distribution for
which E['µ ]’s are close to the observed 'µ ’s. They then
plug-in this estimate to estimate the property. On the other
hand, our approach, by the nature of ML principle, tries to
find the distribution that best explains the entire profile of
the observed data, not just some partial characteristics. It
therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance
measures, competitive with the best possible. For example, the guarantees of the linear program approach are suboptimal in terms of the desired accuracy ". For entropy

def

pz = arg max p(z)
p2P

be the maximum-likelihood estimator of z in P. Upon observing z, f (pz ) is the ML estimator of f . In Theorem 4,
we show that if there is an estimator that achieves error
probability , then the ML estimator has an error probability at most |Z|. We note that variations of this result in the
asymptotic statistics were studied before (see (Lehmann &
Casella, 1998)). Our contribution is to use these results in
the context of symmetric properties and show sample complexity bounds in the non-asymptotic regime.
We emphasize that, throughout this paper Z will be the set
of profiles of length n, and P will be distributions induced
over profiles by length-n i.i.d. samples. Therefore, we
have |Z| = | n |. By Lemma 1, if there is a profile based
estimator with error probability , then thepPML approach
will have error probability at most exp(3 n). Such arguments were used in hypothesis testing to show the existence
of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011; 2012).
At its face value this seems like a weak result. Our second
key step is to prove that for the properties we are interested,
it is possible to obtain very sharp guarantees. For example,
we show that if we can estimate the entropy to an accuracy
±" with error probability 1/3 using n samples, then we can
estimate the entropy to accuracy ±2" with error probability
exp( n0.9 ) using only 2n samples. Using this sharp concentration, the new error probability term dominates | n |,
and we obtain our results. The arguments for sharp concentration are based on modifications to existing estimators
and a new analysis. Most of these results are technical and
are in the appendix.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

6. Maximum Likelihood Property Estimation
We establish performance guarantees of ML property estimation in a general set-up. Recall that P is a collection
of distributions over Z, and f : P ! R. Given a sample Z from an unknown p 2 P, we want to estimate f (p).
The maximum likelihood approach is the following twostep procedure.
1. Find pZ = arg maxp2P p(Z).
2. Output f (pZ ).
We bound the performance of this approach in the following theorem.
Theorem 3. Suppose there is an estimator fˆ : Z ! R,
such that for any p, and Z ⇠ p,
⇣
⌘
Pr f (p) fˆ(Z) > " < ,
(2)
then

Pr (|f (p)

f (pZ )| > 2")  · |Z| .

(3)

Proof. Consider symbols with p(z)
and p(z) <
separately. A distribution p with p(z)
outputs z with
probability at least . For (2) to hold, we must have,
f (p) fˆ(z) < ". By the definition of ML, pz (z)
p(z)
, and for (2) to hold for pz , f (pz )
By the triangle inequality, for all such z,
|f (p)

f (pz )|  f (p)

fˆ(z) + f (pz )

fˆ(z) < ".
fˆ(z)  2".

Thus if p(z)
, then PML satisfies the required guarantee with zero probability of error, and any error occurs
only when p(z) < . We bound this probability as follows.
When Z ⇠ p,
X
Pr (p(Z) < ) 
p(z) < · |Z| .
z2Z:p(z)<

For some problems, it might be easier to just approximate
the ML, instead of finding it exactly. We define an approximation ML as follows:
Definition 1 ( -approximate ML). Let  1. For Z 2 Z,
p̃Z 2 P is a -approximate ML distribution if p̃z (z)
· pz (z). When Z is profiles of length-n, a -approximate
PML is a distribution p̃' such that p̃' (')
· p' (').
The next result proves guarantees for any -approximate
ML estimator.
Theorem 4. Suppose there exists an estimator satisfying (2). For any p 2 P and Z ⇠ p, any -approximate
ML p̃Z satisfies:
Pr (|f (p)

f (p̃Z )| > 2")  · |Z|/ .

The proof is very similar to the previous theorem and is
presented in the Appendix B.
6.1. Competitiveness of ML via Median Trick
Suppose for a property f (p), there is an estimator with
sample complexity n that achieves an accuracy ±" with
probability of error at most 1/3. The standard method to
boost the error probability is the median trick: (i) Obtain
O(log(1/ )) independent estimates using O(n log(1/ ))
independent samples. (ii) Output the median of these estimates. This is an "-accurate estimator of f (p) with error
probability at most . By definition, estimators are a mapping from the samples to R. However, in many applications
the estimators map from a much smaller (some sufficient
statistic) of the samples. Denote by Zn the space consisting of all sufficient statistics that the estimator uses. For
example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence
Zn = n . Using the median-trick, we get the following
result.
Corollary 1. Let fˆ : Zn ! R be an estimator of f (p) with
accuracy " and error-probability 1/3. The ML estimator
achieves accuracy 2" with probability at least 2/3 using
⇢
n0
min n0 :
> n samples.
20 log(3Zn0 )
Proof. Since n is the number of samples to get error probability 1/3, by the Chernoff bound, the error after n0 samples is at most exp( (n0 /(20n))). Therefore, the error probability of the ML estimator for accuracy 2" is at
most exp( (n0 /(20n)))Zn0 , which we desire to be at most
1/3.
For estimators
that use the profile of sequences, | n | <
p
exp(3 n). Plugging this in the previous result shows that
the PML based approach has a sample complexity of at
most O(n2 ). This result holds for all symmetric properties, independent of ", and the alphabet size k. For the
problems mentioned earlier, something much better in possible, namely the PML approach is optimal up to constant
factors.

7. Sample optimality of PML
7.1. Sharp Concentration for Some Properties
To obtain sample-optimality guarantees for PML, we need
to drive the error probability down much faster than the
median trick. We achieve this by using McDiarmid’s inequality stated below. Let fˆ : X ⇤ ! R. Suppose fˆ gets
n independent samples X n from an unknown distribution.
Moreover, changing one of the Xj to any Xj0 changed fˆ by

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

at most c⇤ . Then McDiarmid’s inequality (bounded difference inequality, (Boucheron et al., 2013)) states that,
Pr

⇣

fˆ(X n )

✓
◆
⌘
2t2
E[fˆ(X n )] > t  2 exp
. (4)
nc2⇤

This inequality can be used to show strong error probability
bounds for many problems. We mention a simple application for estimating discrete distributions.
Example 2. It is well known (Devroye & Lugosi, 2001)
that SML requires ⇥(k/"2 ) samples to estimate p in `1 distance
probability at least 2/3. In this case, fˆ(X n ) =
P Nwith
x
p(x) , and therefore c⇤ is at most 2/n. Using
x n
McDiarmid’s inequality, it follows that SML has an error
probability of = 2 exp( k/2), while still using ⇥(k/"2 )
samples.

coverage, and distance to uniformity
p to an accuracy of 4"
with probability at least 1 exp(
n).
Proof. Let ↵ = 0.05. By Lemma 2, for each property of
interest, there are estimators based on the profiles of the
samples such that using near-optimal number of samples,
they have bias " and maximum change if we change any of
the samples is at most c0 n↵ /n for some constant c0 . Hence,
by McDiarmid’s inequality, an accuracy
of 2" is achieved
⇣
⌘
2 1 2↵ 0 2
with probability at least 1 2 exp
2" n
/c . Now
suppose p̃ is any -approximate PML distribution. Then by
Theorem 4



Let Bn be the bias of an estimator fˆ(X n ) of f (p), namely
def
Bn = f (p) E[fˆ(X n )] . By the triangle inequality,
f (p)
 f (p)

E[fˆ(X n )] + fˆ(X n )

E[fˆ(X n )]

E[fˆ(X n )] .

Plugging this in (4),
Pr

⇣

f (p)

n

|

2 exp( 2"2 n1

 exp(

p

✓
◆
⌘
2t2
fˆ(X n )] > t + Bn  2 exp
. (5)
nc2⇤

With this in hand, we need to show that c⇤ can be bounded
for estimators for the properties we consider. In particular,
we will show that
Lemma 2. Let ↵ > 0 be a fixed constant. For entropy,
support, support coverage, and distance to uniformity there
exist profile based estimators that use the optimal number
of samples (given in Table 1), have bias " and if we change
↵
any of the samples, changes by at most c · nn , where c is a
positive constant.
We prove this lemma by proposing several modifications to
the existing sample-optimal estimators. The modified estimators will preserve the sample complexity up to constant
factors and also have a small c⇤ . The proof details are given
in the appendix.
Using (5) with Lemma 2,
Theorem 5. Let n be the optimal sample complexity of estimating entropy, support, support coverage and distance to
uniformity (given in table 1) and c be a large positivep
constant. Let "
c/n0.2 , then any for any > exp (
n),
the -PML estimator estimates entropy, support, support

2↵

p
2
/c0 + 3 n)

n),

where p
in the last step we used "2 n1
exp(
n).

fˆ(X n )

= Bn + fˆ(X n )

·|

Pr (|f (p) f (p̃)| > 4") <

2↵

p
& c0 n, and

>

8. Support and Support Coverage
We analyze both support coverage and the support estimation via a single approach. We first start with support coverage. Recall that the goal is to estimate Sm (p), the expected
number of distinct symbols that we see after observing m
samples from p. By the linearity of expectation,
X
X
Sm (p) =
E[INx (X m )>0 ] =
(1 (1 p(x))m ) .
x2X

x2X

The problem is closely related to the support coverage
problem (Orlitsky et al., 2016), where the goal is to estimate Ut (X n ), the number of new distinct symbols that we
observe in n · t additional samples. Hence
" n
#
X
Sm (p) = E
'i + E[Ut ],
i=1

where t = (m n)/n. We show that the modification of
an estimator in (Orlitsky et al., 2016) is also near-optimal
and satisfies conditions in Lemma 2. We propose to use the
following estimator
Ŝm (p) =

n
X

'i + UtSGT (X n ),

i=1

Pn

where UtSGT (X n ) = i=1 'i ( t)i Pr(Z
i) and Z is a
Poisson random variable with mean r and t = (m n)/n.
The above theorem also works for any " & 1/n0.25
any ⌘ > 0

⌘

for

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

We remark that the proof also holds for Binomial smoothed
random variables as discussed in (Orlitsky et al., 2016).
We need to bound the maximum coefficient and the bias to
apply Lemma 2. We first bound the maximum coefficient
of this estimator.
Lemma 3. For all n  m/2, the maximum coefficient of
Ŝm (p) is at most 1 + er(t 1) .
Proof. For any i, the coefficient of 'i is 1 + ( t)i Pr(Z
r
i
Pt
i). It can be upper bounded as 1 + i=0 e i!(rt) = 1 +
er(t 1) .
The next lemma bounds the bias of the estimator.
Lemma 4. For all n  m/2, the bias of the estimator is
bounded by
|E[Ŝm (p)]

Sm (p)|  2 + 2er(t

Proof. As before let t = (m
E[Ŝm (p)] Sm (p)
n
X
=
E['i ] + E[UtSGT (X n )]
i=1

=

X

E[UtSGT (X n )]

((1

1)

r

+ min(m, S(p))e

.

n)/n.

X

(1

(1

p(x))m )

x2X

p(x))n

(1

p(x))m ) .

8.2. Support Estimator
Recall that the quantity of interest in support estimation is
S̃(p), which we wish to estimate to an accuracy of ".
Proof of Lemma 2 for support. Note that we are interested
in distributions with all the non zero probabilities are at
least 1/k. We propose to estimate S̃(p) using Ŝm (p)/k,
for m = k log 3" . If we choose r = log 3" , then by
Lemma 3, the maximum coefficient of Ŝm (p)/k is at most
3
2 m
k
n log " , which for n
log2 3" is at most
ke
↵ log(k/21/↵ )
k ↵ /k < n↵ /n.
To bound the bias, note that for this choice of m
X
0  S(p) Sm (p) =
(1 p(x))m
x



X
x

e

mp(x)

 ke

log

3
"

=

k"
.
3

Similarly, by Lemma 4,
1
|E[Ŝm (p)] S(p)|
k
1
1
 |E[Ŝm (p)] Sm (p)| + |S(p) Sm (p)|
k
k
1
"
 (2 + 2er(t 1) + ke r ) +  ",
k
3
for all " > 12n↵ /n.

x2X

Hence by Lemma 8 and Corollary 2, in (Orlitsky et al.,
2016), we get
|E[Ŝm (p)]

Sm (p)|  2+2er(t

1)

+ min(m, S(p))e

r

.

Using the above two lemmas we prove results for both the
observed support coverage and support estimator.
8.1. Support Coverage Estimator
Recall that the quantity of interest in support coverage estimation is Sm (p)/m, which we wish to estimate to an accuracy of ".
Proof of Lemma 2 for observed. If we choose r = log 3" ,
then by Lemma 3, the maximum coefficient of Ŝm (p)/m
1/↵

2 n log "
is at most m
e
, which for m  ↵ n log(n/2
log(3/")
↵
↵
most n /m < n /n. Similarly, by Lemma 4,
m

1
|E[Ŝm (p)]
m

3

Sm (p)| 

for all " > 6n↵ /n.

1
(2 + 2er(t
m

1)

+ me

r

)

is at

)  ",

9. Discussion and Future Directions
We studied estimation of symmetric properties of discrete
distributions using the principle of maximum likelihood,
and proved optimality of this approach for a number of
problems. A number of directions are of interest. We believe that the lower bound requirement on " is perhaps an
artifact of our proof technique, and that the PML based approach is indeed optimal for all ranges of ". Approximation
algorithms for estimating the PML distributions would be
a fruitful direction to pursue. Given our results, approximations stronger than exp( "2 n) would be very interesting. In the particular case when the desired accuracy is a
constant, even an exponential approximation would be sufficient for many properties. We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we
consider, and compare with the state of the art provable
methods.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Acknowledgements
The authors thank the reviewers for valuable feedback and
the NSF for support through grants CIF-1564355, CIF1619448, CRII-CIF-1657471, and a Cornell University
start-up grant. Jayadev Acharya thanks Clement Canonne,
Jiantao Jiao, and Pascal Vontobel for interesting discussions.

References

Bu, Yuheng, Zou, Shaofeng, Liang, Yingbin, and Veeravalli, Venugopal V. Estimation of KL divergence between large-alphabet distributions. In ISIT, 2016.
Caferov, Cafer, Kaya, Barış, ODonnell, Ryan, and Say,
AC Cem. Optimal bounds for estimating entropy with
pmf queries. In International Symposium on Mathematical Foundations of Computer Science, pp. 187–198.
Springer, 2015.

Acharya, Jayadev, Das, Hirakendu, Mohimani, Hosein, Orlitsky, Alon, and Pan, Shengjun. Exact calculation of
pattern probabilities. In ISIT, pp. 1498 –1502, 2010.

Cai, T Tony, Low, Mark G, et al. Testing composite hypotheses, hermite polynomials and optimal estimation of
a nonsmooth functional. The Annals of Statistics, 39(2):
1012–1041, 2011.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan, Orlitsky, Alon, and Pan, Shengjun. Competitive closeness
testing. COLT, 19:47–68, 2011.

Canonne, Clément L. A survey on distribution testing:
Your data is big. but is it blue? Electronic Colloquium
on Computational Complexity (ECCC), 22:63, 2015.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan,
Orlitsky, Alon, Pan, Shengjun, and Suresh,
Ananda Theertha.
Competitive classification and
closeness testing. In COLT, 2012.

Colwell, Robert K, Chao, Anne, Gotelli, Nicholas J,
Lin, Shang-Yi, Mao, Chang Xuan, Chazdon, Robin L,
and Longino, John T. Models and estimators linking
individual-based and sample-based rarefaction, extrapolation and comparison of assemblages. Journal of plant
ecology, 5(1):3–21, 2012.

Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. Optimal probability estimation with applications to prediction and classification. In
COLT, 2013a.
Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. A competitive test for uniformity of monotone distributions. In AISTATS, 2013b.
Acharya,
Jayadev,
Orlitsky,
Alon,
Suresh,
Ananda Theertha, and Tyagi, Himanshu.
The
complexity of estimating Rényi entropy. In SODA,
2015.
Aldrich, John. R.a. fisher and the making of maximum likelihood 1912-1922. Statistical Science, 12(3):162–176,
09 1997.
Anevski, Dragi, Gill, Richard D, and Zohren, Stefan. Estimating a probability mass function with unknown labels.
arXiv preprint arXiv:1312.1200, 2013.
Bar-Yossef, Ziv, Kumar, Ravi, and Sivakumar, D. Sampling
algorithms: lower bounds and applications. In Symposium on Theory of computing, pp. 266–275. ACM, 2001.
Batu, Tugkan, Fortnow, Lance, Rubinfeld, Ronitt, Smith,
Warren D., and White, Patrick. Testing that distributions
are close. In FOCS, pp. 259–269, 2000.
Boucheron, S., Lugosi, G., and Massart, P. Concentration
Inequalities: A Nonasymptotic Theory of Independence.
OUP Oxford, 2013.

Cover, Thomas M. and Thomas, Joy A. Elements of information theory (2. ed.). Wiley, 2006.
Devroye, Luc and Lugosi, Gábor. Combinatorial methods
in density estimation. Springer, 2001.
Good, IJ and Toulmin, GH. The number of new species,
and the increase in population coverage, when a sample
is increased. Biometrika, 43(1-2):45–63, 1956.
Hardy, Godfrey H and Ramanujan, Srinivasa. Asymptotic
formulaæ in combinatory analysis. Proceedings of the
London Mathematical Society, 2(1):75–115, 1918.
Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weissman, Tsachy.
Maximum likelihood estimation of
functionals of discrete distributions. arXiv preprint
arXiv:1406.6959, 2014.
Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weissman,
Tsachy. Minimax estimation of functionals of discrete
distributions. IEEE Transactions on Information Theory,
61(5):2835–2885, 2015.
Jiao, Jiantao, Han, Yanjun, and Weissman, Tsachy. Minimax estimation of the L1 distance. In ISIT, pp. 750–754,
2016.
Le Cam, Lucien Marie. Maximum likelihood: an introduction. JSTOR, 1979.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Lehmann, Erich Leo and Casella, George. Theory of point
estimation, volume 31. Springer Science & Business
Media, 1998.
Obremski, Maciej and Skorski, Maciej. Renyi entropy estimation revisited. In APPROX, 2017.
Orlitsky, A., Pan, S., Sajama, Santhanam, P., Viswanathan,
K., and Zhang, J. Pattern maximum likelihood: Computation and experiments. Arxiv, 2017a.
Orlitsky, Alon and Pan, Shengjun. The maximum likelihood probability of skewed patterns. In ISIT, 2009.
Orlitsky, Alon and Suresh, Ananda Theertha. Competitive
distribution estimation: Why is good-turing good. In
NIPS, pp. 2143–2151, 2015.
Orlitsky, Alon, Santhanam, Narayana P., and Zhang, Junan.
Always good turing: Asymptotically optimal probability
estimation. In FOCS, 2003.
Orlitsky, Alon, Sajama, S, Santhanam, NP, Viswanathan,
K, and Zhang, Junan. Algorithms for modeling distributions over large alphabets. In ISIT, 2004a.
Orlitsky, Alon, Santhanam, Narayana P., Viswanathan, Krishnamurthy, and Zhang, Junan. On modeling profiles
instead of values. In UAI, 2004b.
Orlitsky, Alon, Santhanam, Narayana P, and Zhang, Junan. Universal compression of memoryless sources over
unknown alphabets. IEEE Transactions on Information
Theory, 50(7):1469–1481, 2004c.
Orlitsky,
Alon,
Santhanam,
Narayana Prasad,
Viswanathan, Krishna, and Zhang, Junan.
Low
(size) and order in distribution modeling. 2004d.
Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. Convergence of profile based estimators. In Proceedings of the 2005 IEEE
International Symposium on Information Theory (ISIT),
pp. 1843 –1847, 2005.
Orlitsky,
Alon,
Santhanam,
Narayana Prasad,
Viswanathan, Krishna, and Zhang, Junan.
Theoretical and experimental results on modeling low
probabilities. In Information Theory Workshop, 2006.
Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.
Optimal prediction of the number of unseen species.
Proceedings of the National Academy of Sciences, 2016.
doi: 10.1073/pnas.1607774113.
Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. On estimating the probability multiset part i: The pattern maximum likelihood
approach. Arxiv, 2017b.

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. On estimating the probability multiset part ii: Properties of the pattern maximum
likelihood estimator. Arxiv, 2017c.
Pan, Shengjun. On the theory and application of pattern
maximum likelihood. PhD thesis, UC San Diego, 2012.
Pan, Shengjun, Acyarya, Jayadev, and Orlitsky, Alon. The
maximum likelihood probability of unique-singleton,
ternary, and length-7 patterns. pp. 1135–1139, 2009.
Paninski, Liam. Estimation of entropy and mutual information. Neural computation, 15(6):1191–1253, 2003.
Raskhodnikova, Sofya, Ron, Dana, Shpilka, Amir, and
Smith, Adam. Strong lower bounds for approximating
distribution support size and the distinct elements problem. SIAM Journal on Computing, 39(3):813–842, 2009.
Timan, A. F. Theory of Approximation of Functions of a
Real Variable. Pergamon Press, 1963.
Valiant, Gregory and Valiant, Paul. Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts. In STOC, 2011a.
Valiant, Gregory and Valiant, Paul. The power of linear
estimators. In FOCS, pp. 403–412. IEEE, 2011b.
Valiant, Gregory and Valiant, Paul. Instance-by-instance
optimal identity testing. Electronic Colloquium on Computational Complexity (ECCC), 20:111, 2013.
Valiant, Gregory John. Algorithmic approaches to statistical questions. PhD thesis, University of California,
Berkeley, 2012.
Vatedka, Shashank and Vontobel, Pascal O. Pattern maximum likelihood estimation of finite-state discrete-time
markov chains. In ISIT, 2016.
Vontobel, Pascal O. The bethe approximation of the pattern maximum likelihood distribution. In IEEE ISIT, pp.
2012–2016, 2012.
Vontobel, Pascal O. The bethe and sinkhorn approximations of the pattern maximum likelihood estimate and
their connections to the valiant-valiant estimate. In Information Theory and Applications Workshop, ITA, pp.
1–10, 2014.
Wu, Yihong and Yang, Pengkun. Chebyshev polynomials,
moment matching, and optimal estimation of the unseen.
CoRR, abs/1504.01227, 2015.
Wu, Yihong and Yang, Pengkun. Minimax rates of entropy estimation on large alphabets via best polynomial
approximation. IEEE Trans. Information Theory, 62(6):
3702–3720, 2016.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Zou, James, Valiant, Gregory, Valiant, Paul, Karczewski,
Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol,
Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G.
Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects. Nature Communications, 7:13293 EP,
10, 2016.

white

