Spherical Structured Feature Maps for Kernel Approximation

Yueming Lyu 1

Abstract
We propose Spherical Structured Feature (SSF)
maps to approximate shift and rotation invariant kernels as well as bth -order arc-cosine kernels (Cho & Saul, 2009). We construct SSF
maps based on the point set on d − 1 dimensional sphere Sd−1 . We prove that the inner
product of SSF maps are unbiased estimates for
above kernels if asymptotically uniformly distributed point set on Sd−1 is given. According
to (Brauchart & Grabner, 2015), optimizing the
discrete Riesz s-energy can generate asymptotically uniformly distributed point set on Sd−1 .
Thus, we propose an efficient coordinate decent
method to find a local optimum of the discrete
Riesz s-energy for SSF maps construction. Theoretically, SSF maps construction achieves linear
space complexity and loglinear time complexity.
Empirically, SSF maps achieve superior performance compared with other methods.

1. Introduction
Kernel methods such as Gaussian processes (GPs) (Rasmussen, 2006; Srinivas et al., 2009; Snoek et al., 2012)
and support vector machines (SVMs) (Chang & Lin, 2011;
Fan et al., 2008) have been successfully used in many statistical modeling and machine learning tasks. Despite of
strong expressive power, kernel methods usually cannot
scale up to the large scale datasets with L samples due to
the need of manipulating L×L Gram matrix. Recently, random feature maps (Rahimi et al., 2007; Rahimi & Recht,
2009; Sutherland & Schneider, 2015) have demonstrated
their effectiveness on kernel approximation to scale up kernel methods. Roughly speaking, a shift invariant kernel
K(x, z) = K(x − z) : Rd → C can be approximated
by K(x, z) ≈ Ψ(x)T Ψ(z), where Ψ is the√
explicit mapped
feature constructed as Ψ(x) = f (WT x)/ N , where f (·)
1

Department of Computer Science, City University of Hong
Kong, Tat Chee Avenue, Hong Kong . Correspondence to: Yueming Lyu <LV Yueming@outlook.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

denotes the nonlinear function, W ∈ Rd×N is constructed
by N i.i.d samples drawn from a distribution defined by
K. Therefore, the training and inference of kernel methods
can be greatly accelerated by working directly on the primal space of Ψ(·). For example, Gaussian Processes (GPs)
have O(L3 ) computation and O(L2 ) storage complexity.
By using feature maps, it reduces to O(N 2 L + N 3 ) computation and O(N L + N 2 ) storage complexity. All these
elegant properties make random feature maps promising
for large scale kernel methods. Thus, many kernel methods (Le & Wilson, 2015; Cutajar et al., 2016; Oliva et al.,
2016) have been proposed to deal with large scale statistical
learning by directly working on feature maps.
Generally, two aspects of random feature maps are mostly
concerned by literature for scaling up kernel methods. One
is the approximation accuracy of feature maps while the
other is the computational cost of feature maps construction. To achieve better approximation accuracy, (Yang,
2014; Avron et al., 2016) employ QMC (Dick et al., 2013)
sampling instead of standard Monte Carlo sampling to construct feature maps. By mapping QMC points on [0, 1]d
through the inverse cumulative distribution function, they
construct more effective feature maps. To reduce time
complexity, (Le et al., 2013) propose Fastfood to construct
feature maps. Benefiting from the special structured matrix multiplication, it reduces time complexity of feature
maps construction from O(N d) to O(N log d). However,
it achieves computational efficiency at the expense of increasing the variance of approximation. Recently, (Feng
et al., 2015) employ the property of circulant matrix to accelerate feature maps construction of Gaussian kernel without increasing the variance. (Choromanski & Sindhwani,
2016) generalize the Fastfood and circulant feature maps
to P model and particularly discuss the structured matrix
with low-displacement rank. Despite of the success of P
model, it still cannot achieve better approximation accuracy
compared with feature maps obtained with fully Gaussian
matrix.
To achieve better approximation accuracy and loglinear
time complexity, we propose Spherical Structured Feature
(SSF) maps to approximate shift and rotation invariant kernels as well as bth -order arc-cosine kernels (Cho & Saul,
2009). Specifically, We construct SSF maps based on the
point set on d − 1 dimensional sphere Sd−1 , where the

Spherical Structured Feature Maps for Kernel Approximation

points are columns of a particular structured matrix produced by a discrete Fourier matrix. The points on Sd−1
for SSF maps construction can be generated by optimizing the discrete Riesz s-energy. According to (Brauchart
et al., 2014), optimizing the discrete Riesz s-energy (for s
in some ranges) can generate QMC designs on Sd−1 , which
usually can achieve smaller approximation error compared
with fully random methods. Moreover, Because of special structure of the point set, SSF maps construction can
achieve loglinear time complexity via Fast Fourier Transform (FFT).
Our contributions are summarized as follows:
• We propose Spherical Structured Feature (SSF) maps
to approximate shift and rotation invariant kernels as
well as bth -order arc-cosine kernels (Cho & Saul,
2009). We prove that the inner product of SSF maps
are unbiased estimates for above kernels if asymptotically uniformly distributed point set on d − 1 dimensional sphere Sd−1 is given.
• We propose an efficient coordinate decent method
to find a local optimum of the discrete Riesz senergy (Brauchart & Grabner, 2015), thereby approximately generating asymptotically uniformly distributed points on Sd−1 .
• We can construct SSF maps with linear space complexity and loglinear time complexity. Empirically,
SSF maps achieve superior performance compared
with other methods.

2. Background and Preliminaries
We provide a brief review of random feature maps and the
discrete Riesz s-energy in this section as preliminaries.
2.1. Random Feature Maps
Random feature maps can be viewed as equal weight approximation of multidimensional integrals. One earlier
work (Rahimi et al., 2007) approximates the shift invariant
kernels based on the Bochner’s Theorem.
Theorem 2.1 Bochner’s Theorem ((Rudin, 2011)) : A continuous shift invariant scaled kernel function K(x, z) =
K(x − z) : Rd → C is positive definite if and only if
it is the Fourier Transform of a unique finite probability
measure p on Rd .
R
T
(1)
K(x, z) = Rd e−i(x−z) w p(w)dw
For a real valued kernel K(x, z), p(w) = p(−w) ≥ 0 can
ensure the imaginary parts of the integral vanish. According to the Bochner’s theorem, there is a one-to-one corre-

spondence between the kernel functions K(x, z) and probability densities p(w) defined on Rd .
Shift and rotation invariant kernels are shift invariant
kernels with the rotation invariant property, i.e. K(x, z) =
K(Rx, Rz), given any rotation R ∈ SO(d), where SO(d)
denotes rotation groups. The Gaussian kernel K(x, z) =
2
2
e−kx−zk2 /2σ is a member of this family. From Bochner’s
theorem, the corresponding probability density is also
Gaussian. For a general Gaussian RBF kernel K(x, z) =
T
e−(x−z) Σ(x−z)/2 , it can be transformed into rotation invariant form by using y = Σ1/2 x in the original domain.
bth -order arc-cosine kernels are rotation invariant kernels. As discussed in (Cho & Saul, 2009), bth -order arccosine kernels have the following form:
b

b

Kb (x, z) = π1 kxk2 kzk2 Jb (θ)


T
z
where θ = cos−1 kxkx kzk
2

(2)

2

th

b -order arc-cosine kernels have trivial dependence on the
norm of x and z. The dependence on the angle is defined
by function Jb (θ). bth -order arc-cosine kernels are rotation invariant kernels but not shift invariant kernels in general. For example, the zero-order (3) and first-order (4)
arc-cosine kernel are not shift invariant kernels.
K0 (x, z) = 1 −
K1 (x, z) =

1
π

θ
π

kxk2 kzk2 (sin θ + (π − θ) cos θ)

(3)
(4)

The bth -order arc-cosine kernel Kb (x, z) can be reformulated via the integral representation:
R
b
b
Kb (x, z) = 2 Rd s(wT x)s(wT z)(wT x) (wT z) p(w)dw
(5)
where s(·) is a step function (i.e. s(x) = 1 if x > 0 and 0
otherwise) and the density p is standard Gaussian.
Feature maps: Both Monte Carlo and Quasi-Monte Carlo
approximation (Dick et al., 2013) are equal weight approximation to integrals. Based on equal weight approximation,
the feature maps can be constructed as:
K(x, z) ≈

1
N

N
P
i=1



f wiT x f wiT x = Ψ(x)T Ψ(z)

(6)
where wi , i ∈ 1, ..., N are samples constructed by Monte
Carlo or Quasi-Monte Carlo methods. f (·) is a nonlinear
function depending on the kernel. Ψ(·) is the explicit finite
dimensional feature map. For Gaussian kernel with bandwidth σ, the associated nonlinear function is a complex exponential function f (x) = eix/σ . For a zero-order arccosine kernel in (3) and first-order arc-cosine kernel in (4),
the associated nonlinear functions are step function f (x) =
s(x) and ReLU activation function f (x) = max(0, x) respectively.

Spherical Structured Feature Maps for Kernel Approximation

2.2. Discrete Riesz s-energy
The discrete Riesz s-energy is related to the equal weight
numerical integration and uniformly distributed point set.
Equal weight numerical integration over a d-dimensional
sphere Sd := {x ∈ Rd+1 | kxk2 = 1 } uses equal weight
summation of finite point evaluations of the integrands to
approximate the integrals:
R
Sd

f (v)dσ(v) ≈

1
N

N
P

(7)

f (vi )

i=1

where σ denotes the normalized surface area measure on
Sd .
According to (Brauchart & Grabner, 2015), the point set
V = [v1 , ..., vN ] ∈ Sd×N is asymptotically uniformly distributed if equation (8) holds true.
N
P
1
f (vi )
N
N →∞
i=1

lim

=

R
Sd

f (v)dσ(v)

(8)

3.1. Feature Maps for Shift and Rotation Invariant
Kernels
Shift and rotation invariant kernels are highly symmetric and structured because they satisfy both shift invariant property and rotation invariant property. Rotation invariant property means that K(x, z) = K(Rx, Rz), given
any rotation R ∈ SO(d), where SO(d) denotes rotation
groups. To benefit from rotation invariant property, it is
reasonable to construct the feature maps by using spherical
equal weight approximation in equation (7) and (8).
The feature maps for real valued shift and rotation invariant
kernels K(x, z) can be constructed as equation (10):


Ψ (x) = √N1 M [cos Φ− (t1 )xT v1 , sin Φ− (t1 )xT v1 ,


..., cos Φ− (tM )xT vN , sin Φ− (tM )xT vN ]T
(10)
where tj = Mj+1 , V = [v1 , ..., vN ] ∈ Sd−1×N denotes the
point set asymptotically uniformly distributed on Sd−1 and
Φ− (x) denotes the inverse cumulative distribution function
w.r.t the nonnegative radial scale.
T

The discrete Riesz s-energy(Götz, 2003; Brauchart &
Grabner, 2015) is defined as equation (9):

Es (V) :=











N
P

N
P

i=1 j=1,j6=i
N
N
P
P

1
kvi −vj ks2

log

i=1 j=1,j6=i

, s 6= 0
(9)

1
kvi −vj k2 ,

s=0

Theorem 3.1: Ψ(x) Ψ (z) is an unbiased estimate of a
real valued shift and rotation invariant kennel K(x, z).
Proof: From Bochner’s Theorem, a shift invariant kernel
K(x, z) can be written as equation (1). Let r = kwk2 and
p(r) be the density function of r. Because of the rotation
invariant property of K(x, z), we achieve equation (11).
R R
T
K(x, z) = R+ Sd−1 e−ir(x−z) v p(r)drdσ(v)
R
R
−
T
= [0,1] Sd−1 e−i Φ (t)(x−z) v dσ(v)dt
(11)
where R+ denotes the nonnegative real values.

Theorem 2.2 ((Brauchart & Grabner, 2015)): For s > −2,
the optimum N-point configuration of the Riesz s-energy
on Sd is asymptotically uniformly distributed w.r.t the normalized surface area measure σ on Sd .
According to (Brauchart et al., 2014; Brauchart & Grabner,
2015), the discrete Riesz s-energy can serve as a criterion
to construct the point set V = [v1 , ..., vN ] ∈ Sd×N for
QMC designs. Particularly, (Brauchart et al., 2014) have
proved that maximizing the discrete Riesz s-energy with
s ∈ (−2, 0) can generate QMC designs for functions in
Sobolev space. They also prove that QMC designs have
higher convergence rate of worst-case error than fully randomly chosen points for functions in Sobolev space.

3. Spherical Structured Feature Maps
In this section, we propose SSF maps to approximate shift
and rotation invariant kernels as well as bth -order arccosine kernels by employing their rotation invariant property.

For real valued kernel K(x, z), the imaginary parts of the
integral vanish. We can achieve equation (12).


R
R
T
K(x, z) = [0,1] Sd−1 cos Φ− (t)(x − z) v dσ(v)dt
(12)
According to the property of asymptotically uniformly
distributed point set V in equation (8) and the onedimensional QMC rule, we obtain equation (13).
lim

M,N →∞

T

Ψ(x) Ψ (z) =

N P
M
P
1
(cos
M
N
M,N →∞
i=1 j=1
−



Φ− (tj )xT vi cos Φ− (tj )zT vi


+ sin Φ (tj )xT vi sin Φ− (tj )zT vi )


M P
N
P
T
= lim M1N
cos Φ− (tj )(x − z) vi
M,N →∞
j=1
 i=1

R
R
T
= [0,1] Sd−1 cos Φ− (t)(x − z) v dσ(v)dt
= K(x, z)
(13)

lim

Spherical Structured Feature Maps for Kernel Approximation

Proposition 3.1: Let U = [V, −V], using point set U to
approximate a real valued shift and rotation invariant kernel
K(x, z) by using equation (10) is equal to using point set
V to approximate K(x, z):
T

T

Ψ(x; U) Ψ (z; U) = Ψ(x; V) Ψ (z; V)

(14)

Proof: Note that cosine function is an even function. Thus,
we obtain equation (15).




T
T
cos Φ− (tj )(x − z) vi = cos −Φ− (tj )(x − z) vi
(15)
Thus, we achieve equation (16).
T

Ψ(x; U) Ψ (z; U)


N P
M
P
T
= 2N1M
cos Φ− (tj )(x − z) vi


th

i=1 j=1

+ 2N1M
=

1
2N M

N P
M
P



−

T

cos −Φ (tj )(x − z) vi

i=1 j=1
N P
M
P



Let r = kwk2 . Since p is standard Gaussian, by taking
rotation invariant property, we obtain equation (19).


R
Kb (x, z) = R2 Rd Rχ wT x χ wT z p(w)dw

= 2 Sd−1 R+ χ rb vT x χ rb vT z p(r)dσ(v)dr


R
R
= 2 Sd−1 R+ r2b χ vT x χ vT z p(r)dσ(v)dr


R
R
= 2 R+ r2b p(r)dr Sd−1 χ vT x χ vT z dσ(v)


R
= 2Cb Sd−1 χ vT x χ vT z dσ(v)
(19)
Since Kb (x, z) is rotation invariant, we have Kb (x, z) =
Kb (−x, −z). Together with equation (19), we achieve
equation (20).


R
Kb (x, z) = Cb Sd−1 χ vT x χ vT z
(20)
+χ(−vT x)χ(−vT z)dσ(v)

(16)



T
2 cos Φ− (tj )(x − z) vi

i=1 j=1
T

The feature maps for a b -order arc-cosine kernel Kb (x, z)
can be constructed as equation (21).
q


Ψ (x) = CNb [χ v1T x , χ −v1T x , ....,
(21)


T
T
x ]T ∈ R2N
x , χ −vN
χ vN

= Ψ(x; V) Ψ (z; V)

T



Theorem 3.2: Ψ(x) Ψ (z) is an unbiased estimate of a
bth -order arc-cosine kernel Kb (x, z).

Proposition 3.1 shows that for a shift and rotation invariant
kernel, computing N points can achieve the same approximation effect compared with using 2N points.

Proof: According to the Lemma 3.1 and the property of
the asymptotically uniformly distributed point set V, we
obtain equation (22).

3.2. Feature Maps for bth -order Arc-cosine Kernels
In this subsection, we discuss the feature maps for bth order arc-cosine kernels. We discuss them separately because they are rotation invariant kernels but not shift invariant kernels in general. Moreover, they are closely related to
deep neural networks (Cho & Saul, 2009), which demonstrate super performance in many areas.
Lemma 3.1: The bth -order arc-cosine kernels can be calculated as equation (17).


R
Kb (x, z) = Cb Sd−1 χ vT x χ vT z
(17)
+χ(−vT x)χ(−vT z)dσ(v)
R
b
where χ(x) = max(0, sign(x)|x| ), Cb = R+ r2b p(r)dr.
Cb is a constant that is independent of x and z. p(r) is
the density function of the chi-distribution with d degrees
freedom. For example, the constants associated with the
zero, first and second-order arc-cosine kernels are C0 = 1,
C1 = d and C2 = d(d + 2) respectively.
Proof: From equation (5), we can achieve equation (18).
R
b
b
T
T
Kb (x, z) = 2R Rd s(wT x)s(w
z)(w
x) (wT z) p(w)dw


= 2 Rd χ wT x χ wT z p(w)dw
(18)

T

lim Ψ(x) Ψ (z)

N →∞

N


P
= lim CNb
χ viT x χ viT z + χ(−viT x)χ(−viT z)
N →∞
i=1


R
= Cb S d−1 χ vT x χ vT z + χ(−vT x)χ(−vT z)dσ(v)
= Kb (x, z)
(22)


From equation (17) and (22), we observe that the approximation is actually operated on the (d − 1)-dimensional
domain instead of d-dimensional domain (Cho & Saul,
2009). Generally, the approximation error of Quasi Monte
Carlo methods with N points depends on the dimension of
integration. A lower dimension leads to smaller approximation error, thus the feature maps in equation (21) can
achieve lower approximation error.
The feature maps in equation (21) are closely related to
the bidirectional activation neural network. Specifically,
the feature maps for the first-order arc-cosine kernel are
related to the bidirectional ReLU activation function (An
et al., 2015) which has the distance preservation property
compared with ReLU.
From equation (14) and (21), we know that the feature
maps actually rely on the point set U = [V, −V]. The
design of the point set U will be discussed in section 4.

Spherical Structured Feature Maps for Kernel Approximation

4. Design of Matrix U
We have discussed the construction of SSF maps in last
section. However, one unsolved problem is how to obtain
the matrix U = [V, −V]. We employ the discrete Riesz
s-energy as the objective function to obtain matrix U because it can generate asymptotically uniformly distributed
points on Sd−1 (Brauchart & Grabner, 2015). Moreover,
to achieve computation and storage efficiency for feature
maps construction , we add a structured constraint to the
matrix U. In this section, we show the structure of matrix
U first and then the optimization of discrete Riesz s-energy.

Riesz s-energy. Specifically, we will discuss how to minimize the Riesz 0-energy in equation (25). The other Riesz
s-energy can be optimized in a similar way.
E(U) =

2N
P

2N
P

i=1 j=1,j6=i

log

1
kui −uj k

(25)

where U = [V, −V] = [u1 , ..., u2N ].
In the following, we will discuss how to minimize equation (25) by using a coordinate decent method.

It is worth noting that matrix U can be used not only for
kernel approximation, but also for approximation of general integrals over hypersphere. Moreover, by using FFT,
matrix U can accelerate the integral approximation which
involves projection operations. In addition, it only needs to
store the indexes with linear storage cost (i.e. O(d)) instead
of to explicitly store the matrix with cost O(N d).

Theorem 4.1: Let U = [V, −V] with V defined in (23),
the discrete Riesz 0-energy of U can be calculated as equation (26).

2
n−1
m
P
P
2πiks p
1
E(U) = C − 2n
log 1 − (Im m
e n )
p=1
s=1

2
n−1
m
P
P
2πiks p
1
−2n
log 1 − (Re m
e n )

4.1. Structure of Matrix U

where C is a constant independent of the choice of Λ.

Since U can be constructed by V, i.e. U = [V, −V], we
only need to define structured matrix V. To achieve loglinear time complexity of SSF maps construction, we construct V by extracting rows from a discrete Fourier matrix.
The complexity analysis of SSF maps construction based
on matrix V is given in section 5.

Proof: Since U = [V, −V] ∈ S(d−1)×2N , we obtain
equation (27).

p=1

s=1

(26)

Mathematically, the construction of matrix V is shown as
follows. Without loss of generality, we assume that d =
2m, N = 2n, m < n. Let F ∈ Cn×n be a n × n discrete
2πikj
Fourier matrix.
Fk,j = e n is the (k, j)th entry of F ,
√
where i = −1. Let Λ = [k1 , k2 , ..., km ] ⊂ {1, ..., n − 1}
be a subset of indexes.
The structured matrix V can be defined as equation (23).


ReFΛ −ImFΛ
V = √1m
∈ Rd×N
(23)
ImFΛ ReFΛ
where FΛ in equation (24) is the matrix constructed by m
rows of F .

 2πik 1
2πik1 n
1
··· e n
e n


..
..
..
 ∈ Cm×n
FΛ = 
(24)
.
.
.


2πikm n
2πikm 1
e n
··· e n
With the V given in equation (23), it is easy to verify that
kvi k2 = 1 for i ∈ {1, ..., n}. Thus, each column of matrix
V is a point on Sd−1 .
4.2. Minimize the Discrete Riesz s-energy
With structured matrix V defined in equation (23), our goal
is to select a subset of indexes Λ that optimizes the discrete

E(U) = −

2N
P

2N
P

log kui − uj k

i=1 j=1,j6=i

= −2

N
P

log k2vi k

i=1

−2

N
P

N
P

(log kvi − vj k + log kvi + vj k)

i=1 j=1,j6=i
N
N
P
P

=C −2

=C −2

i=1 j=1,j6=i
N
N
P
P

log (kvi − vj k kvi + vj k)
log

i=1 j=1,j6=i

p

2 − 2viT vj


p
2 + 2viT vj
(27)

Recall that N = 2n. By separating the summation term
into two parts (each part has n × n term), we achieve equation (28).
 q

2n
2n
P
P
2
T
E(U) = C − 2
log 2 1 − (vi vj )
i=1 j=1,j6
=i


q
n
2n
P
P
2
=C −4
log 2 1 − (viT vj )
i=1 j=n+1

q
n
n
P
P
2
T
−4
log 2 1 − (vi vj )
i=1 j=1,j6=i

(28)
Let V·,1:n = [v1 , ..., vn ] and V·,n+1:2n = [vn+1 , ..., v2n ]
be the matrix consisting of the first n and last n columns of
V respectively. We can obtain equation (29).
T
V·,1:n
V·,n+1:2n =

T
1
m ReFΛ (−ImFΛ )
1
+m
(ImFΛ )T ReFΛ

(29)

Spherical Structured Feature Maps for Kernel Approximation
T
Note that all diagonal elements of V·,1:n
V·,n+1:2n are
zero. By further separating the first summation term of
equation (28) into two parts, we obtain equation (30).


√
log 2 1 − 0
i=1 j=n+i
 q

n
2n
P
P
2
−4
log 2 1 − (viT vj )
i=1 j=n+1,j6=n+i

 q
n
n
P
P
2
−4
log 2 1 − (viT vj )
i=1 j=1,j6=i
 q

2n
n
P
P
2
log 2 1 − (viT vj )
=C −4
i=1 j=n+1,j6
n+i
 =q

n
n
P
P
2
T
−4
log 2 1 − (vi vj )
E(U) = C − 4

n
2n
P
P

Algorithm 1
Initialization: random sample Λ = [k1 , k2 , ..., km ] from
e = 1T FΛ
{1, 2, ...n − 1} without replacement. Set h
repeat
Set J = J(Λ)
for q = 1 to m do
Set g = [e2πikq /n , e2πikq 2/n ..., e2πikq (n−1)/n ]
e−g
Set h = h
∗
Find kq by kq∗ = arg max J(kq ) in (35)
kq ∈{1,...,n−1}

2πikq∗ /n

Update g = [e
e =h+g
Set h
end for
until J does not change

i=1 j=1,j6=i

∗

∗

, e2πikq 2/n ..., e2πikq (n−1)/n ]

(30)
To be concise, let Z = [z1 , , ..., zn ] =

√1 FΛ .
m

For 1 ≤ j ≤ n, j 6= i, we achieve equation (31).

2
m
P
1
2
∗
2
T
2πiks p/n
(vi vj ) = (Rezi zj ) = m Re
e
s=1

(31)
For n + 1 ≤ j ≤ 2n, , j 6= n + i, we attain equation (32).
2

m
P
1
2πiks p/n
T
2
∗
2
e
(vi vj ) = (Imzi zj−n ) = m Im
s=1

(32)
In equation (31) and (32), p ≡ i − j (mod n), where
mod denotes the modulus operation on integers.
Note that z∗i zj has at most n − 1 distinct values when i 6=
j (mod n) . Together with equation (30), we achieve
equation (33).

 q
n
2n
P
P
2
E(U) = C − 4
log 2 1 − (viT vj )
i=1
j=n+1,j6

q =n+i
n
n
P
P
2
T
−4
log 2 1 − (vi vj )
i=1 j=1,j6=i

 q
n
2n
P
P
2
=C −4
log 2 1 − (Imz∗i zj−n )
i=1 j=n+1,j6
n+i
 =q

n
n
P
P
2
∗
−4
log 2 1 − (Rezi zj )
i=1 j=1,j6=i
s
!
2
n−1
m
P
P
1
2πik
p/n
s
= C − 4n
log 2 1 − (Im m
e
)
p=1
s=1
s
!
2
n−1
m
P
P
1
2πik
p/n
s
−4n
log 2 1 − (Re m
e
)
p=1
s=1

2
n−1
m
P
P
1
2πiks p/n
= C − 2n
log 1 − (Im m
e
)
p=1
s=1


2
n−1
m
P
P
1
−2n
log 1 − (Re m
e2πiks p/n )
p=1

s=1

(33)


From Theorem 4.1, we know that minimizing E(U) is
equivalent to maximizing J(Λ) which is defined in equation (34).

2
n−1
m
P
P
1
2πiks p/n
J(Λ) =
log 1 − (Im m
e
)
p=1
s=1


(34)
2
n−1
m
P
P
1
+
log 1 − (Re m
e2πiks p/n )
p=1

s=1

By keeping all the indexes in Λ = [k1 , k2 , ..., km ] fixed
except the q th element, we can obtain equation (35).
n−1
P




2
log 1 − (Im hp + e2πikq p/n /m)
p=1


n−1

P
2
+
log 1 − (Re hp + e2πikq p/n /m)

J(kq ) =

p=1

where kq ∈ {1, 2, ...n − 1}, hp =

m
P

(35)
e

2πiks p/n

.

s=1,s6=q

With equation (35), we can maximize J(Λ) by maximizing J(kq ) with other indexes fixed each time. Let h =
[h1 , ..., hn−1 ] , g = [e2πikq /n , e2πikq 2/n ..., e2πikq (n−1)/n ].
1 = [1, ..., 1]T ∈ Rm is the vector of all ones. A coordinate
ascent method to maximize J(Λ) is given in Algorithm 1.
Obviously, it is a discrete optimization problem. Algorithm 1 can find a local optimum. The time complexity of
the Algorithm 1 is O(T mn2 ), where T denotes the number
of outer iteration. Empirically, the outer iteration T is less
than ten.

5. Fast Feature Maps Construction
In this section, we will discuss how to construct SSF maps
in loglinear time complexity and linear space complexity
by using the structure property of V.
Theorem 5.1 Assume that d = 2m, N = 2n, m < n. Let

Spherical Structured Feature Maps for Kernel Approximation





x1
∈ R2m and z = x1 + ix2 ∈ Cm . Given
x2
Λ = [k1 , k2 , ..., km ] ⊂ {1, ..., n − 1} , let y ∈ Cn with
yΛ = z. Other elements outside the index set Λ are equal
to zero. Given V defined in equation (23), equation (36)
holds.
x =

VT x =

T
√1 [Re(F ∗ y), Im(F ∗ y)]
m

(36)

culant (Choromanski & Sindhwani, 2016) matrices, QMC
with Halton set and QMC with Sobol set (Avron et al.,
2016). For Halton set and Sobol set, the implementation
in MATLAB are employed in the experiments. The scrambling and shifting techniques are used for Haltonset and
Sobolset. In all the experiments, we fix M = 1 (the number of one-dimensional QMC points) for SSF maps.

Proof: Let Ω ∈ Rn×n be a diagonal matrix with all diagonal elements inside the index set Λ equal to one , the others
equal to zero.

T 

ReF
−ImF
x
Λ
Λ
1
1
T
V x = √m
x2 
 ImFΛ T ReFΛ
T
(ReFΛ )x1 + (ImFΛ )x2
= √1m
T
(−ImF
)x1 + (ReFΛ T )x2
Λ 

∗
Re(FΛ z)
(37)
= √1m
∗
Im(F
Λ z)


Re(F ∗ Ωy)
= √1m
∗
 Im(F ∗ Ωy)
Re(F y)
= √1m
Im(F ∗ y)

Thus, the projection operation VT x (previously mentioned
in equation (10) and (21)) can be calculated by Fast Fourier
Transform algorithm (FFT) in O(n log n) time complexity.
Because scaling and taking nonlinear transform can be finished in O(n), the total time complexity to construct SSF
maps is O(n log n).

Figure 1. Convergence of the Logarithmic Energy

All steps to construct SSF maps are summarized as follows:
e by x
e = Dx, where D ∈ {−1, +1}d×d is
(a) Compute x
a diagonal matrix where diagonal elements are uniformly
sampled from {−1, +1}.
e1 + ie
(b) Construct y such that yΛ = x
x2 , other elements
outside the index set Λ are equal to zero.
e by equation (36) via FFT.
(c) Compute VT x
(d) Construct feature maps Ψ (x) via equation (10) or (21).
For each (m, n) pair , the index set Λ only need to be computed once. It takes O(m) space to store Λ. For shift
and rotation invariant kernels, it takes O(M ) space to store
Φ− (tj ), j ∈ 1, ..., M and takes O(d) (d = 2m) space to
store Λ and D. For bth -order arc-cosine kernels, it only
needs to store one parameter Cb and takes O(d) space to
store Λ and D. By setting M ≤ d, the total space complexity to store the projection matrix is O(d).

6. Empirical Studies
We compare SSF maps with feature maps obtained by fully
Gaussian (Cho & Saul, 2009; Rahimi et al., 2007), the Cir-

Figure 2. Speedup of the Feature Maps Construction

6.1. Convergence and Speedup
First, the convergence of the logarithmic energy ( −J(Λ)
in equation (34)) with (m, n) = (160, 1600) is shown in
Figure 1. From Figure 1, we find that it takes less than
ten iterations (i.e. T < 10) for Algorithm 1 to find a local
optimum.
Second, the speedup results of all methods are shown in

Spherical Structured Feature Maps for Kernel Approximation

(a)

(d)

f
kK−K
kF
kKkF

f
kK−K
k∞
kKk∞

for Gaussian Kernel

(b)

for Gaussian Kernel

(e)

f
kK−K
kF
kKkF

f
kK−K
k∞
kKk∞

for Zero-order Arc Kernel

(c)

for Zero-order Arc Kernel

(f)

f
kK−K
kF
kKkF

f
kK−K
k∞
kKk∞

for First-order Arc Kernel

for First-order Arc Kernel

Figure 3. Relative Mean and Max Reconstruction Error for Gaussian, Zero-order and First-order Arc-cosine Kernel on MNIST

Figure 2. We set N = 2d for all the methods. The speedup
of fully Gaussian projection is the baseline. We can observe
that the speedup of QMC with Halton set is constant as the
dimension d increases and is slower than the baseline. The
speedup of both SSF maps and the Circulant increase fast
as dimension increases, which is consistent with theoretical
analysis. The speedup of Sobol set is not shown because
the inbuilt Sobolset routine of MATLAB does not support
dimension larger than 1,111.

The reconstruction error in the experiments is the mean
value over 10 independent runs. The dimensions of the feature maps are set to {2×d, 3×d, 4×d, 5×d}, where d is the
dimension of the data. For MNIST and CIFAR10 dataset,
each run randomly select 2,000 samples to construct the
Gram matrix. The mean value of the reconstruction errors
with different norms on MNIST are shown in Figure 3. Results on the other datasets are similar to that of Figure 3.
One can refer to the supplementary material for results on
other datasets.

6.2. Approximation Accuracy

Figure 3 shows that the feature maps obtained with fully
Gaussian matrix, the Circulant matrix, QMC with Halton
set and QMC with Sobol set have similar reconstruction
error. SSF maps have the smallest approximation error
among five methods. Especially for the first-order arccosine kernel, it achieves nearly one-fifth relative mean error and one-seventh relative max error of other methods.
Moreover, even if M = 1, SSF maps can achieve about
one-third relative mean error and half of the relative max
error of other methods for Gaussian Kernel approximation.

We evaluate reconstruction error of Gaussian kernel, zeroorder arc-cosine kernel and first-order arc-cosine kernel on
CIFAR10 (Krizhevsky & Hinton, 2009), MNIST (LeCun
& Cortes, 2010), usps and dna dataset. MNIST is a handwritten digit image dataset, which contains 70,000 samples
with 784-dimensional features(pixel). For CIFAR10 with
60,000 samples, the 320-dimensional gist feature (Gong
et al., 2013) are employed in the experiments. Both the
e
kK−K
kF
relative Frobenius error (i.e.
kKkF ) and the relative
e
kK−K
k∞
element-wise maximum error (i.e.
kKk∞ ) are evalue denote the exact and approximated
ated, where K and K
Gram matrices respectively. The Frobenius norm and
the
elementwise maximum norm are defined as kXkF =
qP
P
2
i
j |Xij | and kXk∞ = max |Xij | respectively.
i,j

7. Conclusion
We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bth order arc-cosine kernels. SSF maps can achieve computation and storage efficiency as well as better approximation
accuracy.

Spherical Structured Feature Maps for Kernel Approximation

Acknowledgements
We thank the anonymous reviewers for their valuable comments and suggestions.

References
An, Senjian, Boussaid, Farid, and Bennamoun, Mohammed. How can deep rectifier networks achieve linear
separability and preserve distances? In ICML, pp. 514–
523, 2015.
Avron, Haim, Sindhwani, Vikas, Yang, Jiyan, and Mahoney, Michael W. Quasi-monte carlo feature maps for
shift-invariant kernels. Journal of Machine Learning Research, 17(120):1–38, 2016.
Brauchart, J, Saff, E, Sloan, I, and Womersley, R. Qmc
designs: optimal order quasi monte carlo integration
schemes on the sphere. Mathematics of computation, 83
(290):2821–2851, 2014.
Brauchart, Johann S and Grabner, Peter J. Distributing
many points on spheres: minimal energy and designs.
Journal of Complexity, 31(3):293–326, 2015.
Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.
Cho, Youngmin and Saul, Lawrence K. Kernel methods
for deep learning. In Advances in neural information
processing systems, pp. 342–350, 2009.
Choromanski, Krzysztof and Sindhwani, Vikas. Recycling
randomness with structure for sublinear time kernel expansions. 2016.
Cutajar, Kurt, Bonilla, Edwin V, Michiardi, Pietro, and Filippone, Maurizio. Practical learning of deep gaussian
processes via random fourier features. arXiv preprint
arXiv:1610.04386, 2016.
Dick, Josef, Kuo, Frances Y, and Sloan, Ian H. Highdimensional integration: the quasi-monte carlo way.
Acta Numerica, 22:133–288, 2013.
Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,
Xiang-Rui, and Lin, Chih-Jen. Liblinear: A library for
large linear classification. Journal of machine learning
research, 9(Aug):1871–1874, 2008.
Feng, Chang, Hu, Qinghua, and Liao, Shizhong. Random
feature mapping with signed circulant matrix projection.
In IJCAI, pp. 3490–3496, 2015.
Gong, Yunchao, Lazebnik, Svetlana, Gordo, Albert, and
Perronnin, Florent. Iterative quantization: A procrustean

approach to learning binary codes for large-scale image
retrieval. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 35(12):2916–2929, 2013.
Götz, Mario. On the riesz energy of measures. Journal of
Approximation Theory, 122(1):62–78, 2003.
Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. 2009.
Le, Quoc, Sarlós, Tamás, and Smola, Alex. Fastfoodapproximating kernel expansions in loglinear time. In
Proceedings of the international conference on machine
learning, 2013.
Le, Zichao Yang Alexander J Smola and Wilson, Song Andrew Gordon. A la cartelearning fast kernels. 38, 2015.
LeCun, Yann and Cortes, Corinna. MNIST handwritten
digit database. 2010. URL http://yann.lecun.
com/exdb/mnist/.
Oliva, Junier B, Dubey, Avinava, Poczos, Barnabas,
Schneider, Jeff, and Xing, Eric P. Bayesian nonparametric kernel-learning. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pp. 1078–1086, 2016.
Rahimi, Ali and Recht, Benjamin. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in neural information processing systems, pp. 1313–1320, 2009.
Rahimi, Ali, Recht, Benjamin, et al. Random features for
large-scale kernel machines. In NIPS, volume 3, pp. 5,
2007.
Rasmussen, Carl Edward. Gaussian processes for machine
learning. 2006.
Rudin, Walter. Fourier analysis on groups. John Wiley &
Sons, 2011.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing
systems, pp. 2951–2959, 2012.
Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and
Seeger, Matthias. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv
preprint arXiv:0912.3995, 2009.
Sutherland, Dougal J and Schneider, Jeff.
On the
error of random fourier features.
arXiv preprint
arXiv:1506.02785, 2015.
Yang, J., Sindhwani. V. Avron H. Mahoney M. Quasimonte carlo feature maps for shift-invariant kernels.
ICML, 32, 2014.

