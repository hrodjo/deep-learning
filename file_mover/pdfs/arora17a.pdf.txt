Generalization and Equilibrium in Generative Adversarial Nets (GANs)
Sanjeev Arora 1 Rong Ge 2 Yingyu Liang 1 Tengyu Ma 1 Yi Zhang 1

Abstract
It is shown that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear
successful but the trained distribution may be
far from target distribution in standard metrics.
However, generalization does occur for a weaker
metric called neural net distance. It is also shown
that an approximate pure equilibrium exists in the
discriminator/generator game for a natural training objective (Wasserstein) when generator capacity and training set sizes are moderate. This
existence of equilibrium inspires MIX + GAN protocol, which can be combined with any existing
GAN training, and empirically shown to improve
some of them.

1. Introduction
Generative Adversarial Networks (GANs) (Goodfellow
et al., 2014) have become one of the dominant methods for
fitting generative models to complicated real-life data, and
even found unusual uses such as designing good cryptographic primitives (Abadi & Andersen, 2016). See a survey
by (Goodfellow, 2016). Various novel architectures and
training objectives were introduced to address perceived
shortcomings of the original idea, leading to more stable
training and more realistic generative models in practice
(see (Odena et al., 2016; Huang et al., 2017; Radford
et al., 2016; Tolstikhin et al., 2017; Salimans et al., 2016;
Jiwoong Im et al., 2016; Durugkar et al., 2016) and the reference therein).
The goal is to train a generator deep net whose input is
a standard Gaussian, and whose output is a sample from
some distribution D on Rd , which has to be close to some
target distribution Dreal (which could be, say, real-life imAuthors listed in alphabetical order. 1 Princeton University, Princeton NJ 2 Duke University, Durham NC. Correspondence to: Rong Ge <rongge@cs.duke.edu>, Yi Zhang
<y.zhang@cs.princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. Probability density Dreal with many peaks and valleys

ages represented using raw pixels). The training uses samples from Dreal and together with the generator net also
trains a discriminator deep net trying to maximise its ability
to distinguish between samples from Dreal and D. So long
as the discriminator is successful at this task with nonzero
probability, its success can be used to generate a feedback
(using backpropagation) to the generator, thus improving
its distribution D. Training is continued until the generator
wins, meaning that the discriminator can do no better than
random guessing when deciding whether or not a particular
sample came from D or Dreal . This basic iterative framework has been tried with many training objectives; see Section 2. But it has been unclear what to conclude when the
generator wins this game: is D close to Dreal in some metric? One seems to need some extension of generalization
theory that would imply such a conclusion. The hurdle is
that distribution Dreal could be complicated and may have
many peaks and valleys; see Figure 1. The number of peaks
(modes) may even be exponential in d. (Recall the curse of
dimensionality: in d dimensions there are exp(d) directions
whose pairwise angle exceeds say ⇡/3, and each could be
the site of a peak.) Whereas the number of samples from
Dreal (and from D for that matter) used in the training is
a lot fewer, and thus may not reflect most of the peaks and
valleys of Dreal .
A standard analysis due to (Goodfellow et al., 2014) shows
that when the discriminator capacity (= number of parameters) and number of samples is “large enough”, then a win
by the generator implies that D is very close to Dreal (see
Section 2). But the discussion in the previous paragraph
raises the possibility that “sufficiently large” in this analysis may need to be exp(d).
Another open theoretical issue is whether an equilibrium
always exists in this game between generator and discrim-

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

inator. Just as a zero gradient is a necessary condition for
standard optimization to halt, the corresponding necessary
condition in a two-player game is an equilibrium. Conceivably some of the instability often observed while training GANs could just arise because of lack of equilibrium.
(Recently Arjovsky et al. (2017) suggest that using their
Wasserstein objective in practice reduces instability, but
we still lack proof of existence of an equilibrium.) Standard game theory is of no help here because we need a socalled pure equilibrium, and simple counter-examples such
as rock/paper/scissors show that it doesn’t exist in general1 .
1.1. Our Contributions
We formally define generalization for GANs in Section 3
and show that for previously studied notions of distance
between distributions, generalization is not guaranteed
(Lemma 1). In fact we show that the generator can win
even when D and Dreal are arbitrarily far in any standard
metric.
However, we can guarantee some weaker notion of generalization by introducing a new metric on distributions, the
neural net distance. We show that generalization does happen with moderate number of training examples (i.e., when
the generator wins, the two distributions must be close in
neural net distance). However, this weaker metric comes at
a cost: it can be near-zero even when the trained and target
distributions are very far (Section 3.1)
To explore the existence of equilibria we turn in Section 4
to infinite mixtures of generator deep nets. These are
clearly vastly more expressive than a single generator net:
e.g., a standard result in bayesian nonparametrics says that
every probability density is closely approximable by an infinite mixture of Gaussians (Ghosh et al., 2003). Thus unsurprisingly, an infinite mixture should win the game. We
then prove rigorously that even a finite mixture of fairly
reasonable size can closely approximate the performance
of the infinite mixture (Theorem 4.2).

training that we call MIX+GAN. It can be added on top of
any existing GAN training procedure, including those that
use divergence objectives. Experiments in Section 6 show
that for several previous techniques, MIX+GAN stabilizes
the training, and in some cases improves the performance.

2. Preliminaries
Notations. Throughout the paper we use d for the dimension of samples, and p for the number of parameters in the
generator/discriminator. In Section 3 we use m for number
of samples.
Generators and discriminators. Let {Gu , u 2 U }
(U ⇢ Rp ) denote the class of generators, where Gu is a
function — which is often a neural network in practice —
from R` ! Rd indexed by u that denotes the parameters
of the generators. Here U denotes the possible ranges
of the parameters and without loss of generality we
assume U is a subset of the unit ball2 . The generator Gu
defines a distribution DGu as follows: generate h from
`-dimensional spherical Gaussian distribution and then
apply Gu on h and generate a sample x = Gu (h) of the
distribution DGu . We drop the subscript u in DGu when
it’s clear from context.
Let {Dv , v 2 V} denote the class of discriminators, where
Dv is function from Rd to [0, 1] and v is the parameters of
Dv . The value Dv (x) is usually interpreted as the probability that the sample x comes from the real distribution Dreal
(as opposed to the generated distribution DG ).

We assume Gu and Dv are L-Lipschitz with respect to their
parameters. That is, for all u, u0 2 U and any input h, we
have kGu (h) Gu0 (h)k  Lku u0 k (similar for D).

Notice, this is distinct from the assumption (which we
will also sometimes make) that functions Gu , Dv are Lipschitz: that focuses on the change in function value when
we change x, while keeping u, v fixed3 .

This insight also allows us to show for a natural GAN setting with Wasserstein objective there exists an approximate
equilibrium that is pure. (Roughly speaking, an approximate equilibrium is one in which neither of the players can
gain much by deviating from their strategies.)

Objective functions. The standard GAN training (Goodfellow et al., 2014) consists of training parameters u, v so
as to optimize an objective function:

This existence proof for an approximate equilibrium unfortunately involves a quadratic blowup in the “size” of the
generator (which is still better than the naive exponential
blowup one might expect). Improving this is left for future
theoretical work. But we propose a heuristic approximation to the mixture idea to introduce a new framework for

(1)
Intuitively, this says that the discriminator Dv should give
high values Dv (x) to the real samples and low values
Dv (x) to the generated examples. The log function was
suggested because of its interpretation as the likelihood,

1

Such counterexamples are easily turned into toy GAN scenarios with generator and discriminator having finite capacity, and
the game lacks a pure equilibrium. See supplementary material.

min max

E

u2U v2V x⇠Dreal

2

[log Dv (x)] +

E

x⇠DGu

[log(1

Dv (x))].

Otherwise we can scale the parameter properly by changing
the parameterization.
3
Both Lipschitz parameters can be exponential in the number
of layers in the neural net, however our Theorems only depend on
the log of the Lipschitz parameters

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

and it also has a nice information-theoretic interpretation
described below. However, in practice it can cause problems since log x ! 1 as x ! 0. The objective still
makes intuitive sense if we replace log by any monotone
function : [0, 1] ! R, which yields the objective:
min max

E

u2U v2V x⇠Dreal

[ (Dv (x))]+

E

x⇠DGu

[ (1 Dv (x))]. (2)

We call function the measuring function. It should be
concave so that when Dreal and DG are the same distribution, the best strategy for the discriminator is just to output
1/2 and the optimal value is 2 (1/2). In later proofs, we
will require to be bounded and Lipschitz. Indeed, in practice training often uses (x) = log( + (1
)x) (which
takes values in [log , 0] and is 1/ -Lipschitz) and the recently proposed Wasserstein GAN (Arjovsky et al., 2017)
objective uses (x) = x.
Training with finite samples.
The objective function (2) assumes we have infinite number of samples
from Dreal to estimate the value Ex⇠Dreal [ (Dv (x))].
With finite training
examples x1 , . . . , xm ⇠ Dreal ,
Pm
1
one uses m
[
(D
v (xi ))] to estimate the quantity
i=1
Ex⇠Dreal [ (Dv (x))]. We call the distribution that gives
1/m probability to each of the xi ’s the empirical version
of the real distribution. Similarly, one can use a empirical
version to estimate Ex⇠DGu [ (1 Dv (x))].
Standard interpretation via distance between distributions. Towards analyzing GANs, researchers have assumed
access to infinite number of examples and that the discriminator is chosen optimally within some large class of functions that contain all possible neural nets. This often allows computing analytically the optimal discriminator and
therefore removing the maximum operation from the objective (2), which leads to some interpretation of how and
in what sense the resulting distribution DG is close to the
true distribution Dreal .

Using the original objective function (1), then the optimal
choice among all the possible functions from Rd ! (0, 1)
real (x)
is D(x) = PrealP(x)+P
, as shown in (Goodfellow et al.,
G (x)
2014). Here Preal (x) is the density of x in the real distribution, and PG (x) is the density of x in the distribution generated by generator G. Using this discriminator —
though it’s computationally infeasible to obtain it — one
can show that the minimization problem over the generator correspond to minimizing the Jensen-Shannon (JS) divergence between the true distribution Dreal and the generative distribution DG . Recall that for two distributions
µ and ⌫, the JS divergence is defined by dJS (µ, ⌫) =
µ+⌫
µ+⌫
1
2 (KL(µk 2 ) + KL(⌫k 2 )).
Other measuring functions
and choice of discriminator class leads to different distance function between
distribution other than JS divergence. Notably, (Ar-

jovsky et al., 2017) shows that when (t) = t, and
the discriminator is chosen among all 1-Lipschitz functions, maxing out the discriminator, the generator is
attempting to minimize the Wasserstein distance between Dreal and Du (h). Recall that Wasserstein distance between µ and ⌫ is defined as dW (µ, ⌫) =
supD is 1-Lipschitz |Ex⇠µ [D(x)] Ex⇠⌫ [D(x)]| .

3. Generalization Theory for GANs
The above interpretation of GANs in terms of minimizing
distance (such as JS divergence and Wasserstein distance)
between the real distribution and the generated distribution
relies on two crucial assumptions: (i) very expressive class
of discriminators such as the set of all bounded discriminator or the set of all 1-Lipschitz discriminators, and (ii) very
large number of examples to compute/estimate the objective (1) or (2). Neither assumption holds in practice, and
we will show next that this greatly affects the generalization ability, a notion we introduce in Section 3.1.
3.1. Definition of Generalization
Our definition is motivated from supervised classification,
where training is said to generalize if the training and test
error closely track each other. (Since the purpose of GANs
training is to learn a distribution, one could also consider
a stronger definition of successful training, as discussed in
Section 3.4.)
Let x1 , . . . , xm be the training examples, and let D̂real
denote the uniform distribution over x1 , . . . , xm . Similarly, let Gu (h1 ), . . . , Gu (hr ) be a set of r examples from
the generated distribution DG . In the training of GANs,
one implicitly uses Ex⇠D̂real [ (Dv (x))] to approximate
the quantity Ex⇠Dreal [ (Dv (x))]. Inspired by the observation that the training objective of GANs and its variants is
to minimize some distance (or divergence) d(·, ·) between
Dreal and DG using finite samples, we define the generalization of GANs as follows:
Definition 1. A divergence or distance d(·, ·) between distributions generalizes with m training examples and error
" if for the learnt distribution DG , the following holds with
high probability,
d(Dreal , DG )

d(D̂real , D̂G )  "

(3)

where D̂real is an empirical version of the true distribution
(with m samples) and D̂G is an empirical version of the
generated distribution with polynomial number of samples.
In words, generalization in GANs means that the population distance between the true and generated distribution is
close to the empirical distance between the empirical distributions. Our target is to make the former distance small,

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

whereas the latter one is what we can access and minimize
in practice. The definition allows only polynomial number of samples from the generated distribution because the
training algorithm should run in polynomial time.

When (t) = t, we have that dF , is a distance function 4
, and with slightly abuse of notation we write it simply as
dF (µ, ⌫) = sup

E [D(x)]

D2F x⇠µ

3.2. JS Divergence and Wasserstein don’t Generalize

E [D(x)] .

x⇠⌫

As a warm-up, we show that JS divergence and Wasserstein
distance don’t generalize with any polynomial number of
examples because the population distance (divergence) is
not reflected by the empirical distance.

Example 1. When (t) = log(t) and F
{all functions from Rd to [0, 1]}, we have that dF , is
same as JS divergence. When (t) = t and F
{all 1-Lipschitz functions from Rd to [0, 1]}, then dF ,
the Wasserstein distance.

Lemma 1. Let µ be uniform Gaussian distributions
N (0, d1 I) and µ̂ be an empirical versions of µ with m examples. Then we have dJS (µ, µ̂) = log 2, dW (µ, µ̂) 1.1.

Example 2. Suppose F is a set of neural networks and
(t) = log t, then original GAN objective function is
equivalent to minG dF , (D̂real , D̂G ) .

There are two consequences of Lemma 1. First, consider
the situation where Dreal = DG = µ. Then we have that
dW (Dreal , DG ) = 0 but dW (D̂real , D̂G ) > 1 as long as
we have polynomial number of examples. This violates the
generalization definition equation (3).
Second, consider the case Dreal = µ and DG = D̂real =
µ̂, that is, DG memorizes all of the training examples in
D̂real . In this case, since DG is a discrete distribution with
finite supports, with enough (polynomial) examples, in D̂G ,
effectively we also have that D̂G ⇡ DG . Therefore, we
have that dW (D̂real , D̂G ) ⇡ 0 whereas dW (Dreal , DG ) >
1. In other words, with any polynomial number of examples, it’s possible to overfit to the training examples using
Wasserstein distance. The same argument also applies to
JS divergence. See supplementary material for the formal
proof.
Notice, this result does not contradict the experiments
of (Arjovsky et al., 2017) since they actually use not
Wasserstein distance but a surrogate distance that does generalize, as we show next.
3.3. Generalization Bounds for Neural Net Distance
Which distance measure between Dreal and DG is the
GAN objective actually minimizing and can we analyze
its generalization performance? Towards answering these
questions in full generality (given multiple GANs objectives) we consider the following general distance measure
that unifies JS divergence, Wasserstein distance, and the
neural net distance that we define later in this section.
Definition 2 (F-distance). Let F be a class of functions
from Rd to [0, 1] and be a concave measuring function.
Then the F-divergence with respect to between two distributions µ and ⌫ supported on Rd is defined as
dF , (µ, ⌫) = sup

E [ (D(x))] + E [ (1

D2F x⇠µ

2 (1/2)

x⇠⌫

D(x))]

=
the
=
is

Suppose F is the set of neural networks, and (t) = t, then
the objective function used empirically in (Arjovsky et al.,
2017) is equivalent to minG dF (D̂real , D̂G ) .
GANs training uses F to be a class of neural nets with a
bound p on the number of parameters. We then informally
refer to dF as the neural net distance. The next theorem
establishes generalization in the sense of equation (3) does
hold for it (with a uniform convergence) . We assume that
the measuring function takes values in [
, ] and that it
is L -Lipschitz. Further, F = {Dv , v 2 V} is the class
of discriminators that is L-Lipschitz with respect to the parameters v. As usual, we use p to denote the number of
parameters in v.
Theorem 3.1. In the setting of previous paragraph, let µ, ⌫
be two distributions and µ̂, ⌫ˆ be empirical versions with at
least m samples each. There is a universal constant c such
cp 2 log(LL p/✏)
that when m
, we have with probability
✏2
at least 1 exp( p) over the randomness of µ̂ and ⌫ˆ,
|dF , (µ̂, ⌫ˆ)

dF , (µ, ⌫)|  ✏.

See supplementary material for the proof. The intuition is
that there aren’t too many distinct discriminators, and thus
given enough samples the expectation over the empirical
distribution converges to the expectation over the true distribution for all discriminators.
Theorem 3.1 shows that the neural network divergence (and
neural network distance) has a much better generalization
properties than Jensen-Shannon divergence or Wasserstein
distance. If the GAN successfully minimized the neural
network divergence between the empirical distributions,
that is, d(D̂real , D̂G ), then we know the neural network
divergence d(Dreal , DG ) between the distributions Dreal
and DG is also small. It is possible to change the proof to
also show that this generalization continues to hold at every
iteration of the training. See supplementary material.
4
Technically it is a pseudometric. This is also known as integral probability metrics(Müller, 1997).

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

3.4. Generalization vs Diversity
Since the final goal of GANs training is to learn a distribution, it is worth understanding that though weak generalization in the sense of Section 3.3 is guaranteed, it comes
with a cost (albeit a necessary one). For JS divergence and
Wasserstein distance, when the distance between two distributions µ, ⌫ is small, it is safe to conclude that the distributions µ and ⌫ are almost the same. However, the neural
net distance dN N (µ, ⌫) can be small even if µ, ⌫ are not
very close. As a simple Corollary of Lemma 3.1, we obtain:
Corollary 3.1 (Low-capacity discriminators cannot detect
lack of diversity). Let µ̂ be the empirical version of distribution µ with m samples. There is a some universal concp 2 log(LL p/✏)
stant c such that when m
, we have that
✏2
with probability at least 1 exp( p), dF , (µ, µ̂)  ✏.
That is, the neural network distance for nets with p parameters cannot distinguish between a distribution µ and a distribution with support Õ(p/✏2 ). In fact the proof still works
if the disriminator is allowed to take many more samples
from µ; the reason they don’t help is that its capacity is
limited to p.

4. Expressive Power and Equilibrium
Section 3 clarified the notion of generalization for GANs:
namely, neural-net divergence between the generated distribution D and Dreal on the empirical samples closely
tracks the divergence on the full distribution (i.e., unseen
samples). But this doesn’t explain why in practice the generator usually “wins” so that the discriminator is unable to
do much better than random guessing at the end. In other
words, was it sheer luck that so many real-life distributions
Dreal turned out to be close in neural-net distance to a distribution produced by a fairly compact neural net? This
section suggests no luck may be needed.
The explanation starts with a thought experiment. Imagine allowing a much more powerful generator, namely, an
infinite mixture of deep nets, each of size p. So long as
the deep net class is capable of generating simple gaussians, such mixtures are quite powerful, since a classical
result says that an infinite mixtures of simple gaussians can
closely approximate Dreal . Thus an infinite mixture of
deep net generators will “win” the GAN game, not only
against a discriminator that is a small deep net but also
against more powerful discriminators (e.g., any Lipschitz
function).
The next stage in the thought experiment is to imagine a
much less powerful generator, which is a mix of only a few
deep nets, not infinitely many. Simple counterexamples
show that now the distribution D will not closely approxi-

mate arbitrary Dreal with respect to natural metrics like `p .
Nevertheless, could the generator still win the GAN game
against a deep net of bounded capacity (i.e., the deep net is
unable to distinguish D and Dreal )? We show it can.

INFORMAL THEOREM : If the discriminator is a deep net
with p parameters, then a mixture of Õ(p log(p/✏)/✏2 ) generator nets can produce a distribution D that the discriminator will be unable to distinguish from Dreal with probability more than ✏. (Here Õ(·) notation hides some nuisance factors.)

This informal theorem is also a component of our result below about the existence of an approximate pure
equilibrium. With current technique this existence result
seems sensitive to the measuring function , and works for
(x) = x (i.e., Wasserstein GAN). For other we only
show existence of mixed equilibria with small mixtures.
4.1. General : Mixed Equilibrium
For general measuring function we can only show the
existence of a mixed equilibrium, where we allow the discriminator and generator to be finite mixtures of deep nets.
For a class of generators {Gu , u 2 U } and a class of discriminators {Dv , v 2 V}, we can define the payoff F (u, v)
of the game between generator and discriminator
F (u, v) =

E

x⇠Dreal

[ (Dv (x))] +

E [ (1

x⇠DG

Dv (x)))]. (4)

Of course as we discussed in the previous section, in practice these expectations should be with respect to the empirical distributions. Our discussions in this section does
not depend on the distributions Dreal and Dh , so we define
F (u, v) this way for simplicity.
The well-known min-max theorem (v. Neumann, 1928) in
game theory shows if both players are allowed to play
mixed strategies then the game has a min-max solution. A
mixed strategy for the generator is just a distribution Du
supported on U , and one for discriminator is a distribution
Dv supported on V.

Theorem 4.1 (vonNeumann). There exists value
V , and a pair of mixed strategies (Su ,Sv ) s.t.
8v, Eu⇠Su [F (u, v)]  V , 8u, Ev⇠Sv [F (u, v)] V.
Note that this equilibrium involves both parties announcing their strategies Su , Sv at the start, such that neither will
have any incentive to change their strategy after studying
the opponent’s strategy. The payoff is generated by the generator first sample u ⇠ Su , h ⇠ Dh , and then generate an
example x = Gu (h). Therefore, the mixed generator is just
a linear mixture of generators. The discriminator will first
sample v ⇠ Sv , and then output Dv (x). Note that in general this is very different from a discriminator D that outputs Ev⇠Sv [Dv (x)], because the measuring function is in

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

general nonlinear. In particular, the correct payoff function
for a mixture of discriminator is:
E [F (u, v)]

v⇠Sv

=

E

x⇠Dreal
v⇠Sv

[ (Dv (x))] + E [ (1
h⇠Dh
v⇠Sv

Dv (Gu (h)))].

Of course, this equilibrium involving an infinite mixture
makes little sense in practice. We show that (as is folklore in game theory (Lipton & Young, 1994)) that we can
approximate this min-max solution with mixture of finitely
many generators and discriminators. More precisely we
define ✏-approximate equilibrium:
Definition 3. A pair of mixed strategies (Su , Sv ) is an ✏approximate equilibrium, if for some value V 8v 2 V,
Eu⇠Su [F (u, v)]  V + ✏; 8u 2 U , Ev⇠Sv [F (u, v)]
V ✏. If the strategies Su , Sv are pure strategies, then this
pair is called an ✏-approximate pure equilibrium.
Suppose is L -Lipschitz and bounded in [
, ], the
generator and discriminators are L-Lipschitz with respect
to the parameters and L0 -Lipschitz with respect to inputs, in
this setting we can formalize the above Informal Theorem
as follows:
Theorem 4.2. In the settings above, there is a universal constant C > 0 such that for any ✏, there exists
C 2 p log(LL0 L ·p/✏)
T =
generators Gu1 , . . . , GuT and T
✏2
discriminators Dv1 , . . . , DvT , let Su be a uniform distribution on ui and Sv be a uniform distribution on vi , then
(Su , Sv ) is an ✏-approximate equilibrium. Furthermore, in
this equilibrium the generator “wins,” meaning discriminators cannot do better than random guessing.
The proof uses a standard probabilistic argument and epsilon net argument to show that if we sample T generators
and discriminators from infinite mixture, they form an approximate equilibrium with high probability. For the second part, we use the fact that every distribution can be approximated by infinite mixture of Gaussians, so the generator must be able to approximate the real distribution Dreal
and win. Therefore indeed a mixture of Õ(p) generators
can achieve an ✏-approximate equilibrium. See supplementary material for details.
In the special case of (x) = x (Wasserstein GAN), we
show that a mixture of generator/discriminator is equivalent to a specially designed, larger generator/discriminator,
therefore an approximate pure equilibrium exists. See supplementary material for more details.
Theorem 4.3. Suppose the generator and discriminator
are both k-layer neural networks (k
2) with p parameters, and the last layer uses ReLU activation function. In
the setting of Theorem 4.2, when (x) = x there exists

k + 1-layer neural⇣ networks of generators
⌘ G and discrim2 2
p log(LL0 L ·p/✏)
inator D with O
parameters, such
✏2
that there exists an ✏-approximate pure equilibrium. Furthermore, if the generator is capable of generating a Gaussian then the value V = 1.

5. MIX+GANs
Theorem 4.2 show that using a mixture of (not too many)
generators and discriminators guarantees existence of approximate equilibrium. This suggests that using a mixture
may lead to more stable training.
Of course, it is impractical to use very large mixtures, so
we propose MIX+GAN: use a mixture of T components,
where T is as large as allowed by size of GPU memory
(usually T  5). Namely, train a mixture of T generators {Gui , i 2 [T ]} and T discriminators {Dvi , i 2 [T ]})
which share the same network architecture but have their
own trainable parameters. Maintaining a mixture means
of course maintaining a weight wui for the generator Gui
which corresponds to the probability of selecting the output
of Gui . These weights are also updated via backpropagation. This heuristic can be combined with existing methods like DCGAN (Radford et al., 2016), WASSERSTEIN GAN (Arjovsky et al., 2017) etc., giving us new training
methods MIX+DCGAN, MIX+WASSERSTEIN GAN etc.
We use exponentiated gradient (Kivinen & Warmuth,
1997): store the log-probabilities {↵ui , i 2 [T ]}, and then
obtain the weights
by applying soft-max function on them:
↵u
wui = PTe ei↵uk , i 2 [T ].
k=1

Note that our algorithm is maintaining weights on different
generators and discriminators. This is very different from
the idea of boosting where weights are maintained on samples. A DAGAN (Tolstikhin et al., 2017) uses ideas similar
to boosting and maintains weights on training examples.

Given payoff function F , training MIX+GAN boils down
to optimizing:
min

max

E

{ui },{↵ui } {vj },{↵vj } i,j2[T ]

=

min

max

{ui },{↵ui } {vj },{↵vj }

F (ui , vj )

X

wui wvj F (ui , vj ).

i,j2[T ]

Here the payoff function is the same as Equation (4). We
use both measuring functions (x) = log x (for original GAN) and (x) = x (for WASSERSTEIN GAN). In
our experiments we alternatively update generators’ and
discriminators’ parameters as well as their corresponding
log-probabilities using ADAM (Kingma & Ba, 2015), with
learning rate lr = 0.0001.
Empirically, it is observed that some components of the
mixture tend to collapse and their weights diminish during

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

the training. To encourage full use of the mixture capacity, we add to the training objective an entropy regularizer
that discourages the weightsP
being far away from uniform:
T
Rent ({wui }, {wvi }) = T1 i=1 (log(wui ) + log(wvi )).

6. Experiments

In this section, we first explore the qualitative benefits of our method on image generation tasks: MNIST
dataset (LeCun et al., 1998) of hand-written digits and the
CelebA (Liu et al., 2015) dataset of human faces. Then
for more quantitative evaluation we use the CIFAR-10
dataset (Krizhevsky & Hinton, 2009) and use the Inception
Score introduced in (Salimans et al., 2016). MNIST contains 60,000 labeled 28⇥28-sized images of hand-written
digits, CelebA contains over 200K 108⇥108-sized images
of human faces (we crop the center 64⇥64 pixels for our
experiments), and CIFAR-10 has 60,000 labeled 32⇥32sized RGB natural images which fall into 10 categories.
To reinforce the point that this technique works out of the
box, no extensive hyper-parameter search or tuning is necessary. Please refer to our code for experimental setup. 5
6.1. Qualitative Results
The DCGAN architecture (Radford et al., 2016) uses deep
convolutional nets as generators and discriminators. We
trained MIX+DCGAN on MNIST and CelebA using the
authors’ code as a black box, and compared visual qualities
of generated images to those by DCGAN.
Results on MNIST is shown in Figure 2. In this experiment, the baseline DCGAN consists of a pair of a generator and a discriminator, which are 5-layer deconvoluitonal
neural networks, and are conditioned on image labels. Our
MIX+DCGAN model consists of a mixture of such DCGANs so that it has 3 generators and 3 discriminators. We
observe that our method produces somewhat cleaner digits
than the baseline (note the fuzziness in the latter).
Results on CelebA dataset are also in Figure 2, using the
same architecture as for MNIST, except the models are not
conditioned on image labels anymore. Again, our method
generates more faithful and more diverse samples than the
baseline. Note that one may need to zoom in to fully perceive the difference, since both the two datasets are rather
easy for DCGAN.
6.2. Quantitative Results
Now we turn to quantitative measurement using Inception
Score. Our method is applied to DCGAN and WASSER STEIN GAN (Arjovsky et al., 2017), and throughout, mix5

Related code is public online at https://github.com/
PrincetonML/MIX-plus-GANs.git

Figure 2. MNIST and CelebA Samples. Digits and Faces generated from (a) MIX+DCGAN. (b) DCGAN.

tures of 5 generators and 5 discriminators are used. At first
sight the comparison DCGAN v.s. MIX+DCGAN seems
unfair because the latter uses 5 times the capacity of the former, with corresponding penalty in running time per epoch.
To address this, we also compare our method with larger
versions of DCGAN with roughly the same number of parameters, and we found the former is consistently better
than the later, as detailed below.
To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. (2017),
which is the best variant so far without improved training techniques. The same hyper-parameters are used for
fair comparison. See (Huang et al., 2017) for more details. Similarly, for the MIX+WASSERSTEIN GAN, the
base GAN is identical to that proposed by Arjovsky et al.
(2017) using their hyper-parameter scheme.
For a quantitative comparison, inception score is calculated
for each model, using 50,000 freshly generated samples
that are not used in training. To sample a single image from
our MIX+ models, we first select a generator from the mixture according to their assigned weights {wui }, and then
draw a sample from the selected generator.
Table 1 shows the results on the CIFAR-10 dataset. We find
that, simply by applying our method to the baseline models, our MIX+ models achieve 7.72 v.s. 7.16 on DCGAN,
and 4.04 v.s. 3.82 on WASSERSTEIN GAN. To confirm that
the superiority of MIX+ models is not solely due to more
parameters, we also tested a DCGAN model with 5 times
many parameters (roughly the same number of parameters

Generalization and Equilibrium in Generative Adversarial Nets (GANs)
Table 1. Inception Scores on CIFAR-10. Mixture of DCGANs
achieves higher score than any single-component DCGAN does.
All models except for WASSERSTEIN GAN variants are trained
with labels.
Method
SteinGAN (Wang & Liu, 2016)
Improved GAN (Salimans et al., 2016)
AC-GAN (Odena et al., 2016)
S-GAN (best variant in (Huang et al., 2017))
DCGAN (as reported in (Wang & Liu, 2016))
DCGAN (best variant in (Huang et al., 2017))
DCGAN (5x size)
MIX+DCGAN (with 5 components)
WASSERSTEIN GAN
MIX+WASSERSTEIN GAN (with 5 components)
Real data

Score
6.35
8.09±0.07
8.25 ± 0.07
8.59± 0.12
7.37
7.16±0.10
7.34±0.07
7.72±0.09
3.82±0.06
4.04±0.07
11.24±0.12

Figure 4. Training Curve of MIX+WASSERSTEIN GAN
v.s.
WASSERSTEIN GAN
(Wasserstein
Objective).
MIX+WASSERSTEIN GAN is better towards the end but
loss drops less smoothly, which needs further investigation.

7. Conclusions
The notion of generalization for GANs has been clarified
by introducing a new notion of distance between distributions, the neural net distance. (Whereas popular distances
such as Wasserstein and JS may not generalize.) Assuming the visual cortex also is a deep net (or some network of
moderate capacity) generalization with respect to this metric is in principle sufficient to make the final samples look
realistic to humans, even if the GAN doesn’t actually learn
the true distribution.

Figure 3. Training Curve of MIX+DCGAN v.s. DCGAN (Inception Score). MIX+DCGAN is consistently higher than DCGAN.

as a 5-component MIX+DCGAN), which is tuned using a
grid search over 27 sets of hyper-parameters (learning rates,
dropout rates, and regularization weights). It gets only 7.34
(labeled as ”5x size” in Table 1), which is lower than that
of a MIX+DCGAN. It is unclear how to apply MIX+ to
S-GANs. We tried mixtures of the upper and bottom generators separately, resulting in worse scores somehow. We
leave that for future exploration.
Figure 3 shows how Inception Scores evolve during training. MIX+DCGAN outperforms DCGAN throughout the
entire training process, showing that it makes effective use
of the capacity.
Arjovsky et al. (2017) shows that (approximated) Wasserstein loss, which is the neural network divergence by
our definition, is meaningful because it correlates well
with visual quality of generated samples. Figure 4
shows the training dynamics of neural network divergence
of MIX+WASSERSTEIN GAN v.s. WASSERSTEIN GAN,
which clearly indicates our method is capable of achieving a much lower divergence as well as of improving the
visual quality of generated samples.

One issue raised by our analysis is that the current GANs
objectives cannot even enforce that the synthetic distribution has high diversity (Section 3.4). Furthermore this cannot be fixed by simply providing the discriminator with
more training examples. Possibly some other change to the
GANs setup are needed.
The paper also made progress another unexplained issue
about GANs, by showing that a pure approximate equilibrium exists for a certain natural training objective (Wasserstein) and in which the generator wins the game. No assumption about the target distribution Dreal is needed.
Suspecting that a pure equilibrium may not exist for all objectives, we recommend in practice our MIX+GAN protocol using a small mixture of discriminators and generators.
Our experiments show it improves the quality of several
existing GAN training methods.
Finally, existence of an equilibrium does not imply that a
simple algorithm (in this case, backpropagation) would find
it easily. Understanding convergence remains wide open.

Acknowledgements
This paper was done in part while the authors were hosted
by Simons Institute. We thank Moritz Hardt, Kunal Talwar,
Luca Trevisan, and the referees for useful comments. This
research was supported by NSF, Office of Naval Research,
and the Simons Foundation.

Generalization and Equilibrium in Generative Adversarial Nets (GANs)

References
Abadi, Martı́n and Andersen, David G. Learning to protect communications with adversarial neural cryptography. arXiv preprint arXiv:1610.06918, 2016.
Arjovsky, Martin, Chintala, Soumith, and Bottou, Léon.
Wasserstein gan. arXiv preprint arXiv:1701.07875,
2017.
Durugkar, I., Gemp, I., and Mahadevan, S. Generative
Multi-Adversarial Networks. ArXiv e-prints, November
2016.
Ghosh, Jayanta K, Ghosh, RVJK, and Ramamoorthi, RV.
Bayesian nonparametrics. Technical report, 2003.
Goodfellow, Ian. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,
Bing, Warde-Farley, David, Ozair, Sherjil, Courville,
Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in neural information processing systems,
pp. 2672–2680, 2014.
Huang, Xun, Li, Yixuan, Poursaeed, Omid, Hopcroft, John,
and Belongie, Serge. Stacked generative adversarial
networks. In Computer Vision and Patter Recognition,
2017.
Jiwoong Im, D., Ma, H., Dongjoo Kim, C., and Taylor, G.
Generative Adversarial Parallelization. ArXiv e-prints,
December 2016.
Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. In International Conference on
Learning Representations, 2015.
Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated
gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997.
Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. Technical report,
2009.
LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The mnist database of handwritten digits, 1998.
Lipton, Richard J and Young, Neal E. Simple strategies for
large zero-sum games with applications to complexity
theory. In Proceedings of the twenty-sixth annual ACM
symposium on Theory of computing, pp. 734–740. ACM,
1994.
Liu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou.
Deep learning face attributes in the wild. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 3730–3738, 2015.

Müller, Alfred. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(02):429–443, 1997.
Odena, Augustus, Olah, Christopher, and Shlens, Jonathon.
Conditional image synthesis with auxiliary classifier
gans. arXiv preprint arXiv:1610.09585, 2016.
Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsupervised representation learning with deep convolutional
generative adversarial networks. In International Conference on Learning Representations, 2016.
Salimans, Tim, Goodfellow, Ian, Zaremba, Wojciech, Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016.
Tolstikhin, Ilya, Gelly, Sylvain, Bousquet, Olivier, SimonGabriel, Carl-Johann, and Schölkopf, Bernhard. Adagan: Boosting generative models.
arXiv preprint
arXiv:1701.02386, 2017.
v. Neumann, J. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295–320, 1928.
Wang, Dilin and Liu, Qiang. Learning to draw samples:
With application to amortized mle for generative adversarial learning. Technical report, 2016.

