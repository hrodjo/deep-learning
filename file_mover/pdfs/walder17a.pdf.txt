Fast Bayesian Intensity Estimation for the Permanental Process

Christian J. Walder 1 2 Adrian N. Bishop 1 2 3

Abstract
The Cox process is a stochastic process which
generalises the Poisson process by letting the underlying intensity function itself be a stochastic
process. In this paper we present a fast Bayesian
inference scheme for the permanental process,
a Cox process under which the square root of
the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the
Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple
algorithm which we apply to toy and real-world
problems, obtaining orders of magnitude speed
improvements over previous work.

1. Introduction
The Poisson process is an important model for point data
in which samples of the process are locally finite subsets
of some domain such as time or space. The process is
parametrised by an intensity function, the integral of which
gives the expected number of points in the domain of integration — for a gentle introduction we recommend (Baddeley, 2007). In the typical case of unknown intensity function we may place a non-parametric prior over it via e.g. the
Gaussian Process (GP) and perform Bayesian inference.
Inference under such models is challenging due to both the
GP prior and the non factorial nature of the Poisson process
likelihood (1), which includes an integral of the intensity
function. One may resort to discretising the domain (Rathbun & Cressie, 1994; Møller et al., 1998; Rue et al., 2009)
or performing Monte Carlo approximations (Adams et al.,
2009; Diggle et al., 2013). Fast Laplace approximates were
studied in (Cunningham et al., 2008; Illian et al., 2012;
Flaxman et al., 2015) and variational methods were applied
1

2

Data61, CSIRO, Australia The Australian National University 3 University of Technology Sydney. Correspondence to:
Christian <christian.walder@anu.edu.au>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

in (Lloyd et al., 2015; Kom Samo & Roberts, 2015).
To satisfy non-negativity of the intensity function one
transforms the GP prior. The log-Gaussian Cox Process,
with GP distributed log intensity, has been the subject of
much study; see e.g. (Rathbun & Cressie, 1994; Møller
et al., 1998; Illian et al., 2012; Diggle et al., 2013), Alternative formulations for introducing a GP prior exist, e.g.
(Adams et al., 2009). More recent research has highlighted
the analytical and computational advantages (Lloyd et al.,
2015; 2016; Flaxman et al., 2017; Møller et al., 1998) of the
permanental process, which has GP distributed square root
intensity (Shirai & Takahashi, 2003; McCullagh & Møller,
2006) — we discuss the relationship between these methods and the present work in more detail in subsection 2.2.
In section 2 we introduce the Poisson and permanental processes, and place our work in the context of existing literature. Section 3 reviews Flaxman et al. (2017), slightly
recasting it as regularised maximum likelihood for the permanental process. Our Bayesian scheme is then derived in
section 4. In section 5 we discuss the choice of covariance
function for the GP prior, before presenting some numerical experiments in section 6 and concluding in section 7.

2. The Model
2.1. The Poisson Process
We view the inhomogeneous Poisson process on Ω as a
distribution over locally finite subsets of Ω. The number N (X ) of elements in some X ⊆ Ω is assumed to
be
R distributed as Poisson(Λ(X , µ)), where Λ(S, µ) :=
λ(x)dµ(x) gives the mean of the Poisson distribux∈S
tion. It turns out that this implies the likelihood function
m

p ({xi }i=1 |λ, Ω) =

m
Y

λ(xi ) exp (−Λ(Ω)) .

(1)

i=1

2.2. Latent Gaussian Process Intensities
To model unknown λ(x), we employ a non-parametric
prior over functions, namely the Gaussian process (GP).
To ensure that λ is non-negative valued we include a deterministic “link” function g : R → R+ so that we have
the prior over λ defined by λ = g ◦ f and f ∼ GP(k),
where k is the covariance function for f . The most com-

Fast Bayesian Permanental Processes

mon choice for g is the exponential function exp(·), leading to the log-Gaussian Cox process (LGCP) (Møller et al.,
1998). Recently Adams et al. (2009) employed the transformation g(z) = λ∗ (1 + exp(−z))−1 , which permits efficient sampling via thinning (Lewis & Shedler, 1979) due
to the bound 0 ≤ λ(x) ≤ λ∗ .
2.2.1. P ERMANENTAL P ROCESSES : S QUARED L INK
F UNCTION
In this paper we focus on the choice g(z) = 21 z 2 , known
as the permanental process (Shirai & Takahashi, 2003; McCullagh & Møller, 2006). Two recent papers have demonstrated the analytical and computational advantages of this
link function.
1. Flaxman et al. (2017) derived a non-probabilistic regularisation based algorithm which we review in section 3, and which exploited properties of reproducing kernel Hilbert spaces. The present work generalises their result, providing probabilistic predictions
and Bayesian model selection. Our derivation is by
necessity entirely different to Flaxman et al. (2017), as
their representer theorem (Schölkopf et al., 2001) argument is insufficient for our probabilistic setting (see
e.g. subsubsection 4.1.6).
2. (Lloyd et al., 2015) derived a variational approximation to a Bayesian model with the squared link function, based on an inducing variable scheme similar to
(Titsias, 2009), and exploiting the tractability of certain required integrals. The present work has the advantage of 1) not requiring the inducing point approximation, 2) being free of non-closed form expressions
such as their G̃ and 3) being simpler to implement
and orders of magnitude faster in practice while, as
we demonstrate, exhibiting comparable predictive accuracy.

3. Regularised Maximum Likelihood
Flaxman et al. (2017) combined (1) with the regularisation
term kf k2H(k) , leading to the regularised maximum likelihood estimator for f , namely fˆ :=
argmax
f

m
X


1
1
2
kf kL2 (Ω,µ) + kf k2H(k) ,
log f 2 (xi ) −
2
2|
{z
}
i=1
:=kf k2H(k,Ω,µ)

(2)
where we have implicitly defined the new RKHS
H(k, Ω, µ) := H(k̃). Now, provided we can compute
the associated new reproducing kernel k̃, then we may
appeal to the representer theorem (Kimeldorf & Wahba,
1971) in order to compute the fˆ, which takes the form

Pm

i=1 αi k̃(xi , ·) for some αi . The function k̃ may be expressed in terms of the Mercer expansion (Mercer, 1909)

k(x, y) =

N
X

λi φi (x)φi (y),

(3)

i=1
2
where φi are
Porthonormal in L (Ω, µ). To satisfy for arbitrary f = i wi φi the reproducing property (Aronszajn,
1950)
D
E
X
X
:= f (x) =
k(x, ·),
wi φ(·)i
wi , φi (x) (4)
H(k)

i

i

we let φi be orthogonal in H(k), obtaining hφi , φj i =
P
P 2
2
δij λ−1
i . Hence, k
i wi φi kH(k) =
i wi /λi , and from
P
P 2
2
(2) we have k i wi φi kH(k,Ω,µ) = i wi (1 + λ−1
i ), so
k̃(x, y) =

N
X
i=1

1
φi (x)φi (y).
1 + λ−1
i

(5)

For approximate Bayesian inference however, we cannot
simply appeal to the representer theorem.

4. Approximate Bayesian Inference
In subsection A.3 of the supplementary material, we review the standard Laplace approximation to the GP with
non-Gaussian likelihood. This a useful set-up for what follows, but is not directly generalisable to our case due to the
integral in (1). Instead, in subsection 4.1 we now take a
different approach based on the Mercer expansion.
4.1. Laplace Approximation
It is tempting to naı̈vely substitute k̃ into subsection A.3 of
the supplementary material, and to neglect the integral part
of the likelihood. Indeed, this gives the correct approximate
predictive distribution. The marginal likelihood does not
work in this way however (due to the log determinant in
(18)). We now perform a more direct analysis.
4.1.1. M ERCER E XPANSION S ETUP
Mercer’s theorem allows us to write (3), where for nondegenerate kernels, N = ∞. Assume a linear model in
Φ(x) = (φi (x))i so that1
f (x) = w> Φ(x),

(6)

and let w ∼ N (0, Λ) where Λ = (λi )ii is a diagonal covariance matrix. This is equivalent to f ∼ GP(k) because
cov(f (x), f (z)) = Φ(x)> Λ Φ(z) = k(x, z).
1
We use a sloppy notation where (x)i is the i-th element of x
while (xi )i is a vector with i-th element xi , etc.

Poisson intensity λ

Fast Bayesian Permanental Processes
data locations xi

40

true intensity λ(x)
predictive median
[0.1,0.9] pred. interval
predictive samples

20

0
0.0

0.5

1.0

1.5
2.0
input domain Ω = [0, π]

2.5

3.0

Figure 1. Test function λ0 of subsection 6.2. The vertical grey lines are the input data points, sampled from the ground truth intensity
depicted by the heavy grey line. We plot two samples from the approximate predictive distribution (black lines) along with the median
(solid red) and [0.1, 0.9] interval (filled red). The GP prior is that of subsection 5.1 with hyper-parameters m = 2, number of cosine
frequencies N = 256, and the remaining a, b chosen to maximise the marginal likelihood.

Recall that the Poisson process on Ω with intensity λ(x) =
m
1 2
2 f (x) has likelihood for X := {xi }i=1
log p(X|w, Ω, k) =
Z
m
X
1
1 2
f 2 (x)dµ(x)
log f (xi ) −
2
2
x∈Ω
i=1
|
{z
}
|
{z
}
>
:=log h(X|w)

Crucially, ŵ must satisfy the stationarity condition
ŵ = (I + Λ−1 )−1 ∇w log h(X|w)|w=ŵ ,

(9)

where

w w

∇w log h(X|w)|w=ŵ = 2

m
X
i=1

Φ(xi )
.
Φ(xi )> ŵ

The joint in w, X is
The approximate predictive mean is therefore
log p(w, X|Ω, k)
1
1
N
= log h(X|w)− w> (I+Λ−1 )w− log |Λ|− log 2π.
2
2
2

4.1.2. L APLACE A PPROXIMATION
We make a Laplace approximation to the posterior, which
is the normal distribution
log p(w|X, Ω, k) ≈ log N (w|ŵ, Q)
(7)
1
1
N
= − (w − ŵ)> Q−1 (w − ŵ) − log |Q| −
log 2π
2
2
2

fˆ(x∗ ) := E [f (x∗ )|X, Ω, k]
= Φ(x∗ )> ŵ
m
X
2
· Φ(xi )> (I + Λ−1 )−1 Φ(x∗ )
=
> ŵ
Φ(x
)
i
i=1
:=

m
X

αi k̃(xi , x∗ ).

This reveals the same k̃ as (5). From (10) we have
α̂i = 2/fˆ(xi ).

:= log q(w|X, Ω, k),
where ŵ is chosen as the mode of the true posterior, and Q
is the inverse Hessian of the true posterior, evaluated at ŵ.
4.1.3. P REDICTIVE M EAN

ŵ = argmax log p(w|X, Ω, k)
w

1
= argmax log h(X|w) − w> (I + Λ−1 )w. (8)
2
w

(11)

Putting (9), (10) and (11) into (8), we obtain

α̂ = argmin
α

The mode ŵ is

(10)

i=1

m
X

1
log αi2 + α> K̃α,
2
i=1

where K̃ = (k̃(xi ), xj )ij . This is equivalent to Flaxman
et al. (2017), though slightly simplified by (11). Interestingly, unlike Flaxman et al. (2017) (or the analogous section 3), we did not appeal to the representer theorem.

Fast Bayesian Permanental Processes

4.1.4. P REDICTIVE VARIANCE
We now compute the Q in (7). The Hessian term giving the
inverse covariance becomes

−1

Q



∂2

log
p(w,
X,
Ω,
k)
=−

∂w∂w>
w=ŵ
=I +Λ

−1

+W

2



∂
W =−
log h(X|w)
>
∂w∂w
w=ŵ
m
>
X
Φ(xi )Φ(xi )
=2
>
2
(Φ(x
i ) ŵ)
i=1
:= V DV > ,

Similarly to (19) we get approximate marginal likelihood
log p(X|Ω, k) ≈ log q(ŵ, X|Ω, k) − log q(ŵ|X, Ω, k)

1 >
= log h(X|ŵ) −
ŵ (I + Λ−1 )ŵ − log |Λ|+log |Q|
| P {z
} 2 |
{z
}

σ 2 (x∗ ) := Var [f (x∗ )|X, Ω, k]
= Φ(x∗ )> Q Φ(x∗ )


= k̃(x∗ , x∗ ) − k̃(x∗ , X)  α S −1 α>  k̃(X, x∗ ) ,

where  is the Hadamard product, or element-wise
multi
plication, and S := k̃(X, X)  (αα> ) + 2I .
4.1.5. P REDICTIVE D ISTRIBUTION
Given
the
approximate
predictive
distribution
f (x∗ )|X, Ω, k ∼ N (fˆ(x∗ ), σ 2 (x∗ )) := N (µ, σ 2 ) and the
relation λ(·) = 12 f 2 (·) it is straightforward to derive the
corresponding3 λ(x∗ )|X, Ω, k ∼ Gamma(α, β) where
2
+σ 2 )2
2µ2 σ 2 +σ 4
the shape α = 2σ(µ
2 (2µ2 +σ 2 ) and the scale β =
µ2 +σ 2 .
4.1.6. M ARGINAL L IKELIHOOD
Letting q(ŵ, X|Ω, k) be the Taylor expansion of
log p(w, X|Ω, k) about the mode w = ŵ and evaluating at ŵ gives, as linear and quadratic terms vanish,

log q(ŵ, X|Ω, k) = log p(ŵ, X|Ω, k)
1
1
N
= log h(X|ŵ)− ŵ> (I+Λ−1 )ŵ− log |Λ|− log 2π.
2
2
2
2
3

Where e.g. k̃(X, x∗ ) is an m × 1 matrix of evaluations of k̃.
α−1
1
Gamma(x|α, β) has p.d.f. Γ(k)β
exp(−x/β).
kx

log 2α2i

α> k̃(X,X)α

(12)
We now use the determinant identity |Z + V DV > | =
|Z||D||V > Z −1 V + D−1 | with the same Z, V and D as
subsubsection 4.1.4 to derive
− log |Λ| + log |Q| = − log |Λ| − log |Z + V DV > |




= − log Λ(I + Λ−1 ) − log D−1 + V > Z −1 V  + c
=

where V:i = αi × Φ(xi ) and D = 12 I ∈ Rm×m . The
approximate predictive variance can now be rewritten as an
m-dimensional matrix expression using the identity (Z +
V DV > ) = Z −1 − Z −1 V (V > Z −1 V + D−1 )V > Z −1 with
and Z = I + Λ−1 along with a little algebra, to derive2

m
i=1

−

N
X
i=1

|



1


− log k̃(X, X)  (αα> ) + 2I  + c,
1 + λi
{z
}

log

:=V(k,Ω,µ)

(13)
where c = m log(2). V(k, Ω, µ) is the crucial ingredient,
not accounted for by naı̈vely putting k̃ into subsection A.3.

5. Covariance Functions
To apply our inference scheme we need to compute:
1. The function k̃ from equation (10), studied recently
by Flaxman et al. (2017) and earlier by Sollich &
Williams (2005) as the equivalent kernel.
2. The associated term V(k, Ω, µ) from equation (13),
required for the marginal likelihood.
This is often challenging for compact domains such as the
unit hyper-cube. Such domains are crucial however, if we
are to avoid the well-known edge-effects which arise from
neglecting the fact that our data are sampled from, say, a
two dimensional rectangle. In subsection 5.1 we provide a
simple constructive approach to the case Ω = [0, 1]d . The
following subsection 5.2 presents the general approximation scheme due to Flaxman et al. (2017), for the case when
we have k but not its Mercer expansion.
5.1. Thin-Plate Semi-norms on the Hyper-Cube
Consider the input domain Ω = [0, π]d with Lebesgue measure µ. A classical function regularisation term is the so
called m-th order thin-plate spline semi-norm,
Z
X
∂mf ∂mg
m!
Q
:=
dµ(x)
hf, giT P(m)
α
α
j αj ! x∈Ω ∂x ∂x
|α|=m

= hf, ∆m giL2 (Ω) + B.

(14)

Fast Bayesian Permanental Processes
300

log p(X|Ω, k) and component terms

1.0

500

250

0.8

250
200

0

0.6

−250

− 1 α> k̃(X, X)α
2




− 1 log k̃(X, X)  (αα> ) + 2I 
2
1 V(k, Ω, µ)
2
1 m log(2)
2
log h(X|ŵ)

−500
−750

log p(X|Ω, k)
maximum

10−6

10−5

10−4
10−3
10−2
parameters a and b

10−1

150
0.4
100
0.2
50

100

0.0
0.0

0.2

(a) Decomposition of the marginal likelihood

0.4

0.6

0.8

1.0

(b) Predictive mean intensity.

Figure 2. Modeling the Redwood dataset: (a) the log marginal likelihood log p(X|Ω, k) along with its component terms from (12)
and (13), as a function of the hyper-parameter a from subsection 5.1; (b) the mean intensity corresponding to the maximum marginal
likelihood parameters, with isolines at 50, 100, 150, 200 and 250. See subsection 6.5 for the details.

Here α is a multi-index
running over all indices of total
P
order |α| :=
α
=
m,
and the boundary conditions
j
j
B come from formal integration (see e.g. Wahba (1990,
section 2.4). We neglect B (for reasons explained shortly)
and include the zero-th derivative to define

Now if we restrict the function space to
n
o
X
X
2
H(k) := f =
cβ φβ : kf kH(k) =
c2β /λβ < ∞ ,

hf, giH(k) := hf, (a∆m + b)giL2 (Ω) .

then it is easily verified that the boundary conditions B in
(14) vanish. This is a common approach to solving partial
differential equations with Neumann boundary conditions
(see e.g. Sommerfeld & Straus (1949)). By restricting in
this way, we merely impose zero partial derivatives at the
boundary, while otherwise enjoying the usual Fourier series approximation properties. Hence we can combine the
reproducing property (4) with (14) and (15) to derive
X
k(x, y) =
λβ φβ (x)φβ (y),
(16)

We may select the free parameters a > 0, b > 0 and
m ∈ Z+ using the maximum marginal likelihood criterion.
In general, it is challenging to obtain the expressions we require in closed form for arbitrary d, Ω and m. The analytical limit in the literature appears to be the case m = 2 with
dimension d = 1 along with so-called Neumann boundary conditions (which impose a vanishing gradient on the
boundary (Sommerfeld & Straus, 1949)). That k̃ has been
derived in closed form as the reproducing kernel of an associated Sobolev space by Thomas-Agnan (1996).
We now present a simple but powerful scheme which sidesteps these challenges via a well chosen series expansion.
Consider the basis function
φβ (x) := (2/π)d/2

d p
Y

1/2

[βj =0]

cos(βj xj ),

β≥0

β≥0

β≥0

where λβ := 1/(a

Pd

j=1

βj2

m

+ b).

The above covariance function is not required for our inference algorithm. Rather, the point is that since the basis
is also orthonormal, we may substitute λβ and φβ into (10)
and (13) to obtain k̃ and V(k), as required.

j=1

where β is a multi-index with non-negative (integral) values, and [·] denotes the indicator function (which is one if
the condition is satisfied and zero otherwise). The φβ form
a convenient basis for our purposes. They are orthonormal:
hφβ , φγ iL2 (Ω) = [β = γ],
and also eigenfunctions of our regularisation operator with
d
 X

m
(a∆m + b)φβ = a
βj2 + b φβ .
j=1

(15)

Series truncation. We have discovered closed form expressions for k̃ only for m ≤ 2 and d = 1. In practice
we may truncate the series at any order and still obtain a
valid model due to the equivalence with the linear model
(6). Hence, a large approximation error (in terms of k̃)
due to truncation may be irrelevant from a machine learning perspective, merely implying a different GP prior over
functions. Indeed, the maximum marginal likelihood criterion based on subsubsection 4.1.6 may guide the selection of an appropriate truncation order, although some care
needs to be taken in this case.

Fast Bayesian Permanental Processes

5.2. Arbitrary Covariances and Domains

Tk : H(k) → H(k)

`2 Error (normalised)

Flaxman et al. (2017) suggested the following approximation for k̃, for the case when k is known but the associated
Mercer expansion is not. The approximation is remarkably general and elegant, and may even be applied to nonvectorial data by employing, say, a kernel function defined
on strings (Lodhi et al., 2002). The idea is to note that the
φi , λi pairs are eigenfunctions of the integral operator (see
Rasmussen & Williams (2006) section 4.3)

1.0

λ0
λ1
λ2
λ3
λ4

0.8
0.6
0.4
0.2
0.0

Z
f 7→ Tk f :=

0.00

k(x, ·)f (x)p(x) dx,
x∈Ω

√

(X)

:=

(X)

:= λ(mat)
/m.
i

φi
λi

(mat)

m/λi

(mat)

k(·, X)ei

These approximations may be used for k̃, as in (Flaxman
et al., 2017), as well as our V(k, Ω, µ).

1.0

λ0
λ1
λ2
λ3
λ4

0.8

0.6

0.4

0.2

0.0
0.0

6. Experiments

1.00

(a) `2 error vs. marginal likelihood.
Expected Test Log Likelihood (normalised)

where p is related to µ of the previous subsection by
µ(x) = p(x) dx. The Nyström approximation (Nyström,
1928) to Tk draws m samples X from p and defines
P
(X)
1
Tk g := m
x∈X k(x, ·)g(x). Then the eigenfunctions
and eigenvectors of Tk may be approximated via the eigen(mat)
(mat)
vectors ei
and eigenvalues λi
of 2 k(X, X), as

0.25
0.50
0.75
log p(X|Ω, k) (normalised)

0.2
0.4
0.6
0.8
log p(X|Ω, k) (normalised)

1.0

(b) Expected log-loss vs. marginal likelihood.

6.1. Setup
Evaluation We use two metrics. The `2 Error is the
squared difference
to the ground truth w.r.t. the Lebesgue
R
measure: x∈Ω (λ(x) − λtrue (x))2 dx. The test log likelihood is the logarithm of (1) at an independent test
sample (one sample being a set of points, i.e. a sample from the process), which we summarise by averaging over a finite number of test sets (for real data where
the ground truth intensity is unknown) and otherwise (if
we have the
h ground truth) by
i the analytical expression
EX∼PP(λ) log pX∼PP(λ̂) (X) =
Z




λ(x) log λ̂(x) − λ̂(x) dx,

x∈Ω

where P P (λ) is the process with intensity λ (see the supplementary subsection A.1). This evaluation metric is
novel in this context, yet more accurate and computationally cheaper than the sampling of e.g. (Adams et al., 2009).
Decision Theory The above metrics are functions of a
single estimated intensity. In all cases we use the predictive
mean intensity for evaluation. We demonstrate in subsection A.2 of the supplementary material that this is optimal

Figure 3. The relationship between the log marginal likelihood
and the `2 error (both scaled and shifted to [0, 1]), on the benchmark problems of subsection 6.2 — see figure 3 for the details.

for the expected test log likelihood evaluation (the `2 error
cases is similar as is trivial to show).
Algorithms We compare our new Laplace Bayesian
Point Process (LBPP) with two covariances: the cosine
kernel of subsection 5.1 with fixed m = 2 and hyperparameters a and b (LBPP-Cos), and the Gaussian kernel
2
k(x, z) = γ 2 exp(|x − y| /(2β 2 )) with the method of
subsection 5.2 (LBPP-G). We compared with the Variational Bayesian Point Process (VBPP) (Lloyd et al., 2015)
using the same Gaussian kernel. LBPP-G and VBPP use
a regular grid for X (of subsection 5.2) and the inducing
points, respectively. To compare timing we vary the number of basis functions, i.e. the number of grid points for
LBPP-G and VBPP, and cosine terms for LBPP-Cos. We
include the baseline kernel smoothing with edge correction
(KS+EC) method (Diggle, 1985; Lloyd et al., 2015). All

Fast Bayesian Permanental Processes

Normalised Metric

Expected Test Log Likelihood

`2 Error

0.20

Fit Time

100

0.18

0.96

10−1

KS+EC
LBPP-Cos
LBPP-G
VBPP

0.16
0.94

10−2

0.14
0.92

0.12
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

10−3
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

Number of basis functions / inducing points

Figure 4. Mean ± one standard error performance on the toy problems of subsection 6.2 as a function of the number of basis functions
(with KS+EC results replicated along the horizontal axis for comparison). The vertical axes of each figure are normalised scores: see
subsubsection 6.2.2 for the details.

inference is performed with maximum marginal likelihood,
except for KS+EC where we maximise the leave one out
metric described in (Lloyd et al., 2015).
6.2. 1D Toy Examples
We drew five toy intensities, λ0 , λ2 , . . . , λ4 as 12 f 2 where f
was sampled from the GP of Gaussian covariance (defined
above) with γ = 5 and β = 0.5. Figure 1 depicts λ0 — see
the caption for a description. The remaining test functions
are shown in figure 6 of the supplementary material.
6.2.1. M ODEL S ELECTION
As the marginal likelihood log p(X|Ω, k) is a key advantage of our method over the non-probabilistic approach
of Flaxman et al. (2017), we investigated its efficacy for
model selection. Figure 3 plots log p(X|Ω, k) against our
two error metrics, both rescaled to [0, 1] for effective visualisation, based on a single training sample per test function. We observe a strong relationship, with larger values of
log p(X|Ω, k) generally corresponding to lower error. This
demonstrates the practical utility of both the marginal likelihood itself, and our Laplace approximation to it.
6.2.2. E VALUATION
We sampled 100 training sets from each of our five toy
functions. Figure 4 shows our evaluation metrics along
with the fitting time as a function of the number of basis functions. For visualisation all metrics (including fit
time) are scaled to [0, 1] by dividing by the maximum for
the given test function, over data replicates and algorithms.
LBBP-G and and VBPP achieve the best performance, but
our LBPP-G is two orders of magnitude faster. Our KS+EC
implementation follows the methodology of Lloyd et al.
(2015): we fit the kernel density bandwidth using average leave one out log likelihood. This involves a quadratic
number of log p.d.f. of the truncated normal calculations,

and log-sum-exp calculations, both of which involve large
time constants, but are asymptotically superior to the other
methods we considered. LBBP-Cos is slightly inferior in
terms of expected test log likelihood, which is expected due
to the toy functions having been sampled according to the
same Gaussian kernel of LBPP-G and VBPP (as well as the
density estimator of KS+EC).
6.3. Real Data
We compared the methods on three real world datasets,
• coal: 190 points in one temporal dimension, indicating the time of fatal coal mining accidents in the
United Kingdom, from 1851 to 1962 (Collins, 2013);
• redwood: 195 California redwood tree locations from
a square sampling region (Ripley, 1977);
• cav: 138 caveolae locations from a square sampling
region of muscle fiber (Davison & Hinkley, 2013).
6.4. Computational Speed
Similarly to subsubsection 6.2.2 we evaluate the fitting
speed and statistical performance vs. number of basis functions — see figure 5. We omit the `2 error as the ground
truth is unknown. Instead we generate 100 test problems
by each time randomly assigning each original datum to either the training or the testing set with equal probability.
Again we observe similar predictive performance of LBPP
and VBPP, but with much faster fit times for our LBPP. Interestingly LBPP-Cos slightly outperform LBPP-G.
6.5. 2D California Redwood Dataset
We conclude by further investigating the redwood dataset.
Once again we employed the ML-II procedure to determine
a and b, fixing m = 2, for the covariance function of subsection 5.1, using the lowest 32 cosine frequencies in each

Normalised Metric (ratio)

Fast Bayesian Permanental Processes

Test Log Likelihood

0.9
0.8

Fit Time
100
10−1

0.7
0.6

KS+EC
LBPP-Cos
LBPP-G
VBPP

10−2

0.5
10−3

0.4
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

Normalised Metric (ratio)

(a) cas dataset
100
10−1

0.8

10−2
10−3
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75

Normalised Metric (ratio)

(b) coal dataset
0.8
0.7

10−1

0.6

10−2

0.5

10−3
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
Number of basis functions / inducing points

5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
Number of basis functions / inducing points

(c) redwood dataset
Figure 5. Mean ± one standard error performance of the different methods on real data, as a function of the number of basis functions
(with KS+EC results replicated along the horizontal axis for comparison). See subsection 6.4 for the details.

dimension for a total of N = 322 basis functions in the expansion (16). For ease of visualisation we also fixed a = b.
Figure 2 plots the results, including a decomposition of the
log marginal likelihood log p(X|Ω, k) and a visualisation
of the predictive mean. The mean function strongly resembles the result presented by Adams et al. (2009), where
computationally expensive MCMC was employed.
The decomposition of the marginal likelihood on the left
of figure 2 provides insight into the role of the individual
terms in (12) and (13) which make up log p(X|Ω, k). In
particular, the term V(k, Ω, µ) from (13) acts as a regulariser, guarding against over-fitting, and balancing against
the data term h of (12).

7. Conclusion
We have discussed the permanental process, which places
a Gaussian Process prior over the square root of the inten-

sity function of the Poisson process, and derived the equations required for empirical Bayes under a Laplace posterior approximation. Our analysis provides 1) an alternative derivation and probabilistic generalization of (Flaxman
et al., 2017), and 2) an efficient and easier to implement
alternative which does not rely on inducing inputs (but
rather reproducing kernel Hilbert space theory), to the related Bayesian approach of Lloyd et al. (2015). This further
demonstrates, in a new way, the mathematical convenience
and practical utility of the permanental process formulation
(in comparison with say log Gaussian Cox processes).

Acknowledgements
Thanks to Young Lee, Kar Wai Lim and Cheng Soon Ong
for useful discussions. Adrian is supported by the Australian Research Council (ARC) via a Discovery Early Career Researcher Award (DE-120102873).

Fast Bayesian Permanental Processes

References
Adams, Ryan P., Murray, Iain, and MacKay, David J. C.
Tractable nonparametric Bayesian inference in Poisson
processes with gaussian process intensities. In Bottou, Léon and Littman, Michael (eds.), ICML, pp. 9–16,
Montreal, June 2009.
Aronszajn, N. Theory of reproducing kernels. Transactions
of the American Mathematical Society, 68, 1950.
Baddeley, Adrian. Spatial point processes and their applications. Lecture Notes in Mathematics: Stochastic Geometry, 1892:1–75, 2007.
Collins, Michael.
The Inside-Outside Algorithm.
Columbia University Lecture Notes, 2013.
Cont, Rama and Tankov, Peter. Financial modelling with
jump processes. Financial mathematics series. Chapman
and Hall/CRC, London, 2004. ISBN 1-5848-8413-4.
Cunningham, J.P., Shenoy, K.V., and Sahani, M. Fast
Gaussian Process Methods for Point Process Intensity
Estimation. In Proceedings of the 25th International
Conference on Machine Learning (ICML), pp. 192–199,
Helsinki, Finland, 2008.
Davison, A. C. and Hinkley, D. V. Bootstrap Methods and Their Application. Cambridge University
Press, New York, NY, USA, 2013. ISBN 0511802846,
9780511802843.
Diggle, Peter. A kernel method for smoothing point process
data. Applied Statistics, 34:138–147, 1985.
Diggle, P.J., Moraga, P., Rowlingson, B., and Taylor,
B.M. Spatial and spatio-temporal log-Gaussian Cox processes: Extending the geostatistical paradigm. Statistical
Science, 28(4):542–563, 2013.
Flaxman, S., Wilson, A.G., Neill, D.B., Nickisch, H., and
Smola, A.J. Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods. In Proceedings
of the 32nd International Conference on Machine Learning (ICML), pp. 607–616, Lille, France, 2015.
Flaxman, S., Teh, Y.W., and Sejdinovic, D. Poisson Intensity Estimation with Reproducing Kernels. In International Conference on Artificial Intelligence and Statistics
(AISTATS), 2017.
Illian, J.B., Sørbye, S.H., and Rue, H. A toolbox for fitting
complex spatial point process models using integrated
nested Laplace approximation (INLA). The Annals of
Applied Statistics, 6(4):1499–1530, 2012.
Kimeldorf, G. and Wahba, G. Some results on Tchebycheffian spline functions. Journal of Mathematical Analysis
and Appl., 33:82–95, 1971.

Kom Samo, Y.-L. and Roberts, S. Scalable nonparametric
bayesian inference on point processes with gaussian processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp. 2227–2236,
Lille, France, 2015.
Lewis, P. A. W. and Shedler, G. S. Simulation of nonhomogeneous Poisson processes by thinning. Naval Res.
Logistics Quart, 26:403–413, 1979.
Lloyd, Chris M., Gunter, Tom, Osborne, Michael A.,
Roberts, Stephen J., and Nickson, Tom. Latent point process allocation. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, pp. 389–397,
2016.
Lloyd, C.M., Gunter, T., Osborne, M.A., and Roberts,
S.J. Variational inference for gaussian process modulated poisson processes. In Proceedings of the 32nd International Conference on Machine Learning (ICML),
pp. 1814–1822, Lille, France, 2015.
Lodhi, Huma, Saunders, Craig, Shawe-Taylor, John, Cristianini, Nello, and Watkins, Chris. Text Classification
using String Kernels. Journal of Machine Learning Research, 2:419–444, February 2002.
McCullagh, Peter and Møller, Jesper. The permanental process. Advances in Applied Probability, 38(4):873–888,
2006.
Mercer, J. Functions of positive and negative type, and their
connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London A:
Mathematical, Physical and Engineering Sciences, 209
(441-458):415–446, 1909. ISSN 0264-3952.
Møller, J, Syversveen, A, and Waagepetersen, R. Log gaussian cox processes. Scandanavian Journal of Statistics,
25:451–482, 1998.
Nyström, Evert Johannes. Über die praktische auflösung
von linearen integralgleichungen mit anwendungen auf
randwertaufgaben der potentialtheorie. Commentationes
physico-mathematicae, 4:1–52, 1928.
Rasmussen, C. E. and Williams, C. K.I. Gaussian Processes for Machine Learning. Adaptive Computation
and Machine Learning. The MIT Press, Cambridge,
Massachusetts, 2006.
Rathbun, Stephen L. and Cressie, Noel. Asymptotic properties of estimators for the parameters of spatial inhomogeneous poisson point processes. Advances in Applied
Probability, 26:122–154, 1994.

Fast Bayesian Permanental Processes

Ripley, B. D. Modeling Spatial Patterns. Journal of the
Royal Statistical Society Series B Statistical Methodolology, 39(2), 1977.
Rue, Håvard, Martino, Sara, and Chopin, Nicolas. Approximate bayesian inference for latent gaussian models by
using integrated nested laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 71(2):319–392, 2009.
Schölkopf, Bernhard, Herbrich, Ralf, and Smola, Alex J.
A generalized representer theorem. In Proc. of the 14th
Annual Conf. on Computational Learning Theory, pp.
416–426, London, UK, 2001. Springer-Verlag. ISBN 3540-42343-5.
Shirai, Tomoyuki and Takahashi, Yoichiro. Random point
fields associated with certain fredholm determinants ii:
Fermion shifts and their ergodic and gibbs properties.
Ann. Probab., 31(3):1533–1564, 07 2003. doi: 10.1214/
aop/1055425789.
Sollich, Peter and Williams, Christopher K. I. Understanding Gaussian Process Regression Using the Equivalent Kernel, pp. 211–228. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2005.
Sommerfeld, Arnold and Straus, Ernst Gabor. Partial differential equations in physics. Pure and applied mathematics. Academic Press, New York, 1949.
Thomas-Agnan, Christine. Computing a family of reproducing kernels for statistical applications. Numerical Algorithms, 13(1):21–32, 1996.
Titsias, Michalis K. Variational learning of inducing variables in sparse gaussian processes. In In Artificial Intelligence and Statistics 12, pp. 567–574, 2009.
Wahba, G. Spline Models for Observational Data. Series
in Applied Math., Vol. 59, SIAM, Philadelphia, 1990.

