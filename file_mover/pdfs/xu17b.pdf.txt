Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Hongteng Xu 1 Dixin Luo 2 Hongyuan Zha 1

Abstract
Many real-world applications require robust algorithms to learn point processes based on a type
of incomplete data — the so-called short doublycensored (SDC) event sequences. We study this
critical problem of quantitative asynchronous
event sequence analysis under the framework
of Hawkes processes by leveraging the idea of
data synthesis. Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method,
sampling predecessors and successors for each
SDC event sequence from potential candidates
and stitching them together to synthesize long
training sequences. The rationality and the feasibility of our method are discussed in terms
of arguments based on likelihood. Experiments
on both synthetic and real-world data demonstrate that the proposed data synthesis method
improves learning results indeed for both timeinvariant and time-varying Hawkes processes.

1. Introduction
Real-world interactions among multiple entities are often
recorded as asynchronous event sequences, such as user behaviors in social networks, job hunting and hopping among
companies, and diseases and their complications. The entities or event types in the sequences often exhibit selftriggering and mutually-triggering patterns. For example,
a tweet of a twitter user may trigger further responses from
her friends (Zhao et al., 2015). A disease of a patient may
trigger other complications (Choi et al., 2015). Hawkes
processes, an important kind of temporal point process
model (Hawkes & Oakes, 1974), have capability to describe the triggering patterns quantitatively and capture the
infectivity network of the entities.
1

Georgia Institute of Technology, Atlanta, Georgia, USA
University of Toronto, Toronto, Ontario, Canada. Correspondence to: Hongteng Xu <hxu42@gatech.edu>, Dixin Luo
<dixin.luo@utoronto.ca>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Figure 1. For each SDC sequence, i.e., incomplete disease history
of a person in his lifetime, we design a mechanism to select other
SDC sequences as predecessors/successors and synthesize a long
sequence. Then, we can estimate the unobserved triggering patterns among diseases, i.e., the red dashed arrows, and construct a
disease network.

Despite the usefulness of Hawkes processes, robust learning of Hawkes processes often needs many event sequences
with events occurring over a long observation window. Unfortunately, the observation window is likely to be very
short and sequence-specific in many important practical applications, i.e., within an imagined universal window, each
sequence is only observed with a corresponding short subinterval of it, and the events outside this sub-interval are
not observed — we call them short doubly-censored (SDC)
event sequences. Existing learning algorithms of Hawkes
processes directly applied to SDCs may suffer from overfitting, and what is worse, the triggering patterns between
historical events and current ones are lost, so that the triggering patterns learned from SDC event sequences are often unreliable. This problem is a thorny issue in several practical applications, especially in those having timevarying triggering patterns. For example, the disease networks of patients should evolve with the increase of age.
However, it is very hard to track and record people’s diseases on a life-time scale. Instead, we can only obtain their
several admissions (even only one admission) in a hospital during one or two years, which are just SDC event
sequences. Therefore, it is highly desirable to propose a
method to learn Hawkes processes having a longtime support from a collection of SDC event sequences
In this paper, we propose a novel and simple data synthesis
method to enhance the robustness of learning algorithms
for Hawkes processes. Fig. 1 illustrates the principle of our
method. Given a set of SDC event sequences, we sample
predecessor for each event sequence from potential candidates and stitch them together as new training data. In
the sampling step, the distribution of predecessor (and successor) is estimated according to the similarities between

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

current sequence and its candidates, and the similarity is
defined based on the information of time stamps and (optional) features of event sequences. We analyze the rationality and the feasibility of our data synthesis method and
discuss the necessary condition for using the method. Experimental results show that our data synthesis method indeed helps to improve the robustness of various learning
algorithms for Hawkes processes. Especially in the case
of time-varying Hawkes processes, applying our method in
the learning phase achieves much better results than learning directly from SDC event sequences, which is meaningful for many practical applications, e.g., constructing dynamical disease network, and learning long-term infectivity among different IT companies.

2. Related Work
An event sequence can be represented as s = {(ti , ci )}M
i=1 ,
where time stamps ti ’s are in an observation window
[Tb , Te ] and events ci ’s are in a set of event types C =
{1, ..., C}. A point process {Nc }c∈C is a random process model taking event sequences as instances, where
Nc = {Nc (t)|t ∈ [Tb , Te ]} and Nc (t) is the number of
type-c events occurring at or before time t. A point process can be characterized via its conditional intensity function λc (t) = E[dNc (t)|HtC ]/dt, where c ∈ C and HtC =
{(ti , ci )|ti < t, ci ∈ C} is the set of history. It represents
the expected instantaneous happening rate of events given
historical record (Daley & Vere-Jones, 2007). The intensity is often modeled with certain parameters Θ to capture
the phenomena of interests, i.e., self-triggering (Hawkes &
Oakes, 1974) or self-correcting (Xu et al., 2015). Based on
{λc (t)}c∈C , the likelihood of an event sequence s is
 X Z Te

Y
L(s; Θ) =
λci (ti ) exp −
λc (s)ds . (1)
i

c

Tb

Hawkes Processes. Hawkes processes (Hawkes & Oakes,
1974) have a particular form of intensity:
XC Z t
(2)
λc (t) = µc +
φcc0 (t, s)dNc0 (s),
0
c =1

0

where µc is theR exogenous base intensity independent of the
t
history while 0 φcc0 (t, s)dNc0 (s) is the endogenous intensity capturing the influence of historical events on type-c
ones at time t (Xu et al., 2016a). Here, φcc0 (t, s) ≥ 0
is called impact function. It quantifies the influence of
the type-c0 event at time s to the type-c event at time t.
Hawkes processes provide us with a physically-meaningful
model to capture the infectivity among various events,
which are used in social network analysis (Zhou et al.,
2013b; Zhao et al., 2015), behavior analysis (Yang & Zha,
2013; Luo et al., 2015) and financial analysis (Bacry et al.,
2013). However, the methods in these references assume

that the impact function is shift-invariant (i.e., φcc0 (t, s) =
φcc0 (t − s), t ≥ s), which limits their applications on longtime scale. Recently, the time-dependent Hawkes process
(TiDeH) in (Kobayashi & Lambiotte, 2016) and the neural
network-based Hawkes process in (Mei & Eisner, 2016)
learn very flexible Hawkes processes with complicated intensity functions. Because they highly depend on the size
and the quality of data, they may fail in the case of SDC
event sequences.
Learning from Imperfect Observations. In practice, we
need to learn sequential models from imperfect observations (e.g., interleaved (Xu et al., 2016b), aggregated (Luo
et al., 2016) and extremely-short sequences (Xu et al.,
2016c)). Multiple imputation (MI) (Rubin, 2009) is a general framework to build surrogate observations from the
current model. For time series, bootstrap method (Efron,
1982; Politis & Romano, 1994; Gonçalves & Kilian, 2004)
and its variants (Paparoditis & Politis, 2001; Guan & Loh,
2007) have been used to improve learning results when observations are insufficient. In survival analysis, many techniques have been made to deal with truncated and censored
data (Turnbull, 1974; De Gruttola & Lagakos, 1989; Klein
& Moeschberger, 2005; Van den Berg & Drepper, 2016).
For point processes, the global (Streit, 2010) or local (Fan,
2009) likelihood maximization estimators (MLE) are used
to learn Poisson processes. Nonparametric approaches
for non-homogeneous Poisson processes use the pseudo
MLE (Sun & Kalbfleisch, 1995) or full MLE (Wellner &
Zhang, 2000). The bootstrap methods above are also used
to learn point processes (Cowling et al., 1996; Guan & Loh,
2007; Kirk & Stumpf, 2009). To learn Hawkes processes
robustly, structural constraints, e.g., low-rank (Luo et al.,
2015) and group-sparse regularizers (Xu et al., 2016a), are
introduced. However, all of these methods do not consider
the case of SDC event sequences for Hawkes processes.

3. Learning from SDC Event Sequences
Suppose that the original complete event sequences are
in a long observation window. However, the observation
window in practice might be segmented into several intervals {Tbn , Ten }N
n=1 , and we can only observe Kn SDC sen
quences {snk }K
k=1 in the n-th interval, n = 1, ..., N . Although we can still apply maximum likelihood estimator to
learn Hawkes processes, i.e.,
X
minΘ −
log L(snk ; Θ),
(3)
n,k

the SDC event sequences would lead to over-fitting problem and the loss of triggering patterns. Can we do better in
such a situation? In this work, we propose a data synthesis
method based on a sampling-stitching mechanism, which
extends SDC event sequences to longer ones and enhances
the robustness of learning algorithms.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences
0

3.1. Data Synthesis via Sampling-Stitching
Denote the k-th SDC event sequence in the n-th interval as
snk . Because its predecessor is unavailable, if we learn the
parameters of our model via (3) directly, we actually impose a strong assumption on our data that there is no event
happening before snk (or previous events are too far away
from snk to have influences on snk ). Obviously, this assumption is questionable — it is likely that there are influential
events happening before snk . A more reasonable strategy is
enumerating potential predecessors and maximizing the expected log-likelihood over the whole observation window:
X
minΘ −
Es∼HC n [log L([s, snk ]; Θ)].
(4)
T
n,k
b

Here Ex∼D [f (x)] represents the expectation of function
f (x) with random variable x yielding to a distribution
D. s ∼ HTC n means all possible history before Tbn , and
b
L([s, snk ]; Θ) is the likelihood of stitched sequence [s, snk ].
The stitched sequence [s, snk ] can be generated via sampling SDC sequence s from previous 1st, ..., (k − 1)-th intervals and stitching s to snk . The sampling process yields
to the probabilistic distribution of the stitched sequences.
Given snk , we can compute its similarity between its poten0
0
0
tial predecessor snk0 in [Tbn , Ten ] as
(
0
0, when Ten ≥ Tbn
n0
n
w(sk0 , sk ) =
(5)
0
0
S(Tbn , Ten )S(fkn , fkn0 ), o.w.
Here, S(a, b) = exp(−kb − ak22 /σs ) is a predefined similarity function with parameter σs . fkn is the feature of
snk , which is available in some applications. Note that the
availability of feature is optional — even if the feature of
sequence is unavailable, we can still define the similarity
measurement purely based on time stamps. The normal0
0
ized {w(snk0 , snk )} provides us with the probability that snk0
0
0
appears before snk , i.e., p(snk0 |snk ) ∝ w(snk0 , snk ). Then, we
0
can sample snk0 according to the categorical distribution,
0
i.e., snk0 ∼ Category(w(·, snk )).
We can apply such a sampling-stitching mechanism L
times iteratively to the SDC sequences in both backward
and forward directions and get long stitched event sequences. Specifically, we represent a stitched event sequence as sstitch = [s1 , ..., s2L+1 ], sl ∈ {snk }, l =
1, ..., 2L + 1, whose probability is
p(sstitch ) ∝

Y2L
l=1

w(sl , sl+1 ).

(6)

Note that our data synthesis method naturally contains two
variants. When the starting (the ending) point of time window is unavailable, we use the time stamp of the first (the
last) event of SDC sequence instead. Additionally, we can

relax the constraint Ten ≤ Tbn in (5) and allow a SDC sequence to have an overlap with its predecessor/successor.
In this case, we preserve the overlap part randomly either
from itself or its predecessor/successor before applying our
sampling-stitching method. These two variants ensure that
our data synthesis method is doable in practice, which are
used in the following experiments on real-world data.
3.2. Justification
After applying our data synthesis method, we obtain many
stitched event sequences, which can be used as instances
for estimating Es∼HC n [log L([s, snk ]; Θ)]. Specifically,
T

b

taking advantage of stitched sequences, we can rewrite the
learning problem in (4) approximately as
X
minΘ −
p(sstitch ) log L(sstitch ; Θ), (7)
sstitch ∈S

which is actually the minimum cross-entropy estimation.
p(sstitch ) represents the “true” probability that the stitched
sequence happens, which is estimated via the predefined
similarity measurement and the sampling mechanism. The
likelihood L(sstitch ; Θ) represents the “unnatural” probability that the stitched sequence happens, which is estimated based on the definition in (1). Our data synthesis
method takes advantage of the information of time stamps
and (optional) features and makes p(sstitch ) suitable for
practical situations. For example, the likelihood of a sequence generally reduces with the increase of observation
time window. The proposed probability p(sstitch ) yields to
the same pattern — according to (6), the longer a stitched
sequence is, the smaller its probability becomes.
The set of all possible stitched sequences, i.e., the
QNS in (7),
is very large, whose cardinality is |S| = O( n=1 Kn ).
In practice, we cannot and do not need to enumerate all
possible combinations. An empirical setting is making the
number of stitched sequences comparable to that
PNof original SDC event sequences, i.e., generating O( n=1 Kn )
stitched sequences. In the following experiments, we just
apply 5(= U ) trials and generate 5 stitched sequences for
each original SDC event sequence, which achieves a tradeoff between computational complexity and performance.
3.3. Feasibility
It should be noted that our data synthesis method is only
suitable for those complicated point processes whose historical events have influences on current and future ones.
Specifically, we analyze the feasibility of our method for
several typical point processes.
Poisson Processes. Our data synthesis method cannot improve learning results if the event sequences are generated
via Poisson processes. For Poisson processes, the happening rate of future events is independent of historical

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

events. In other words, the intensity function of each interval can be learned independently based on the SDC event
sequences. The stitched sequences do not provide us with
any additional information.
Hawkes Processes. For Hawkes processes, whose intensity function is defined as (2), our data synthesis method
can enhance the robustness of learning algorithm generally.
In particular, consider a “long” event sequence generated
via a Hawkes process in the time window [Tb , Te ]. If we
divide the time window into 2 intervals, i.e., [Tb , T ] and
(T, Te ], the intensity function corresponding to the second
interval can be written as
X
X
φcci (t, ti ). (8)
λc (t) = µc +
φcci (t, ti ) +
ti ≤T

T <ti ≤Te

If the events in the first interval are unobserved, we just
have a SDC event sequence, and the second term in (8) is
unavailable. Learning Hawkes processes directly from the
SDC event sequence ignores the information of the second
term, which has a negative influence on learning results.
Our data synthesis method leverages the information from
other potential predecessors and generates multiple candidate long sequences. As a result, we obtain multiple intensity functions sharing the second interval and maximize
the weighted sum of their log-likelihood functions (i.e., an
estimated expectation of the log-likelihood of the real long
sequence), as (7) does.
Compared with learning from SDC event sequences directly, applying our data synthesis method can improve
learning results in general, unless the term
P
ti ≤T φcci (t, ti ) is ignorable. Specifically, we can model
the impact functions {φcc0 (t, s)}c,c0 ∈C of Hawkes processes based on basis representation:
φcc0 (t, s) = ψcc0 (t) × g(t − s)
| {z }
| {z }
Infectivity

=

XM
m=1

Triggering kernel

Hawkes process is a kind of physically-interpretable model
for many natural and social phenomena. The proposed
model in (9) reflects many common properties of realworld event sequences. First, the infectivity among various event types often changes smoothly in practice: in social networks, the interaction between two users changes
smoothly, which is not established or blocked suddenly;
in disease networks, the infectivity among diseases should
change smoothly with the increase of patient’s age. Applying Gaussian basis representation guarantees the smoothness of infectivity function. Second, the triggering kernel measures the decay of infectivity over time. According to existing work, the decay of infectivity is exponential approximately, which has been verified in many
real-world data (Zhou et al., 2013a; Kobayashi & Lambiotte, 2016; Choi et al., 2015). For learning Hawkes
processes from SDC event sequences, we combine our
data synthesis method with an EM-based learning algorithm of Hawkes processes. Applying our data synthesis method, we obtain a set of stitched event sequences
S = {sn } and their appearance probabilities {pn }, where
n
sn = {(tni , cni )Ii=1
|tni ∈ [Tbn , Ten ], cni ∈ C} and pn is calculated based on (5). According to (7, 9), we can learn
target Hawkes process via
min

µ≥0, A≥0

−

X|S|
n=1

pn log L(sn ; Θ) + γR(A).

(9)

acc0 m κm (t)g(t − s).

When M = 1 and κm (t) ≡ 1, we obtain the simplest timeinvariant Hawkes process. Relaxing the shift-invariant assumption, i.e., M > 1 and κm (t) is Gaussian, we obtain a flexible
time-varying Hawkes process model.

(10)

Here Θ = {µ, A} represents the parameters of our model.
The vector µ = [µc ] and the tensor A = [acc0 m ] are nonnegative. Based on (1, 9), the log-likelihood function is
log L(sn ; Θ)
XIn
XC
=
log λci (ti ) −
i=1

Here, we decomposeP
impact functions into two parts: 1)
M
Infectivity ψcc0 (t) = m=1 acc0 m κm (t) represents the in0
fectivity of event type c to c at time t.1 2) Triggering kernel
g(t) = exp(−βt) measures the time decay of infectivity. It
means that the infectivity of a historical event to current
one reduces exponentially with the increase of temporal
distance between them. When β is very large, φcc0 (t, s)
decays rapidly with the increase of t − s, and the events
happening long ago can be ignored. In such a situation, our
data synthesis method is unable to improve learning results.
1

4. Implementation for Hawkes Processes

=

XIn
i=1

Z

c=1


X
log µcni +

j<i

XC 
X
−
∆n µc −
c=1

Ten

λc (s)ds
Tbn

n
g(τij
)

X
m



acni cnj m κm (tni )

XIn X
m

i=1

j≤i


accnj m Gij ,

R tni+1

n
where τij
= tni − tnj , Gij = tn κm (s)g(s − tnj )ds,
i
and ∆n = Ten − Tbn . R(A) represents the regularizer
of parameters, whose weight is γ. Following existing
work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al.,
2016a), we assume the infectivity connections among different event types to be sparse and impose a `1 -norm regularizer on the coefficient tensor A, i.e., R(A) = kAk1 =
P
0
c,c0 ,m |acc m |.

We can solve the problem via an EM algorithm. Specifically, when sparse regularizer is applied, we take advantage of ADMM method, introducing auxiliary variable
Z = [zcc0 m ] and dual variable U = [ucc0 m ] for A and

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

rewriting the objective function in (10) as
X
−
pn log L(sn ; Θ) + 0.5ρkA − Zk2F
n

+ ρtr(U > (A − Z)) + γkZk1 .
Here ρ controls the weights of regularization terms, which
increases with the number of EM iterations. tr(·) computes
the trace of matrix. Then, we can update {µ, A}, Z, and
U alternatively.
Update µ and A: Given the parametersPin the k-th iteration, we apply Jensen’s inequality to − n log L(sn ; Θ)
and obtain a surrogate objective function for µ and A:
Q(µ, A; µk , Ak , Z k , U k )
X
In X X
N
M
n
X
)acni cnj m κm (tni )
g(τij
=−
pn
qijm log
qijm
n=1
i=1 j<i m=1


C X
M X
C
X
X
µcni
n
−
accnj m Gij − ∆
µc
+ qi log
qi
c=1 m=1
c=1
j≤i

k

+ 0.5ρkA − Z +
where qi =

µk
cn
i
k
λcn (tn
i )
i

U k k2F ,
n
n
g(τij
)ak
cn cn m κm (ti )
i j
k
n
λcn (ti )
i

, and

λkcni (tni ) is calculated based on µk and Ak . Then, we can
∂Q
update µ and A via solving ∂Q
∂µ = 0 and ∂A = 0. Both of
these two equations have closed-form solution:
P
P
qi
n pn
cn
k+1
i =c
P
µc =
,
n p n ∆n
(11)
p
B 2 − 4ρC − B
k+1
,
acc0 m =
2ρ
where

n

X
n

pn

X

0
ci =c, cn
j =c , j≤i

n
0
cn
i =c, cj =c , j≤i

Gij ,

qijm .

Update Z: Given Ak+1 and U k , we can update Z via
solving the following optimization problem:
minZ γkZk1 + 0.5ρkAk+1 − Z + U k k2F .
Applying soft-thresholding method, we have
Z k+1 = F γρ (Ak+1 + U k ),

(12)

where Fη (x) = sign(x) min{|x| − η, 0} is the softthresholding function.
k+1

Update U : Given A
dual variable as

In summary, Algorithm 1 shows the scheme of our learning
method. Note that the algorithm can be applied to SDC
event sequences directly via ignoring pn ’s.

5. Experiments
5.1. Implementation Details

and qijm =

k
B = ρ(ukcc0 m − zcc
0m) +
X
X
C=−
pn
n

Algorithm 1 Learning Algorithm of Hawkes Processes
1: Input: Event sequences S. The threshold V . Predefined parameters β, σκ , and γ.
2: Output: Parameters A and µ.
3: Initialize Ak and µk randomly. Z k = Ak , U k = 0.
k = 0, ρ = 1.
4: repeat
5:
Obtain Ak+1 and µk+1 via (11).
6:
Obtain Z k+1 via (12).
7:
Obtain U k+1 via (13).
8:
k = k + 1, ρ = 1.5ρ.
9: until kAk − Ak−1 kF < V
10: A = Ak , µ = µk .

and Z

k+1

, we can further update

U k+1 = U k + (Ak+1 − Z k+1 ).

(13)

To demonstrate the usefulness of our data synthesis
method, we combine it with various learning algorithms of
Hawkes processes and learn different models accordingly
from SDC event sequences. For time-invariant Hawkes
processes, we consider two learning algorithms — our EMbased learning algorithm and the least squares (LS) algorithm in (Eichler et al., 2016). For time-varying Hawkes
processes, we apply our EM-based learning algorithm. In
the following experiments, we use Gaussian basis functions: κm (t) = exp((t − tm )2 /σκ ) with center tm and
bandwidth σκ . The number and the bandwidth of basis can
be set according to the basis selection method proposed
in (Xu et al., 2016a). Additionally, we set V = 10−4 ,
γ = 1, and σs = 1 in our algorithm. Given SDC event
sequences, we learn Hawkes processes in three ways: 1)
learning directly from SDC event sequences; 2) applying the stationary bootstrap method in (Politis & Romano,
1994) to generate more synthetic SDC event sequences
and learning from these sequences accordingly; 3) learning from stitched sequences generated via our data synthesis method. For real-world data, whose SDC sequences do
not have predefined starting and ending time stamps, we
applied the variants of our method mentioned in the end of
Section 3.1.
5.2. Synthetic Data
The synthetic SDC event sequences are generated via the
following method: 2000 complete event sequences are simulated in the time window [0, 50] based on a 2-dimensional
Hawkes process. The base intensity {µc }2c=1 are randomly
generated in the range [0.1, 0.2]. The parameter of trigger-

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

(a) Our learning algorithm
Figure 3. Comparisons on log-likelihood and relative error.

Figure 4. Comparisons on infectivity functions {ψcc0 (t)}. The
number of original SDC sequences is 200 and stitched via our
method once.
(b) Least squares algorithm
Figure 2. Comparisons on log-likelihood and relative error.

ing kernel, β, is set to be 0.2. For time-invariant Hawkes
processes, we set the infectivity {ψcc0 (t)} to be 4 constants
randomly generated in the range [0, 0.2]. For time-varying
cc0
Hawkes processes, we set ψcc0 (t) = 0.2 cos(2π ω50
t),
where {ωcc0 } are randomly generated in the range [1, 4].
Given these complete event sequences, we select 1000 sequences as testing set while the remaining 1000 sequences
as training set. To generate SDC event sequences, we segment time window into 10 intervals, and just randomly preserve the data in one interval for each training sequences.
We test all methods in 10 trials and compare them on the
relative error between real parameters Θ and their estimab 2
b i.e., kΘ−Θk
tion results Θ,
kΘk2 , and the log-likelihood of testing sequences.
Time-invariant Hawkes Processes. Fig. 2 shows the comparisons on log-likelihood and relative error for various
methods. In Fig. 2(a) we can find that compared with the
learning results based on complete event sequences, the results based on SDC event sequences degrade a lot (lower
log-likelihood and higher relative error) because of the loss
of information. Our data synthesis method improves the
learning results consistently with the increase of training
sequences, which outperforms its bootstrap-based competitor (Politis & Romano, 1994) as well. To demonstrate the
universality of our method, besides our EM-based algorithm, we apply our method to the Least Squares (LS) algo-

rithm (Eichler et al., 2016). Fig. 2(b) shows that our method
also improves the learning results of the LS algorithm in
the case of SDC event sequences. Both the log-likelihood
and the relative error obtained from the stitched sequences
approach to the results learned from complete sequences.
Time-varying Hawkes Processes. Fig. 3 shows the comparisons on log-likelihood and relative error for various
methods. Similarly, the learning results are improved because of applying our method — higher log-likelihood and
lower relative error are obtained and their standard deviation (the error bars associated with curves) is shrunk. In
this case, applying our method twice achieves better results
than applying once, which verifies the usefulness of the iterative framework in our sampling-stitching algorithm. Besides objective measurements, in Fig. 4 we visualize the
infectivity functions {ψcc0 (t)}. It is easy to find that the
infectivity functions learned from stitched sequences (red
curves) are comparable to those learned from complete
event sequences (yellow curves), which have small estimation errors of the ground truth (black curves).
Note that our iterative framework is useful, especially
for time-varying Hawkes processes, when the number of
stitches is not very large. In our experiments, we fixed
the maximum number of synthetic sequences. As a result,
Figs. 2 and 3 show that the likelihoods first increase (i.e.,
stitching once or twice) and then decrease (i.e., stitching
more than three times) while the relative errors have opponent changes w.r.t. the number of stitches. These phenomena imply that too many stitches introduce too much unreliable interdependency among events. Therefore, we fix

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

(a) LinkedIn data

(a) LinkedIn data

(b) MIMIC III data

Figure 5. Comparisons on log-likelihood.

the number of stitches to 2 in the following experiments.
5.3. Real-World Data
Besides synthetic data, we also test our method on realworld data, including the LinkedIn data collected by ourselves and the MIMIC III data set (Johnson et al., 2016).
LinkedIn Data. The LinkedIn data we collected online
contain job hopping records of 3, 000 LinkedIn users in 82
IT companies. For each user, her/his check-in time stamps
corresponding to different companies are recorded as an
event sequence, and her/his profile (e.g., education background, skill list, etc.) is treated as the feature associated
with the sequence. For each person, the attractiveness of
a company is always time-varying. For example, a young
man may be willing to join in startup companies and increase his income via jumping between different companies. With the increase of age, he would more like to stay
in the same company and achieve internal promotions. In
other words, the infectivity network among different companies should be dynamical w.r.t. the age of employees.
Unfortunately, most of the records in LinkedIn are short
and doubly-censored — only the job hopping events in recent years are recorded. How to construct the dynamical infectivity network among different companies from the SDC
event sequences is still an open problem.
Applying our data synthesis method, we can stitch different users’ job hopping sequences based on their ages (time
stamps) and their profile (feature) and learn the dynamical
network of company over time. In particular, we select 100
users with relatively complete job hopping history (i.e., the
range of their working experience is over 25 years) as testing set. The remaining 2, 900 users are randomly selected
as training set. The log-likelihood of testing set in 10 trials
is shown in Fig. 5(a). We can find that the log-likelihood
obtained from stitched sequences is higher than that obtained from original SDC sequences or that obtained from
the sequences generated via the bootstrap method (Politis
& Romano, 1994), and its standard deviation is bounded

(b) MIMIC III data
Figure 6. Comparisons on infectivity functions {ψcc0 (t)}.

stably. Fig. 6(a) visualizes the adjacent matrix of infectivity network. The properties of the network verifies the
rationality of our results: 1) the diagonal elements of the
adjacent matrix are larger than other elements in general,
which reflects the fact that most employees would like to
stay in the same company and achieve a series of internal
promotions; 2) with the increase of age, the infectivity network becomes sparse, which reflects the fact that users are
more likely to try different companies in the early stages of
their careers.
MIMIC III Data. The MIMIC III data contain admission records of over 40, 000 patients in the Beth Israel
Deaconess Medical Center between 2001 and 2012. For
each patient, her/his admission time stamps and diseases
(represented via the ICD-9 codes (Deyo et al., 1992)) are
recorded as an event sequence, and her/his profile (including gender, race and chronic history) is represented as binary feature of the sequence. As aforementioned, some
work (Choi et al., 2015) has been done to extract timeinvariant disease network from admission records. However, the real disease network should be time-varying w.r.t.
the age of patient. Similar to the LinkedIn data, here we
only obtain SDC event sequences — the ranges of most
admission records are just 1 or 2 years.
Applying our data synthesis method, we can leverage the
information from different patients and stitch their sequences based on their ages and their profile. Focusing on
600 common diseases in 12 categories, we select 15, 000
patients’ admission records randomly as training set and
1, 000 patients with relatively complete records as testing set. Fig. 5(b) shows that applying our data synthesis
method indeed helps to improve log-likelihood of testing
data. Compared with our bootstrap-based competitor, our
data synthesis method gets more obvious improvements.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Figure
7. The diseases (nodes) are labeled with ICD-9 codes. The colors indicate their sub-categories. The size of the c-th node is
P
0 (t)). The width of edge is ψcc0 (t).
ψ
0
cc
c

Furthermore, we visualize the adjacent matrix of dynamical network of disease categories in Fig. 6(b). We can find
that: 1) with the increase of age the disease network becomes dense, which reflects the fact that the complications
of diseases are more and more common when people become old; 2) the networks show that neoplasms and the
diseases of circulatory, respiratory, and digestive systems
have strong self-triggering patterns because the treatments
of these diseases often include several phases and require
patients to make admission multiple times; 3) for kids and
teenagers, their disease networks (i.e., “Age 0” and “Age
10” networks) are very sparse, and their common diseases
mainly include neoplasms and the diseases of circulatory,
respiratory, and digestive systems; 4) for middle-aged people, the reasons for their admissions are diverse and complicated so that their disease networks are dense and include many mutually-triggering patterns; 5) for longevity
people, their disease networks (i.e., “Age 80” and “Age 90”
networks) are relatively sparser than those of middle-aged
people, because their admissions are generally caused by
elderly chronic diseases.
Additionally, we visualize the dynamical networks of the
diseases of circulatory systems in Fig. 7, and find some interesting triggering patterns. For example, for kids (“Age
0” network), the typical circulatory diseases are “diseases
of mitral and aortic valves” (ICD-9 396) and “cardiac dysrhythmias” (ICD-9 427), which are common for premature
babies and the kids having congenital heart disease. For
the old (“Age 80” network), the network becomes dense.
We can find that 1) as a main cause of death, “heart failure” (ICD-9 428) is triggered via multiple other diseases,
especially “secondary hypertension” (ICD-9 405); 2) “sec-

ondary hypertension” is also likely to cause “other and illdefined cerebrovascular disease” (ICD-9 437); 3) “Hemorrhoids” (ICD-9 455), as a common disease with strong
self-triggering pattern, will cause frequent admissions of
patients. In summary, the analysis above verifies the rationality of our result — the dynamical disease networks we
learned indeed reflect the properties of human’s health trajectory. The list of ICD-9 codes and the complete enlarged
network over age are shown in the supplementary file.

6. Conclusion
In this paper, we propose a novel data synthesis method to
learn Hawkes processes from SDC event sequences. With
the help of temporal information and optional features,
we measure the similarities among different SDC event
sequences and estimate the distribution of potential long
event sequences. Applying a sampling-stitching mechanism, we successfully synthesize a large amount of long
event sequences and learn point processes robustly. We
test our method for both time-invariant and time-varying
Hawkes processes. Experiments show that our data synthesis method improves the robustness of learning algorithms
for various models. In the future, we plan to provide more
theoretical and quantitative analysis to our data synthesis
method and apply it to more applications.

Acknowledgements
This work is supported in part via NSF IIS-1639792,
DMS-1317424, NIH R01 GM108341, NSFC 61628203,
U1609220 and the Key Program of Shanghai Science and
Technology Commission 15JC1401700.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

References
Bacry, Emmanuel, Delattre, Sylvain, Hoffmann, Marc, and
Muzy, Jean-Francois. Some limit theorems for hawkes
processes and application to financial statistics. Stochastic Processes and their Applications, 123(7):2475–2499,
2013.
Choi, Edward, Du, Nan, Chen, Robert, Song, Le, and Sun,
Jimeng. Constructing disease network and temporal progression model via context-sensitive hawkes process. In
ICDM, 2015.
Cowling, Ann, Hall, Peter, and Phillips, Michael J. Bootstrap confidence regions for the intensity of a poisson
point process. Journal of the American Statistical Association, 91(436):1516–1524, 1996.
Daley, Daryl J and Vere-Jones, David. An introduction to
the theory of point processes: volume II: general theory and structure. Springer Science & Business Media,
2007.
De Gruttola, Victor and Lagakos, Stephen W. Analysis of
doubly-censored survival data, with application to aids.
Biometrics, pp. 1–11, 1989.
Deyo, Richard A, Cherkin, Daniel C, and Ciol, Marcia A.
Adapting a clinical comorbidity index for use with icd9-cm administrative databases. Journal of clinical epidemiology, 45(6):613–619, 1992.
Efron, Bradley. The jackknife, the bootstrap and other resampling plans, volume 38. SIAM, 1982.
Eichler, Michael, Dahlhaus, Rainer, and Dueck, Johannes.
Graphical modeling for multivariate hawkes processes
with nonparametric link functions. Journal of Time Series Analysis, 2016.
Fan, Chun-Po Steve. Local Likelihood for Intervalcensored and Aggregated Point Process Data. PhD thesis, University of Toronto, 2009.
Gonçalves, Sılvia and Kilian, Lutz. Bootstrapping autoregressions with conditional heteroskedasticity of unknown form. Journal of Econometrics, 123(1):89–120,
2004.

Johnson, Alistair EW, Pollard, Tom J, Shen, Lu, Lehman,
Li-wei H, Feng, Mengling, Ghassemi, Mohammad,
Moody, Benjamin, Szolovits, Peter, Celi, Leo Anthony,
and Mark, Roger G. Mimic-iii, a freely accessible critical care database. Scientific data, 3, 2016.
Kirk, Paul DW and Stumpf, Michael PH. Gaussian process regression bootstrapping: exploring the effects of
uncertainty in time course data. Bioinformatics, 25(10):
1300–1306, 2009.
Klein, John P and Moeschberger, Melvin L. Survival
analysis: techniques for censored and truncated data.
Springer Science & Business Media, 2005.
Kobayashi, Ryota and Lambiotte, Renaud. Tideh: Timedependent hawkes process for predicting retweet dynamics. arXiv preprint arXiv:1603.09449, 2016.
Luo, Dixin, Xu, Hongteng, Zhen, Yi, Ning, Xia, Zha,
Hongyuan, Yang, Xiaokang, and Zhang, Wenjun. Multitask multi-dimensional hawkes processes for modeling
event sequences. In IJCAI, 2015.
Luo, Dixin, Xu, Hongteng, Zhen, Yi, Dilkina, Bistra, Zha,
Hongyuan, Yang, Xiaokang, and Zhang, Wenjun. Learning mixtures of markov chains from aggregate data with
structural constraints. Transactions on Knowledge and
Data Engineering, 28(6):1518–1531, 2016.
Mei, Hongyuan and Eisner, Jason. The neural hawkes process: A neurally self-modulating multivariate point process. arXiv preprint arXiv:1612.09328, 2016.
Paparoditis, Efstathios and Politis, Dimitris N. Tapered
block bootstrap. Biometrika, 88(4):1105–1119, 2001.
Politis, Dimitris N and Romano, Joseph P. The stationary
bootstrap. Journal of the American Statistical association, 89(428):1303–1313, 1994.
Rubin, Donald B. Multiple Imputation for Nonresponse in
Surveys, volume 307. John Wiley & Sons, 2009.
Streit, Roy L. Poisson point processes: imaging, tracking,
and sensing. Springer Science & Business Media, 2010.
Sun, J and Kalbfleisch, JD. Estimation of the mean function
of point processes based on panel count data. Statistica
Sinica, pp. 279–289, 1995.

Guan, Yongtao and Loh, Ji Meng. A thinned block bootstrap variance estimation procedure for inhomogeneous
spatial point patterns. Journal of the American Statistical
Association, 102(480):1377–1386, 2007.

Turnbull, Bruce W. Nonparametric estimation of a survivorship function with doubly censored data. Journal of
the American Statistical Association, 69(345):169–173,
1974.

Hawkes, Alan G and Oakes, David. A cluster process representation of a self-exciting process. Journal of Applied
Probability, pp. 493–503, 1974.

Van den Berg, Gerard J and Drepper, Bettina. Inference for
shared-frailty survival models with left-truncated data.
Econometric Reviews, 35(6):1075–1098, 2016.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

Wellner, Jon A and Zhang, Ying. Two estimators of the
mean of a counting process with panel count data. Annals of Statistics, pp. 779–814, 2000.
Xu, Hongteng, Zhen, Yi, and Zha, Hongyuan. Trailer generation via a point process-based visual attractiveness
model. In IJCAI, 2015.
Xu, Hongteng, Farajtabar, Mehrdad, and Zha, Hongyuan.
Learning granger causality for hawkes processes. In
ICML, 2016a.
Xu, Hongteng, Ning, Xia, Zhang, Hui, Rhee, Junghwan,
and Jiang, Guofei. Pinfer: Learning to infer concurrent request paths from system kernel events. In ICAC,
2016b.
Xu, Hongteng, Wu, Weichang, Nemati, Shamim, and Zha,
Hongyuan. Icu patient flow prediction via discriminative learning of mutually-correcting processes. arXiv
preprint arXiv:1602.05112, 2016c.
Yang, Shuang-Hong and Zha, Hongyuan. Mixture of mutually exciting processes for viral diffusion. In ICML,
2013.
Zhao, Qingyuan, Erdogdu, Murat A, He, Hera Y, Rajaraman, Anand, and Leskovec, Jure. Seismic: A selfexciting point process model for predicting tweet popularity. In KDD, 2015.
Zhou, Ke, Zha, Hongyuan, and Song, Le. Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes. In AISTATS, 2013a.
Zhou, Ke, Zha, Hongyuan, and Song, Le. Learning triggering kernels for multi-dimensional hawkes processes. In
ICML, 2013b.

