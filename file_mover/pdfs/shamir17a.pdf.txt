Online Learning with Local Permutations and Delayed Feedback

Ohad Shamir * 1 Liran Szlak * 1

Abstract
We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner
is allowed to slightly permute the order of the
loss functions generated by an adversary. On one
hand, this models natural situations where the exact order of the learner’s responses is not crucial,
and on the other hand, might allow better learning and regret performance, by mitigating highly
adversarial loss sequences. Also, with random
permutations, this can be seen as a setting interpolating between adversarial and stochastic
losses. In this paper, we consider the applicability of this setting to convex online learning with
delayed feedback, in which the feedback on the
prediction made in round t arrives with some delay τ . With such delayed feedback, the best
√ possible regret bound is well-known to be O( τ T ).
We prove that by being able to permute losses by
a distance of at most M√(for M ≥
pτ ), the regret
can be improved to O( T (1 + τ 2 /M )), using a Mirror-Descent based algorithm which can
be applied for both Euclidean and non-Euclidean
geometries. We also prove a lower bound, showing that for M <√τ /3, it is impossible to improve
the standard O( τ T ) regret bound by more than
constant factors. Finally, we provide some experiments validating the performance of our algorithm.

1. Introduction
Online learning is traditionally posed as a repeated game
where the learner has to provide predictions on an arbitrary sequence of loss functions, possibly even generated
adversarially. Although it is often possible to devise algorithms with non-trivial regret guarantees, these have to cope
with arbitrary loss sequences, which makes them conserva*

Equal contribution 1 Weizmann Institute of Science, Rehovot, Israel.
Correspondence to:
Liran Szlak <liran.szlak@weizmann.ac.il>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tive and in some cases inferior to algorithms not tailored to
cope with worst-case behavior. Indeed, an emerging line of
work considers how better online learning can be obtained
on “easy” data, which satisfies some additional assumptions. Some examples include losses which are sampled
i.i.d. from some distribution, change slowly in time, have
a consistently best-performing predictor across time, have
some predictable structure, mix adversarial and stochastic
losses, etc. (e.g. (Sani et al., 2014; Karnin and Anava,
2016; Bubeck and Slivkins, 2012; Seldin and Slivkins,
2014; Hazan and Kale, 2010; Chiang et al., 2012; Steinhardt and Liang, 2014; Hazan and Kale, 2011; Rakhlin and
Sridharan, 2013; Seldin and Slivkins, 2014)).
In this paper, we take a related but different direction:
Rather than explicitly excluding highly adversarial loss sequences, we consider how slightly perturbing them can mitigate their worst-case behavior, and lead to improved performance. Conceptually, this resembles smoothed analysis (Spielman and Teng, 2004), in which one considers the
worst-case performance of some algorithm, after performing some perturbation to their input. The idea is that if the
worst-case instances are isolated and brittle, then a perturbation will lead to easier instances, and better reflect the
attainable performance in practice.
Specifically, we propose a setting, in which the learner
is allowed to slightly reorder the sequence of losses generated by an adversary: Assuming the adversary chooses
losses h1 , . . . , hT , and before any losses are revealed, the
learner may choose a permutation σ on {1, . . . , T }, satisfying maxt |t − σ(t)| ≤ M for some parameter M ,
and then play a standard online learning game on losses
hσ(1) , . . . , hσ(T ) . We denote this as the Online Learning
with Local Permutations (OLLP) setting. Here, M controls
the amount of power given to the learner: M = 0 means
that no reordering is performed, and the setting is equivalent to standard adversarial online learning. At the other
extreme, M = T means that the learner can reorder the
losses arbitrarily. For example, the learner may choose to
order the losses uniformly at random, making it a quasistochastic setting (the only difference compared to i.i.d.
losses is that they are sampled without-replacement rather
than with-replacement).
We argue that allowing the learner some flexibility in the

Online Learning with Local Permutations and Delayed Feedback

order of responses is a natural assumption. For example,
when the learner needs to provide rapid predictions on a
high-frequency stream of examples, it is often immaterial
if the predictions are not provided in the exact same order at
which the examples arrived. Indeed, by buffering examples
for a few rounds before being answered, one can simulate
the local permutations discussed earlier.
We believe that this setting can be useful in various online learning problems, where it is natural to change a bit
the order of the loss functions. In this paper, we focus on
one well-known problem, namely online learning with delayed feedback. In this case, rather than being provided
with the loss function immediately after prediction is made,
the learner only receives the loss function after a certain
number τ ≥ 1 of rounds. This naturally models situations where the feedback comes much more slowly than
the required frequency of predictions: To give a concrete
example, consider a web advertisement problem, where an
algorithm picks an ad to display, and then receives a feedback from the user in the form of a click. It is likely that
the algorithm will be required to choose ads for new users
while still waiting for the feedback from the previous user.
Web advertisement, answering user queries and many other
regimes where the feedback come from a user, are all relevant to this setting. It is also plausible that the distribution
of examples in the above scenarios is not stochastic neither
is it adversarial, and thus the proposed setting of Online
Learning With Local Permutation is relevant to these cases.
For convex online learning with delayed feedback, in a
standard adversarial setting, it √
is known that the attainable regret is on the order of O( τ T ), and this is also the
best possible in the worst case (Weinberger and Ordentlich,
2002; Mesterharm, 2005; Langford et al., 2009; Joulani
et al., 2013; Quanrud and Khashabi, 2015). On the other
hand, in a stochastic setting where the losses are sampled
i.i.d. from some distribution, (Agarwal and Duchi, 2011)
show √
that the attainable regret is much better, on the order
of O( T +τ ). This gap between the worst-case adversarial
setting, and the milder i.i.d. setting, hints that this problem
is a good fit for our OLLP framework.
Thus, in this paper, we focus on online learning with feedback delayed up to τ rounds, in the OLLP framework
where the learner is allowed to locally permute the loss
functions (up to a distance of M ). First, we devise an algorithm, denoted as Delayed Permuted Mirror Descent, and
prove
p that it achieves an expected regret bound of order
O( T (τ 2 /M + 1)) assuming M ≥ τ . As M increases
compared to τ , this regret
bound interpolates between
the
√
√
standard adversarial τ T regret, and a milder T regret,
typical of i.i.d. losses. As its name implies, the algorithm
is based on the well-known online mirror descent (OMD)
algorithm (see (Hazan et al., 2016; Shalev-Shwartz et al.,

2012)), and works in the same generality, involving both
Euclidean and non-Euclidean geometries. The algorithm
is based on dividing the entire sequence of functions into
blocks of size M and performing a random permutation
within each block. Then, two copies of OMD are ran
on different parts of each block, with appropriate parameter settings. A careful analysis, mixing adversarial and
stochastic elements, leads to the regret bound.
In addition, we provide a lower bound complementing our
upper bound analysis, showing that when M is significantly smaller than τ (specifically, τ /3), then even with
local permutations, it√is impossible to obtain a worse-case
regret better than Ω( τ T ), matching (up to constants) the
attainable regret in the standard adversarial setting where
no permutations are allowed. Finally, we provide some experiments validating the performance of our algorithm.
The rest of the paper is organized as follows: in section 2
we formally define the Online Learning with Local Permutation setting, section 3 describes the Delayed Permuted
Mirror Descent algorithm and outlines its regret analysis,
section 4 discusses a lower bound for the delayed setting
with limited permutation power, section 5 shows experiments, and finally section 6 provides concluding remarks,
discussion, and open questions. Appendix A contains most
of the proofs.

2. Setting and Notation
Convex Online Learning. Convex online learning is posed
as a repeated game between a learner and an adversary
(assumed to be oblivious in this paper). First, the adversary chooses T convex losses h1 , . . . , hT which are
functions from a convex set W to R. At each iteration
t ∈ {1, 2, . . . , T }, the learner makes a prediction wt , and
suffers a loss of ht (wt ). To simplify the presentation, we
use the same notation ∇ht (w) to denote either a gradient
of ht at w (if the loss is differentiable) or a subgradient at
w otherwise, and refer to it in both cases as a gradient. We
assume that both w ∈ W and the gradients of any function ht in any point w ∈ W are bounded w.r.t. some norm:
Given a norm k · k with a dual norm k · k∗ , we assume that
the diameter of the space W is bounded by B 2 and that
∀w ∈ W, ∀h ∈ {h1 , h2 , ..., hT } : k∇h (w) k∗ ≤ G. The
purpose of the learner is to minimize her (expected) regret,
i.e.
" T
#
T
X
X
∗
R(T ) = E
ht (wt ) −
ht (w )
t=1

t=1

where w∗ = argmin
w∈W

T
X

ht (w)

t=1

where the expectation is with respect to the possible randomness of the algorithm.

Online Learning with Local Permutations and Delayed Feedback

Learning with Local Permutations. In this paper, we introduce and study a variant of this standard setting, which
gives the learner a bit more power, by allowing her to
slightly modify the order in which the losses are processed, thus potentially avoiding highly adversarial but
brittle loss constructions. We denote this setting as the
Online Learning with Local Permutations (OLLP) setting.
Formally, letting M be a permutation window parameter, the learner is allowed (at the beginning of the game,
and before any losses are revealed) to permute h1 , . . . , hT
to hσ−1 (1) , . . . , hσ−1 (T ) , where σ is a permutation from
the set P erm := {σ : ∀t, |σ (t) − t| ≤ M }. After this
permutation is performed, the learner is presented with
the permuted sequence as in the standard online learning setting, with the same regret as before. To simplify notation, we let ft = hσ−1 (t) , so the learner is
presented with the loss sequence f1 , . . . , fT , and the regret
hPis the same asPthe standard
i regret, i.e. R(T ) =
T
T
∗
E
f
(w
)
−
f
(w
)
. Note that if M = 0
t
t=1 t
t=1 t
then we are in the fully adversarial setting (no permutation is allowed). At the other extreme, if M = T and σ
is chosen uniformly at random, then we are in a stochastic
setting, with a uniform distribution over the set of functions
chosen by the adversary (note that this is close but differs a
bit from a setting of i.i.d. losses). In between, as M varies,
we get an interpolation between these two settings.
Learning with Delayed Feedback. The OLLP setting can
be useful in many applications, and can potentially lead to
improved regret bounds for various tasks, compared to the
standard adversarial online learning. In this paper, we focus
on studying its applicability to the task of learning from
delayed feedback.
Whereas in standard online learning, the learner gets to observe the loss ft immediately at the end of iteration t, here
we assume that at round t, she only gets to observe ft−τ for
some delay parameter τ < T (and if t < τ , no feedback
is received). For simplicity, we focus on the case where
τ is fixed, independent of t, although our results can be
easily generalized (as discussed in subsection 3.3). We emphasize that this is distinct from another delayed feedback
scenario sometimes studied in the literature (Agarwal and
Duchi, 2011; Langford et al., 2009), where rather than receiving ft−τ the learner only receives a (sub)gradient of
ft−τ at wt−τ . This is a more difficult setting, which is relevant for instance when the delay is due to the time it takes
to compute the gradient.

3. Algorithm and Analysis
Our algorithmic approach builds on the well-established
online mirror descent framework. Thus, we begin with a
short reminder of the Online Mirror Descent algorithm. For

a more extensive explanation refer to (Hazan et al., 2016).
Readers who are familiar with the algorithm are invited to
skip to Subsection 3.1.
The online mirror descent algorithm is a generalization of
online gradient descent, which can handle non-Euclidean
geometries. The general idea is the following: we start
with some point wt ∈ W, where W is our primal space.
We then map this point to the dual space using a (strictly
convex and continuously differentiable) mirror map ψ, i.e.
∇ψ (wt ) ∈ W ∗ , then perform the gradient update in the
dual space, and finally map the resulting new point back
to our primal space W again, i.e. we want to find a point
wt+1 ∈ W s.t. ∇ψ (wt+1 ) = ∇ψ (wt ) − η · gt where gt
denotes the gradient. Denoting by wt+ 21 the point satisfying ∇ψ(wt+ 21 ) = ∇ψ (wt ) − η · gt , it can be shown that
wt+ 12 = (∇ψ ∗ ) (∇ψ (wt ) − η · gt ), where ψ ∗ is the dual
function of ψ. This point, wt+ 12 , might lie outside our hypothesis class W, and thus we might need to project it back
to our space W. We use the Bregman divergence associated
to ψ to do this:
wt+1 = argmin4ψ (w, wt+ 21 ),
w∈W

where the Bregman divergence ∆ψ is defined as
4ψ (x, y) = ψ (x) − ψ (y) − h∇ψ(y), x − yi.
Specific choices of the mirror map ψ leads to specific instantiations of the algorithms for various geometries. Perhaps the simplest example is ψ (x) = 21 kxk22 , with associated Bregman divergence 4ψ (x, y) = 21 · kx − yk2 . This
leads us to the standard and well-known online gradient descent algorithm, where wt+1 is the Euclidean projection on
the set W of
wt − η · gt .
Another example
is the negative entropy mirror map
Pn
ψ (x) =
x
i=1 i · log (xi ), which is 1-strongly convex
with
respect

Pn to the	 1-norm on the simplex W =
x ∈ Rn+ : i=1 xi = 1 . In that case, the resulting algorithm is the well-known multiplicative updates algorithm,
where
wt+1,i = wt,i · exp(−ηgt,i )/

n
X

wt,i · exp(−ηgt,j ).

j=1

Instead of the 1-norm on the simplex, one can also consider
arbitrary p-norms, and take ψ(x) = 21 · kxk2q , where q is the
dual norm (satisfying 1/p + 1/q = 1).
3.1. The Delayed Permuted Mirror Descent Algorithm
Before describing the algorithm, we note that we will focus
here on the case where the permutation window parameter

Online Learning with Local Permutations and Delayed Feedback

M is larger than the delay parameter τ . If M < τ√, then our
regret bound is generally no better than the O( τ T ) obtainable by a standard algorithm without any permutations,
and this is actually tight as shown in Section 4.
We now turn to present our algorithm, denoted as The Delayed Permuted Mirror Descent algorithm (see algorithm 1
below as well as figure 1 for a graphical illustration). First,
the algorithm splits the time horizon T into M consecutive blocks, and performs a uniformly random permutation
on the loss functions within each block. Then, it runs two
online mirror descent algorithms in parallel, and uses the
delayed gradients in order to update two separate predictors – wf and ws , where wf is used for prediction in the
first τ rounds of each block, and ws is used for prediction
in the remaining M − τ rounds (here, f stands for “first”
and s stands for “second”). The algorithm maintaining ws
crucially relies on the fact that the gradient of any two functions in a block (at some point w) is equal, in expectation
over the random permutation within each block. This allows us to avoid most of the cost incurred by delays within
each block, since the expected gradient of a delayed function and the current function are equal. A complicating factor is that at the first τ rounds of each block, no losses from
the current block has been revealed so far. To tackle this,
we use another algorithm (maintaining wf ), specifically to
deal with the losses at the beginning of each block. This
algorithm does not benefit from the random permutation,
and its regret scales the same as standard adversarial online learning with delayed feedback. However, as the block
size M increases, the proportion of losses handled by wf
decreases, and hence its influence on the overall regret diminishes.
The above refers to how the blocks are divided for purposes of prediction. For purposes of updating the predictor
of each algorithm, we need to use the blocks a bit differently. Specifically, we let T1 and T2 be two sets of indices.
T1 includes all indices from the first τ time points of every block, and is used to update wf . T2 includes the first
M −τ indices of every block, and is used to update ws (see
figure 1). Perhaps surprisingly, note that T1 and T2 are not
disjoint, and their union does not cover all of {1, . . . , T }.
The reason is that due to the random permutation in each
block, the second algorithm only needs to update on some
of the loss functions in each block, in order to obtain an
expected regret bound on all the losses it predicts on.
3.2. Analysis
The regret analysis of the Delayed Permuted Mirror Descent algorithm is based on a separate analysis of each
of the two mirror descent sub-algorithms, where in the
first sub-algorithm the delay parameter τ enters multiplicatively, but doesn’t play a significant role in the regret of

Algorithm 1 Delayed Permuted Mirror Descent
Input: M , ηf , ηs
Init: w1f = 0, w1s = 0, jf = js = 1
Divide T to consecutive blocks of size M , and permute
the losses uniformly at random within each block. Let
f1 , . . . , fT denote the resulting permuted losses.
for t = 1..., T do
if t ∈ first τ rounds of the block then
Predict using wjff
Receive a loss function from τ places back:
ft−M = fT1 (jf −τ ) . If none exists (in the first τ
iterations), take the 0 function.


Compute: ∇fT1 (jf −τ ) wjff −τ
Update: wjf + 1 =
 f 2 


(∇ψ ∗ ) ∇ψ wjff − ηf ∇fT1 (jf −τ ) wjff −τ


Project: wjf +1 = argmin4ψ w, wjf + 1
w∈W

f

2

jf = jf + 1
else
Predict using wjss
Receive a loss function from τ places back: ft−τ =
fT2 (js )


Compute: ∇ft−τ wjss = ∇fT2 (js ) wjss
Update: wjss + 1 =
2


(∇ψ ∗ ) ∇ψ wjss − ηs · ∇fT2 (js ) wjss


Project: wjs +1 = argmin4ψ w, wjss + 1
w∈W

2

js = js + 1
end if
end for

the second sub-algorithm (which utilizes the stochastic nature of the permutations). Combining the regret bound of
the two sub-algorithms, and using the fact that the portion
of losses predicted by the second algorithm increases with
M , leads to an overall regret bound improving in M .
In the proof, to analyze the effect of delay, we need a
bound on the distance between any two consequent predictors wt , wt+1 generated by the sub-algorithm. This depends on the mirror map and Bregman divergence used for
the update, and we currently do not have a bound holding
in full generality. Instead, we let Ψ(ηf ,G) be some upper
bound on kwt+1 − wt k, where the update is using step-size
ηf and gradients of norm ≤ G. Using Ψ(ηf ,G) we prove a
general bound for all mirror maps. In Lemmas 3 and 4 in
Appendix A.1, we show that for two common mirror maps
(corresponding to online gradient descent and multiplicative weights), Ψ(ηf ,G) ≤ c · ηf G for some numerical conp
stant c, leading to a regret bound of O( T (τ 2 /M + 1)).
Also, we prove theorem 1 for 1-strongly convex mirror

Online Learning with Local Permutations and Delayed Feedback
Block 3

Block 2

Block 1
𝑻𝟐
𝑻𝟏
𝒘𝒔

𝒘𝒇

Predictions
1

𝒘𝒔

𝒘𝒇

𝒘𝒔

𝒘𝒇

𝝉

𝝉

𝝉

𝝉

𝝉

𝝉

...

T

𝝉

...

T

𝝉

...

T

Updates for 𝒘𝒇
1

Updates for 𝒘𝒔
1

𝝉

M

M

M

Figure 1. Scheme of predictions and updates of both parallel algorithms (best viewed in color; see text for details). Top color bars mark
which iterations are in T1 (purple lines) and which are in T2 (green lines). Top timeline shows which predictor, wf or ws , is used to
predict in each iteration. Middle timeline shows where gradients for updating wf come from (first τ iterations of the previous block),
and lower timeline shows where gradients for updating ws come from (first M − τ iterations of the same block, each gradient from
exactly τ rounds back).

maps, although it can be generalized to any λ-strongly convex mirror map by scaling.
Theorem 1. Given a norm k · k, suppose that we run the
Delayed Permuted Mirror Descent algorithm using a mirror map ψ which is 1-strongly convex w.r.t. k · k, over a
domain W with diameter B 2 w.r.t the bregman divergence
of ψ: ∀w, v ∈ W : 4ψ (w, v) ≤ B 2 , and such that the
(sub)-gradient g of each loss function on any w ∈ W satisfies kgk∗ ≤ G (where k · k∗ is the dual norm of k · k).
Then the expected regret, given a delay parameter τ and
step sizes ηf , ηs satisfies:

E

" T
X

#
∗

ft (wt ) − ft (w ) ≤

t=1

B2
T τ G2
+ ηf ·
·
ηf
M
2

B2
Tτ2
· G · Ψ(ηf ,G) +
M
ηs
2
T · (M − τ ) G
+ ηs ·
·
M
2
+

Furthermore, if Ψ(ηf ,G) ≤ c · ηf G for some constant c,
√
√
B· 2M
√
and ηf = q B· M
,
η
=
, the regret
s
G· T ·(M −τ )
G· T ·τ ·( 12 +c·τ )
is bounded by
r
r
Tτ
1
2T (M − τ )
c
· BG
+c·τ +
· BG
M
2
M
!!
r
√
τ2
=O
T·
+1
M
r

√
When M = O(τ ), this bound is O( τ T ). similar to
the standard adversarial learning case. However,
as M in√
creases, the regret gradually improves to O( T +τ ), which
is the regret attainable in a purely stochastic setting with
i.i.d. losses. The full proof can be found in appendix A.1.1,
and we sketch below the main ideas.
First, using the definition of regret, we show that it is
enough to upper-bound the regret of each of the two subalgorithms separately. Then, by a standard convexity argument, we reduce this to bounding sums of terms of the
form E[hwtf − wf∗ , ∇ft (wtf )i] for the first sub-algorithm,
and E [hwts − ws∗ , ∇ft (wts )i] for the second sub-algorithm
(where wf∗ and ws∗ are the best fixed points in hindsight
for the losses predicted on by the first and second subalgorithms, respectively, and where for simplicity we assume the losses are differentiable). In contrast, we can
use the standard analysis of mirror descent, using delayed gradients, to get a bound for the somewhat differf
ent terms E[hwtf − wf∗ , ∇ft−τ (wt−τ
)i] for the first subalgorithm, and E [hwts − ws∗ , ∇ft−τ (wts )i] for the second
sub-algorithm. Thus, it remains to bridge between these
terms.
Starting with the second sub-algorithm, we note that since
we performed a random permutation within each block, the
expected value of all loss functions within a block (in expectation over the block, and evaluated at a fixed point) is
equal. Moreover, at any time point, the predictor ws maintained by the second sub-algorithm does not depend on the

Online Learning with Local Permutations and Delayed Feedback

delayed nor the current loss function. Therefore, conditioned on wts , and in expectation over the random permutation in the block, we have that
E[∇ft (wts )] = E[ft−τ (wts )]
from which it can be shown that
E [hwts − ws∗ , ∇ft (wts )i] = E [hwts − ws∗ , ∇ft−τ (wts )i]
Thus, up to a negligible factor having to do with the first
few rounds of the game, the second sub-algorithm’s expected regret does not suffer from the delayed feedback.
For the first sub-algorithm, we perform an analysis which
does not rely on the random permutation. Specifically, we first show that since we care just about the
sum of the losses, it is sufficient to bound the differf
ence between E[hwtf − wf∗ , ∇ft (wtf )i] and E[hwt+τ
−
f
wf∗ , ∇ft (wt )i]. Using Cauchy-Shwartz, this difference
f
k·k∇ft (wtf )k, which
can be upper bounded by kwtf −wt+τ
2
in turn is at most c · τ · ηf · G using our assumptions on the
gradients of the losses and the distance between consecutive predictors produced by the first sub-algorithm.
Overall, we get two regret bounds, one for each subalgorithm. The regret of the first sub-algorithm scales
with τ , similar to the no-permutation setting, but the subalgorithm handles only a small fraction of the iterations (the
first τ in every block of size M ). In the rest of the iterations,
where we use the second sub-algorithm, we get a bound
that resembles more the stochastic case, without such dependence on τ . Combining the two, the result stated in
Theorem 1 follows.
3.3. Handling Variable Delay Size
So far, we discussed a setting where the feedback arrives
with a fixed delay of size τ . However, in many situations
the feedback might arrive with a variable delay size τt at
any iteration t, which may raise a few issues.
First, feedback might arrive in an asynchronous fashion,
causing us to update our predictor using gradients from
time points further in past after already using more recent
gradients. This complicates the analysis of the algorithm.
A second, algorithmic problem, is that we could also possibly receive multiple feedbacks simultaneously, or no feedback at all, in certain iterations, since the delay is of variable size. One simple solution is to use buffering and reduce the problem to a constant delay setting. Specifically,
we assume that all delays are bounded by some maximal
delay size τ . We would like to use one gradient to update our predictor at every iteration (this is mainly for ease
of analysis, practically one could update the predictor with
multiple loss functions in a single iteration). In order to
achieve this, we can use a buffer to store loss functions that

were received but have not been used to update the predictors yet. We define Gradf and Grads , two buffers that
will contain gradients from time points in T1 or T2 , correspondingly. Each buffer is of size τ . If we denote by Ft
the set of function that have arrived in time t, we can simply store loss functions that have arrived asynchronously
in the buffers defined above, sort them in ascending order,
and take the delayed loss function from exactly τ iterations
back in the update step. This loss function must be in the
appropriate buffer since the maximal delay size is τ . From
this moment on, the algorithm can proceed as usual and its
analysis still applies.

4. Lower Bound
In this section, we give a lower bound in the setting where
M < τ3 with all feedback having delay of exactly τ . We
will show that for this case, the regret bound cannot be improved by more than a constant factor over the bound of
the adversarial onlinelearning
problem with a fixed delay
√ 
of size τ , namely Ω
τ T for a sequence of length T .
We hypothesize that this regret bound also cannot be significantly improved for any M = O(τ ) (and not just τ /3).
However, proving this remains an open problem.
Theorem 2. For every (possible randomized) algorithm A
with a permutation window of size M ≤ τ3 , there exists a
choice of linear, 1-Lipschitz functions over [−1, 1] ⊂ R,
such that the expected regret of A after T rounds (with respect to the algorithm’s randomness), is
" T
#
T
√ 
X
X
∗
E
ft (wt ) −
ft (w ) = Ω
τT
t=1

t=1

where w∗ = argmin
w∈W

T
X

ft (w)

t=1

For completeness, we we also provide in appendix A.2 a
proof that when M = 0 (i.e. no permutations
√ allowed),
then the worst-case regret is no better than Ω( τ T ). This
is of course a special case of Theorem 2, but applies to
the standard adversarial online setting (without any local
permutations), and the proof is simpler. The proof sketch
for the setting where no permutation is allowed was already
provided in (Langford et al., 2009), and our contribution is
in providing a full formal proof.
The proof in the case where M = 0 is based on linear
losses of the form ft = αt · wt over [−1, +1], where
αt ∈ {−1,√+1}. Without permutations, it is possible to
prove a Ω( τ T ) lower bound by dividing the T iterations
into blocks of size τ , where the α values of all losses at
each block is the same and randomly chosen to equal either
+1 or −1. Since the learner does not obtain any informa-

Online Learning with Local Permutations and Delayed Feedback

tion about this value until the block is over, this reduces to
adversarial online learning over T /τ rounds, where the regret at each p
round scales linearly
with τ , and overall regret
√
at least Ω(τ T /τ ) = Ω( τ T ).
In the proof of theorem 2, we show that by using a similar
construction, even with permutations, having a permutation
window less than τ /3 still means that the α values would
still be unknown until all loss functions of the block are
processed, leading to the same lower bound up to constants.
The formal proof appears in the appendix, but can be
sketched as follows: first, we divide the T iterations into
blocks of size τ /3. Loss functions within each block are
identical, of the form ft = αt · wt , and the value of α per
block is chosen uniformly at random from {−1, +1}, as
before. Since here, the permutation window M is smaller
than τ /3, then even after permutation, the time difference
between the first and last time we encounter an α that originated from a single block is less than τ . This means that
by the time we get any information on the α in a given
block, the algorithm already had to process all the losses
in the block, which leads to the same difficulty as the nopermutation setting. Specifically, since the predictors chosen by the algorithm when handling the losses of the block
do not depend on the α value in that block, and that α is
chosen randomly, we get that the expected loss of the algorithm at any time point t equals 0. Thus, the cumulative
loss across the entire loss sequence is also 0. In contrast,
for w∗ , the optimal predictor in hindsight over the entire
sequence,
we can prove an expected accumulated loss of
√
−Ω( τ T ) after T iterations, using Khintchine inequality
and the fact that the α’s were randomly chosen per block.
This
√ leads us to a lower bound of expected regret of order
τ T , for any algorithm with a local permutation window
of size M < τ /3.

5. Experiments
We consider the adversarial setting described in section 4,
where an adversary chooses a sequence of functions such
that every τ functions are identical, creating blocks of size
τ of identical loss functions, of the form ft (wt ) = αt · wt
where αt is chosen randomly in {−1, +1} for each block.
In all experiments we use T = 105 rounds, a delay parameter of τ = 200, set our step sizes according to the
theoretical analysis, and report the mean regret value over
1000 repetitions of the experiments.
In our first experiment, we considered the behavior of our
Delayed Permuted Mirror Descent algorithm, for window
sizes M > τ , ranging from τ + 1 to T . In this experiment,
we chose the α values randomly, while ensuring a gap of
200 between the number of blocks with +1 values and the
number of blocks with −1 values (this ensures that the op-

timal w∗ is a sufficiently strong competitor, since otherwise
the setting is too “easy” and the algorithm can attain negative regret in some situations). The results are shown in
Figures 2 and 3, where the first figure presents the accumulated regret of our algorithm over time, whereas the second
figure presents the overall regret after T rounds, as a function of the window size M .
When applying our algorithm in this setting with different values of M > τ , ranging from M = τ + 1 and up
to M = T , we get a regret that scales from the order of
the adversarial bound to the order of the stochastic bound
depending on the window size, as expected by our analysis. For all window sizes greater than 5 · τ , we get a regret
that is in the order of the stochastic bound - this is not surprising, since after the permutation we get a sequence of
functions that is very close to an i.i.d. sequence,
√ in which
case any algorithm can be shown to achieve O( T ) regret
in expectation. Note that this performance is better than
that√predicted by our theoretical analysis, which implies an
O( T ) behavior only when M ≥ Ω(τ 2 ). It is an open
and interesting question whether it means that our analysis
can be improved, or whether there is a harder construction
leading to a tighter lower bound.
In our second experiment, we demonstrate the brittleness
of the lower bound construction for standard online learning with delayed feedback, focusing on the M < τ regime.
Specifically, we create loss functions with blocks as before
(where following the lower bound construction, the α values in each block of size τ = 200 is chosen uniformly at
random). Then, we perform a random permutation over
consecutive windows of size M (ranging from M = 0 up
9
1
to M = 10
τ in intervals of 10
τ ). Finally, we run standard
Online Gradient
Descent
with
delayed
gradients (and fixed
√
step size 1/ T ), on the permuted losses. The results are
presented in Figure 4.
For window sizes M < τ2 we see that the regret is close to
the adversarial bound, whereas as we increase the window
size the regret decreases towards the stochastic bound. This
experiment evidently shows that this hardness construction
is indeed brittle, and easily breaks in the face of local permutations, even for window sizes M < τ .

6. Discussion
We presented the OLLP setting, where a learner can locally
permute the sequence of examples from which she learns.
This setting can potentially allow for improved learning in
many problems, where the worst-case regret is based on
highly adversarial yet brittle constructions. In this paper,
we focused on the problem of learning from delayed feedback in the OLLP setting, and showed how it is possible to
improve the regret by allowing local permutations. Also,

Online Learning with Local Permutations and Delayed Feedback

Figure 2. Regret of the Delayed Permuted Mirror Descent algorithm,√with local permutation in window sizes ranging from M = τ + 1
to√M = T . A pink ∗ indicates the order of the stochastic bound ( T + τ ), and a red ∗ indicates the order of the adversarial bound
( τ T ). Regret is averaged over 1000 repetitions. Best viewed in color.

Figure 3. Regret of the Delayed Permuted Mirror Descent algorithm for different window sizes, after T = 105 iterations, with
local permutation window sizes ranging from M = τ + 1 to
M = T . Red√dashed line (top) indicates the order of the adversarial bound ( τ T ) and green √
dashed line (bottom) indicates the
order of the stochastic bound ( T + τ ). Regret is averaged over
1000 repetitions, error bars indicate standard error of the mean.
Best viewed in color.

we proved a lower bound in the situation where the permutation window is significantly smaller than the feedback
delay, and showed that in this case, permutations cannot
allow for a better regret bound than the standard adversarial setting. We also provided some experiments, demonstrating the power of the setting as well as the feasibility
of the proposed algorithm. An interesting open question
is what minimal permutation size allows non-trivial regret
improvement, and whether our upper bound in Theorem 1
is tight. As suggested by our empirical experiments, it
is possible that even small local permutations are enough
to break highly adversarial sequences and improve performance in otherwise worst-case scenarios. Another interest-

Figure 4. Regret of the standard Online Gradient Descent algorithm, in a adversarialy designed setting as described in 4, and
with local permutation in window sizes ranging from M = 0 to
9
M = 10
τ . Red dashed line (top) indicates the order of the adver√
sarial bound ( τ T ) and green √
dashed line (bottom) indicates the
order of the stochastic bound ( T + τ ). Regret is averaged over
1000 repetitions, error bars indicate standard error of the mean.
Best viewed in color.

ing direction is to extend our results to a partial feedback
(i.e. bandit) setting. Finally, it would be interesting to study
other cases where local permutations allow us to interpolate
between fully adversarial and more benign online learning
scenarios.
Acknowledgements
OS is supported in part by an FP7 Marie Curie CIG grant,
the Intel ICRI-CI Institute, and Israel Science Foundation
grant 425/13. LS is an ISEF fellow.

Online Learning with Local Permutations and Delayed Feedback

References
Alekh Agarwal and John C Duchi. Distributed delayed
stochastic optimization. In Advances in Neural Information Processing Systems, pages 873–881, 2011.
Sébastien Bubeck and Aleksandrs Slivkins. The best of
both worlds: Stochastic and adversarial bandits. In
COLT, pages 42–1, 2012.

Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1287–1295, 2014.
Shai Shalev-Shwartz et al. Online learning and online conR in Machine
vex optimization. Foundations and Trends
Learning, 4(2):107–194, 2012.

Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad
Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu.
Online optimization with gradual variations. In COLT,
pages 6–1, 2012.

Daniel A Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the simplex algorithm usually
takes polynomial time. Journal of the ACM (JACM), 51
(3):385–463, 2004.

Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine
learning, 80(2-3):165–188, 2010.

Jacob Steinhardt and Percy Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm.
In ICML, pages 1593–1601, 2014.

Elad Hazan and Satyen Kale. Better algorithms for benign bandits. Journal of Machine Learning Research, 12
(Apr):1287–1311, 2011.

Marcelo J Weinberger and Erik Ordentlich. On delayed
prediction of individual sequences. IEEE Transactions
on Information Theory, 48(7):1959–1976, 2002.

Elad Hazan et al. Introduction to online convex optimizaR in Optimization, 2(3-4):
tion. Foundations and Trends
157–325, 2016.
Pooria Joulani, András György, and Csaba Szepesvári. Online learning under delayed feedback. In ICML (3),
pages 1453–1461, 2013.
Zohar S Karnin and Oren Anava. Multi-armed bandits:
Competing with optimal sequences. In NIPS, 2016.
John Langford, Alexander Smola, and Martin Zinkevich.
Slow learners are fast. arXiv preprint arXiv:0911.0491,
2009.
Ishai Menache, Ohad Shamir, and Navendu Jain. Ondemand, spot, or both: Dynamic resource allocation for
executing batch jobs in the cloud. In 11th International
Conference on Autonomic Computing (ICAC 14), pages
177–187, 2014.
Chris Mesterharm. On-line learning with delayed label
feedback. In International Conference on Algorithmic
Learning Theory, pages 399–413. Springer, 2005.
Kent Quanrud and Daniel Khashabi. Online learning with
adversarial delays. In Advances in Neural Information
Processing Systems, pages 1270–1278, 2015.
Alexander Rakhlin and Karthik Sridharan. Online learning
with predictable sequences. In COLT, pages 993–1019,
2013.
Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In Advances in
Neural Information Processing Systems, pages 810–818,
2014.

