Leveraging Union of Subspace Structure to Improve Constrained Clustering

John Lipor 1 Laura Balzano 1

Abstract
Many clustering problems in computer vision and
other contexts are also classification problems,
where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often
applied to problems that fit this description, for
example with face images or handwritten digits.
While it is straightforward to request human input on these datasets, our goal is to reduce this
input as much as possible. We present a pairwiseconstrained clustering algorithm that actively selects queries based on the union-of-subspaces
model. The central step of the algorithm is in
querying points of minimum margin between estimated subspaces; analogous to classifier margin,
these lie near the decision boundary. We prove
that points lying near the intersection of subspaces
are points with low margin. Our procedure can be
used after any subspace clustering algorithm that
outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the stateof-the-art active query algorithms on datasets with
subspace structure and is competitive on other
datasets.

1. Introduction
The union of subspaces (UoS) model, in which data vectors
lie near one of several subspaces, has been used actively in
the computer vision community on datasets ranging from
images of objects under various lighting conditions (Basri
& Jacobs, 2003) to visual surveillance tasks (Oliver et al.,
2000). The recent textbook (Vidal et al., 2016) includes a
number of useful applications for this model, including lossy
image compression, clustering of face images under different lighting conditions, and video segmentation. Subspace
clustering algorithms utilize the UoS model to cluster data
1

Department of Electrical and Computer Engineering, University Michigan, Ann Arbor, MI, USA. Correspondence to: John
Lipor <lipor@umich.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

vectors and estimate the underlying subspaces, achieving excellent performance on a variety of real datasets. However,
as we will show in Section 4, even oracle UoS classifiers
do not achieve perfect clustering on these datasets. While
current algorithms for subspace clustering are unsupervised,
in many cases a human could provide relevant information
in the form of pairwise constraints between points, e.g., answering whether two images are of the same person or
whether two objects are the same.
The incorporation of pairwise constraints into clustering algorithms is known as pairwise-constrained clustering (PCC).
PCC algorithms use supervision in the form of must-link
and cannot-link constraints by ensuring that points with
must-link constraints are clustered together and points with
cannot-link constraints are clustered apart. In (Davidson
et al., 2006), the authors investigate the phenomenon that
incorporating poorly-chosen constraints can lead to an increase in clustering error, rather than a decrease as one
would expect from additional label information. This is
because points constrained to be in the same cluster that
are otherwise dissimilar can confound the constrained clustering algorithm. For this reason, researchers have turned
to active query selection methods, in which constraints are
intelligently selected based on a number of heuristics. These
algorithms perform well across a number of datasets but do
not take advantage of any known structure in the data. In the
case where data lie on a union of subspaces, one would hope
that knowledge of the underlying geometry could give hints
as to which points are likely to be clustered incorrectly.

	N
Let X = xi ∈ RD i=1 be a set of data points lying near
a union of K linear subspaces of the ambient space. We
K
denote the subspaces by {Sk }k=1 , each having dimension
dk . An example union of subspaces is shown in Fig. 1,
where d1 = 2, d2 = d3 = 1. The goal of subspace clustering algorithms has traditionally been to cluster the points
in X according to their nearest subspace without any supervised input. We turn this around and ask whether this
model is useful for active clustering, where we request a
very small number of intelligently selected labels. A key observation when considering data well-modeled by a union of
subspaces is that uncertain points will be ones lying equally
distant to multiple subspaces. Using a novel definition of
margin tailored for the union of subspaces model, we incorporate this observation into an active subspace clustering

Leveraging Union of Subspace Structure to Improve Constrained Clustering
S2

S3

S1

Figure 1. Example union of K = 3 subspaces of dimensions d1 =
2, d2 = 1, and d3 = 1.

algorithm.
Our contributions are as follows. We introduce a novel algorithm for pairwise constrained clustering that leverages
UoS structure in the data. A key step in our algorithm is
choosing points of minimum margin, i.e., those lying near
a decision boundary between subspaces. We define a notion of margin for the UoS model and provide theoretical
insight as to why points of minimum margin are likely to be
misclustered by unsupervised algorithms. We show through
extensive experimental results that when the data lie near
a union of subspaces, our method drastically outperforms
existing PCC algorithms, requiring far fewer queries to
achieve perfect clustering. Our datasets range in dimension
from 256-2016, number of data points from 320-9298, and
number of subspaces from 5-100. On ten MNIST digits
with a modest number of queries, we get 5% classification
error with only 500 pairwise queries compared to about 20%
error for current state-of-the-art PCC algorithms and 35%
for unsupervised algorithms. We also achieve 0% classification error on the full Yale, COIL, and USPS datasets
with a small fraction of the number of queries needed by
competing algorithms. In datasets where we do not expect
subspace structure, our algorithm still achieves competitive
performance. Further, our algorithm is agnostic to the input subspace clustering algorithm and can therefore take
advantage of any future algorithmic advances for subspace
clustering.

2. Related Work
A survey of recently developed subspace clustering algorithms can be found in (Vidal, 2011) and the textbook (Vidal et al., 2016). In these and more recent work, clustering algorithms that employ spectral methods achieve
the best performance on most datasets. Notable examples of such algorithms include Sparse Subspace Clustering
(SSC) (Elhamifar & Vidal, 2013) and its extensions (You
et al., 2016b;a), Low-Rank Representation (LRR) (Liu et al.,
2010), Thresholded Subspace Clustering (TSC) (Heckel &
Bölcskei, 2015), and Greedy Subspace Clustering (GSC)

(Park et al., 2014). Many recent algorithms exist with both
strong theoretical guarantees and empirical performance,
and a full review of all approaches is beyond the scope
of this work. However, the core element of all recent algorithms lies in the formation of the affinity matrix, after
which spectral clustering is performed to obtain label estimates. In SSC, the affinity matrix is formed via a series
of `1 -penalized regressions. LRR uses a similar cost function but penalizes the nuclear norm instead of the `1 . TSC
thresholds the spherical distance between points, and GSC
works by successively (greedily) building subspaces from
points likely to lie in the same subspace. Of these methods,
variants of SSC achieve the best overall performance on
benchmark datasets and has the strongest theoretical guarantees, which were introduced in (Elhamifar & Vidal, 2013)
and strengthened in numerous recent works (Soltanolkotabi
& Candes, 2012; 2014; Wang & Xu, 2013; Wang et al.,
2016). While the development of efficient algorithms with
stronger guarantees has received a great deal of attention,
very little attention has been paid to the question of what to
do about data that cannot be correctly clustered. Thus, when
reducing clustering error to zero (or near zero) is a priority,
users must look beyond unsupervised subspace clustering
algorithms to alternative methods. One such method is
to request some supervised input in the form of pairwise
constraints, leading to the study of pairwise-constrained
clustering (PCC).
PCC algorithms work by incorporating must-link and
cannot-link constraints between points, where points with
must-link constraints are forced (or encouraged in the case
of spectral clustering) to be clustered together, and points
with cannot-link constraints are forced to be in separate
clusters. In many cases, these constraints can be provided
by a human labeler. For example, in (Biswas & Jacobs,
2014), the authors perform experiments where comparisons
between human faces are provided by users of Amazon
Mechanical Turk with an error rate of 1.2%. Similarly, for
subspace clustering datasets such as Yale B and MNIST, a
human could easily answer questions such as, “Are these
two faces the same person?” and “Are these two images
the same number?” An early example of PCC is found
in (Wagstaff et al., 2001), where the authors modify the
K-means cost function to incorporate such constraints. In
(Basu et al., 2004), the authors utilize active methods to initialize K-means in an intelligent “E XPLORE” phase, during
which neighborhoods of must-linked points are built up. After this phase, new points are queried against representatives
from each neighborhood until a must-link is obtained. A
similar explore phase is used in (Mallapragada et al., 2008),
after which a min-max approach is used to select the most
uncertain sample. Early work on constrained spectral clustering appears in (Xu et al., 2005; Wang & Davidson, 2010),
in which spectral clustering is improved by examining the

Leveraging Union of Subspace Structure to Improve Constrained Clustering

eigenvectors of the affinity matrix in order to determine the
most informative points. However, these methods are limited to the case of two clusters and therefore impractical in
many cases.
More recently, the authors in (Xiong et al., 2016; Biswas
& Jacobs, 2014) improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in (Xiong et al., 2016),
referred to as Uncertainty Reducing Active Spectral Clustering (URASC). URASC works by maintaining a set of
certain sets, whereby points in the same certain set are mustlinked and points in different certain sets are cannot-linked.
A test point xT is selected via an uncertainty-reduction
model motivated by matrix perturbation theory, after which
queries are presented in an intelligent manner until xT is
either matched with an existing certain set or placed in its
own new certain set. In practice (Xiong, 2016), the certain
sets are initialized using the E XPLORE algorithm of (Basu
et al., 2004).
While we are certainly not the first to consider actively selecting labels to improve clustering performance, to the best
of our knowledge we are the first to do so with structured
clusters. Structure within and between data clusters is often
leveraged for unsupervised clustering (Wright et al., 2009),
and that structure is also leveraged for adaptive sampling of
the structured signals themselves (e.g., see previous work
on sparse (Haupt et al., 2011; Indyk et al., 2011), structured
sparse (Soni & Haupt, 2014), and low rank signals (Krishnamurthy & Singh, 2013)). This paper emphasizes the power
of that structure for reducing the number of required labels
in an active learning algorithm as opposed to reducing the
number of samples of the signal itself, and points to exciting open questions regarding the tradeoff between signal
measurements and query requirements in semi-supervised
clustering.

3. UoS-Based Pairwise-Constrained
Clustering

	N
Recall that X = xi ∈ RD i=1 is a set of data points lying
K
on a union of K subspaces {Sk }k=1 , each having dimension
d. In this work, we assume all subspaces have the same
dimension, but it is possible to extend our algorithm to deal
with non-uniform dimensions. The goal is to cluster the data
points according to this generative model, i.e., assigning
each data point to its (unknown) subspace. In this section
we describe our algorithm, which actively selects pairwise
constraints in order to improve clustering accuracy. The key
step is choosing an informative query test point, which we
do using a novel notion of minimum subspace margin.
Denote the true clustering of a point x ∈ X by C(x). Let

the output of a clustering algorithm (such as SSC) be an
affinity/similarity matrix A and a set of label estimates
n
oN
. These are the inputs to our algorithm. The
Ĉ(xi )
i=1
high-level operation of our algorithm is as follows. To initialize, we build a set of certain sets Z using an E XPLORE-like
algorithm similar to that of (Basu et al., 2004). Certain sets
are in some sense equivalent to labels in that points within
a certain set belong to the same cluster and points across
certain sets belong to different clusters. Following this, the
following steps are repeated until a maximum number of
queries has been made:
1. Spectral Clustering: Obtain label estimates via spectral clustering.
2. PCA on each cluster: Obtain a low-dimensional subspace estimate from points currently sharing the same
estimated cluster label.
3. Select Test Point: Obtain a test point xT using subspace margin with respect to the just estimated subspaces.
4. Assign xT to Certain Set: Query the human to compare the test point with representatives from certain
sets until a must-link is found or all certain sets have
been queried, in which case the test point becomes its
own certain set.
5. Impute Label Information: Certain sets are used to
impute must-link and cannot-link values in the affinity
matrix.
We refer to our algorithm as SUPERPAC (SUbsPace clustERing with Pairwise Active Constraints). A diagram of the
algorithm is given in Fig. 2, and we outline each of these
steps below and provide pseudocode in Algorithm 1.
3.1. Sample Selection via Margin
Min-margin points have been studied extensively in active
learning; intuitively, these are points that lie near the decision boundary of the current classifier. In (Settles, 2012),
the author notes that actively querying points of minimum
margin (as opposed to maximum entropy or minimum confidence) is an appropriate choice for reducing classification error. In (Wang & Singh, 2016), the authors present a
margin-based binary classification algorithm that achieves
an optimal rate of convergence (within a logarithmic factor).
In this section, we define a novel notion of margin for
the UoS model and provide theoretical insight as to
why points of minimum margin are likely to be misclustered. For a subspace Sk with orthonormal basis Uk , let the distance of a point to that subspace
be

dist(x, Sk ) = miny∈Sk kx − yk2 = x − Uk UkT x2 . Let
k ∗ = arg mink∈[K] dist(x, Sk ) be the index of the closest
subspace, where [K] = {1, 2, · · · , K}. Then the subspace

Leveraging Union of Subspace Structure to Improve Constrained Clustering
Form Unsupervised
Affinity Matrix

Run PCA on
Individual Clusters

Spectral
Clustering

Select Test Point

0.8
0.6

xT = argminx ^
μ(x)

1

0.4
0.2

A

0.5
0
0

-0.2
-0.4
-0.5
-0.6
-0.8

Impute Label Information

Assign xT to Certain Set (actively query human)
No
No

xT

Yes

Z1
Z2
Z3

Figure 2. Diagram of SUPERPAC algorithm for pairwise constrained clustering.
0.8

Then

0.6

p
(1 + ε) σ 2 (D − d)
p
≤ µ(y)
1−
(1 − ε) σ 2 (D − d) + dist(x, S2 )2

0.4

0.2

0

and
-0.2

p
(1 − ε) σ 2 (D − d)
p
µ(y) ≤ 1 −
,
(1 + ε) σ 2 (D − d) + dist(x, S2 )2

-0.4

-0.6

-0.8
-1

-0.8

-0.6

-0.4

-0.2

0

0.2

0.4

0.6

0.8

1

Figure 3. Illustration of subspace margin. The blue and red lines
are the generative subspaces, with corresponding disjoint decision
regions. The yellow-green color shows the region within some
margin of the decision boundary, given by the dotted lines.

margin of a point x ∈ X is the ratio of closest and second
closest subspaces, defined as
µ̂(x) = 1 −

max

j6=k∗ ,j∈[K]

dist(x, Sk∗ )
.
dist(x, Sj )

(1)

The point of minimum margin is then defined as
arg minx∈X µ̂(x). Note that the fraction is a value in [0, 1],
where the a value of 0 implies that the point x is equidistant
to its two closest subspaces. This notion is illustrated in
Figure 3, where the yellow-green color shows the region
within some margin of the decision boundary.
In the following theorem, we show that points lying near
the intersection of subspaces are included among those of
minimum margin with high probability. This method of
point selection is then motivated by the fact that the difficult
points to cluster are those lying near the intersection of
subspaces [12]. Further, theory for SSC ([11],[15]) shows
that problematic points are those having large inner product
with some or all directions in other subspaces. Subspace
margin captures exactly this phenomenon.
Theorem 1. Consider two d-dimensional subspaces S1 and
S2 . Let y = x + n, where x ∈ S1 and n ∼ N (0, σ 2 ID ).
Define
dist(y, S1 )
µ(y) = 1 −
.
dist(y, S2 )

with probability at least 1 − 4e−cε
absolute constant.

2

(D−d)

, where c is an

The proof is given in the supplementary material. Note that
if dist(y, S1 ) ≤ dist(y, S2 ), then µ(y) = µ̂(y). In this case,
Thm. 1 states that under the given noise model, points with
small residual to the incorrect subspace (i.e., points near the
intersection of subspaces) will have small margin. These are
exactly the points for which supervised label information
will be most beneficial.
The statement of Thm. 1 allows us to quantify exactly how
near a point must be to the intersection of two subspaces to
be considered a point of minimum margin. Let φ1 ≤ φ2 ≤
· · · ≤ φd be the d principal angles1 between S1 and S2 . If
Pd
the subspaces are very far apart, d1 i=1 sin2 (φi ) is near
P
d
1, and if they are very close d1 i=1 sin2 (φi ) is near zero.
Note that, for any x ∈ S1 ,
sin2 (φ1 ) ≤ dist(x, S2 )2 ≤ sin2 (φd ) ;
that is, there are bounds on dist(x, S2 ) depending on the
relationship of the two subspaces. We also know that if x is
drawn using isotropic Gaussian weights from a basis for S1 ,
then
d

 1X
E dist(x, S2 )2 =
sin2 (φi ) .
d i=1
Given this, we might imagine that margin of the noisy points
is a useful indicator of points near the intersection in a scePd
nario where sin2 (φ1 ) is small but d1 i=1 sin2 (φi ) is not,
1

See (Golub & Loan, 2012) for a definition of principal angles.

Leveraging Union of Subspace Structure to Improve Constrained Clustering

e.g., when the subspaces have an intersection but are distant
in other directions. With this in mind we state the following
corollary, whose proof can be found in the supplementary
material.
Corollary 1. Suppose x1 ∈ S1 is such that
d

1X 2
sin (φi )
d i=1

dist(x1 , S2 )2 = sin2 (φ1 ) + δ

!
(2)

for some small δ ≥ 0; that is, x1 is close to the intersection
of S1 and S2 . Let x2 be a random point in S1 generated as
x2 = U1 w where U1 is a basis for S1 and w ∼ N (0, d1 Id ).
We observe yi = xi + ni , where ni ∼ N (0, σ 2 ), i = 1, 2.
If there exists τ > 1 such that
δ<

5 1
−
7 τ

and
τ



d
1
1X 2
sin2 (φ1 ) + σ 2 (D − d) <
sin (φi ) ,
6
d i=1

(3)

that is, the average angle is sufficiently larger than the
smallest angle, then
2

P {µ(y1 ) < µ(y2 )} ≥ 1 − e−c( 100 )
7

ds

2

− 4e−c( 50 )
1

(D−d)

where µ(y) is defined as in Thm. 1, c is an absolute constant,
Pd
and s = d1 i=1 sin2 (φi ).
We make some remarks first to connect our results to other
subspace distances that are often used. Perhaps the most
intuitive form of subspace distance between that spanned by
U1 and U2 is d1 k(I − U1 U1 )T U2 k2F ; if the two subspaces
are the same, the projection onto the orthogonal complement is zero; if they are orthogonal, we get the norm of U2
alone, giving a distance of 1. This is equal to the more visually symmetric 1 − d1 kU1T U2 k2F , another common distance.
Further we note that, by the definition of principal angles
(Golub & Loan, 2012),
1
1
1 − kU1T U2 k2F = 1 −
d
d

d
X
i=1

cos2 (φi ) =

1
d

d
X

sin2 (φi ) .

i=1

From Equation (2), we see that the size of δ determines
how close x1 ∈ S1 is to S2 ; if δ = 0, x1 is as close to S2
as possible. For example, if φ1 = 0, the two subspaces
intersect, and δ = 0 implies that x1 ∈ S1 ∩ S2 . Equation
(3) captures the gap between average principal angle and
the smallest principal angle. We conclude that if this gap
is large enough and δ is small enough so that x1 is close to
S2 , then the observed y1 will have smaller margin than the
average point in S1 , even when observed with noise.

Algorithm 1 SUPERPAC
Input: X = {x1 , x2 , . . . , xN }: data, K: number of
clusters, d: subspace dimension, A: affinity matrix, maxQueries: maximum number of pairwise comparisons
Estimate Labels: Ĉ ← S PECTRAL C LUSTERING(A,K)
Initialize Certain Sets: Initialize Z = {Z1 , · · · , Znc }
and numQueries via U O S-E XPLORE in supplementary
material.
while numQueries < maxQueries do
PCA on Each Cluster: Solve
X
Sk = min
kxi − U U 0 xi k2 .
U ∈RD×d

i:Ĉ(xi )=k

Obtain Test Point: select xT ← arg minx∈X µ̂(x)
Assign xT to Certain Set:
Sort {Z1 , · · · , Znc } in order of most likely mustlink (via subspace residual for xT ), query xT against
representatives from Zk until must-link constraint is
found or k = nc . If no must-link constraint is found,
set Z ← {Z1 , · · · , Znc , {xT }} and increment nc .
Impute Constraints: Set Aij = Aji = 1 for (xi , xj )
in the same certain set and Aij = Aji = 0 for (xi , xj )
in different certain sets (do not impute for points absent
from certain sets).
Estimate Labels: Ĉ ← S PECTRAL C LUSTER ING (A,K)
end while
For another perspective, consider that in the noiseless case,
for x1 , x2 ∈ S1 , the condition dist(x1 , S2 ) < dist(x2 , S2 )
is enough to guarantee that x1 lies nearer to S2 . Under the
given additive noise model (yi = xi + ni for i = 1, 2) the
gap between dist(x1 , S2 ) and dist(x2 , S2 ) must be larger
by some factor depending on the noise level. After two
applications of Thm. 1 and rearranging terms, we have that
µ(y1 ) < µ(y2 ) with high probability if
βdist(x2 , S2 )2 − dist(x1 , S2 )2 > (1 − β)σ 2 (D − d). (4)
4

where β = ((1 − ε)/(1 + ε)) , a value near 1 for small
ε. Equation (4) shows that the gap dist(x2 , S2 )2 −
dist(x1 , S2 )2 must grow (approximately linearly) with the
noise level σ 2 . The relationship of this gap to the subspace
distances is quantified by Corollary 1; plugging sin2 (φ1 )
from Equation (2) into Equation (3) and rearranging yields
a statement of the form in Equation (4).
3.2. Pairwise Constrained Clustering with SUPERPAC
We now describe SUPERPAC in more detail, our algorithm
for PCC when data lie near a union of subspaces, given in
Algorithm 1. The algorithm begins by initializing a set of
disjoint certain sets, an optional process described in the

Leveraging Union of Subspace Structure to Improve Constrained Clustering

missclassification %

Yale, K = 5

Yale, K = 10

8

SUPERPAC
URASC-N
Random
Oracle UoS

30

10
20

6

20

15

4

5

10

2
0
0

MNIST, K = 10

MNIST, K = 5
25

10

5
500

0
1000 0

1000

0
2000 0

500

0
1000 0

1000

2000

3000

number of pairwise comparisons

Figure 4. Misclassification rate for Yale B and MNIST datasets with many pairwise comparisons. Left-to-right: Yale B K = 5 (input
from SSC), Yale B K = 10 (input from SSC), MNIST K = 5 (input from TSC), MNIST K = 10 (input from TSC).

supplementary material due to space constraints. Next our
algorithm assigns the points most likely to be misclassified
to certain sets by presenting a series of pairwise comparisons. Finally, we impute values onto the affinity matrix for
all points in the certain sets and perform spectral clustering.
The process is then repeated until the maximum number of
pairwise comparisons has been reached.
Let xT be the test point chosen as the min-margin point.
Our goal is to assign xT to a certain set using as the fewest
number of queries possible. For each certain set Zk , the
representative xk is chosen as the maximum-margin point
within the set. Next, for each k, we let Uk be the ddimensional PCA
n estimate of the matrix
o whose columns
are the points x ∈ X : Ĉ(x) = Ĉ(xk ) . We then query
our test point
xT against the

 representatives xk in order of
residual xT − Uk UkT xT 2 (smallest first). If a must-link
constraint is found, we place xT in the corresponding certain set. Otherwise, we place xT in its own certain set and
update the number of certain sets. Pseudocode for the complete algorithm is given in Algorithm 1. As a technical note,
we first normalize the input affinity matrix A so that the
maximum value is 2. For must-link constraints, we impute
a value of 1 in the affinity matrix, while for cannot-link
constraints we impute a 0. The approach of imputing values
in the affinity matrix is common in the literature but does
not strictly enforce the constraints. Further, we found in
our experiments that imputing the maximum value in the
affinity matrix resulted in unstable results. Thus, users must
be careful to not only choose the correct constraints as noted
in (Basu et al., 2004), but to incorporate these constraints in
a way that allows for robust clustering.
SUPERPAC can be thought of as an extension of ideas from
PCC literature (Basu et al., 2004; Biswas & Jacobs, 2014;
Xiong et al., 2016) to leverage prior knowledge about the
underlying geometry of the data. For datasets such as Yale B
and MNIST, the strong subspace structure makes Euclidean
distance a poor proxy for similarity between points in the
same cluster, leading to the superior performance of our
algorithm demonstrated in the following sections. This

structure does not exist in all datasets, in which case we
do not expect our algorithm to outperform current PCC
algorithms. The reader will note we made a choice to order
the certain sets according to the UoS model; this is similar
to the choice in (Xiong et al., 2016) to query according to
similarity, where our notion of similarity here is based on
subspace distances. We found this resulted in significant
performance benefits, matching our intuition that points are
clustered based on their nearest subspace. In contrast to
(Biswas & Jacobs, 2014; Xiong et al., 2016), where the test
point is chosen according to a global improvement metric,
we choose test points according to their classification margin.
In our experiments, we found subspace margin to be a strong
indicator of which points are misclassified, meaning that our
algorithm rapidly corrects the errors that occur as a result of
unsupervised subspace clustering.
Finally, note that the use of certain sets relies on the assumption that the pairwise queries are answered correctly—an
assumption that is common in the literature (Basu et al.,
2004; Mallapragada et al., 2008; Xiong et al., 2016). However, in (Xiong et al., 2016), the authors demonstrate that
an algorithm based on certain sets still yields significant improvements under a small error rate. The study of robustly
incorporating noisy pairwise comparisons is an interesting
topic for further study.

4. Experimental Results
We compare the performance of our method and the nonparametric version of the URASC algorithm (URASC-N)2
over a variety of datasets. Note that while numerous PCC
algorithms exist, URASC achieves both the best empirical results and computational complexity on a variety of
datasets. We also compared with the methods from (Basu
et al., 2004) and (Biswas & Jacobs, 2014) but found both
to perform significanly worse than URASC on all datasets
considered, with a far greater computational cost in the case
2

In our experiments, the parametric version of URASC was
found to be numerically unstable and did not have significantly
different performance from URASC-N in the best cases.

Leveraging Union of Subspace Structure to Improve Constrained Clustering

missclassification %

Yale, K = 38

Dataset
Yale
MNIST
COIL-20
COIL-100
USPS

25
20
15
10
5
0

SUPERPAC
URASC-N
Random
Oracle UoS
1000

N
320-2432
500-1000
1440
7200
9298

K
5,10,38
5,10
20
100
10

D
2016
784
1024
1024
256

d
9
3
9
9
15

Table 1. Datasets used for experiments with relevant parameters;
N : total number of samples, K: number of clusters, D: ambient
dimension, d: estimated subspace dimension.
2000

3000

number of pairwise comparisons
Figure 5. Misclassification rate versus number of pairwise comparisons for extended Yale face database B with K = 38 subjects.
Input affinity matrix is taken from SSC-OMP.

of (Biswas & Jacobs, 2014). We use a maximum query
budget of 2K for U O S-E XPLORE and E XPLORE. For completeness, we also compare to random constraints, in which
queries are chosen uniformly at random from the set of
unqueried pairs.
Finally, we compare against the oracle PCA classifier, which
we now define. Let Uk be the d-dimensional PCA estimate
of the points whose true label C(x)
 = k. Then
 the oracle
label is Ĉo (x) = arg mink∈[K] x − Uk UkT x2 . This allows us to quantitatively capture the idea that, because the
true classes are not perfectly low-rank, some points would
not be clustered with the low-rank approximation of their
own true cluster. In our experiments, we also compared
with oracle robust PCA (Candes et al., 2011) implemented
via the augmented Lagrange multiplier method (Lin et al.,
2011) but did not find any improvement in classification
error.
Datasets We consider five datasets commonly used as
benchmarks in the subspace clustering literature3 , with a
summary of the datasets and their relevant parameters are
given in Table 1. The Yale B dataset consists of 64 images
of size 192 × 168 of each of 38 different subjects under
a variety of lighting conditions. For values of K less than
38, we follow the methodology of (Zhang et al., 2012) and
perform clustering on 100 randomly selected subsets of size
K. We choose d = 9 as is common in the literature (Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2015). The MNIST
handwritten digit database test dataset consists of 10,000
centered 28 × 28 pixel images of handwritten digits 0-9.
We follow a similar methodology to the previous section
and select 100 random subsets of size K, using subspace
dimension d = 3 as in (Heckel & Bölcskei, 2015). The
COIL-20 dataset (Nene et al., 1996b) consists of 72 images
3

The validity of the UoS assumption for two of these datasets
is investigated in (Elhamifar & Vidal, 2013; Heckel & Bölcskei,
2015).

of size 32 × 32 of each of 20 objects. The COIL-100 dataset
(Nene et al., 1996a) contains 100 objects (distinct from the
COIL-20 objects) of the same size and with the same number of images of each object. For both datasets, we use
subspace dimension d = 9. Finally, we apply our algorithm
to the USPS dataset provided by (Cai et al., 2011), which
contains 9,298 total images of handwritten digits 0-9 of size
16 × 16 with roughly even label distribution. We again use
subspace dimension d = 9.
Input Subspace Clustering Algorithms A major
strength of our algorithm is that it is agnostic to the initial
subspace clustering algorithm used to generate the input
affinity matrix. To demonstrate this fact, we apply our
algorithm with an input affinity matrix obtained from a
variety of subspace clustering methods, summarized in
Table 1. Note that some recent algorithms are not included
in the simulations here. However, the simulations show
that our algorithm works well with any initial clustering,
and hence we expect similar results as new algorithms are
developed.
Experimental Results Fig. 4 shows the clustering error
versus the number of pairwise comparisons for the Yale and
MNIST datasets. The input affinity matrix is obtained by
running SSC for the Yale datset and by running TSC for
the MNIST dataset. The figure clearly demonstrates the
benefits of leveraging UoS structure in constrained clustering—in all cases, SUPERPAC requires roughly half the
number of queries needed by URASC to achieve perfect
clustering. For the Yale dataset with K = 5, roughly 2Kd
queries are required to surpass oracle performance, and for
K = 10 roughly 3Kd queries are required. Note that for the
Yale dataset, the clustering error increases using URASC.
This is due to the previously mentioned fact that imputing
the wrong constraints can lead to worse clustering performance. For sufficiently many queries, the error decreases
as expected. Fig. 5 shows the misclassification rate versus
number of points for all K = 38 subjects of the Yale databse,
with the input affinity matrix taken from SSC-OMP (You
et al., 2016b). We space out the markers for clearer plots.
In this case, URASC performs roughly the same as random
query selection, while SUPERPAC performs significantly

Leveraging Union of Subspace Structure to Improve Constrained Clustering

missclassification %

COIL-20

COIL-100

COIL-20 Smoothing
15

15

SUPERPAC
URASC-N
Random
SUPERPAC-S
Oracle UoS

30
25

10

10

20
15

5

5

10
5

0
0

200

400

600

0

2000

4000

0
0

50

100

150

number of pairwise comparisons

USPS

50

SUPERPAC
URASC-N
Random
Oracle UoS

40
30
20
10
0

500

1000

1500

2000

number of pairwise comparisons
Figure 7. Misclassification rate versus number of pairwise comparisons for USPS dataset with K = 10 digits, 9,298 total samples.
Input affinity matrix is taken from EnSC. URASC did not complete
after 48 hours of run time.

missclassification %

missclassification %

Figure 6. Misclassification rate versus number of pairwise comparisons for COIL-20 (K = 20) and COIL-100 (K = 100)) databases.
Input affinity matrix is taken from EnSC. Rightmost plot shows proposed smoothing heuristic.

50

Sonar
SUPERPAC
URASC-N
Random

40
30
20
10
0
0

100

200

300

number of pairwise comparisons
Figure 8. Misclassification rate for Sonar dataset from (Xiong et al.,
2016), where there is not reason to believe the clusters have subspace structure. We are still very competitive with state-of-the-art.

better.

5. Conclusion

Fig. 6 demonstrates the continued superiority of our algorithm in the case where UoS structure exists. In the case
of COIL-20, the clustering is sometimes unstable, alternating between roughly 0% and 7% clustering error for both
active algorithms. This further demonstrates the observed
phenomenon that spectral clustering is sensitive to small
perturbations. To avoid this issue, we kept track of the
K-subspaces cost function (see (Bradley & Mangasarian,
2000)) and ensured the cost decreased at every iteration. We
refer to this added heuristic as SUPERPAC-S in the figure.
The incorporation of this heuristic into our algorithm is a
topic for further study.

We have presented a method of selecting and incorporating
pairwise constraints into subspace clustering that considers
the underlying geometric structure of the problem. The
union of subspaces model is often used in computer vision
applications where it is possible to request input from human labelers in the form of pairwise constraints. We showed
that labeling is often necessary for subspace classifiers to
achieve a clustering error near zero; additionally, these constraints can be chosen intelligently to improve the clustering
procedure overall and allow for perfect clustering with a
modest number of requests for human input.

Fig. 7 shows the resulting error on the USPS dataset, again
indicating the superiority of our method. Note that N is
large for this dataset, making spectral clustering computationally burdensome. Further, the computational complexity
of URASC is dependent on N . As a result, URASC did not
complete 2000 queries in 48 hours of run time when using
10 cores, so we compare to the result after completing only
1000 queries. Finally, in Fig. 8, we demonstrate that even
on data without natural subspace structure, SUPERPAC
performs competitively with URASC.

Developing techniques for handling noisy query responses
will allow extension to undersampled or compressed data.
One may assume that compressed data would be harder to
distinguish, leading to noisier query responses. Finally, we
saw that for datasets with different types of cluster structure, the structure assumptions of each algorithm had direct
impact on performance; in the future we plan to additionally develop techniques for learning from unlabeled data
whether the union of subspace model or a standard clustering approach is more appropriate.

Leveraging Union of Subspace Structure to Improve Constrained Clustering

Acknowledgements
This work was supported by NSF F031543-071159-GRFP
and US ARO Grant W911NF1410634.

References
Basri, R. and Jacobs, D. Lambertian reflectance and linear
subspaces. IEEE TPAMI, 25(2):218–233, February 2003.
Basu, Sugato, Banerjee, Arindam, and Mooney, Raymond J.
Active semi-supervision for pairwise constrained clustering. In Proc. SIAM Int. Conf. on Data Mining, 2004.
Biswas, Arjit and Jacobs, David. Active image clustering
with pairwise constraints from humans. International
Journal on Computer Vision, 108:133–147, 2014.
Bradley, Paul S. and Mangasarian, Olvi L. k-Plane clustering. Journal of Global Optimization, 16:23–32, 2000.
Cai, Deng, He, Xiaofei, Han, Jiawei, and Huang, Thomas S.
Graph regularized non-negative matrix factorization for
data representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1548–1560, 2011.
Candes, Emmanuel J., Li, Xiadong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM, 58(3), May 2011.
Davidson, Ian, Wagstaff, Kiri L., and Basu, Sugato. Measuring constraint-set utility for partitional clustering algorithms. In Proc. European Conf. on Machine Learning
and Prinicpals and Practice of Knowledge Discovery in
Databases, 2006.
Elhamifar, Ehsan and Vidal, Renee. Sparse subspace clustering: Algorithm, theory, and applications. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 35:2765–
2781, November 2013.
Golub, Gene and Loan, Charles Van. Matrix Computations.
Johns Hopkins University Press, 2012.
Haupt, Jarvis, Castro, Rui M, and Nowak, Robert. Distilled
sensing: Adaptive sampling for sparse detection and estimation. IEEE Transactions on Information Theory, 57
(9):6222–6235, 2011.
Heckel, Reinhard and Bölcskei, Helmut. Robust subspace
clustering via thresholding. IEEE Trans. Inf. Theory, 24
(11):6320–6342, 2015.
Indyk, Piotr, Price, Eric, and Woodruff, David P. On the
power of adaptivity in sparse recovery. In Foundations
of Computer Science (FOCS), 2011 IEEE 52nd Annual
Symposium on, pp. 285–294. IEEE, 2011.

Krishnamurthy, Akshay and Singh, Aarti. Low-rank matrix
and tensor completion via adaptive sampling. In Advances
in Neural Information Processing Systems, pp. 836–844,
2013.
Lin, Zhouchen, Chen, Minming, Wu, Leqin, and Ma, Yi.
Linearized alternating direction method with adaptive
penalty for low-rank representation. In Advances in Neural Information Processing Systems, 2011.
Liu, Guangcan, Lin, Zhouchen, and Yu, Yong. Robust
subspace segmentation by low-rank representation. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 663–670, 2010.
Mallapragada, Pavan Kumar, Jin, Rong, and Jain, Anil K.
Active query selection for semi-supervised clustering. In
Proc. Int. Conf. on Pattern Recognition, 2008.
Nene, S. A., Nayar, S. K., and Murase, H. Columbia object
image library (COIL-100). Technical report, Columbia
University, 1996a.
Nene, S. A., Nayar, S. K., and Murase, H. Columbia object
image library (COIL-20). Technical report, Columbia
University, 1996b.
Oliver, N.M., Rosario, B., and Pentland, A.P. A bayesian
computer vision system for modeling human interactions.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):831–843, 2000.
Park, Dohyung, Caramanis, Constantine, and Sanghavi, Sujay. Greedy subspace clustering. In Advances in Neural
Information Processing Systems, pp. 2753–2761, 2014.
Settles, Burr. Active Learning. Morgan & Claypool, 2012.
Soltanolkotabi, Mahdi and Candes, Emmanuel J. A Geometric Analysis of Subspace Clustering with Outliers. The
Annals of Statistics, 40(4):2195–2238, 2012.
Soltanolkotabi, Mahdi and Candes, Emmanuel J. Robust
Subspace Clustering. The Annals of Statistics, 42(2):
669–699, 2014.
Soni, Akshay and Haupt, Jarvis. On the fundamental limits
of recovering tree sparse vectors from noisy linear measurements. IEEE Transactions on Information Theory,
60(1):133–149, 2014.
Vidal, Rene, Sastry, S. Shankar, and Ma, Yi. Generalized
Principal Component Analysis. Springer-Verlag, 2016.
Vidal, Renee. Subspace clustering. IEEE Signal Processing
Magazine, 28:52–68, March 2011.

Leveraging Union of Subspace Structure to Improve Constrained Clustering

Wagstaff, Kiri, Cardie, Claire, Rogers, Seth, and Schroedl,
Stefan. Constrained K-means clustering with background
knowledge. In Proc. Int. Conf. on Machine Learning,
2001.
Wang, Xiang and Davidson, Ian. Active spectral clustering.
In Proc. 10th Int. Conf. on Data Mining, 2010.
Wang, Y. and Singh, A. Noise-adaptive margin-based active
learning for multi-dimensional data and lower bounds
under tsybakov noise. In Proc. AAAI Conference on
Artificial Intellgence, 2016.
Wang, Yining, Wang, Yu-Xiang, and Singh, Aarti. Graph
connectivity in noisy sparse subspace clustering. In Proceedings of The 19th International Conference on Artificial Intelligence and Statistics, 2016.
Wang, Yu-Xiang and Xu, Huan. Noisy sparse subspace
clustering. In Proceedings of The 30th International
Conference on Machine Learning, pp. 89–97, 2013.
Wright, John, Yang, Allen Y, Ganesh, Arvind, Sastry,
S Shankar, and Ma, Yi. Robust face recognition via
sparse representation. IEEE transactions on pattern analysis and machine intelligence, 31(2):210–227, 2009.
Xiong, Caiming, 2016. personal correspondence.
Xiong, Caiming, Johnson, David M., and Corso, Jason J.
Active clustering with model-based uncertainty reduction. IEEE Trans. Pattern Anal. Mach. Intelligence, 2016.
Accepted for publication.
Xu, Qianjun, desJardins, Marie, and Wagstaff, Kiri L. Active constrained clustering by examining spectral eigenvectors. In Proc. 8th Int. Conf. on Discovery Science,
2005.
You, Chong, Li, Chun-Guang, Robinson, Daniel P., and
Vidal, Rene. Oracle based active set algorithm for scalable elastic net subspace clustering. In Proc. IEEE International Conference on Computer Vision and Pattern
Recognition, 2016a.
You, Chong, Robinson, Daniel P., and Vidal, Rene. Scalable sparse subspace clustering by orthogonal matching
pursuit. In Proc. IEEE International Conference on Computer Vision and Pattern Recognition, 2016b.
Zhang, Teng, Szlam, Arthur, Wang, Yi, and Lerman, Gilad.
Hybrid linear modeling via local best-fit flats. International Journal of Computer Vision, 100:217–240, 2012.

