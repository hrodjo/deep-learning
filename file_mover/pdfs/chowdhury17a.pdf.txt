On Kernelized Multi-armed Bandits
Sayak Ray Chowdhury 1 Aditya Gopalan 1

Abstract
We consider the stochastic bandit problem with
a continuous set of arms, with the expected reward function over the arms assumed to be fixed
but unknown. We provide two new Gaussian
process-based algorithms for continuous bandit
optimization – Improved GP-UCB (IGP-UCB)
and GP-Thomson sampling (GP-TS), and derive
corresponding regret bounds. Specifically, the
bounds hold when the expected reward function
belongs to the reproducing kernel Hilbert space
(RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new selfnormalized concentration inequality for vectorvalued martingales of arbitrary, possibly infinite,
dimension. Finally, experimental evaluation and
comparisons to existing algorithms on synthetic
and real-world environments are carried out that
highlight the favorable gains of the proposed
strategies in many cases.

1. Introduction
Optimization over large domains under uncertainty is an
important subproblem arising in a variety of sequential decision making problems, such as dynamic pricing in economics (Besbes & Zeevi, 2009), reinforcement learning
with continuous state/action spaces (Kaelbling et al., 1996;
Smart & Kaelbling, 2000), and power control in wireless
communication (Chiang et al., 2008). A typical feature of
such problems is a large, or potentially infinite, domain of
decision points or covariates (prices, actions, transmit powers), together with only partial and noisy observability of
the associated outcomes (demand, state/reward, communication rate); reward/loss information is revealed only for
decisions that are chosen. This often makes it hard to bal1
Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, 560012, India.
Correspondence to:
Sayak Ray Chowdhury <srchowdhury@ece.iisc.ernet.in>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ance exploration and exploitation, as available knowledge
must be transferred efficiently from a finite set of observations so far to estimates of the values of infinitely many
decisions. A classic case in point is that of the canonical
stochastic MAB with finitely many arms, where the effort
to optimize scales with the total number of arms or decisions; the effect of this is catastrophic for large or infinite
arm sets.
With suitable structure in the values or rewards of arms,
however, the challenge of sequential optimization can be
efficiently addressed. Parametric bandits, especially linearly parameterized bandits (Rusmevichientong & Tsitsiklis, 2010), represent a well-studied class of structured decision making settings. Here, every arm corresponds to a
known, finite dimensional vector (its feature vector), and
its expected reward is assumed to be an unknown linear
function of its feature vector. This allows for a large, or
even infinite, set of arms all lying in space of finite dimension, say d, and a rich line of work gives algorithms that
attain sublinear regret with a polynomial dependence on
the dimension, e.g., Confidence Ball (Dani et al., 2008),
OFUL (Abbasi-Yadkori et al., 2011) (a strengthening of
Confidence Ball) and Thompson sampling for linear bandits (Agrawal & Goyal, 2013)1 The insight here is that even
though the number of arms can be large, the number of unknown parameters (or degrees of freedom) in the problem
is really only d, which makes it possible to learn about the
values of many other arms by playing a single arm.
A different approach to modelling bandit problems with a
continuum of arms is via the framework of Gaussian processes (GPs) (Rasmussen & Williams, 2006). GPs are
a flexible class of nonparametric models for expressing
uncertainty over functions on rather general domain sets,
which generalize multivariate Gaussian random vectors.
GPs allow tractable regression for estimating an unknown
function given a set of (noisy) measurements of its values
at chosen domain points. The fact that GPs, being distributions on functions, can also help quantify function uncertainty makes it attractive for basing decision making strategies on them. This has been exploited to great advantage to
1

Roughly, for rewards bounded
in [−1, 1], these algorithms
√ 
achieve optimal regret Õ d T , where Õ (·) hides polylog(T )
factors.

On Kernelized Multi-armed Bandits

build nonparametric bandit algorithms, such as GP-UCB
(Srinivas et al., 2009), GP-EI and GP-PI (Hoffman et al.,
2011). In fact, GP models for bandit optimization, in terms
of their kernel maps, can be viewed as the parametric linear
bandit paradigm pushed to the extreme, where each feature
vector associated to an arm can have infinite dimension 2 .
Against this backdrop, our work revisits the problem of
bandit optimization with stochastic rewards. Specifically,
we consider stochastic multiarmed bandit (MAB) problems
with a continuous arm set, and whose (unknown) expected
reward function is assumed to lie in a reproducing kernel
Hilbert space (RKHS), with bounded RKHS norm – this
effectively enforces smoothness on the function3 . We make
the following contributions• We design a new algorithm – Improved Gaussian
Process-Upper Confidence Bound (IGP-UCB) – for
stochastic bandit optimization. The algorithm can be
viewed as a variant of GP-UCB (Srinivas et al., 2009),
but uses a significantly reduced confidence interval
width resulting in an order-wise improvement in regret compared to GP-UCB. IGP-UCB also shows a
markedly improved numerical performance over GPUCB.
• We develop a nonparametric version of Thompson
sampling, called Gaussian Process Thompson sampling(GP-TS),and show that enjoys a regret bound
√
of Õ γT dT . Here, T is the total time horizon and
γT is a quantity depending on the RKHS containing
the reward function. This is, to our knowledge, the
first known regret bound for Thompson sampling in
the agnostic setup with nonparametric structure.
• We prove a new self-normalized concentration inequality for infinite-dimensional vector-valued martingales, which is not only key to the design and
analysis of the IGP-UCB and GP-TS algorithms, but
also potentially of independent interest. The inequality generalizes a corresponding self-normalized bound
for martingales in finite dimension proven by AbbasiYadkori et al. (2011).
• Empirical comparisons of the algorithms developed
above, with other GP-based algorithms, are presented,
over both synthetic and real-world setups, demonstrating performance improvements of the proposed algorithms, as well as their performance under misspecification.
2
The completion of the linear span of all feature vectors (images of the kernel map) is precisely the reproducing kernel Hilbert
space (RKHS) that characterizes the GP.
3
Kernels, and their associated RKHSs,

2. Problem Statement
We consider the problem of sequentially maximizing a
fixed but unknown reward function f : D → R over a
(potentially infinite) set of decisions D ⊂ Rd , also called
actions or arms. An algorithm for this problem chooses, at
each round t, an arm xt ∈ D, and subsequently observes
a reward yt = f (xt ) + εt , which is a noisy version of the
function value at xt . The arm xt is chosen causally depending upon the arms played and rewards obtained upto
round t − 1, denoted by the history Ht−1 = {(xs , ys ) : s =
1, . . . , t−1}. We assume that the noise sequence {εt }∞
t=1 is
conditionally R-sub-Gaussian for a fixed constant R ≥ 0,
i.e.,
 2 2
h
i

λ R
∀t ≥ 0, ∀λ ∈ R, E eλεt  Ft−1 ≤ exp
,
2

(1)

where Ft−1 is the σ-algebra generated by the random variables {xs , εs }t−1
s=1 and xt .This is a mild assumption on the
noise (it holds, for instance, for distributions bounded in
[−R, R]) and is standard in the bandit literature (AbbasiYadkori et al., 2011; Agrawal & Goyal, 2013).
Regret. The goal of an algorithm is to maximize its cumulative reward or alternatively minimize its cumulative
regret – the loss incurred due to not knowing f ’s maximum point beforehand. Let x? ∈ argmaxx∈D f (x) be
a maximum point of f (assuming the maximum is attained). The instantaneous regret incurred at time t is
rt = f (x? ) − f (xt ), and the cumulative regret in a time
horizonP
T (not necessarily known a priori) is defined to be
T
RT = t=1 rt . A sub-linear growth of RT in T signifies
that RT /T → 0 as T → ∞, or vanishing per-round regret.
Regularity Assumptions. Attaining sub-linear regret is
impossible in general for arbitrary reward functions f and
domains D, and thus some regularity assumptions are in
order. In what follows, we assume that D is compact. The
smoothness assumption we make on the reward function f
is motivated by Gaussian processes4 and their associated
reproducing kernel Hilbert spaces (RKHSs, see Schölkopf
& Smola (2002)). Specifically, we assume that f has small
norm in the RKHS of functions D → R, with positive
semi-definite kernel function k : D ×D → R. This RKHS,
denoted by Hk (D), is completely specified by its kernel
function k(·, ·) and vice-versa, with an inner product h·, ·ik
obeying the reproducing property: f (x) = hf, k(x, ·)ik for
all f ∈ Hk (D). In other words, the kernel plays the role
of delta functions to represent the evaluation map at each
point x ∈ D via
p the RKHS inner product. The RKHS
norm kf kk = hf, f ik is a measure of the smoothness5
4
Other work has also studied continuum-armed bandits with
weaker smoothness assumptions such as Lipschitz continuity –
see Related work for details and comparison.
5
One way to see this is that for every element g in
the RKHS, |g(x) − g(y)| = |hg, k(x, ·) − k(y, ·)i| ≤
kgkk kk(x, ·) − k(y, ·)kk by Cauchy-Schwarz.

On Kernelized Multi-armed Bandits

of f , with respect to the kernel function k, and satisfies:
f ∈ Hk (D) if and only if kf kk < ∞.
We assume a known bound on the RKHS norm of the unknown target function6 : kf kk ≤ B. Moreover, we assume
bounded variance by restricting k(x, x) ≤ 1, for all x ∈ D.
Two common kernels that satisfy bounded variance property are Squared Exponential and Matérn, defined as
0

kSE (x, x )
0

kM atérn (x, x )

=
=



2

2

µt (x)
0

kt (x, x )
σt2 (x)

= kt (x)T (Kt + λI)−1 y1:t ,
0

T

(2)
−1

= k(x, x ) − kt (x) (Kt + λI)

0

kt (x ),(3)

= kt (x, x).

(4)



− s /2l ,
√
√

21−ν s 2ν ν  s 2ν 
Bν
,
Γ(ν)
l
l

exp

where kt (x) = [k(x1 , x), . . . , k(xt , x)]T . Therefore conditioned on the history Ht , the posterior distribution over f
is GPD (µt (·), v 2 kt (·, ·)), where

where l > 0 and ν > 0 are hyperparameters, s =
kx − x0 k2 encodes the similarity between two points
x, x0 ∈ D, and Bν (·) is the modified Bessel function. Generally the bounded variance property holds for any stationary kernel, i.e. kernels for which k(x, x0 ) = k(x − x0 ) for
all x, x0 ∈ Rd . These assumptions are required to make
the regret bounds scale-free and are standard in the literature (Agrawal & Goyal, 2013). Instead if k(x, x) ≤ c or
kf kk ≤ cB, then our regret bounds would increase by a
factor of c.

3. Algorithms
Design philosophy. Both the algorithms we propose
use Gaussian likelihood models for observations, and
Gaussian process (GP) priors for uncertainty over reward functions. A Gaussian process over D, denoted
by GPD (µ(·), k(·, ·)), is a collection of random variables
(f (x))x∈D , one for each x ∈ D, such that every finite
sub-collection of random variables (f (xi ))m
i=1 is jointly
Gaussian with mean E [f (xi )] = µ(xi ) and covariance
E [(f (xi ) − µ(xi ))(f (xj ) − µ(xj ))] = k(xi , xj ), 1 ≤
i, j ≤ m, m ∈ N. The algorithms use GPD (0, v 2 k(·, ·)),
v > 0, as an initial prior distribution for the unknown reward function f over D, where k(·, ·) is the kernel function associated with the RKHS Hk (D) in which f is assumed to have ‘small’ norm at most B. The algorithms
also assume that the noise variables εt = yt − f (xt )
are drawn independently, across t, from N (0, λv 2 ), with
λ ≥ 0. Thus, the prior distribution for each f (x), is assumed to be N (0, v 2 k(x, x)), x ∈ D. Moreover, given
a set of sampling points At = (x1 , . . . , xt ) within D, it
follows under the assumption that the corresponding vector of observed rewards y1:t = [y1 , . . . , yt ]T has the multivariate Gaussian distribution N (0, v 2 (Kt + λI)), where
Kt = [k(x, x0 )]x,x0 ∈At is the kernel matrix at time t. Then,
by the properties of GPs, we have that y1:t and f (x) are
jointly Gaussian given At :


  2

f (x)
v k(x, x)
v 2 kt (x)T
∼ N 0,
,
y1:t
v 2 kt (x) v 2 (Kt + λI)
6
This is analogous to the bound on the weight θ typically assumed in regret analyses of linear parametric bandits.

Thus for every x ∈ D, the posterior distribution of f (x),
given Ht , is N (µt (x), v 2 σt2 (x)).
Remark. Note that the GP prior and Gaussian likelihood
model described above is only an aid to algorithm design,
and has nothing to do with the actual reward distribution
or noise model as in the problem statement (Section 2).
The reward function f is a fixed, unknown, member of the
RKHS Hk (D), and the true sequence of noise variables εt
is allowed to be a conditionally R-sub-Gaussian martingale
difference sequence (Equation 1). In general, thus, this represents a misspecified prior and noise model, also termed
the agnostic setting by Srinivas et al. (2009).
The proposed algorithms, to follow, assume the knowledge
of only the sub-Gaussianity parameter R, kernel function k
and upper bound B on the RKHS norm of f . Note that v, λ
are free parameters (possibly time-dependent) that can be
set specific to the algorithm.
3.1. Improved GP-UCB (IGP-UCB) Algorithm
We introduce the IGP-UCB algorithm (Algorithm 1), that
uses a combination of the current posterior mean µt−1 (x)
and standard deviation vσt−1 (x) to (a) construct an upper
confidence bound (UCB) envelope for the actual function
f over D, and (b) choose an action to maximize it. Specifically it chooses, at each round t, the action
xt = argmax µt−1 (x) + βt σt−1 (x),

(5)

x∈D

with the scale parameter v set to be 1. Such a rule
trades off exploration (picking points with high uncertainty
σt−1 (x)) with exploitation (picking
p points with high reward µt−1 (x)), with βt = B + R 2(γt−1 + 1 + ln(1/δ))
being the parameter governing the tradeoff, which we later
show is related to the width of the confidence interval for f
at round t. δ ∈ (0, 1) is a free confidence parameter used
by the algorithm, and γt is the maximum information gain
at time t, defined as:
γt :=

max
A⊂D:|A|=t

I(yA ; fA ).

Here, I(yA ; fA ) denotes the mutual information between
fA = [f (x)]x∈A and yA = fA + εA , where εA ∼
N (0, λv 2 I) and quantifies the reduction in uncertainty
about f after observing yA at points A ⊂ D. γt is
a problem dependent quantity and can be found given
the knowledge of domain D and kernel function k. For

On Kernelized Multi-armed Bandits

a compact subset D of Rd , γT is O((ln T )d+1 ) and
O(T d(d+1)/(2ν+d(d+1)) ln T ), respectively, for the Squared
Exponential and Matérn kernels (Srinivas et al., 2009), depending only polylogarithmically on the time T .
Algorithm 1 Improved-GP-UCB (IGP-UCB)
Input: Prior GP (0, k), parameters B, R, λ, δ.
for t = 1, 2, 3 . . . T p
do
Set βt = B + R 2(γt−1 + 1 + ln(1/δ)).
Choose xt = argmax µt−1 (x) + βt σt−1 (x).
x∈D

Observe reward yt = f (xt ) + εt .
Perform update to get µt and σt using 2, 3 and 4.
end for
Discussion. Srinivas et al. (2009) have proposed the GPUCB algorithm, and Valko et al. (2013) the KernelUCB
algorithm, for sequentially optimizing reward functions lying in the RKHS Hk (D). Both algorithms play an arm
at time t using the rule: xt = argmaxx∈D µt−1 (x) +
β̃t σt−1 (x). GP-UCB uses the exploration parameter β̃t =
q

2B 2 + 300γt−1 ln3 (t/δ), with λ set to σ 2 , where σ is
additionally assumed to be a known, uniform (i.e., almostsure) upper bound on all noise variables εt (Srinivas et al.,
2009, Theorem 3). Compared to GP-UCB, IGP-UCB (Algorithm 1) reduces the width of the confidence interval by
a factor roughly O(ln3/2 t) at every round t, and, as we
will see, this small but critical adjustment leads to much
better theoretical and empirical performance compared to
GP-UCB. In KernelUCB, β̃t is set as η/λ1/2 , where η is
the exploration parameter and λ is the regularization constant. Thus IGP-UCB can be viewed as a special case of
KernelUCB where η = βt .
3.2. Gaussian Process Thompson Sampling (GP-TS)

Our second algorithm, GP-TS (Algorithm 2), inspired
by the success of Thompson sampling for standard and
parametric bandits (Agrawal & Goyal, 2012; Kaufmann
et al., 2012; Gopalan et al., 2014; Agrawal & Goyal,
2013),
p uses the time-varying scale parameter vt = B +
R 2(γt−1 + 1 + ln(2/δ)) and operates as follows. At
each round t, GP-TS samples a random function ft (·) from
the GP with mean function µt−1 (·) and covariance function
vt2 kt−1 (·, ·). Next, it chooses a decision set Dt ⊂ D, and
plays the arm xt ∈ Dt that maximizes ft 7 . We call it GPThompson-Sampling as it falls under the general framework of Thompson Sampling, i.e., (a) assume a prior on the
underlying parameters of the reward distribution, (b) play
the arm according to the prior probability that it is optimal,
7
If Dt = D for all t, then this is simply exact Thompson
sampling. For technical reasons, however, our regret bound is
valid when Dt is chosen as a suitable discretization of D, so we
include Dt as an algorithmic parameter.

and (c) observe the outcome and update the prior. However,
note that the prior is nonparametric in this case.
Algorithm 2 GP-Thompson-Sampling (GP-TS)
Input: Prior GP (0, k), parameters B, R, λ, δ.
for t = 1, 2, 3 . . . , do
p
Set vt = B + R 2(γt−1 + 1 + ln(2/δ)).
Sample ft (·) from GPD (µt−1 (·), vt2 kt−1 (·, ·)).
Choose the current decision set Dt ⊂ D.
Choose xt = argmax ft (x).
x∈Dt

Observe reward yt = f (xt ) + εt .
Perform update to get µt and kt using 2 and 3.
end for

4. Main Results
We begin by presenting two key concentration inequalities
which are essential in bounding the regret of the proposed
algorithms.
d
Theorem 1 Let {xt }∞
t=1 be an R -valued discrete time
stochastic process predictable with respect to the filtration
∞
{Ft }∞
t=0 , i.e., xt is Ft−1 -measurable ∀t ≥ 1. Let {εt }t=1
be a real-valued stochastic process such that for some R ≥
0 and for all t ≥ 1, εt is (a) Ft -measurable, and (b) R-subGaussian conditionally on Ft−1 . Let k : Rd ×Rd → R be a
symmetric, positive-semidefinite kernel, and let 0 < δ ≤ 1.
For a given η > 0, with probability at least 1 − δ, the following holds simultaneously over all t ≥ 0:

p

det((1 + η)I + Kt )
.
δ
(6)
(Here, Kt denotes the t × t matrix Kt (i, j) = k(xi , xj ),
1 ≤ i, j ≤ t and for any x ∈ Rt and A ∈ Rt×t , kxkA :=
√
xT Ax). Moreover, if Kt is positive definite ∀t ≥ 1 with
probability 1, then the conclusion above holds with η = 0.
2
kε1:t k((Kt +ηI)−1 +I)−1

2

≤ 2R ln

Theorem 1 represents a self-normalized concentration inequality: the ‘size’ of the increasing-length sequence {εt }t
of martingale differences is normalized by the growing
quantity ((Kt + ηI)−1 + I)−1 that explicitly depends on
the sequence. The following lemma helps provide an alternative, abstract, view of the self-normalized process of
Theorem 1, based on the feature space representation induced by a kernel.
Lemma 1 Let k : Rd × Rd → R be a symmetric, positivesemidefinite kernel, with associated feature map ϕ : Rd →
Hk and the reproducing kernel Hilbert space8 (RKHS) Hk .
8
Such a pair (ϕ, Hk ) always exists, see e.g., Rasmussen &
Williams (2006).

On Kernelized Multi-armed Bandits

Pt
Letting St = s=1 εs ϕ(xs ) and the (possibly infinite diPt
mensional) matrix9 Vt = I+ s=1 ϕ(xs )ϕ(xs )T , we have,
whenever Kt is positive definite, that
kε1:t k(K −1 +I )−1 = kSt kV −1 ,
t

t



 −1/2 
St 
where kSt kV −1 := Vt
t

denotes the norm of

Hk

−1/2
Vt
St

in the RKHS Hk .

 

Observe that St is Ft -measurable and also E St  Ft−1 =
St−1 . The process {St }t≥0 is thus a martingale with values10 in the RKHS H, which can possibly be infinitedimensional, and moreover, whose deviation is measured
by the norm weighted by Vt−1 , which is itself derived from
St . Theorem 1 represents the kernelized generalization
of the finite-dimensional result of Abbasi-Yadkori et al.
(2011), and we recover their result under the special case
of a linear kernel: ϕ(x) = x for all x ∈ Rd .
We remark that when ϕ is a mapping to a finite-dimensional
Hilbert space, the argument of Abbasi-Yadkori et al. (2011,
Theorem 1) can be lifted to establish Theorem 1, but
it breaks down in the generalized, infinite-dimensional
RKHS setting, as the self-normalized bound in their paper has an explicit, growing dependence on the feature dimension. Specifically, the method of mixtures (de la Pena
et al., 2009) or Laplace method, as dubbed by Maillard
(2016), fails to hold in infinite dimension. The primary reason for this is that the mixture distribution for finite dimensional spaces can be chosen independently of time, but in a
nonparametric setup like ours, where the dimensionality of
−1
the self-normalizing factor Kt−1 + I
itself grows with
time, the use of (random) stopping times, precludes using
time-dependent mixtures. We get around this difficulty by
applying a novel ‘double mixture’ construction, in which a
pair of mixtures on (a) the space of real-valued functions on
Rd , i.e., the support of a Gaussian process, and (b) on real
sequences is simultaneously used to obtain a more general
result, of potentially independent interest.
Our next result shows that how the posterior mean is concentrated around the unknown reward function f .
Theorem 2 Under the same hypotheses as those of Theorem 1, let D ⊂ Rd , and f : D → R be a member of the
RKHS of real-valued functions on D with kernel k, with
RKHS norm bounded by B. Then, with probability at least
1 − δ, the following holds for all x ∈ D and t ≥ 1:


p
|µt−1 (x) − f (x)| ≤ B + R 2(γt−1 + 1 + ln(1/δ)) σt−1 (x),

where γt−1 is the maximum information gain after t − 1
2
rounds and µt−1 (x), σt−1
(x) are mean and variance of
9

More formally,
P Vt : Hk → Hk is the linear operator defined
by Vt (z) = z + ts=1 ϕ(xs )hϕ(xs ), zi ∀z ∈ Hk .
10
We ignore issues of measurability here.

posterior distribution defined as in Equation 2, 3, 4, with λ
set to 1 + η and η = 2/T .
Theorem 3.5 of Maillard (2016) states a similar result on
the estimation of the unknown reward function from the
RKHS. We improve upon it in the sense that the confidence
bound in Theorem 2 is simultaneous over all x ∈ D, while
the bound has been shown only for a single, fixed x in the
Kernel Least-squares setting. We are able to achieve this
result by virtue of Theorem 1.
4.1. Regret Bound of IGP-UCB
Theorem 3 Let δ ∈ (0, 1), kf kk ≤ B and εt is conditionally R-sub-Gaussian. Running IGP-UCB for a function f
lying
RKHS Hk(D), we obtain a regret bound of
√ in the
√
O T (B γT + γT ) with high probability. More pre √
cisely, with probability at least 1 − δ, RT = O B T γT +

p
T γT (γT + ln(1/δ)) .
Improvement over GP-UCB. Srinivas et al. (2009), in
the course of analyzing the GP-UCB algorithm, show
that when the reward function
√ lies√in the RKHS Hk (D),

GP-UCB obtains regret O T (B γT + γT ln3/2 (T ))
with high probability (see Theorem 3 therein for the exact bound). Furthermore, they assume that the noise εt
is uniformly bounded by σ, while our sub-Gaussianity assumption (see Equation 1) is slightly more general, and
we are able to obtain a O(ln3/2 T ) multiplicative factor
improvement in the final regret bound thanks to the new
self-normalized inequality (Theorem 1). Additionally, in
our numerical experiments, we observe a significantly improved performance of IGP-UCB over GP-UCB, both on
synthetically generated function, and on real-world sensor
measurement data (see Section 6).
Comparison with KernelUCB. Valko et al. (2013)pshow
˜ ),
that the cumulative regret of KernelUCB is Õ( dT
˜ defined as the effective dimension, measures, in
where d,
a sense, the number of principal directions over which
the projection of the data in the RKHS is spread. They
show that d˜ is at least as good as γT , precisely γT ≥
Ω(d˜ln ln T )√and thus the regret bound of KernelUCB is
√
roughly Õ( T γT ), which is γT factor better than IGPUCB. However, KernelUCB requires the number of actions
to be finite, so the regret bound is not applicable for infinite
or continuum action spaces.
4.2. Regret Bound of GP-TS
For technical reasons, we will analyze the following version of GP-TS. At each round t, the decision set used
by GP-TS is restricted to be a unique discretization Dt

On Kernelized Multi-armed Bandits

of D with the property that |f (x) − f ([x]t )| ≤ 1/t2
for all x ∈ D, where [x]t is the closest point to x in
Dt . This can always be achieved by choosing a compact and convex domain D ⊂ [0, r]d and discretization
Dt with size |Dt | = (BLrdt2 )d such that kx − [x]t k1 ≤
rd/BLrdt2 = 1/BLt2 for all x ∈ D, where L =
1/2
 2
|
. This implies, for every x ∈
sup sup ∂∂pk(p,q)
p=q=x
j ∂qj
x∈D j∈[d]

ilar O(ln3/2 T ) factor improvement, as obtained by IGPUCB over GP-UCB, was achieved in the linear parametric
setting by (Abbasi-Yadkori et al., 2011) in the OFUL algorithm, over its predecessor ConfidenceBall (Dani et al.,
2008). Finally we see that the for linear bandit problem
with infinitely many actions, IGP-UCB
√ attains the information theoretic lower bound of √
Ω(d T ) (see (Dani et al.,
2008)), but GP-TS is a factor of d away from it.

D,
|f (x) − f ([x]t )| ≤ kf kk L kx − [x]t k1 ≤ 1/t2 ,

(7)

as any f ∈ Hk (D) is Lipschitz continuous with constant
kf kk L (De Freitas et al., 2012, Lemma 1).
Theorem 4 (Regret bound for GP-TS) Let δ ∈ (0, 1),
D ⊂ [0, r]d be compact and convex, kf kk ≤ B and
{εt }t a conditionally R-sub-Gaussian sequence. Running GP-TS for a function f lying in the RKHS Hk (D)
and with decision sets Dt chosen as above, with probability at least  1 − δ, the regret of GP-TS
√ satisp
(γT + ln(2/δ))d ln(BdT ) T γT +
fies RT = O

p
B T ln(2/δ) .
Comparison with IGP-UCB.
Observe that regret scal√
ing of GP-TS is Õ(γT dT ) which is√ a multiplicative
√
d factor away from the bound Õ(γT T ) obtained for
IGP-UCB and similar behavior is reflected in our simulations on
psynthetic data. The additional multiplicative factor of d ln(BdT ) in the regret bound of GP-TS is essentially a consequence of discretization. How to remove
this extra logarithmic dependency, and make the analysis
discretization-independent, remains an open question.
Remark. The regret bound for GP-TS is inferior compared
to IGP-UCB in terms of the dependency on dimension d,
but to the best of our knowledge, Theorem 4 is the first
(frequentist) regret guarantee of Thompson Sampling in the
agnostic, non-parametric setting of infinite action spaces.
Linear Models and a Matching Lower Bound. If the
mean rewards are perfectly linear, i.e. if there exists a
θ ∈ Rd such that f (x) = θT x for all x ∈ D, then we
are in the parametric setup, and one way of casting this
in the kernelized framework is by using the linear kernel
k(x, x0 ) = xT x0 . For this kernel, γ√
T = O(d ln T ), and the
regret scaling
of
IGP-UCB
is
Õ(d
T ) and that of GP-TS
√
is Õ(d3/2 T ), which recovers the regret bounds of their
linear, parametric analogues OFUL (Abbasi-Yadkori et al.,
2011) and Linear Thompson sampling (Agrawal & Goyal,
2013), respectively. Moreover,
in this case d˜ = d, thus
√
the regret of IGP-UCB is d factor away from that of KernelUCB. But the regret bound of KernelUCB also depends
on the number of arms√N , and if N is exponential in d,
then it also suffers Õ(d T ) regret. We remark that a sim-

5. Overview of Techniques
We briefly outline here the key arguments for all the theorems in Section 4. See Chowdhury & Gopalan (2017) for
complete proofs.
Proof Sketch for Theorem 1. It is convenient to assume that Kt , the induced kernel matrix at time t, is invertible, since this is where the crux of the argument lies.
First we show that for any function g : D → R and
for all t ≥n0, thanks to the sub-Gaussian property
(1),
o
2
g
1
T
the process Mt := exp(ε1:t g1:t − 2 kg1:t k ) is a nont
negative super-martingale with respect to the filtration Ft ,
where g1:t := [g(x1 ), . . . , g(xt )]T and in fact satisfies
E [Mtg ] ≤ 1. The chief difficulty is to handle the behavior of Mt at a (random) stopping time, since the sizes of
quantities such as ε1:t at the stopping time will be random.
We next construct a mixture martingale Mt by mixing Mtg over g drawn from an independent GPD (0, k)
Gaussian process, which is a measure over a large
space of functions, i.e., the space RD . Then, by a
change of measure argument, we show that this induces
a mixture distribution which is essentially N (0, Kt ) over
any desired finite
 dimension t, thus
 obtaining Mt =
2
√ 1
exp 12 kε1:t k(I+K −1 )−1 . Next from the fact
det(I+Kt )

t

that E [Mτ ] ≤ 1 and from Markov’s inequality, for any
δ ∈ (0, 1), we obtain
p
h
i
2
P kε1:τ k(Kτ−1 +I)−1 > 2 ln
det(I + Kτ )/δ ≤ δ.
Finally, we lift this bound for all t through a standard stopping time construction as in Abbasi-Yadkori et al. (2011).
Proof Sketch for Theorem 2. Here we sketch the
special case of η = 0, i.e.
λ = 1.
Observe that |µt (x) − f (x)| is upper bounded
by
sum
of

kt (x)T (Kt + I)−1 ε1:t  and Q :=
two
terms,
P
:=


kt (x)T (Kt + I)−1 f1:t − f (x). Now we observe that
−1
σt2 (x) = ϕ(x)T (ΦTt Φt + I)
and use this obser-
 ϕ(x)
T
T
−1 T


vation to show
that
P
=
ϕ(x)
(Φ

 t Φt + I) Φt ε1:t
T
T
−1
and Q = ϕ(x) (Φt Φt + I) f , which are in turn upper bounded by the terms σt (x) kSt kV −1 and kf kk σt (x)
t
respectively. Then the result follows using Theorem 1,
along with the assumption that kf kk ≤ B and the fact that
1
2 ln(det(I + Kt )) ≤ γt a.s. when Kt is invertible.

On Kernelized Multi-armed Bandits
4

4

x 10

x 10

4

2.5

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

3

2.5

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

2

Cumulative Regret

3.5

Cumulative Regret

Proof Sketch for Theorem 3. First from Theorem 2 and
the choice of xt in Algorithm 1, we show that the instantaneous regret rt at round t is upper bounded by 2βt σt−1 (xt )
with probability at P
least 1 − δ. Then the √
result follows by
T
bounding the term t=1 σt−1 (xt ) by O( T γT ).

2

1.5

1

1.5

1

0.5

First we lower bound the probability of playing an unsatu0
rated arm at round t. We define a filtration Ft−1 as the history Ht−1 up to round t − 1 and hprove that for “most”i(in
 0
0
a high probability sense) Ft−1 , P xt ∈ Dt \ St  Ft−1 ≥
√
p−1/t2 , where p = 1/4e π. This observation, along with
concentration bounds for ft (x) and f (x) and “smoothness”
of f , allow us to show that the expected regret at round
t is upper bounded in terms of σt−1 (xt ), i.e. in terms
of regret due to playing an unsaturated arm.
h More
ipre0
0

cisely, we show that for “most” Ft−1 , E rt Ft−1 ≤
h
 0 i
11ct
2B+1

p E σt−1 (xt ) Ft−1 +
t2 , and use it to prove
2B+1
t
that Xt ' rt − 11c
p σt−1 (xt ) −
t2 ; t ≥ 1 is a
super-martingale difference sequence adapted to filtration
0
{Ft }t≥1 . Now, using the Azuma-Hoeffding inequality,
PT
along with the bound on t=1 σt−1 (xt ), we obtain the desired high-probability regret bound.

6. Experiments
In this section we provide numerical results on both synthetically generated test functions and functions from realworld data. We compare GP-UCB, IGP-UCB and GP-TS
with GP-EI and GP-PI11 .
11
GP-EI and PI perform similarly and thus are not separately
distinguishable in the plots.

0.5

0

0
0

0.5

1

1.5

2

2.5

3

Rounds

3.5

0

0.5

1

1.5

2

2.5

3

Rounds

4

x 10

(a)

3.5
4

x 10

(b)

Figure 1. Cumulative regret for functions lying in the RKHS corresponding to (a) Squared Exponential kernel and (b) Matérn kernel.
4

4

x 10

x 10

3

3.5

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

2

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

3

Cumulative Regret

2.5

Cumulative Regret

Proof Sketch for Theorem 4. We follow a similar approach given in Agrawal & Goyal (2013) to prove the regret bound of GP-TS. First observe that from our choice of
discretization sets Dt , the instantaneous regret at round t
is given by rt = f (x? ) − f ([x? ]t ) + f ([x? ]t ) − f (xt ) ≤
1
?
?
t2 + ∆t (xt ), where ∆t (x) := f ([x ]t ) − f (x) and [x ]t
?
is the closest point to x in Dt . Now at each round t, after an action is chosen, our algorithm improves the confidence about true reward function f , via an update of µt (·)
and kt (·, ·). However, if we play a suboptimal arm, the regret suffered can be much higher than the improvement of
our knowledge. To overcome this difficulty, at any round
t, we divide the arms (in the present discretization Dt )
into two groups: saturated arms, St , defined as those with
∆t (x) > ct σt−1 (x) and unsaturated otherwise, where ct
is an appropriate constant. The idea is to show that the
probability of playing a saturated arm is small and then
bound the regret of playing an unsaturated arm in terms
of
This is useful because the inequality
PTstandard deviation. √
T γT ) allows us to bound the total
σ
(x
)
≤
O(
t−1
t
t=1
regret due to unsaturated arms.

1.5

1

0.5

2.5

2

1.5

1

0.5

0

0
0

0.5

1

1.5

2

Rounds

(a)

2.5

3

3.5
4

x 10

0

0.5

1

1.5

2

Rounds

2.5

3

3.5
4

x 10

(b)

Figure 2. Cumulative regret for functions lying in the GP corresponding to (a) Squared Exponential kernel and (b) Matérn kernel.

Synthetic Test Functions. We use the following procedure
to generate test functions from the RKHS. First we sample
100 points uniformly from the interval [0, 1] and use that as
our decision set. Then we compute a kernel matrix K on
those points and draw reward vector y ∼ N (0, K). Finally,
the mean of the resulting posterior distribution is used as
the test function f . We set noise parameter R2 to be 1%
of function range and use λ = R2 . We used Squared Exponential kernel with lengthscale parameter l = 0.2 and
Matérn kernel with parameters ν = 2.5, l = 0.2. Parameters βt , β̃t , vt of IGP-UCB, GP-UCB and GP-TS are
chosen as given in Section 3, with δ = 0.1, B 2 = f T Kf
and γt set according to theoretical upper bounds for corresponding kernels. We run each algorithm for T = 30000
iterations, over 25 independent trials (samples from the
RKHS) and plot the average cumulative regret along with
standard deviations (Figure 1). We see a significant improvement in the performance of IGP-UCB over GP-UCB.
In fact IGP-UCB performs the best in the pool of competitors, while GP-TS also fares reasonably well compared to
GP-UCB and GP-EI/GP-PI.
We next sample 25 random functions from the GP (0, K)
and perform the same experiment (Figure 2) for both kernels with exactly same set of parameters. The relative performance of all methods is similar to that in the previous
experiment, which is the arguably harder “agnostic” setting
of a fixed, unknown target function.
Standard Test Functions. We consider 2 well-known

On Kernelized Multi-armed Bandits
4

x 10

6000
5000

7000

Cumulative Regret

Cumulative Regret

7000

8000

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

4000
3000
2000

6000

5000

2.5

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

7000

4000

3000

2000

6000

5000

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

2

Cumulative Regret

8000

GP-PI
GP-EI
GP-TS
GP-UCB
IGP-UCB

8000

Cumulative Regret

9000

4000

3000

2000

1.5

1

0.5

0
1000

1000
0

1000

0
0

0.5

1

1.5

2

Rounds

(a)

2.5

3

3.5
4

x 10

0
0

0.5

1

1.5

2

Rounds

2.5

3

3.5
4

x 10

(b)

Figure 3. Cumulative regret for (a) Rosenbrock and (b) Hartman3
benchmark function.

synthetic benchmark functions for Bayesian Optimization:
Rosenbrock and Hartman3 (see Azimi et al. (2012) for exact analytical expressions). We sample 100 d points uniformly from the domain of each benchmark function, d being the dimension of respective domain, as the decision set.
We consider the Squared Exponential kernel with l = 0.2
and set all parameters exactly as in previous experiment.
The cumulative regret for 25 independent trials on Rosenbrock and Hartman3 benchmarks is shown in Figure 3. We
see GP-EI/PI perform better than the rest, while IGP-UCB
and GP-TS show competitive performance. Here no algorithm is aware of the underlying kernel function, hence
we conjecture that the UCB- and TS- based algorithms are
somewhat less robust on the choice of kernel than EI/PI.
Temperature Sensor Data. We use temperature data12
collected from 54 sensors deployed in the Intel Berkeley
Research lab between February 28th and April 5th, 2004
with samples collected at 30 second intervals. We tested all
algorithms in the context of learning the maximum reading of the sensors collected between 8 am to 9 am. We
take measurements of first 5 consecutive days (starting Feb.
28th 2004) to learn algorithm parameters. Following Srinivas et al. (2009), we calculate the empirical covariance matrix of the sensor measurements and use it as the kernel
matrix in the algorithms. Here R2 is set to be 5% of the
average empirical variance of sensor readings and other algorithm parameters is set similarly as in the previous experiment with γt = 1 (found via cross-validation). The functions for testing consist of one set of measurements from all
sensors in the two following days and the cumulative regret
is plotted over all such test functions. From Figure 4, we
see that IGP-UCB and GP-UCB performs the same, while
GP-TS outperforms all its competitors.
Light Sensor Data. We take light sensor data collected
in the CMU Intelligent Workplace in Nov 2005, which is
available online as Matlab structure13 and contains locations of 41 sensors, 601 train samples and 192 test samples.
12
http://db.csail.mit.edu/labdata/labdata.
html
13
http://www.cs.cmu.edu/˜guestrin/Class/
10708-F08/projects/lightsensor.zip

−0.5
0

0.5

1

1.5

2

Rounds

(a)

2.5

3

3.5
4

x 10

0

0.5

1

1.5

2

Rounds

2.5

3

3.5
4

x 10

(b)

Figure 4. Cumulative regret plots for (a) temperature data and (b)
light sensor data.

We compute the kernel matrix, estimate the noise and set
other algorithm parameters exactly as in the previous experiment. Here also GP-TS is found to perform better than
the others, with IGP-UCB performing better than GP-EI/PI
(Figure 4).
Related work. An alternative line of work pertaining to
X -armed bandits (Kleinberg et al., 2008; Bubeck et al.,
2011; Carpentier & Valko, 2015; Azar et al., 2014) studies continuum-armed bandits with smoothness structure.
For instance, Bubeck et al. (2011) show that with a Lipschitzness assumption on the reward function, algorithms
based on discretizing the domain yield nontrivial regret
d+1
guarantees, of order Ω(T d+2 ) in Rd . Other Bayesian
approaches to function optimization are GP-EI (Močkus,
1975), GP-PI (Kushner, 1964), GP-EST (Wang et al.,
2016) and GP-UCB, including the contextual (Krause &
Ong, 2011), high-dimensional (Djolonga et al., 2013; Wang
et al., 2013), time-varying (Bogunovic et al., 2016) safetyaware (Gotovos et al., 2015), budget-constraint (Hoffman
et al., 2013) and noise-free (De Freitas et al., 2012) settings. Other relevant work focuses on best arm identification problem in the Bayesian setup considering pure exploration (Grünewälder et al., 2010). For Thompson sampling
(TS), Russo & Van Roy (2014) analyze the Bayesian regret
of TS, which includes the case where the target function is
sampled from a GP prior. Our work obtains the first frequentist regret of TS for unknown, fixed functions from an
RKHS.

7. Conclusion
For bandit optimization, we have improved upon the existing GP-UCB algorithm, and introduced a new GP-TS algorithm. The proposed algorithms perform well in practice
both on synthetic and real-world data. An interesting case
is when the kernel function is also not known to the algorithms a priori and needs to be learnt adaptively. Moreover,
one can consider classes of time varying functions from the
RKHS, and general reinforcement learning with GP techniques. There are also important questions on computational aspects of optimizing functions drawn from GPs.

On Kernelized Multi-armed Bandits

Acknowledgement
Aditya Gopalan was supported by the DST INSPIRE faculty grant IFA13-ENG-69. The authors are grateful to O.A. Maillard for helpful discussions, and to anonymous reviewers for providing useful comments.

References
Abbasi-Yadkori, Yasin, Pál, Dávid, and Szepesvári, Csaba.
Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pp.
2312–2320, 2011.

Dani, Varsha, Hayes, Thomas P, and Kakade, Sham M.
Stochastic linear optimization under bandit feedback. In
COLT, pp. 355–366, 2008.
De Freitas, Nando, Smola, Alex, and Zoghi, Masrour.
Exponential regret bounds for gaussian process bandits with deterministic observations. arXiv preprint
arXiv:1206.6457, 2012.
de la Pena, Victor H, Lai, Tze Leung, and Shao, Qi-Man.
Self-normalized processes. probability and its applications, 2009.

Agrawal, Shipra and Goyal, Navin. Analysis of thompson
sampling for the multi-armed bandit problem. In COLT,
pp. 39–1, 2012.

Djolonga, Josip, Krause, Andreas, and Cevher, Volkan.
High-dimensional gaussian process bandits. In Advances
in Neural Information Processing Systems, pp. 1025–
1033, 2013.

Agrawal, Shipra and Goyal, Navin. Thompson sampling
for contextual bandits with linear payoffs. In ICML, pp.
127–135, 2013.

Gopalan, Aditya, Mannor, Shie, and Mansour, Yishay.
Thompson sampling for complex online problems. In
ICML, volume 14, pp. 100–108, 2014.

Azar, Mohammad Gheshlaghi, Lazaric, Alessandro, and
Brunskill, Emma. Online stochastic optimization under
correlated bandit feedback. In ICML, pp. 1557–1565,
2014.

Gotovos, Alkis, CH, ETHZ, and Burdick, Joel W. Safe
exploration for optimization with gaussian processes.
2015.

Azimi, Javad, Jalali, Ali, and Fern, Xiaoli. Hybrid batch
bayesian optimization. arXiv preprint arXiv:1202.5597,
2012.
Besbes, Omar and Zeevi, Assaf. Dynamic pricing without
knowing the demand function: Risk bounds and nearoptimal algorithms. Operations Research, 57(6):1407–
1420, 2009.
Bogunovic, Ilija, Scarlett, Jonathan, and Cevher, Volkan.
Time-varying gaussian process bandit optimization.
arXiv preprint arXiv:1601.06650, 2016.
Bubeck, Sébastien, Munos, Rémi, Stoltz, Gilles, and
Szepesvári, Csaba. X-armed bandits. Journal of Machine Learning Research, 12(May):1655–1695, 2011.

Grünewälder, Steffen, Audibert, Jean-Yves, Opper, Manfred, and Shawe-Taylor, John. Regret bounds for gaussian process bandit problems. In AISTATS, pp. 273–280,
2010.
Hoffman, Matthew D, Brochu, Eric, and de Freitas, Nando.
Portfolio allocation for bayesian optimization. In UAI,
pp. 327–336, 2011.
Hoffman, Matthew W, Shahriari, Bobak, and de Freitas,
Nando. Exploiting correlation and budget constraints
in bayesian multi-armed bandit optimization. arXiv
preprint arXiv:1303.6746, 2013.
Kaelbling, Leslie Pack, Littman, Michael L, and Moore,
Andrew W. Reinforcement learning: A survey. Journal
of artificial intelligence research, 4:237–285, 1996.

Carpentier, Alexandra and Valko, Michal. Simple regret for
infinitely many armed bandits. In ICML, pp. 1133–1141,
2015.

Kaufmann, Emilie, Korda, Nathaniel, and Munos, Rémi.
Thompson sampling: An asymptotically optimal finitetime analysis. In International Conference on Algorithmic Learning Theory, pp. 199–213. Springer, 2012.

Chiang, Mung, Hande, Prashanth, Lan, Tian, and Tan,
Chee Wei. Power control in wireless cellular networks.
Foundations and Trends in Networking, 2(4):381–533,
2008. ISSN 1554-057X. doi: 10.1561/1300000009.

Kleinberg, Robert, Slivkins, Aleksandrs, and Upfal, Eli.
Multi-armed bandits in metric spaces. In Proceedings of
the fortieth annual ACM symposium on Theory of computing, pp. 681–690. ACM, 2008.

Chowdhury, Sayak Ray and Gopalan, Aditya.
On
kernelized multi-armed bandits.
arXiv preprint
arXiv:1704.00445, 2017.

Krause, Andreas and Ong, Cheng S. Contextual gaussian
process bandit optimization. In Advances in Neural Information Processing Systems, pp. 2447–2455, 2011.

On Kernelized Multi-armed Bandits

Kushner, Harold J. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86(1):97–
106, 1964.
Maillard, Odalric-Ambrym. Self-normalization techniques
for streaming confident regression. 2016.
Močkus, J. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical
Conference, pp. 400–404. Springer, 1975.
Rasmussen, Carl Edward and Williams, Christopher KI.
Gaussian processes for machine learning. 2006.
Rusmevichientong, Paat and Tsitsiklis, John N. Linearly
parameterized bandits. Math. Oper. Res., 35(2):395–
411, May 2010.
Russo, Daniel and Van Roy, Benjamin. Learning to optimize via posterior sampling. Mathematics of Operations
Research, 39(4):1221–1243, 2014.
Schölkopf, Bernhard and Smola, Alexander J. Learning
with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, 2002.
Smart, William D and Kaelbling, Leslie Pack. Practical
reinforcement learning in continuous spaces. In ICML,
pp. 903–910, 2000.
Srinivas, Niranjan, Krause, Andreas, Kakade, Sham M, and
Seeger, Matthias. Gaussian process optimization in the
bandit setting: No regret and experimental design. arXiv
preprint arXiv:0912.3995, 2009.
Valko, Michal, Korda, Nathaniel, Munos, Rémi, Flaounas,
Ilias, and Cristianini, Nelo.
Finite-time analysis
of kernelised contextual bandits.
arXiv preprint
arXiv:1309.6869, 2013.
Wang, Zi, Zhou, Bolei, and Jegelka, Stefanie. Optimization
as estimation with gaussian processes in bandit settings.
In International Conf. on Artificial and Statistics (AISTATS), 2016.
Wang, Ziyu, Zoghi, Masrour, Hutter, Frank, Matheson, David, Freitas, N, et al. Bayesian optimization
in high dimensions via random embeddings. AAAI
Press/International Joint Conferences on Artificial Intelligence, 2013.

