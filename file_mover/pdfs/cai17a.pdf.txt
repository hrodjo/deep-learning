Priv‚ÄôIT: Private and Sample Efficient Identity Testing

Bryan Cai * 1 Constantinos Daskalakis * 1 Gautam Kamath * 1

Abstract
We develop differentially private hypothesis testing methods for the small sample regime. Given
a sample D from a categorical distribution p over
some domain Œ£, an explicitly described distribution q over Œ£, some privacy parameter Œµ, accuracy parameter Œ±, and requirements Œ≤I and
Œ≤II for the type I and type II errors of our test,
the goal is to distinguish between p = q and
dTV (p, q) ‚â• Œ±. We provide theoretical bounds
for the sample size |D| so that our method both
satisfies (Œµ, 0)-differential privacy, and guarantees Œ≤I and Œ≤II type I and type II errors. We
show that differential privacy may come for free
in some regimes of parameters, and we always
beat the sample complexity resulting from running the œá2 -test with noisy counts, or standard
approaches such as repetition for endowing nonprivate œá2 -style statistics with differential privacy guarantees. We experimentally compare the
sample complexity of our method to that of recently proposed methods for private hypothesis
testing (Gaboardi et al., 2016; Kifer & Rogers,
2017).

1. Introduction
Hypothesis testing is the age-old problem of deciding
whether observations from an unknown phenomenon p
conform to a model q. Often p can be viewed as a distribution over some alphabet Œ£, and the goal is to determine,
using samples from p, whether it is equal to some model
distribution q or not. This type of test is the lifeblood of
the scientific method and has received tremendous study in
statistics since its very beginnings. Naturally, the focus has
been on minimizing the number of observations from the
unknown distribution p that are needed to determine, with
*
1
Equal contribution
Massachusetts Institute of Technology, Cambridge, Massachusetts, USA. Correspondence
to: Bryan Cai <bcai@mit.edu>, Constantinos Daskalakis
<costis@csail.mit.edu>, Gautam Kamath <g@csail.mit.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

confidence, whether p = q or p 6= q.
In several fields of research and application, however, samples may contain sensitive information about individuals;
consider for example, individuals participating in some
clinical study of a disease that carries social stigma. It may
thus be crucial to guarantee that operating on the samples
needed to test a statistical hypothesis protects sensitive information about the samples. This is not at odds with the
goal of hypothesis testing itself, since the latter is about
verifying a property of the population p from which the
samples are drawn, and not of the samples themselves.
Without care, however, sensitive information about the
sample might actually be divulged by statistical processing that is improperly designed. As recently exhibited, for
example, it may be possible to determine whether individuals participated in a study from data that would typically
be published in genome-wide association studies (Homer
et al., 2008). Motivated in part by this realization, there has
been increased recent interest in developing data sharing
techniques which are private (Johnson & Shmatikov, 2013;
Uhler et al., 2013; Yu et al., 2014; Simmons et al., 2016).
Protecting privacy when computing on data has been extensively studied in several fields ranging from statistics to
diverse branches of computer science including algorithms,
cryptography, database theory, and machine learning; see,
e.g., (Dalenius, 1977; Adam & Worthmann, 1989; Agrawal
& Aggarwal, 2001; Dinur & Nissim, 2003; Dwork, 2008;
Dwork & Roth, 2014) and their references. A notion of privacy proposed by theoretical computer scientists which has
found a lot of traction is that of differential privacy (Dwork
et al., 2006). Roughly speaking, it requires that the output
of an algorithm on two neighboring datasets D and D0 that
differ in the value of one element be statistically close. For
a formal definition see Section 2.
Our goal in this paper is to develop tools for privately performing statistical hypothesis testing. In particular, we are
interested in studying the tradeoffs between statistical accuracy, power, significance, and privacy in the sample size.
To be precise, given samples from a categorical distribution
p over some domain Œ£, an explicitly described distribution
q over Œ£, some privacy parameter Œµ, accuracy parameter Œ±,
and requirements Œ≤I and Œ≤II for the type I and type II errors of our test, the goal is to distinguish between p = q

Priv‚ÄôIT

and dTV (p, q) ‚â• Œ±. We want that the output of our test
be (Œµ, 0)-differentially private, and that the probability we
make a type I or type II error be Œ≤I and Œ≤II respectively.
Treating these as hard constraints, we want to minimize the
number of samples that we draw from p.
Notice that the correctness constraint on our test pertains
to whether we draw the right conclusion about how p compares to q, while the privacy constraint pertains to whether
we respect the privacy of the samples that we draw from p.
The pertinent question is how much the privacy constraint
increases the number of samples that are needed to guarantee correctness. Our main result is that privacy may come
for free in certain regimes of parameters, and has a mild
cost for all regimes of parameters.
To be precise, without privacy constraints, it is well
known
‚àö
that identity testing can be performed from O( Œ±2n ¬∑ log Œ≤1 )
samples, where n is the size of Œ£ and Œ≤ = min{Œ≤I , Œ≤II },
and that this is tight (Batu et al., 2001; Paninski, 2008;
Valiant & Valiant, 2014; Acharya et al., 2015). Our main
theoretical result is that, with privacy constraints, the number of samples that are needed is

‚àö


‚àö
n
n
n1/3
OÃÉ max
,
,
¬∑ log(1/Œ≤) . (1)
Œ±2 Œ±3/2 Œµ Œ±5/3 Œµ2/3
Our statistical test is provided in Section 5 where the above
upper bound on the number of samples that it requires is
proven as Theorem 3. Notice that privacy
‚àö comes for free
when the privacy requirement Œµ is ‚Ñ¶( Œ±) ‚Äì for example
when Œµ = 10% and the required statistical accuracy is 3%.
The precise constants sitting in the O(¬∑) notation of Eq. (1)
are given in the proof of Theorem 3. We experimentally
verify the sample efficiency of our tests by comparing them
to recently proposed private statistical tests (Gaboardi et al.,
2016; Kifer & Rogers, 2017), discussed in more detail
shortly. Fixing a differential privacy and type I, type II error
constraints, we compare how many samples are required
by our and their methods to distinguish between hypotheses that are Œ± = 0.1 apart in total variation distance. We
find that different algorithms are more efficient depending
on the regime and properties desired by the analyst. Our
experiments and further discussion of the tradeoffs are presented in Section 6.
Approach. A standard approach to turn an algorithm differentially private is to use repetition. As already mentioned above, absent differential privacy constraints, statistical
tests have been provided that use an optimal m =
‚àö
O( Œ±2n ¬∑ log Œ≤1 ) number of samples. A trivial way to get
(Œµ, 0)-differential privacy using such a non-private test is to
create O(1/Œµ) datasets, each comprising m samples from
p, and run the non-private test on one of these datasets, chosen randomly. It is clear that changing the value of a single

element in the combined dataset may only affect the output
of the test with probability at most Œµ. Thus the output is
(Œµ, 0)-differentially private; see Section 3 for a proof. The
issue with this approach is ‚àöthat the total number of samples
that it draws is m/Œµ = O( ŒµŒ±n2 ¬∑ log Œ≤1 ), which is higher than
our target. See Corollary 1.
A different approach towards private hypothesis testing is
to look deeper into the non-private tests and try to ‚Äúprivatize‚Äù them. The most sample-efficient tests are variations
of the classical œá2 -test. They compute the number of times,
Ni , that element i ‚àà Œ£ appears in the sample and aggregate
those counts using a statistic that equals, or is close to, the
œá2 -divergence between the empirical distribution defined
by these counts and the hypothesis distribution q. They accept q if the statistic is low and reject q if it is high, using
some threshold.
A reasonable approach to privatize such a test is to add
noise, e.g. Laplace(1/Œµ) noise, to each count Ni , before
running the test. It is well known that adding Laplace(1/Œµ)
noise to a set of counts makes them differentially private,
see Theorem 1. However, it also increases the variance of
the statistic. This has been noticed empirically in recent
work of (Gaboardi et al., 2016) for the œá2 -test. We show
that the variance of the optimal œá2 -style test statistic significantly increases if we add Laplace noise to the counts,
in Section
4.1, thus increasing the sample complexity from
‚àö
O( n) to ‚Ñ¶(n3/4 ). So this route, too, seems problematic.
A last approach towards designing differentially private
tests is to exploit the distance beween the null and the alternative hypotheses. A correct test should accept the null
with probability close to 1, and reject an alternative that is
Œ±-far from the null with probability close to 1, but there are
no requirements for correctness when the alternative is very
close to the null. We could thus try to interpolate smoothly
between datasets that we expect to see when sampling the
null and datasets that we expect to see when sampling an
alternative that is far from the null. Rather than outputting
‚Äúaccept‚Äù or ‚Äúreject‚Äù by merely thresholding our statistic,
we would like to tune the probability that we output ‚Äúreject‚Äù based on the value of our statistic, and make it so that
the ‚Äúreject‚Äù probability is Œµ-Lipschitz as a function of the
dataset. Moreover, the probability should be close to 0 on
datasets that we expect to see under the null and close to 1
on datasets that we expect to see under an alternative that
2
is Œ±-far. As we show in Section
‚àö 4.2, œá -style statistics have
high sensitivity, requiring œâ( n) samples to be made appropriately Lipschitz.
While both the approach of adding noise to the counts, and
that of turning the output of the test Lipschitz fail in isolation, our test actually goes through by intricately combining these two approaches. It has two steps:

Priv‚ÄôIT

1. A filtering step, whose goal is to ‚Äúreject‚Äù when p is
blatantly far from q. This step is performed by comparing the counts Ni with their expectations under q,
after having added Laplace(1/Œµ) noise to these counts.
If the noisy counts deviate from their expectation, taking into account the extra variance introduced by the
noise, then we can safely ‚Äúreject.‚Äù Moreover, because
noise was added, this step is differentially private.
2. If the filtering step fails to reject, we perform a statistical step. This step just computes the œá2 -style statistic
from (Acharya et al., 2015), without adding noise to
the counts. The crucial observation is that if the filtering step does not reject, then the statistic is actually
Œµ-Lipschitz with respect to the counts, and thus the
value of the statistic is still differentially private. We
use the value of the statistic to determine the bias of a
coin that outputs ‚Äúreject.‚Äù
Details of our test are given in Section 5.
Related Work. Identity testing is one of the most classical problems in statistics, where it is traditionally called
hypothesis or goodness-of-fit testing, see (Pearson, 1900;
Fisher, 1935; Rao & Scott, 1981; Agresti, 2012) for some
classical and contemporary references. In this field, the focus is often on asymptotic analysis, where the number of
samples goes to infinity, and we wish to get a grasp on
their asymptotic distributions and error exponents (Agresti,
2012; Tan et al., 2010). In the past twenty years, this
problem has enjoyed significant interest in the theoretical
computer science community (see, i.e., (Batu et al., 2001;
Paninski, 2008; Levi et al., 2013; Valiant & Valiant, 2014;
Acharya et al., 2015; Canonne et al., 2016; Diakonikolas &
Kane, 2016; Daskalakis et al., 2016), and (Canonne, 2015)
for a survey), where the focus has instead been on the finite
sample regime, rather than asymptotics. Specifically, the
goal is to minimize the number of samples required, while
still remaining computationally tractable.
A number of recent works (Wang et al., 2015; Gaboardi
et al., 2016; Kifer & Rogers, 2017) (and a simultaneous
work, focused on independence testing (Kakizaki et al.,
2017)) investigate differential privacy with the former set
of goals. In particular, their algorithms focus on fixing a
desired significance (type I error) and privacy requirement,
and study the asymptotic distribution of the test statistics.
On the other hand, we are the first work to apply differential privacy to the latter line of inquiry, where our goal is
to minimize the number of samples required to ensure the
desired significance, power and privacy. As a point of comparison between these two worlds, we provide an empirical
evaluation of our method versus their methods.
The problem of distribution estimation (rather than testing)
has also recently been studied under the lens of differential

privacy (Diakonikolas et al., 2015). This is another classical statistics problem which has recently piqued the interest of the theoretical computer science community. We
note that the techniques required for this setting are quite
different from ours, as we must deal with issues that arise
from very sparsely sampled data.

2. Preliminaries
In this paper, we will focus on discrete probability distributions over [n]. For a distribution p, we will use the notation
pi to denote the mass p places on symbol i.
Definition 1. The total variation distance between p and q
is defined as
dTV (p, q) =

1 X
|pi ‚àí qi | .
2
i‚àà[n]

Definition 2. A randomized algorithm M with domain Nn
is (Œµ, Œ¥)-differentially private if for all S ‚äÜ Range(M ) and
for all pairs of inputs D, D0 such that kD ‚àí D0 k1 ‚â§ 1:
Pr [M (D) ‚àà S] ‚â§ eŒµ Pr [M (D0 ) ‚àà S] + Œ¥.
If Œ¥ = 0, the guarantee is called pure differential privacy.
In the context of distribution testing, the neighboring
dataset definition corresponds to two datasets where one
dataset is generated from the other by removing one sample. Up to a factor of 2, this is equivalent to the alternative
definition where one dataset is generated from the other by
arbitrarily changing one sample.
Definition 3. An algorithm for the (Œ±, Œ≤I , Œ≤II )-identity
testing problem with respect to a (known) distribution q
takes m samples from an (unknown) distribution p and has
the following guarantees:
‚Ä¢ If p = q, then with probability at least 1‚àíŒ≤I it outputs
‚Äúp = q;‚Äù
‚Ä¢ If dTV (p, q) ‚â• Œ±, then with probability at least 1‚àíŒ≤II
it outputs ‚Äúp 6= q.‚Äù
In particular, Œ≤I and Œ≤II are the type I and type II errors of
the test. Parameter Œ± is the radius of distinguishing accuracy. Notice that, when p satisfies neither of cases above,
the algorithm‚Äôs output may be arbitrary.
We note that if an algorithm is to satisfy both these definitions, the latter condition (the correctness property) need
only be satisfied when p falls into one of the two cases,
while the former condition (the privacy property) must be
satisfied for all realizations of the samples from p (and in
particular, for p which do not fall into the two cases above).

Priv‚ÄôIT

We recall the classical Laplace mechanism, which states
that applying independent Laplace noise to a set of counts
is differentially private.
Theorem 1 (Theorem 3.6 of (Dwork & Roth, 2014)).
Given a set of counts N1 , . . . , Nn , the noised counts (N1 +
Y1 , . . . , Nn + Yn ) are (Œµ, 0)-differentially private when the
Yi ‚Äôs are i.i.d. random variables drawn from Laplace(1/Œµ).
Finally, we recall the definition of zero-concentrated differential privacy from (Bun & Steinke, 2016) and its relationship to differential privacy.
Definition 4. A randomized algorithm M with domain Nn
is œÅ-zero-concentrated differentially private (œÅ-zCDP) if for
all pairs of inputs D, D0 such that kD ‚àí D0 k1 ‚â§ 1 and all
Œ± ‚àà (1, ‚àû):
0

DŒ± (M (D)||M (D )) ‚â§ œÅŒ±,
where DŒ± is the Œ±-ReÃÅnyi divergence between the distribution of M (D) and M (D0 ).
Proposition 1 (Propositions 1.3 and 1.4 of (Bun & Steinke,
2016)). If a mechanism M1 satisfies (Œµ, 0)-differential pri2
vacy, then M1 satisfies Œµ2 -zCDP. If a mechanism M2 satp
isfies œÅ-zCDP, then M2 satisfies (œÅ + 2 œÅ log(1/Œ¥), Œ¥)differential privacy for any Œ¥ > 0.

3. A Simple Upper Bound
‚àö 
n
Œ±2 Œµ

In this section, we provide an O
upper bound for the
differentially private identity testing problem. More generally, we show that if an algorithm requires a dataset of
size m for a decision problem, then it can be made (Œµ, 0)differentially private at a multiplicative cost of 1/Œµ in the
sample size. This is a folklore result, but we include and
prove it here for completeness.
Theorem 2. Suppose there exists an algorithm for a decision problem P which succeeds with probability at least
1 ‚àí Œ≤ and requires a dataset of size m. Then there exists an
(Œµ, 0)-differentially private algorithm for P which succeeds
with probability at least 54 (1 ‚àí Œ≤) + 1/10 and requires a
dataset of size O(m/Œµ).
Proof. First, with probability 1/5, we flip a coin and output yes or no with equal probability. This guarantees that
we have probability at least 1/10 of either outcome, which
will allow us to satisfy the multiplicative guarantee of differential privacy.
We then draw 10/Œµ datasets of size m, and solve the decision problem (non-privately) for each of them. Finally, we
select a random one of these computations and output its
outcome.
The correctness follows, since we randomly choose the
right answer with probability 1/10, or with probability 4/5,

we solve the problem correctly with probability 1 ‚àí Œ≤. As
for privacy, we note that, if we remove a single element
of the dataset, we may only change the outcome of one
of these computations. Since we pick a random computation, this is selected with probability Œµ/10, and thus the
probability of any outcome is additively shifted by at most
Œµ/10. Since we know the minimum probability of any output is 1/10, this gives the desired multiplicative guarantee
required for (Œµ, 0)-differential privacy.
We obtain the following corollary by noting that the
tester
‚àö of (Acharya et al., 2015) (among others) requires
O( n/Œ±2 ) samples for identity testing.
Corollary 1. There exists an (Œµ, 0)-differentially private
testing algorithm for the (Œ±, Œ≤I , Œ≤II )-identity testing problem for any distribution q which requires
‚àö
m=O


n
¬∑ log(1/Œ≤)
ŒµŒ±2

samples, where Œ≤ = min (Œ≤I , Œ≤II ).

4. Roadblocks to Differentially Private
Testing
In this section, we describe roadblocks which prevent two
natural approaches to differentially private testing from
working.
In Section 4.1, we show that if one simply adds Laplace
noise to the empirical counts of a dataset (i.e., runs the
Laplace mechanism of Theorem 1) and then attempts to
run an optimal identity tester, the variance of the statistic
increases dramatically, and thus results in a much larger
sample complexity, even for the case of uniformity testing. The intuition behind this phenomenon is as follows.
When performing uniformity testing in the small sample
regime (when the number of samples m is the square root
of the domain
‚àö size n), we will see a (1 ‚àí o(1))n elements
0 times, O( n) elements 1 time, and O(1) elements 2
times. If we add Laplace(10) noise to guarantee (0.1, 0)differential privacy, this obliterates the signal provided by
these collision statistics, and thus many more samples are
required before the signal prevails.
In Section 4.2, we demonstrate that œá2 statistics have high
sensitivity, and thus are not naturally differentially private. In other words, if we consider a œá2 statistic Z
on two datasets D and D0 which differ in one record,
|Z(D) ‚àí Z(D0 )| may be quite large. This implies that
methods such as rescaling this statistic and interpreting it
as a probability, or applying noise to the statistic, will not
be differentially private until we have taken a large number
of samples.

Priv‚ÄôIT

4.1. A Laplaced œá2 -statistic has large variance
Proposition 2. Applying the Laplace mechanism to a
dataset before applying the identity tester of (Acharya
et al., 2015) results in a significant increase in the variance, even when considering the case of uniformity. More
precisely, if we consider the statistic
Z 0 (D) =

X (Ni + Yi ‚àí m/n)2 ‚àí (Ni + Yi )
m/n

i‚àà[n]

where Ni is the number of occurrences of symbol i in
the dataset D (which is of size P oisson(m)) and Yi ‚àº
Laplace(1/Œµ), then
‚Ä¢ If p is uniform, then E[Z 0 ] =
20n3
Œµ4 m 2 .

2

2n
Œµ2 m

and Var[Z 0 ] ‚â•

‚Ä¢ If p is a particular distribution which is Œ±-far in total variation distance from uniform, then E[Z 0 ] =
2
4mŒ±2 + Œµ2n
2m .
The variance of the statistic can be compared to that of the
unnoised statistic, which is upper bounded by m2 Œ±4 . We
can see that the noised statistic has larger variance until
m = ‚Ñ¶(n3/4 ).

Next, we examine the variance of Z 0 . Let Œªi = mpi and
Œª0i = mqi = m/n. By a similar computation as before, we
have that
X 1 
2Œª2i + 4Œªi (Œªi ‚àí Œª0i )2
Var[Z 0 ] =
Œª02
i‚àà[n] i

1
20
0
2
+ 2 (8Œªi + 2(2Œªi ‚àí 2Œªi ‚àí 1) ) + 4 .
Œµ
Œµ
Since all four summands of this expression are nonnegative, we have that
Var[Z 0 ] ‚â•

20 X 1
20n3
=
.
02
Œµ4
Œªi
Œµ4 m2
i‚àà[n]

If we wish to use Chebyshev‚Äôs inequality to separate these
two cases, we require that Var[Z 0 ] is at most the square
of the mean separation. In other
we require that
 3/4words,

n
20n3
2 4
.
Œµ4 m2 ‚â§ m Œ± , or that m = ‚Ñ¶
ŒµŒ±
4.2. A œá2 -statistic has high sensitivity
Consider the primary statistic which we use in Algorithm
1:
1 X (Ni ‚àí mqi )2 ‚àí Ni
Z(D) =
.
mŒ±2
mqi
i‚àà[n]

0

Proof. First, we compute the mean of Z . Note that since
|D| ‚àº P oisson(m), the Ni ‚Äôs will be independently distributed as P oisson(mpi ) (see, i.e., (Acharya et al., 2015)
for additional discussion).
X

(Ni + Yi ‚àí m/n)2 ‚àí (Ni + Yi )
0
E[Z ] = E
m/n
i‚àà[n]
X
(Ni ‚àí m/n)2 ‚àí Ni
=E
m/n
i‚àà[n]
X Y 2 + 2Yi (Ni ‚àí m/n) ‚àí Yi 
i
+
m/n
i‚àà[n]

= m ¬∑ œá2 (p, q) +

X
i‚àà[n]

= m ¬∑ œá2 (p, q) +

As shown in Section 5, E[Z] = 0 if p = q and E[Z] ‚â• 1
if dTV (p, q) ‚â• Œ±, and the variance of Z is such that these
two cases can be separated with constant probability. A
natural approach is to truncate this statistic to the range
[0, 1], interpret it as a probability and output the result of
Bernoulli(Z) ‚Äì if p = q, the result is likely to be 0, and if
dTV (p, q) ‚â• Œ±, the result is likely to be 1. One might hope
that this statistic is naturally private. More specifically, we
would like that the statistic Z has low sensitivity, and does
not change much if we remove a single individual. Unfortunately, this is not the case. We consider datasets D, D0 ,
where D0 is identical to D, but with one fewer occurrence
of symbol i. It can be shown that the difference in Z is

2
Œµ2

|Z(D) ‚àí Z(D0 )| =

m/n

2n2
Œµ2 m

In other words, the mean is a rescaling of the œá2 distance
between p and q, shifted by some constant amount. When
p = q, the œá2 -distance between p and q is 0, and the expectation is just the second term. Focus on the case where n is
even, and consider p such that pi = (1 + 2Œ±)/n if i is even,
and (1 ‚àí 2Œ±)/n otherwise. This is Œ±-far from uniform in
total variation distance. Furthermore, by direct calculation,
œá2 (p, q) = 4Œ±2 , and thus the expectation of Z 0 in this case
2
is 4mŒ±2 + Œµ2n
2m .

2|Ni ‚àí mqi ‚àí 1|
m2 Œ± 2 qi

Letting q be the uniform distribution and requiring that
this is at most Œµ (for the sake of privacy), we have a con2Ni n
straint which
is roughly of the form m
2 Œ±2 ‚â§ Œµ, or that

‚àö

m=‚Ñ¶

‚àö
Ni n
Œµ0.5 Œ±

.

In particular, if Ni = ‚àönc for any c > 0, this does not
achieve the desired O( n) sample complexity. One may
observe that, if Ni is this large, looking at symbol i alone
is sufficient to conclude p is not uniform, even if the count
Ni had Laplace noise added. Indeed, our main algorithm of
Section 5 works in part due to our formalization and quantification of this intuition.

Priv‚ÄôIT

5. Priv‚ÄôIT: A Differentially Private Identity
Tester
In this section, we sketch the proof of our main testing upper bound:
Theorem 3. There exists an (Œµ, 0)-differentially private
testing algorithm for the (Œ±, Œ≤I , Œ≤II )-identity testing problem for any distribution q which requires



‚àö
‚àö
n
n
n1/3
¬∑
log(1/Œ≤)
,
,
m = OÃÉ max
Œ±2 Œ±3/2 Œµ Œ±5/3 Œµ2/3
samples, where Œ≤ = min (Œ≤I , Œ≤II ).
The full details of the proof are provided in the supplementary materials.
The pseudocode for this algorithm is provided in Algorithm
1. We fix the constants c1 = 1/4 and c2 = 3/40. For a
high-level overview of our algorithm‚Äôs approach, we refer
the reader to the Approach paragraph in Section 1.
Algorithm 1 Priv‚ÄôIT: A differentially private identity tester
1: Input: Œµ; an explicit distribution q; sample access to a
distribution p
2: Define A ‚Üê {i : qi ‚â• c1 Œ±/n}, AÃÑ ‚Üê [n] \ A
3: Sample Yi ‚àº Laplace(2/c2 Œµ) for all i ‚àà A
4: if there
 exists i ‚àà A such that |Yi | ‚â•
2
c2 Œµ

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

log 1‚àí(1‚àíc12 )1/|A| then
return either ‚Äúp 6= q‚Äù or ‚Äúp = q‚Äù with equal probability
end if
Draw a multiset S of P oisson(m) samples from p
Let Ni be the number of occurrences of the ith domain
element in S
for i ‚àà A do


if |Ni + Yi ‚àí mqi | ‚â• c22Œµ log 1‚àí(1‚àíc12 )1/|A| +
 ‚àö
	
max 4 mqi log n, log n then
return ‚Äúp 6= q‚Äù
end if
end for P
(Ni ‚àímqi )2 ‚àíNi
2
Z ‚Üê mŒ±
2
i‚ààA
mqi
Let T be the closest value to Z which is contained in
the interval [0, 1]
Sample b ‚àº Bernoulli(T )
if b = 1 then
return ‚Äúp 6= q‚Äù
else
return ‚Äúp = q‚Äù
end if

Proof of Theorem 3 (sketch): We focus on the case where
Œ≤ = 1/3, the general case follows at the cost of a multiplicative log(1/Œ≤) in the sample complexity from a stan-

dard amplification argument. We will require the following
tail bounds on Ni and Yi .


Claim 1. |Yi | ‚â§ c22Œµ log 1‚àí(1‚àíc12 )1/|A| simultaneously
for all i ‚àà A with probability exactly 1 ‚àí c2 .
	
 ‚àö
Claim 2. |Ni ‚àí mpi | ‚â§ max 4 mpi log n, log n simultaneously for all i ‚àà A with probability at least
2
‚àí 1.1
1 ‚àí n0.84
n .
Correctness. Correctness can be shown in a similar
‚àö way
to (Acharya et al., 2015) ‚Äì in short, if m = ‚Ñ¶( n/Œ±2 ),
then the expectations are separated in the two cases, and the
variance is bounded. A careful combination of the previous
claims and Chebyshev‚Äôs inequality guarantee correctness.
Privacy. We will prove (0, c2 Œµ/2)-differential privacy,
which in our setting, will imply (Œµ, 0)-differential privacy
(due to Claim 1).
We first consider the possibility of rejecting in line 11.
Noising our counts by the random variables Yi ensures that
this step is (0, c2 Œµ/4)-differentially private.
Consider the difference in value of Z for two neighboring datasets D and D0 , differing in i: Z(D) ‚àí Z(D0 ) =
2(Ni ‚àímqi ‚àí1)
. Conditioning on the event that we did not
m2 Œ±2 qi
return in line 11, we can show
o
n p
4 log(n/c2 )
|Ni ‚àí mqi | ‚â§
+ max 4 mqi log n, log n .
c2 Œµ
This
2
m2 Œ±2 qi

implies

|Z(D)  ‚àí Z(D0 )|
+ 4 mqi log n . Enforcing

that

6 log(n/c2 )
c2 Œµ

‚àö

‚â§
that

each of these
the condition
qterms‚àöare at most c2 Œµ/8 gives

2/3
n log(n/c2 )
(n log n)1/3
64
96
‚àö
,
m ‚â• max
.
Œ±1.5 Œµ
c2 c1
c2 c1
Œ±5/3 Œµ2/3
2

Since both terms are at most c2 Œµ/8, this step is (0, c2 Œµ/4)differentially private. By composition of differential privacy, this gives the desired overall (0, c2 Œµ/2)-differential
privacy and thus Œµ-pure differential privacy.

6. Experiments
We performed an empirical evaluation of our algorithm,
Priv‚ÄôIT, on synthetic datasets. All experiments were
performed on a laptop computer with a 2.6 GHz Intel Core
i7-6700HQ CPU and 8 GB of RAM. Significant discussion
is required to provide a full comparison with prior work in
this area, since performance of the algorithms varies depending on the regime.
We compared our algorithm with two recent algorithms for
differentially private hypothesis testing:
1. The Monte Carlo Goodness of fit test with Laplace
noise from (Gaboardi et al., 2016), MCGOF;

Priv‚ÄôIT
Uniformity Testing

2. The projected Goodness of Fit test from (Kifer &
Rogers, 2017), zCDP-GOF.

Priv‚ÄôIT
zCDP-GOF
MCGOF

250000

Z=

X (Ni ‚àí mqi )2 ‚àí Ni
i‚ààA

mqi

.

We add Laplace noise to Z, with scale parameter Œò(‚àÜ/Œµ),
where ‚àÜ is the sensitivity of Z, which guarantees (Œµ/2, 0)differential privacy. Then, similar to the other algorithms,
we choose a threshold for this noised statistic such that
we have the desired type I error. This algorithm can be
analyzed to provide identical theoretical guarantees as Algorithm 1, but with the practical advantage that there are
fewer parameters to tune.
To begin our experimental evaluation, we started with uniformity testing. Our experimental setup was as follows.
The algorithms were provided q as the uniform distribution
over [n]. The algorithms were also provided with samples
from some distribution p. This (unknown) p was q for the
case p = q, or a distribution which we call the ‚ÄúPaninski
construction‚Äù for the case dTV (p, q) ‚â• Œ±. The Paninski
construction is a distribution where half the elements of the
support have mass (1+Œ±)/n and half have mass (1‚àíŒ±)/n.
We use this name for the construction as (Paninski, 2008)
showed that this example is one of the
‚àö hardest to distinguish from uniform: one requires ‚Ñ¶( n/Œ±2 ) samples to
(non-privately) distinguish a random permutation of this
construction from the uniform distribution. We fixed parameters Œµ = 0.1 and Œ± = 0.1. In addition, recall that
Proposition 1 implies that pure differential privacy (the privacy guaranteed by Priv‚ÄôIT) is stronger than zCDP (the
privacy guaranteed by zCDP-GOF). In particular, our guarantee of Œµ-pure differential privacy implies Œµ2 /2-zCDP. As
a result, we ran zCDP-GOF with a privacy parameter of
0.005-zCDP, which is equivalent to the amount of zCDP
our algorithm provides. Our experiments were conducted
on a number of different support sizes n, ranging from 10
to 10600. For each n, we ran the testing algorithms with increasing sample sizes m in order to discover the minimum
sample size when the type I and type II errors were both
empirically below 1/3. To determine these empirical error
rates, we ran all algorithms 1000 times for each n and m,
and recorded the fraction of the time each algorithm was
correct. As the other algorithms take a parameter Œ≤I as a
target type I error, we input 1/3 as this parameter.
The results of our first test are provided in Figure 1. The
x-axis indicates the support size, and the y-axis indicates
the minimum number of samples required. We plot three
lines, which demonstrate the empirical number of samples

200000
Sample Complexity (m)

We note that we implemented a modified version of
Priv‚ÄôIT, which differs from Algorithm 1 in lines 14 to
21. In particular, we instead consider a statistic

150000

100000

50000

0
0

2000

4000

6000
Support Size (n)

8000

10000

Figure 1. The sample complexities of Priv‚ÄôIT, MCGOF, and
zCDP-GOF for uniformity testing

required to obtain 1/3 type I and type II error for the different algorithms. We can see that in this case, zCDP-GOF
is the most statistically efficient, followed by MCGOF and
Priv‚ÄôIT.
To explain this difference in statistical efficiency, we note
that the theoretical guarantees of Priv‚ÄôIT imply that it
performs well even when data is sparsely sampled. More
precisely, one of the benefits of our tester is that it can
reduce the variance induced by elements whose expected
number of occurrences is less than 1. Since none of these
testers reach this regime (i.e., even zCDP-GOF at n =
10000 expects to see each element 10 times), we do not
reap the benefits of Priv‚ÄôIT. Ideally, we would run these
algorithms on the uniform distribution at sufficiently large
support sizes. However, since this is prohibitively expensive to do with thousands of repetitions (for any of these
methods), we instead demonstrate the advantages of our
tester on a different distribution.
Our second test is conducted with q being a 2-histogram1 ,
where all but a vanishing fraction of the probability mass is
concentrated on a small, constant fraction of the support2 .
This serves as our proxy for a very large support, since now
we will have elements which have a sub-constant expected
number of occurrences. The algorithms are provided with
samples from a distribution p, which is either q or a similar
Paninski construction as before, where the total variation
distance from q is placed on the support elements containing non-negligible mass. We ran the test on support sizes n
ranging from 10 to 6800. All other parameters are the same
1
A k-histogram is a distribution where the domain can be partitioned into k intervals such that the distribution is uniform over
each interval.
2
In particular, in Figure 3, n/200 support elements contained
1 ‚àí 10/n probability mass, but similar trends hold with modifications of these parameters.

Priv‚ÄôIT
Identity Testing on a 2-Histogram

Uniformity Testing, Revisited

14000

Priv‚ÄôIT
zCDP-GOF

zCDP-GOF
Priv‚ÄôIT (Œ¥ = 1/e)

80000

12000

Priv‚ÄôIT (Œ¥ = 1/e2 )
Priv‚ÄôIT (Œ¥ = 1/e4 )
Priv‚ÄôIT (Œ¥ = 1/e8 )
60000
Sample Complexity (m)

Sample Complexity (m)

10000

8000

6000

Priv‚ÄôIT (Œ¥ = 1/e16 )

40000

4000
20000

2000

0
0
0

1000

2000

3000
4000
Support Size (n)

5000

6000

7000

0

1000

2000

3000
Support Size (n)

4000

5000

6000

Figure 2. The sample complexities of Priv‚ÄôIT and zCDP-GOF
for identity testing on a 2-histogram

Figure 3. The sample complexities of Priv‚ÄôIT and zCDP-GOF
for uniformity testing, with approximate differential privacy

as in the previous test.

The results of our third test are provided in Figure 3. We
found that, for all Œ¥ tested, Priv‚ÄôIT required fewer samples than zCDP-GOF. This is unsurprising for Œ¥ very large
and small, since the differential privacy guarantees become
very easy to satisfy, but we found it to be true for even
‚Äúmoderate‚Äù values of Œ¥. This implies that if an analyst is
satisfied with approximate differential privacy, she might
be better off using Priv‚ÄôIT, rather than an algorithm
which guarantees zCDP.

The results of our second test are provided in Figure 2. In
this case, we compare Priv‚ÄôIT and zCDP-GOF, and note
that our test is slightly better for all support sizes n, though
the difference can be pronounced or diminished depending on the construction of the distribution q. We found
that MCGOF was incredibly inefficient on this construction
‚Äì even for n = 400 it required 130000 samples, which is
a factor of 10 worse than zCDP-GOF on a support of size
n = 6800. To explain this phenomenon, we can inspect the
contribution of a single domain element i to their statistic:
(Ni + Yi ‚àí mqi )2
.
mqi
In the case where mqi  1 and p = q, this is approxiY2
mately equal to mqi i . The standard deviation of this term
will be of the order mq1i Œµ2 , which can be made arbitrarily
large as mqi ‚Üí 0. While zCDP-GOF may naively seem
susceptible to this same pitfall, their projection method appears to elegantly avoid it.
As a final test, we note that zCDP-GOF guarantees zCDP,
while Priv‚ÄôIT guarantees (vanilla) differential privacy.
In our previous tests, our guarantee was Œµ-differential pri2
vacy, while theirs was Œµ2 -zCDP: by Proposition 1, our
guarantees imply theirs. In the third test, we revisit uniformity testing, but when their guarantees imply ours. More
specifically, again with Œµ = 0.1, we ran zCDP-GOF with
2
the guarantee of Œµ2 -zCDP and Priv‚ÄôIT with the guaranp
2
tee of ( Œµ2 + Œµ 2 log(1/Œ¥), Œ¥) for various Œ¥ > 0. We note
that Œ¥ is often thought in theory to be ‚Äúcryptographically
small‚Äù (such as 2‚àí100 ), but we compare with a wide range
of Œ¥, both large and small: Œ¥ = 1/et for t ‚àà {1, 2, 4, 8, 16}.
This test was conducted on support sizes n ranging from 10
to 6000.

While the main focus of our evaluation was statistical in nature, we will note that Priv‚ÄôIT was more efficient in runtime than our implementation of MCGOF, and more efficient
in memory usage than our implementation of zCDP-GOF.
The former point was observed by noting that, in the same
amount of time, Priv‚ÄôIT was able to reach a trial corresponding to a support size of 20000, while MCGOF was
only able to reach 10000. The latter point was observed by
noting that zCDP-GOF ran out of memory at a support size
of 11800. This is likely because zCDP-GOF requires matrix computations on a matrix of size O(n2 ). It is plausible
that all of these implementations could be made more time
and memory efficient, but we found our implementations
to be sufficient for the sake of our comparison.

Acknowledgments
The authors would like to thank Jon Ullman for helpful discussions in the early stages of this work. The authors were
supported by NSF CCF-1551875, CCF-1617730, CCF1650733, and ONR N00014-12-1-0999.

Priv‚ÄôIT

References
Acharya, Jayadev, Daskalakis, Constantinos, and Kamath,
Gautam. Optimal testing for properties of distributions.
In Advances in Neural Information Processing Systems
28, NIPS ‚Äô15, pp. 3577‚Äì3598. Curran Associates, Inc.,
2015.
Adam, Nabil R. and Worthmann, John C. Security-control
methods for statistical databases: A comparative study.
ACM Computing Surveys (CSUR), 21(4):515‚Äì556, 1989.
Agrawal, Dakshi and Aggarwal, Charu C. On the design
and quantification of privacy preserving data mining algorithms. In Proceedings of the 20th ACM SIGMODSIGACT-SIGART Symposium on Principles of Database
Systems, PODS ‚Äô01, pp. 247‚Äì255, New York, NY, USA,
2001. ACM.
Agresti, Alan. Categorical Data Analysis. Wiley, 2012.
Batu, Tugkan, Fischer, Eldar, Fortnow, Lance, Kumar,
Ravi, Rubinfeld, Ronitt, and White, Patrick. Testing
random variables for independence and identity. In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science, FOCS ‚Äô01, pp. 442‚Äì451,
Washington, DC, USA, 2001. IEEE Computer Society.
Bun, Mark and Steinke, Thomas. Concentrated differential
privacy: Simplifications, extensions, and lower bounds.
In Proceedings of the 14th Conference on Theory of
Cryptography, TCC ‚Äô16-B, pp. 635‚Äì658, Berlin, Heidelberg, 2016. Springer.
Canonne, CleÃÅment L. A survey on distribution testing:
Your data is big. but is it blue? Electronic Colloquium
on Computational Complexity (ECCC), 22(63), 2015.
Canonne, CleÃÅment L. A short note on Poisson tail bounds.
http://www.cs.columbia.edu/Àúccanonne/
files/misc/2017-poissonconcentration.
pdf, 2017.
Canonne, CleÃÅment L., Diakonikolas, Ilias, Gouleakis,
Themis, and Rubinfeld, Ronitt. Testing shape restrictions of discrete distributions. In Proceedings of the 33rd
Symposium on Theoretical Aspects of Computer Science,
STACS ‚Äô16, pp. 25:1‚Äì25:14, 2016.
Dalenius, Tore. Towards a methodology for statistical disclosure control. Statistisk Tidskrift, 15:429‚Äì444, 1977.
Daskalakis, Constantinos, Dikkala, Nishanth, and Kamath, Gautam. Testing Ising models. arXiv preprint
arXiv:1612.03147, 2016.

Diakonikolas, Ilias and Kane, Daniel M. A new approach
for testing properties of discrete distributions. In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science, FOCS ‚Äô16, pp. 685‚Äì694,
Washington, DC, USA, 2016. IEEE Computer Society.
Diakonikolas, Ilias, Hardt, Moritz, and Schmidt, Ludwig.
Differentially private learning of structured discrete distributions. In Advances in Neural Information Processing Systems 28, NIPS ‚Äô15, pp. 2566‚Äì2574. Curran Associates, Inc., 2015.
Dinur, Irit and Nissim, Kobbi. Revealing information while
preserving privacy. In Proceedings of the 22nd ACM
SIGMOD-SIGACT-SIGART Symposium on Principles of
Database Systems, PODS ‚Äô03, pp. 202‚Äì210, New York,
NY, USA, 2003. ACM.
Dwork, Cynthia. Differential privacy: A survey of results. In Proceedings of the 5th International Conference on Theory and Applications of Models of Computation, TAMC ‚Äô08, pp. 1‚Äì19, Berlin, Heidelberg, 2008.
Springer.
Dwork, Cynthia and Roth, Aaron. The Algorithmic Foundations of Differential Privacy. Now Publishers, Inc.,
2014.
Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In Proceedings of the 3rd Conference on
Theory of Cryptography, TCC ‚Äô06, pp. 265‚Äì284, Berlin,
Heidelberg, 2006. Springer.
Fisher, Ronald A. The Design of Experiments. Macmillan,
1935.
Gaboardi, Marco, Lim, Hyun-Woo, Rogers, Ryan M., and
Vadhan, Salil P. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In Proceedings of the 33rd International Conference on Machine Learning, ICML ‚Äô16, pp. 1395‚Äì1403.
JMLR, Inc., 2016.
Homer, Nils, Szelinger, Szabolcs, Redman, Margot, Duggan, David, Tembe, Waibhav, Muehling, Jill, Pearson,
John V., Stephan, Dietrich A., Nelson, Stanley F., and
Craig, David W. Resolving individuals contributing trace
amounts of dna to highly complex mixtures using highdensity snp genotyping microarrays. PLoS Genetics, 4
(8):1‚Äì9, 2008.
Johnson, Aaron and Shmatikov, Vitaly. Privacy-preserving
data exploration in genome-wide association studies. In
Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ‚Äô13, pp. 1079‚Äì1087, New York, NY, USA, 2013.
ACM.

Priv‚ÄôIT

Kakizaki, Kazuya, Sakuma, Jun, and Fukuchi, Kazuto. Differentially private chi-squared test by unit circle mechanism. In Proceedings of the 34th International Conference on Machine Learning, ICML ‚Äô17. JMLR, Inc.,
2017.

Valiant, Gregory and Valiant, Paul. An automatic inequality prover and instance optimal identity testing. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science, FOCS ‚Äô14, pp. 51‚Äì60,
Washington, DC, USA, 2014. IEEE Computer Society.

Kifer, Daniel and Rogers, Ryan M. A new class of private
chi-square tests. In Proceedings of the 20th International
Conference on Artificial Intelligence and Statistics, AISTATS ‚Äô17, pp. 991‚Äì1000. JMLR, Inc., 2017.

Wang, Yue, Lee, Jaewoo, and Kifer, Daniel. Differentially private hypothesis testing, revisited. arXiv preprint
arXiv:1511.03376, 2015.

Klar, Bernhard. Bounds on tail probabilities of discrete
distributions. Probability in the Engineering and Informational Sciences, 14(02):161‚Äì171, 2000.
Levi, Reut, Ron, Dana, and Rubinfeld, Ronitt. Testing
properties of collections of distributions. Theory of Computing, 9(8):295‚Äì347, 2013.
Paninski, Liam. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE
Transactions on Information Theory, 54(10):4750‚Äì4755,
2008.
Pearson, Karl. On the criterion that a given system of deviations from the probable in the case of a correlated system
of variables is such that it can be reasonably supposed to
have arisen from random sampling. Philosophical Magazine Series 5, 50(302):157‚Äì175, 1900.
Pollard, David.
A few good inequalities.
http:
//www.stat.yale.edu/Àúpollard/Books/
Mini/Basic.pdf, 2015.
Rao, Jon N.K. and Scott, Alastair J. The analysis of categorical data from complex sample surveys: Chi-squared
tests for goodness of fit and independence in two-way tables. Journal of the Americal Statistical Association, 76
(374):221‚Äì230, 1981.
Simmons, Sean, Sahinalp, Cenk, and Berger, Bonnie. Enabling privacy-preserving gwass in heterogeneous human populations. Cell Systems, 3(1):54‚Äì61, 2016.
Tan, Vincent Y.F., Anandkumar, Animashree, and Willsky, Alan S. Error exponents for composite hypothesis
testing of Markov forest distributions. In Proceedings
of the 2010 IEEE International Symposium on Information Theory, ISIT ‚Äô10, pp. 1613‚Äì1617, Washington, DC,
USA, 2010. IEEE Computer Society.
Uhler, Caroline, SlavkovicÃÅ, Aleksandra, and Fienberg,
Stephen E. Privacy-preserving data sharing for genomewide association studies. The Journal of Privacy and
Confidentiality, 5(1):137‚Äì166, 2013.

Yu, Fei, Fienberg, Stephen E., SlavkovicÃÅ, Aleksandra B.,
and Uhler, Caroline. Scalable privacy-preserving data
sharing methodology for genome-wide association studies. Journal of Biomedical Informatics, 50:133‚Äì141,
2014.

