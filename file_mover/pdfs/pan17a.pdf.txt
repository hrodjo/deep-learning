Prediction under Uncertainty in Sparse Spectrum Gaussian Processes
with Applications to Filtering and Control

Yunpeng Pan 1 2 Xinyan Yan 1 3 Evangelos A. Theodorou 1 2 Byron Boots 1 3

Abstract

ization,
Z

Sparse Spectrum Gaussian Processes (SSGPs)
are a powerful tool for scaling Gaussian processes (GPs) to large datasets. Existing SSGP
algorithms for regression assume deterministic
inputs, precluding their use in many real-world
robotics and engineering applications where accounting for input uncertainty is crucial. We
address this problem by proposing two analytic
moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs. Our methods are more general and scalable than their standard GP counterparts, and are
naturally applicable to multi-step prediction or
uncertainty propagation. We show that efficient
algorithms for Bayesian filtering and stochastic
model predictive control can use these methods,
and we evaluate our algorithms with comparative
analyses and both real-world and simulated experiments.

1. Introduction
The problem of prediction under uncertainty, appears in
many fields of science and engineering that involve sequential prediction including state estimation (Ko & Fox,
2009; Deisenroth et al., 2012), time series prediction (Girard et al., 2003), stochastic process approximation (Archambeau et al., 2007), and planning and control (Deisenroth et al., 2015; Pan et al., 2015). In these problems, uncertainty can be found in both the predictive models and the
model’s inputs. Formally, we are often interested in finding
the probability density of a prediction y, given a distribution p(x) and a probabilistic model p(y|x). By marginal1
Georgia Institute of Technology, Atlanta, Georgia,
USA 2 School of Aerospace Engineering 3 School of Interactive Computing.
Correspondence to: Yunpeng Pan
<ypan37@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

p(y) =

p(y|x)p(x) dx.

(1)

Unfortunately, computing this integral exactly is often intractable. In this paper, we tackle a subfamily of (1) where:
1) the probabilistic model is learned from data and specified by a sparse spectrum representation of a Gaussian process (SSGP); and 2) the input x is normally distributed. We
show that analytic expressions of the moments of p(y) can
be derived and that these are directly applicable to sequential prediction problems like filtering and control.
1.1. Related work
Gaussian Process (GP) regression with uncertain inputs
has been addressed by Candela et al. (2003); Girard et al.
(2003), and extended to the multivariate outputs by Kuss
(2006). These methods have led to the development of
many algorithms in reinforcement learning (Rasmussen &
Kuss, 2004; Deisenroth et al., 2015), Bayesian filtering
(Ko & Fox, 2009; Deisenroth et al., 2009), and smoothing
(Deisenroth et al., 2012). However, these approaches have
two major limitations: 1) they are not directly applicable
to large datasets, due to the polynomial time complexity
for exact inference (Williams & Rasmussen, 2006); and 2)
analytic moment expressions, when used, are restricted to
squared exponential (SE) kernels (Kuss, 2006) and cannot
be generalized to other kernels in a straightforward way.
A common method for approximating large-scale kernel
machines is through random Fourier features (Rahimi &
Recht, 2007). The key idea is to map the input to a lowdimensional feature space yielding fast linear methods. In
the context of GP regression (GPR), this idea leads to the
sparse spectrum GPR (SSGPR) algorithm (Lázaro-Gredilla
et al., 2010). SSGP has been extended in a number of ways
for, e.g. incremental model learning (Gijsberts & Metta,
2013), and large-scale GPR (Dai et al., 2014; Yan et al.,
2015). However, to the best of our knowledge, prediction
under uncertainty for SSGPs has not been explored. Although there are several alternative approximations to exact
GP inference including approximating the posterior distribution using inducing points, e.g., (Snelson & Ghahramani,
2006; Titsias, 2009; Cheng & Boots, 2016), comparing different GP approximations is not the focus of this paper.

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

1.2. Applications
We consider two key problems that are widely encountered
in robotics and engineering: Bayesian filtering and stochastic model predictive control.
The goal of Bayesian filtering is to infer a hidden system state through the recursive application of Bayes’ rule.
Well-known frameworks for Bayesian filtering include unscented Kalman Filtering (UKF), particle filtering (PF), extended Kalman filtering (EKF), and assumed density filtering (ADF). GP-based Bayesian filtering with SE kernels
has been developed for these frameworks by (Ko & Fox,
2009; Deisenroth et al., 2009). We extend this work with
highly efficient SSGP-based EKF and ADF algorithms.
The goal of stochastic model predictive control (MPC) is
to find finite horizon optimal control at each time instant.
Due to the high computational cost of GP inference and
real-time optimization requirements in MPC, most GPbased control methods (Deisenroth et al., 2015; Pan &
Theodorou, 2014; Kupcsik et al., 2014) are restricted to
episodic reinforcement learning tasks. To cope with this
challenge, we present an SSGP-based MPC algorithm that
is fast enough to perform probabilistic trajectory optimization and model adaptation on-the-fly.
1.3. Our contributions
• We propose two approaches to prediction under uncertainty in SSGPs with closed-form expressions for
the predictive distribution. Compared to previous GP
counterparts, our methods: 1) are more scalable, and
2) can be generalized to any continuous shift-invariant
kernels with a Fourier feature representation.
• We demonstrate successful applications of the proposed approaches by presenting scalable algorithms
for 1) recursive Bayesian filtering and 2) stochastic
model predictive control via probabilistic trajectory
optimization.
The rest of the paper is organized as follows. In §2, we give
an introduction to SSGPs, which serves as our probabilistic model. Derivation and expressions of the two proposed
prediction methods are detailed in §3. Applications to filtering and control, and experimental results are presented
in §4 and §5 respectively. Finally §6 concludes the paper.

2. Sparse Spectral Representation of GPs
Consider the task of learning the function f : Rd → R,
given IID data D = {xi , yi }ni=1 , with each pair related by
y = f (x) + ,

 ∼ N (0, σn2 ),

(2)

where  is IID additive Gaussian noise. Gaussian process regression (GPR) is a principled way of performing
Bayesian inference in function space, assuming that function f has a prior distribution f ∼ GP(m, k), with mean

function m : Rd → R and kernel k : Rd × Rd → R.
Without loss of generality, we assume m(x) = 0. Exact GPR is challenging for large datasets due to its O(n3 )
time and O(n2 ) space complexity (Williams & Rasmussen,
2006), which is a direct consequence of having to store and
invert an n × n Gram matrix.
Random features can be used to form an unbiased approximation of continuous shift-invariant kernel functions, and
are proposed as a general mechanism to accelerate largescale kernel machines (Rahimi & Recht, 2007), via explicitly mapping inputs to low-dimensional feature space.
Based on Bochner’s theorem, the Fourier transform of a
continuous shift-invariant positive definite kernel k(x, x0 )
is a proper probability distribution p(ω), assuming k(x, x0 )
is properly scaled (Rahimi & Recht, 2007):
Z
T
0
k(x, x0 ) = p(ω)ejω (x−x ) dω
(3)
= E(φω (x)φω (x0 )∗ ), ω ∼ p(ω),
T

where φω (x) = ejω x , and we can see that k(x, x0 ) only
depends on the lag vector separating x and x0 : x−x0 . Equation (3) leads to an P
unbiased finite sample approximation
1
φωi (x)φωi (x0 )∗ , where random freof k: k(x, x0 ) ≈ m
m
quencies {ωi }i=1 are drawn IID from p(ω). Utilizing the
fact that φω can be replaced by sinusoidal functions since
both p(ω) and k(x, x0 ) are reals, and concatenating features
{φωi }m
i=1 into a succinct vector form, an approximation for
k(x, x0 ) is expressed as
 c 
φ (x)
0
T
0
k(x, x ) ≈ φ(x) φ(x ), φ(x) = s
,
(4)
φ (x)
φci (x) = σk cos(ωiT x), φsi (x) = σk sin(ωiT x), ωi ∼ p(ω),
where σk is a scaling coefficient. For the commonly
used Squared Exponential (SE) kernel: k(x, x0 ) =
σf2 exp(− 21 kx − x0 k2Λ−1 ), p(ω) = N (0, Λ−1 ) and σk =
σ
√ f , where the coefficient σf and the diagonal matrix Λ are
m
the hyperparameters, examples of kernels and corresponding spectral densities can be found in Table 1.
In accordance with this feature map (4), Sparse Spectrum
GPs are defined as follows
Definition 1. Sparse Spectrum GPs (SSGPs) are GPs with
kernels defined on the finite-dimensional and randomized
feature map φ (4):
k(x, x0 ) = φ(x)T φ(x0 ) + σn2 δ(x − x0 ),

(5)

where the function δ is the Kronecker delta function.
The second term in (5) accounts for the additive zero mean
Gaussian noise in (2), if the goal is to learn the correlation between x and y directly as in our case of learning the
probabilistic model p(y|x), instead of learning the latent
function f .

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

Because of the explicit finite-dimensional feature map (4),
each SSGP is equivalent to a Gaussian distribution over the
weights of features w ∈ R2m . Assuming that prior distribution of weights w is N (0, I) 1 and the feature map is
fixed, after conditioning on the data D = {xi , yi }ni=1 , the
posterior distribution of w is 2
w ∼ N (α, σn2 A−1 ),
α=A

−1

ΦY,

T

A = ΦΦ +

(6)
σn2 I,

which can be derived through Bayesian linear regression. In (6), the column vector Y and the matrix Φ are

T
specified
by the data D: Y = y1 . . . yn , Φ =

φ(x1 ) . . . φ(xn ) . Consequently, the posterior distribution over the output y in (2) at a test point x is exactly
Gaussian, in which the posterior variance explicitly captures the model uncertainty in prediction with input x:
p(y|x) = N (αT φ(x), σn2 + σn2 kφ(x)k2A−1 ).

(7)

This Bayesian linear regression method for SSGP is proposed in Lázaro-Gredilla et al. (2010). Its time complexity
is O(nm2 + m3 ), which is significantly more efficient than
standard GPR’s O(n3 ) when m  n.
Remark It’s worth noting that the methods proposed in
this paper are not tied to specific algorithms for SSGP
regression such as Bayesian linear regression (LázaroGredilla et al., 2010), but able to account for any SSGP
with specified feature weights distribution (6), where posterior α and A can be computed by any means. Variations on
A include sparse approximations by a low rank plus diagonal matrix, or iterative solutions by optimization methods
like doubly stochastic gradient descent (Dai et al., 2014).

3. Prediction under Uncertainty
Two methods for prediction under uncertainty are presented under two conditions: 1) the uncertain input is normally distributed: x ∼ N (µ, Σ), and 2) probabilistic models are in the form of (7) specified by SSGPs. Despite
these conditions, evaluating the integral in (1) is still intractable. In this work, we approximate the true predictive distribution p(y) by a Gaussian distribution with moments that are analytically computed through: 1) exact moment matching, and 2) linearization of posterior mean function. Closed-form expressions for predictive mean, variance, covariance, and input-prediction cross-covariance are
derived. We consider multivariate outputs by utilizing con-

ditionally independent scalar models for each output dimension, i.e., assuming for outputs in different dimension
ya and yb , p(ya , yb |x) = p(ya |x)p(yb |x). Discussions on
this assumption can be found in Appendix §6.1. For notational simplicity, we suppress the dependency of φ(x) on
x, and treat y as a scalar by default.
3.1. Exact moment matching (SSGP-EMM)
We derive the closed-form expressions for exact moments:
1) the predictive mean E y, 2) the predictive variance Var y
and covariance Cov(ya , yb ), which in the multivariate case
correspond to the diagonal and off-diagonal entries of the
predictive covariance matrix, and 3) the cross-covariance
between input and prediction Cov(x, y).
Using the expressions for SSGP (4), (7), and the law of
total expectation, the predictive mean becomes
 c

φ
E y = E E(y|x) = E αT φ = αT E s ,
(8)
φ
E φci = σk E cos(ωiT x),

E φsi = σk E sin(ωiT x),

where i = 1, . . . , m, and in the nested expectation
E E(y|x), the outer expectation is over the input distribution p(x) = N (µ, Σ), and the inner expectation is over the
conditional distribution p(y|x) (7).
By observing (8), we see that the expectation of sinusoids
under the Gaussian distribution is the key to computing the
predictive mean. Thus, we state the following proposition:
Proposition 1. The expectation of sinusoids over multivariate Gaussian distributions: x ∼ N (µ, Σ), x ∈ Rd ,
1
d
i.e., p(x) = (2π)− 2 (det Σ)− 2 exp(− 12 kx − µk2Σ−1 ), can
be computed analytically:
1
E cos(ω T x) = exp(− kωk2Σ ) cos(ω T µ),
2
1
T
E sin(ω x) = exp(− kωk2Σ ) sin(ω T µ).
2
To prove it, we invoke Euler’s formula to transform the lefthand-side to complex domain, apply identities involving
quadratic exponentials, and then convert back to real numbers (see Appendix §3.2 for details). In Proposition 1, the
expectations depend on the mean and variance of the input
Gaussian distribution. Intuitively, after passing a Gaussian
distributed input through a sinusoidal function, the expectation of the output is equal to passing the mean of the input through the sinusoid, and then scaling it by a constant
exp(− 21 kωk2Σ ), which depends on the variance of the input.
Expectations are smaller with larger input variance due to
the periodicity of sinusoids.

1

I is the identity matrix with proper size. The prior covari
ance is identity since E (f (x)f (x)) = E φ(x)T wwT φ(x0 ) =
φ(x)T E(wwT )φ(x0 ), and E (f (x)f (x0 )) = φ(x)T φ(x0 ) (see
§2.2 in Rasmussen & Kuss (2004) for details.)
2
Conditioning on data D is omitted, e.g., in w|D, for simplicity in notation.

The exact moments are then derived using Proposition 1.
By the law of total variance, the predictive variance is
Var y = E Var(y|x) + Var E(y|x)
(9)

= σn2 + σn2 Tr A−1 Ψ + αT Ψα − (E y)2 ,

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

where Ψ is defined as the expectation of the outer product
of feature vectors over input distribution p(x). Specifically,
we compute Ψ by applying the product-to-sum trigonometric identities:
 cc


Ψ
Ψcs
T
E φφ = Ψ =
,
Ψsc Ψss


σk2
E cos(ωi + ωj )T x + E cos(ωi − ωj )T x ,
Ψcc
=
ij
2


σk2
ss
Ψij =
E cos(ωi − ωj )T x − E cos(ωi + ωj )T x ,
2
2


σ
k
E sin(ωi + ωj )T x − E sin(ωi − ωj )T x ,
Ψcs
ij =
2
where Ψcc , Ψss , Ψcs are m × m matrices, and i, j =
1, . . . , m, on whose terms Proposition 1 can be directly applied.
Next, we derive the covariance for different output dimensions for multivariate prediction. These correspond to the
off-diagonal entries of the predictive covariance matrix.
We show that, despite the conditional independence assumption for different outputs given a deterministic input,
outputs become coupled with uncertain inputs. Using the
law of total covariance, the covariance is
Cov(ya , yb ) = Cov (E(ya |x), E(yb |x))
= E (E(ya |x), E(yb |x))−(E ya )(E yb ) (10)
= αaT Ψab αb − (αaT E φa )(αbT E φb ),
where matrix Ψab is the expectation of the outer product
of feature vectors corresponding to different feature maps
φa , φb for outputs ya , yb , computed similarly as in (3.1)
with corresponding random frequencies {ωi }, and the scaling coefficient σk (4). Vectors αa and αb are the corresponding weight vectors for ya and yb (7). Compared to
the expression for the variance of a single output in (9), the
term E (Cov(ya |x) Cov(yb |x)) that is included in the law
of total covariance is neglected due to the assumption of
conditional independence of different outputs (§2), so (10)
does not have the corresponding first two terms in (9).
Finally, we compute the cross-covariance between input
and each output dimension. Invoking the law of total covariance:
Cov(x, y) = Cov(x, E(y|x))
= E (x E(y|x)) − (E x)(E y)

(11)

= Υα − (E y)µ,
where matrix Υ is the expectation of the outer product of
the input x and the feature vector φ(x) over input distribution x ∼ N (µ, Σ):


. . . Υsm ,
E(xφT ) = Υ = Υc1 . . . Υcm Υs1


Υci = σk E cos(ωiT x)x , Υsi = σk E cos(ωiT x)x ,

k(x, x0 )
exp(− 21 kx − x0 k2Λ−1 )
exp(−kx − x0 k1 )

Kernel
Gaussian
Laplacian

21−ν
Γ(ν)

Matérn

rν Kν (r)

p(ω)
N (0, Λ−1 )
Qd
1

i=1 π(1+ωi )

d

h( 2ν
+ 4π 2 kωk22 )ν+ 2
`2

Table 1: Examples of continuous shift-invariant positivedefinite kernels
and their corresponding spectral densities,
√
2νkx−x0 k2
where r =
, Kν is a modified Bessel function,
`
d

and h =

ν
2d π 2 Γ(ν+ d
2 )(2ν)
.
Γ(ν)`2ν

where i = 1, . . . , m. We state the following proposition
to compute each column in Υ consisting of expectations of
the product sinusoidal functions and inputs.
Proposition 2. The expectation of the multiplication of sinusoids and linear functions over multivariate Gaussian
distributions: x ∼ N (µ, Σ), can be computed analytically:


E cos(ω T x)x = E cos(ω T x) µ − (E(sin(ω T x))Σω,



E sin(ω T x)x = E sin(ω T x) µ + E cos(ω T x) Σω,
where the right-hand-side expectations have analytical expressions (Proposition 1).

To prove it, we find an expression for E aT x cos(ω T x) ,
for any a, through the complex domain trick used to
prove Proposition
 1. Next, the result is extended to
E x cos(ω T x) , by setting a to consist of indicator vectors (see Appendix §3.3 for details). Applying Proposition
1 and 2, we complete the derivation of Cov(x, y) in (11).
Remark In summary, SSGP-EMM computes the exact
posterior moments. This is equivalent to expectation propagation (Minka, 2001) by minimizing the Kullback-Leibler
divergence between the true distribution and its Gaussian approximation with respect to the natural parameters.

SSGP-EMM’s computation complexity is O m2 k 2 d2 ,
where m is the number of features, k is the output dimension, and d is the input dimension. The most computationally demanding part is constructing matrices Ψab (10) for
each output pair, where each requires O m2 d2 .
Compared to the multivariate moment-matching approach
for GPs (GP-EMM)
(Girard et al., 2003; Kuss, 2006) with

O n2 k 2 d2 time complexity, SSGP-EMM is more efficient when m  n. Moreover, our approach is applicable to any positive-definite continuous shift-invariant kernel with different spectral densities (see examples in Table
1), while previous approaches like GP-EMM (Kuss, 2006)
are only derived for squared exponential (SE) or polynomial kernels. Next we introduce a more computationally
efficient but less accurate approach that avoids the computation of Ψab ’s.

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

3.2. Linearization (SSGP-Lin)
An alternative approach to computing the exact moments
of the predictive distribution is based on the linearization
of the posterior mean function in (7) at the input mean µ:
T

T

m(x) = α φ(x) ≈ m(µ) + α Dφ(µ)(x − µ),
| {z }

(12)

Method
Time
Applicable
kernels

SSGP-EMM
O(m2 k2 d2 )

SSGP-Lin
O(m2 k + mk2 d)

GP-EMM
O(n2 k2 d2 )

continuous shiftinvariant kernels

continuous shiftinvariant kernels

SE or polynomial kernels

Table 2: Comparison of our proposed methods and GPEMM (Girard et al., 2003; Kuss, 2006) in terms of computational complexity and generalizability.

M

where Dφ(µ) denotes taking the derivative of function φ
at µ. Given the definition of φ in (4), Dφ can be found
by chain rule: Dφci (x) = −σk sin(ωiT x)ωiT , Dφsi (x) =
σk cos(ωiT x)ωiT .
Utilizing the linearized posterior mean function (12), the
predictive moments can be approximated. The predictive
mean approximation is
E y = E E(y|x) ≈ m(µ),
(13)
and the predictive variance approximation is
Var y = E Var(y|x) + Var E(y|x)
≈ Var(y|µ) + Var(αT M x)
=

σn2

+

σn2 kφ(µ)k2A−1

T

(14)
T

+ α M ΣM α.

and the approximate covariance between output dimension
a and b is
Cov(ya , yb ) = Cov (E(ya |x), E(yb |x))

= E αaT Ma (x − µ)(x − µ)T MbT αb (15)
≈ αaT Ma ΣMbT αb ,
where Ma and Mb are defined as M in (12), except that
they correspond to feature maps φa and φb . Notice that the
assumption of conditional independence between different
outputs is invoked here again, cf., (10).
Finally, the cross-covariance between the input and output
can be approximated as
Cov(x, y) = Cov(x, E(y|x))

≈ E (x − µ)(αT M (x − µ))
(16)
= αT M Σ
Unlike SSGP-EMM, which computes exact moments
(§3.1), this linearization-based approach SSGP-Lin computes an approximation of the predictive
moments. In

contrast to SSGP-EMM’s O m2 k 2 d computational complexity, the
 computation time of SSGP-Lin is reduced to
O m2 kd , as a direct consequence of avoiding the construction of Ψ (3.1) in SSGP-EMM (10), which makes
SSGP-Lin more efficient than SSGP-EMM, especially
when the output dimension is high.
Both SSGP-EMM and SSGP-Lin are applicable to a general family of kernels. See Table 2 for a comparison between our methods and GP-EMM (Girard et al., 2003;
Kuss, 2006). In the next section, we compare these approaches in applications of filtering and control.

4. Applications
We focus on the application of the proposed methods to
Bayesian filtering and predictive control. We begin by introducing Gauss-Markov models, which can be expressed
by the following discrete-time nonlinear dynamical system:
xt+1 = f (xt , ut ) + xt ,
yt = g(xt ) +

yt ,

xt ∼ N (0, Σx ),

(17)

yt

(18)

∼ N (0, Σy ),

where xt ∈ Rd is state, ut ∈ Rr is control, yt ∈ Rk is
observation or measurement, xt ∈ Rd is IID process noise,
yt ∈ Rk is IID measurement noise, and subscript t denotes
discrete time index. We call the probabilistic models (17)
and (18) the dynamics and observation models, and the corresponding deterministic functions f and g the dynamics
and observation functions.
We consider scenarios where f and g are unknown
but

n
a dataset D = {(xt , ut ), xt+1 }n−1
,
{x
,
y
}
is
prot t t=1
t=1
vided. The probabilistic models specified by SSGPs can
be learned from the dataset, and then used to model the
dynamics and observation (17) (18). More concretely, the
dynamics model p(xt+1 |xt , ut ) is learned using state trann−1
sition pairs {(xt , ut ), xt+1 }t=1
, and the observation model
p(yt |xt ) is learned separately from state-observation pairs
{xt , yt }nt=1 .
4.1. Bayesian filtering
The task of Bayesian filtering is to infer the posterior distribution of the current state of a dynamical system based
on the current and past noisy observations, i.e., finding
p(xt|t ), where the notation xt|s denotes the random variable xt |y0 , . . . , ys . Due to the Markov property of the process x, i.e., xt |x0 , . . . , xt−1 = xt |xt−1 , in Gauss-Markov
models, p(xt|t ) can be computed recursively through alternating prediction step and correction step.
4.1.1. P REDICTION STEP (xt−1|t−1 → xt|t−1 )
In the prediction step, xt−1|t−1 is propagated through the
dynamics model p(xt |xt−1 , ut−1 ):
Z
p(xt|t−1 ) = p(xt |xt−1 , ut−1 )p(xt−1|t−1 ) dxt−1 ,
which can be viewed as prediction under uncertainty
(1). Suppose that p(xt−1|t−1 ) = N (µ̂t−1|t−1 , Σ̂t−1|t−1 ),
with learned SSGP representation for the dynamics,
Gaussian approximations of the output: p(xt|t−1 ) ≈
N (µ̂t|t−1 , Σ̂t|t−1 ) can be obtained by either SSGP-EMM

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

(§3.1) using (8), (9) and (10), or SSGP-Lin (§3.2) using
(13), (14) and (15).
4.1.2. C ORRECTION STEP (xt|t−1 → xt|t )
The correction step conditions xt|t−1 on the current observation yt using Bayes’ rule:
p(yt |xt|t−1 )p(xt|t−1 )
p(xt|t ) = R
.
(19)
p(yt |xt|t−1 )p(xt|t−1 ) dxt
In the preceding prediction step, we obtain p(xt|t−1 ) ≈
N (µ̂t|t−1 , Σ̂t−1|t−1 ), which serves as a prior on xt in this
correction step. Due to the intractability of the integral in
the denominator, to apply Bayes’ rule we first seek Gaussian approximations for the joint distribution, as in the previous work on Bayesian filtering relying on GPs (Deisenroth et al., 2009;
Ko
& Fox, 2009):

 

Σ̂
Σ̂xy
µ̂t|t−1
xt|t−1
∼N
, t|t−1
,
(20)
µ̂y
yt|t−1
Σ̂Txy
Σ̂y
R
Invoking p(yt|t−1 ) = p(yt |xt|t−1 )p(xt|t−1 ) dxt , the moments µ̂y , Σ̂y , and Σ̂xy in the joint Gaussian approximation
can be computed as the predictive mean, predictive covariance, and input-prediction cross-covariance, for the observation model p(yt |xt ) with input p(xt|t−1 ), using SSGPEMM or SSGP-Lin. Having all terms in (20) determined,
we condition xt|t−1 exactly on current observation yt :
µ̂t|t = µ̂t|t−1 + Σ̂xy Σ̂−1
y (y − µ̂y ),
(21)
Σ̂t|t = Σ̂t|t−1 − Σ̂xy Σ̂−1
y Σ̂xy .
This Gaussian approximation p(xt|t ) ≈ N (µ̂t|t , Σ̂t|t ) is
then used as input to the prediction step. Thus, we have
shown that starting from p(x0 ) = N (µ0 , Σ0 ), by consecutively applying prediction and correction steps presented
above, we recursively obtain state estimates for xt|t−1 and
xt|t . Rather than using a finite sample-based approximation
such as in the GP-UKF (Ko & Fox, 2009), the Gaussian approximations of the full densities p(xt|t ) and p(xt|t−1 ) are
propagated.
Algorithm 1 SSGP-ADF and SSGP-EKF
1: Model learning: collect dataset D, and learn SSGP dy-

namics and observations models (§2.)
2: Initialization: set prior p(x0 ).
3: for t = 1, . . . do
4:
Prediction: compute µ̂t|t−1 and Σ̂t|t−1 . by either

SSGP-EMM (§3.1) or SSGP-Lin (§3.2).
Measurement: make an observation yt .
Correction: compute µ̂t|t and Σ̂t|t according to (21)
by either SSGP-EMM (§3.1) or SSGP-Lin (§3.2).
7: end for
5:
6:

We summarize the resulting filtering algorithm SSGPADF (assumed density filtering) and SSGP-EKF (extended
Kalman filtering), based on SSGP-EMM and SSGP-Lin,
respectively, in Algorithm 1. These are analogs of GP-ADF
(Deisenroth et al., 2009) and GP-EKF (Ko & Fox, 2009).

4.2. Stochastic Model Predictive Control
The stochastic model predictive control (MPC) problem is
to choose a control sequence that minimizes the expected
cost, provided p(xt ):
i+T
X

u?t+1:t+T = argmin E h(xt+T ) +
l(xt+i , ut+i ) ,
ut+1:t+T

i

at each time step, subject to stochastic system dynamics
(17), where function h : Rd → R and l : Rd × Rr → R
are the final and running cost respectively. There are two
main challenges to applying MPC in practice: 1) MPC requires an accurate dynamics model for multi-step prediction, and 2) online optimization is very computationally expensive. For clarity in presentation, we will assume that the
state is fully observable henceforth.
Algorithm 2 MPC via probabilistic trajectory optimization
(1-3: offline optimization, 4-8: online optimization)
1: Model learning: collect dataset D, and learn SSGP dy-

namics model (§2).
2: Initialization: set t = 0, and estimate p(x0 ).
3: Trajectory optimization: perform trajectory optimiza4:
5:
6:
7:

8:

tion in belief space, obtain u?t+1:t+T .
repeat
Policy execution: apply one-step control u?t+1 to the
system and move one step forward, update t = t+1.
Model adaptation: incorporate new data and update
SSGP dynamics model.
Trajectory optimization: perform re-optimization
with the updated model. Initialize with the previously optimized trajectory and obtain new u?t+1:t+T .
until Task terminated

4.2.1. MPC VIA PROBABILISTIC TRAJECTORY
OPTIMIZATION

We address the aforementioned challenges by employing
a combination of prediction under uncertainty and trajectory optimization. More precisely, we use SSGP-EMM or
SSGP-Lin to efficiently obtain approximate Gaussian distribution over trajectory of states and perform trajectory optimization in the resultant Gaussian belief space based on
differential dynamic programming (DDP) (Abbeel et al.,
2007; Tassa et al., 2007). Note that DDP-related methods
require computation of first and second order derivatives of
the dynamics and cost. Our analytic moment expressions
provide a robust and efficient way to compute these derivatives. Details are omitted due to space limit, but they can
be found in Appendix §4.
Within the SSGP framework, we may incrementally update the posterior distribution over the feature weights w
(6) given a new sample without storing or inverting the
matrix A explicitly, Instead we keep track of its upper triangular Cholesky factor A = RT R (Gijsberts & Metta,
2013). Given a new sample, a rank-1 update is applied to

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes
Method
N Lx
RM SE

SSGP-ADF
2.5003
4.6822

SSGP-EKF
3.415467
5.1451

GP-ADF
2.489385
4.6854

GP-EKF
3.396343
5.1012

Table 3: Comparison of our methods with GP-ADF
(Deisenroth et al., 2009) and GP-EKF (Ko & Fox, 2009)
in terms of average N Lx (negative log-likelihood) of the
ground truth states given estimates and RMSE (root-meansquare error). Lower values are better. The results correspond to the filtering task in sec 5.1.1.
the Cholesky factor R, which requires O(m2 ) time. To
cope with time-varying systems and to make the method
more adaptive, we employ a forgetting factor λ ∈ (0, 1),
such that the impact of the previous samples decays exponentially in time (Ljung, 1998).
Our proposed MPC algorithm, summarized in Algorithm
2, is related to several algorithms and differs in both model
and controller learning. First, SSGPs are more robust to
modeling error than Locally Weighted Projection Regression (LWPR) used in iLQG-LD (Mitrovic et al., 2010). See
a numerical comparison in (Gijsberts & Metta, 2013). Second, we efficiently propagate uncertainty in multi-step prediction which is crucial in MPC. In contrast, AGP-iLQR
(Boedecker et al., 2014) drops the input uncertainty and
uses subset of regressors (SoR-GP) which lacks a principled way to select reference points. In addition, PDDP (Pan
& Theodorou, 2014) uses GPs which are computationally
expensive for online optimization. Two deep neural networks are used for modeling in (Yamaguchi & Atkeson,
2016), which make it difficult to perform online incremental learning, as we do here.

5. Experimental Results
5.1. Bayesian filtering
5.1.1. 1D O NE - STEP FILTERING
We consider a synthetic dynamical system with ground25x
truth dynamics f (x) = 12 x + 1+x
2 and observation g(x) =
2
6 sin(2x) with Σx = 1.5 and Σy = 1 in (17,18), in a
similar setting to Deisenroth et al. (2009). We compare the
performance of four filters, SSGP-ADF, SSGP-EKF, GPADF (Deisenroth et al., 2009) and GP-EKF (Ko & Fox,
2009). All models are trained using 800 samples. However, for SSGP models, only 10 random Fourier features
of a SE kernel are used. Figure 1 illustrates the comparison of filtered state distribution of a typical realization.
We evaluate the methods by computing N Lx (the negative
log-likelihood of the ground truth samples in the filtered
distribution) and RMSE (root-mean-square error between
filtered mean and ground truth samples). See Table 3 for a
detailed comparison. Our methods SSGP-ADF and SSGPEKF are able to offer close performance with their full GP
counterparts but with greatly reduced computational cost.
See Appendix §6.2 for further discussions on the comparison between SSGP-ADF and SSGP-EKF.

5.1.2. R ECURSIVE FILTERING
We next consider a state estimation task in high-speed autonomous driving on a dirt track (Figure 2a). The goal is
to recursively estimate the state of an autonomous rallycar
given noisy measurements. The vehicle state consists of
linear velocities (x and y), heading rate, and roll angle, in
body frame. Controls are steering and throttle. Measurements are collected by wheel speed sensors. This filtering
task is challenging because of the complex nonlinear dynamics and the amount of noise in the measurements. We
do not use any prior model of the car, but learn the model
from ground truth estimates of vehicle state generated by
integrating GPS and IMU data via iSAM2 (Kaess et al.,
2012). 50,000 samples are collected from wheel speed sensors and ground truth state estimates from iSAM2 for training. Because of the sample size, it is too computationally
expensive to use GP-based filter such as GP-ADF (Deisenroth et al., 2009). Instead, we use SSGP-ADF to perform
1,200 recursive filtering steps which correspond to 30 seconds of high-speed driving. Filtered distributions using 80
features are shown in Figure 2b, and Figure 2c shows the
mean and twice the standard deviation of N Lx over six 30
seconds driving with different number of features. Surprisingly, only need a small number of features is necessary for
satisfactory results.
5.2. Model Predictive Control
5.2.1. T RACKING A MOVING TARGET
We consider the Puma-560 robotic arm and quadrotor systems with dynamics model specified by SSGPs. For both
tasks the goal is to track a moving target. In addition, the
true system dynamics vary online, which necessitates both
online optimization and model update, as we do here. See
Appendix §5.2 for detailed task descriptions. Results in
terms of cost l(xt , ut ) are shown in Figure 4. Figure 4a
shows that our methods outperform iLQG-LD (Mitrovic
et al., 2010) and AGP-iLQR (Boedecker et al., 2014). The
similarities and differences between these methods have
been discussed in §4.2. Figure 4b shows that model update
is necessary and more features could improve performance.
5.2.2. AUTONOMOUS DRIFTING
We study the control of an autonomous car during extreme
operating conditions (powerslide). The task is to stabilize
the vehicle to a specified steady-state using purely longitudinal control during high-speed cornering. This problem
has been studied in Velenis et al. (2010) where the authors
developed a LQR control scheme based on a physics-based
dynamics model. We apply our MPC algorithm to this task
without any prior model knowledge and 2,500 data points
generated by the model in Velenis et al. (2010). SSGP-Lin
is used for multi-step prediction. Results and comparison
to Velenis et al. (2010) are illustrated in Figure 3.

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes
30

30

30

GP-ADF

30

SSGP-ADF

GP-EKF

SSGP-EKF

20

20

20

20

10

10

10

10

0

0

0

0

-10

-10

-10

-10

-20

-20

-20

-20

-30
-10

-30
-10

-30
-10

-5

0

5

10

(a) GP-ADF (800 data points)

-5

0

5

10

(b) SSGP-ADF (10 features)

-5

0

5

-30
-10

10

(c) GP-EKF (800 data points)

-5

0

5

10

(d) SSGP-EKF (10 features)

Figure 1: Black points are ground truth states, red areas are filter distributions for (a) GP-ADF (Deisenroth et al., 2009),
(c) GP-EKF (Ko & Fox, 2009), our proposed methods (b) SSGP-ADF and (d) SSGP-EKF. The x-axis is the mean of initial
belief p(x0 ), which is randomly distributed in [−10, 10] and y-axis shows the mean and twice the standard deviation of
filtered distribution p(x1 |y1 ) after observing y1 .

(a) Autonomous driving task

(b) Filtered distribution vs. ground truth

(c) N Lx vs. # of features

300

400

500

-30
-40
-50
-60

V

V
200

0

100

200

300

500

400

0

100

200

300

400

500

2
1.5
1
0.5

β
0

100

200

300

500

400

0

100

200

300

400

500

10

V/R

2
1.5
1
0.5

100

β

-30
-40
-50
-60

0

9
8
7
6

V/R

β

9
8
7
6

V/R

V

Figure 2: Recursive filtering task for high-speed autonomous driving. Figure (b) shows trajectories of all the states of a 30
seconds continuous driving (1,200 steps), where blue lines are the ground truth, and red lines and red areas are the mean
and twice the standard deviation of the filtered distributions respectively. In (c), the red line and area are the mean and
twice the standard deviation of N Lx over six 30 seconds driving with varying number of features.

0

100

200

300

500

400

10

Start

5

-30
-40
-50
-60
2
1.5
1
0.5

6. Discussion and Conclusion
0

100

200

300

400

500

0

100

200

300

400

500

100

200

300

400

500

0
10

Start

5

0

9
8
7
6

0

0

-5

-5

-5

-10

-10

-10

-20

-15

-10

-5

0

-20

5

Start

5

-15

-10

-5

0

5

-20

-15

-10

-5

0

5

Figure 3: Comparison of the drifting performance using 50
(left), 150 (middle) and 400 (right) random features. Blue
lines are the solution provided in Velenis et al. (2010). Performance improves with a larger number of features, and
with a moderate number of features, MPC with SSGP-Lin
behaves very closely to the ground truth solution.
10 4

25

10

20

Cost

Cost

10 3

2

With model adaptation (400 feat)
With model adaptation (100 feat)
Without model adaptation (100 feat)
RH-DDP with known model

15
10

10 1

10 0

Our method (SSGP-EMM)
Our method (SSGP-Lin)
iLQG-LD (LWPR)
AGP-iLQR (SoR-GP)
100

200

300

400

5
500

600

700

Time step

(a) Robotic arm task cost

800

100

200

300

400

We introduced two analytic moment-based approaches to
prediction under uncertainty in sparse spectrum Gaussian
processes (SSGPs). Compared to their full GP counterparts, our methods are more general: they are applicable
to any continuous shift-invariant kernel. They also scale
to larger datasets by leveraging random features with frequencies sampled from the spectral density of a given kernel (see Table 1, 2). Although we adopt the name SSGP,
our proposed methods are not tied to specific model learning methods such as linear Bayesian regression (LázaroGredilla et al., 2010). They can be applied to any SSGP
with a specified feature weight distribution (6), and α and
A can be computed via different approaches. For example, A can be iteratively computed by methods like doubly
stochastic gradient descent (Dai et al., 2014). We studied
the application of the proposed methods to Bayesian filtering and model predictive control. Our methods directly
address the challenging aspects of these problems: model
uncertainty and real-time execution constraints. We evaluated our algorithms on real-world and simulated examples
and showed that SSGP-EMM (§3.1) and SSGP-Lin (§3.2)
are accurate alternatives to their full GP counterparts when
learning from large amounts of data.

Time step

(b) Quadrotor task cost

Figure 4: Cost comparison for arm and quadrotor tasks.

Acknowledgements
This work was supported by NSF NRI awards 1637758 and
1426945.

Prediction under Uncertainty in Sparse Spectrum Gaussian Processes

References
Abbeel, P., Coates, A., Quigley, M., and Ng, A. Y. An application
of reinforcement learning to aerobatic helicopter flight. NIPS,
19:1, 2007.
Archambeau, Cedric, Cornford, Dan, Opper, Manfred, and
Shawe-Taylor, John. Gaussian process approximations of
stochastic differential equations. Gaussian Processes in Practice, 1:1–16, 2007.

Kuss, Malte. Gaussian process models for robust regression, classification, and reinforcement learning. PhD thesis, Technische
Universität, 2006.
Lázaro-Gredilla, M., Quiñonero-Candela, J., Rasmussen, C. E.,
and Figueiras-Vidal, A. R. Sparse spectrum Gaussian process
regression. The Journal of Machine Learning Research, 99:
1865–1881, 2010.
Ljung, Lennart. System identification. Springer, 1998.

Boedecker, J., Springenberg, JT., Wulfing, J., and Riedmiller, M.
Approximate real-time optimal control based on sparse Gaussian process models. In ADPRL 2014, pp. 1–8. IEEE, 2014.

Minka, Thomas P. A family of algorithms for approximate
Bayesian inference. PhD thesis, Massachusetts Institute of
Technology, 2001.

Candela, J. Quinonero, Girard, A., Larsen, J., and Rasmussen,
C. E. Propagation of uncertainty in Bayesian kernel modelsapplication to multiple-step ahead forecasting. In IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE, 2003.

Mitrovic, D., Klanke, S., and Vijayakumar, S. Adaptive optimal
feedback control with learned internal dynamics models. In
From Motor Learning to Interaction Learning in Robots, pp.
65–84. Springer, 2010.

Cheng, Ching-An and Boots, Byron. Incremental variational
sparse Gaussian process regression. In Proceedings of Advances in Neural Information Processing Systems 30 (NIPS),
2016.
Dai, Bo, Xie, Bo, He, Niao, Liang, Yingyu, Raj, Anant, Balcan,
Maria-Florina F, and Song, Le. Scalable kernel methods via
doubly stochastic gradients. In Advances in Neural Information Processing Systems, pp. 3041–3049, 2014.
Deisenroth, M., Fox, D., and Rasmussen, C. Gaussian processes
for data-efficient learning in robotics and control. IEEE Transsactions on Pattern Analysis and Machine Intelligence, 27:75–
90, 2015.
Deisenroth, Marc Peter, Huber, Marco F, and Hanebeck, Uwe D.
Analytic moment-based Gaussian process filtering. In Proceedings of the 26th annual international conference on machine learning, pp. 225–232. ACM, 2009.

Pan, Y. and Theodorou, E. Probabilistic differential dynamic programming. In Advances in Neural Information Processing Systems (NIPS), pp. 1907–1915, 2014.
Pan, Yunpeng, Theodorou, Evangelos, and Kontitsis, Michail.
Sample efficient path integral control under uncertainty. In Advances in Neural Information Processing Systems, pp. 2314–
2322, 2015.
Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Advances in neural information processing
systems, pp. 1177–1184, 2007.
Rasmussen, C. and Kuss, M. Gaussian processes in reinforcement
learning. In NIPS, volume 4, pp. 1, 2004.
Snelson, E. and Ghahramani, Z. Sparse Gaussian processes using
pseudo-inputs. NIPS, 18:1257, 2006.
Tassa, Y., Erez, T., and Smart, W. D. Receding horizon differential
dynamic programming. In NIPS, 2007.

Deisenroth, Marc Peter, Turner, Ryan Darby, Huber, Marco F,
Hanebeck, Uwe D, and Rasmussen, Carl Edward. Robust filtering and smoothing with Gaussian processes. IEEE Transactions on Automatic Control, 57(7):1865–1871, 2012.

Titsias, Michalis K. Variational learning of inducing variables in
sparse gaussian processes. In AISTATS, volume 5, pp. 567–
574, 2009.

Gijsberts, A. and Metta, G. Real-time model learning using incremental sparse spectrum Gaussian process regression. Neural
Networks, 41:59–69, 2013.

Velenis, E., Frazzoli, E., and Tsiotras, P. Steady-state cornering
equilibria and stabilisation for a vehicle during extreme operating conditions. International Journal of Vehicle Autonomous
Systems, 8(2-4):217–241, 2010.

Girard, A., Rasmussen, C.E., Quinonero-Candela, J., and MurraySmith, R. Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting. In NIPS,
2003.

Williams, C.K.I and Rasmussen, C.E. Gaussian processes for
machine learning. MIT Press, 2006.

Kaess, Michael, Johannsson, Hordur, Roberts, Richard, Ila,
Viorela, Leonard, John J, and Dellaert, Frank. isam2: Incremental smoothing and mapping using the bayes tree. The International Journal of Robotics Research, 31(2):216–235, 2012.
Ko, J. and Fox, D. Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models. Autonomous
Robots, 27(1):75–90, 2009.
Kupcsik, A., Deisenroth, M.P., Peters, J., Loh, AP, Vadakkepat,
P., and Neumann, G. Model-based contextual policy search for
data-efficient generalization of robot skills. Artificial Intelligence, 2014.

Yamaguchi, Akihiko and Atkeson, Christopher G. Neural networks and differential dynamic programming for reinforcement learning problems. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pp. 5434–5441.
IEEE, 2016.
Yan, Xinyan, Xie, Bo, Song, Le, and Boots, Byron. Large-scale
Gaussian process regression via doubly stochastic gradient descent. The ICML Workshop on Large-Scale Kernel Learning,
2015.

