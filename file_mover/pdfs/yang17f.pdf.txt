A Richer Theory of Convex Constrained Optimization
with Reduced Projections and Improved Rates

Tianbao Yang 1 Qihang Lin 1 Lijun Zhang 2

Abstract
This paper focuses on convex constrained optimization problems, where the solution is subject
to a convex inequality constraint. In particular,
we aim at challenging problems for which both
projection into the constrained domain and a linear optimization under the inequality constraint
are time-consuming, which render both projected
gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced
optimization algorithms for both smooth and
non-smooth optimization with improved convergence rates under a certain regularity condition
of the constraint function. We first present a general theory of optimization with only one projection. Its application to smooth optimization
with only one projection yields O(1/) iteration
complexity, which improves over the O(1/2 )
iteration complexity established before for nonsmooth optimization and can be further reduced
under strong convexity. Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at
the price of a logarithmic number of projections.
In particular, we achieve an iteration complex2(1−θ)
e
ity of O(1/
) for non-smooth optimization
1−θ
e
and O(1/
) for smooth optimization, where
θ ∈ (0, 1] appearing the local error bound condition characterizes the functional local growth
rate around the optimal solutions. Novel applications in solving the constrained `1 minimization
problem and a positive semi-definite constrained
distance metric learning problem demonstrate
that the proposed algorithms achieve significant
speed-up compared with previous algorithms.
1
The University of Iowa, Iowa City, IA 52242, USA 2 National
Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China. Correspondence to: Tianbao Yang
<tianbao-yang@uiowa.edu>.

This is the long version of our paper appearing in the Proceedings
of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).

1. Introduction
In this paper, we aim at solving the following convex constrained optimization problem:
min

x∈Rd

f (x),

s.t. c(x) ≤ 0,

(1)

where f (x) is a smooth or non-smooth convex function and
c(x) is a lower-semicontinuous and convex function. The
problem can find applications in machine learning, signal
processing, statistics, marketing optimization, and etc. For
example, in distance metric learning one needs to learn a
positive semi-definite (PSD) matrix such that similar examples are close to each other and dissimilar examples are
far from each other (Weinberger et al., 2006; Xing et al.,
2003), where the positive semi-definite constraint can be
cast into a convex inequality constraint. Another example
arising in compressive sensing is to minimize the `1 norm
of high-dimensional vector subject to a measurement constraint (Candès & Wakin, 2008). Although general interiorpoint methods can be applied to solve the problem with linear convergence, they suffer from exceedingly high computational cost per-iteration. Another solution is to employ the projected gradient (PG) method (Nesterov, 2004)
or the conditional gradient (CG) method (Frank & Wolfe,
1956), where the PG method needs to compute the projection into the constrained domain at each iteration and
CG needs to solve a linear optimization problem under
the constraint. However, for many constraints (e.g., PSD,
quadratic constraints) both projection into the constrained
domain and the linear optimization under the constraint are
time-consuming, which restrict their capabilities to solving
these problems.
Recently, there emerges a new direction towards addressing the challenge of expensive projection that is to reduce
the number of projections. In the seminal paper (Mahdavi
et al., 2012), the authors have proposed two algorithms with
only one projection at the end of iterations for non-smooth
convex and strongly convex optimization, respectively. The
idea of both algorithms is to move the constraint function
into the objective function and to control the violation of
constraint for intermediate solutions. While their developed algorithms enjoy an optimal convergence rate for nonsmooth optimization (i.e., O(1/2 ) iteration complexity)

Convex Constrained Optimization with Reduced Projections and Improved Rates

and a close-to-optimal convergence rate for strongly con1
e
vex optimization (i.e., O(1/)
), there still lack of theory
and algorithms with reduced projections and faster rates
for smooth convex optimization and for convex optimization without strong convexity assumptions.
In this paper, we make significant contributions by developing a richer theory of convex constrained optimization
with reduced projections and faster rates. To be specific,
• we develop a general framework and theory of optimization with only one projection, where any favorable
smooth or non-smooth convex optimization algorithms
can be employed to solve the intermediate augmented
unconstrained objective function. We discuss in full details the applicability of the proposed algorithms to problems with polyhedral, quadratic or PSD constraints.
• Applying the general theory to smooth convex optimization 2 with Nesterov’s accelerated gradient methods yields an iteration complexity of O(1/) with only
one projection. In addition, when equipped with an optimal algorithm for strongly convex optimization the general theory implies the optimal iteration complexity of
O(1/) for strongly convex optimization with only one
projection. For smooth and strongly convex optimization, the general theory implies an iteration complexity
of O(1/β ) where β ∈ (1/2, 1) with only one projection
and a sufficiently large number of iterations.
• Building on the general framework and theory, we further develop an improved theory with faster convergence
rates for non-strongly convex optimization at the price
of a logarithmic number of projections. In particular, we
show that under a mild local error bound condition, the
2(1−θ)
e
iteration complexities can be reduced to O(1/
)
1−θ
e
for non-smooth optimization and O(1/ ) for smooth
optimization, where θ ∈ (0, 1] is a constant in the local
error bound condition that characterizes the local growth
rate of functional values. To our knowledge, these are the
best convergence results with only a logarithmic number
of projections for non-strongly convex optimization. We
also demonstrate their effectiveness for solving compressive sensing and distance metric learning problems.

2. Related Work
The issue of high projection cost in projected gradient
descent has received increasing attention in recent years.
Most studies are based on the Frank-Wolfe technique that
eschews the projection in favor of a linear optimization
over the constrained domain (Jaggi, 2013; Hazan & Kale,
2012; Lacoste-Julien et al., 2013; Garber & Hazan, 2015).
It happens that for many bounded domains (e.g., bounded
1

e suppresses a logarithmic factor.
where O()
2
where the constraint function is assumed to be smooth.

balls for vectors and matrices, a PSD constraint with a
bounded trace norm) the linear optimization over the constrained domain is much cheaper than projection into the
constrained domain (Jaggi, 2013). However, there still exist many constraints that render both projection into the
constrained domain and linear optimization under the constraint are comparably expensive. Examples include polyhedral constraints, quadratic constraints and a PSD constraint 3 .
To tackle these complex constraints, the idea of optimization with a reduced number of projections was explored in
several studies since (Mahdavi et al., 2012). In a recent paper (Chen et al., 2016), the authors show that for stochastic
strongly convex optimization, the optimal convergence rate
can be achieved using a logarithmic number of projections.
In contrast, the developed theory in this paper implies that
only one projection is sufficient to achieve the optimal convergence rate for strongly convex optimization, and a logarithmic number of projections can be used to accelerate
convergence rates for non-strongly convex optimization.
Cotter et al. (2016) proposed a stochastic algorithm for
solving heavily constrained problems with many constraint
functions by extending the work of (Mahdavi et al., 2012).
Nonetheless, their focus is not to improve the convergence
rates. Zhang et al. (2013) studied the smooth and strongly
convex optimization and they proposed a stochastic algorithm with O(κ log(T )) projections and proved an O(1/T )
convergence rate, where κ is the condition number and T
is the total number of iterations. Nonetheless, if the condition number is high the number of projections could be
very large. In addition, their algorithm utilizes the minibatch to avoid frequent projections in stochastic optimization, which is different from the present paper.
We note that several recent works also exploit different
forms of error bound conditions to improve the convergence (Wang & Lin, 2014; So, 2013; Hou et al., 2013; Zhou
et al., 2015; Yang & Lin, 2016; Xu et al., 2016). Most
notably, the technique used in our work is closely related
to (Yang & Lin, 2016). However, for constrained optimization problems the methods in (Yang & Lin, 2016) still need
to conduct projections at each iteration.
Finally, we comment on the differences between the proposed methods and the classical penalty methods that also
move the constraint into the objective using a penalty function (Bertsekas, 1996). The major differences are that (i)
the classical penalty methods typically require solving each
subproblem exactly while our methods do not require that;
and (ii) the classical penalty methods typically guarantee
asymptotic convergence while our methods have explicit
convergence rates.
3

Indeed, a linear optimization over a PSD constraint is illposed because the PSD domain is unbounded.

Convex Constrained Optimization with Reduced Projections and Improved Rates

3. Preliminaries
Let Ω = {x ∈ Rd : c(x) ≤ 0} denote the constrained domain, Ω∗ denote the optimal solution set and f∗ denote the
optimal objective value. We denote by ∇f (x) the gradient
and by ∂f (x) the subgradient of a smooth or non-smooth
function, respectively. When f (x) is a non-smooth function, we consider the problem as non-smooth constrained
optimization. When both f (x) and c(x) are smooth, we
consider the problem as smooth constrained optimization.
A function f (x) is L-smooth if it has a Lipschitz continuous gradient, i.e., k∇f (x) − ∇f (y)k ≤ Lkx − yk, where
k · k denotes the Euclidean norm. A function f (x) is µstrongly convex if it satisfies f (x) ≥ f (y) + ∂f (y)> (x −
y) + µ2 kx − yk2 .
In the sequel, dist(x, Ω) denotes the distance of x to a set
Ω, i.e., dist(x, Ω) = minu∈Ω kx − uk. Let [s]+ be a hinge
operator that is defined as [s]+ = s if s ≥ 0, and [s]+ = 0
if s < 0.
Throughout the paper, we make the the following assumptions to facilitate the development of our algorithms and
theory.
Assumption 1. For a convex minimization problem (1), we
assume (i) there exists a positive value ρ > 0 such that
min
c(x)=0
v∈∂c(x),v6=0

kvk ≥ ρ,

(2)

or more generally there exists a constant ρ > 0 for any
x ∈ Rd , such that x\ = arg minu∈Rd ,c(u)≤0 ku − xk2
satisfies
kx\ − xk ≤ [c(x)]+ /ρ.
(3)
(ii) there exists a strictly feasible solution such that c(x) <
0; (iii) both f (x) and c(x) are defined everywhere and are
Lipschitz continuous with their Lipschitz constants denoted
by G and Gc , respectively.
We make several remarks about the assumptions. The inequality in (2) is introduced in (Mahdavi et al., 2012),
which is to ensure the distance from the final solution before projection to constrained domain Ω is not too large.
Note that the inequality in (3) is a more general condition
than (2) as seen from the following lemma.
Lemma 1. For any x ∈ Rd , let x\ = arg minc(u)≤0 ku −
xk2 . If (2) holds, then (3) holds.
The above lemma is implicit in the proof of (Mahdavi et al.,
2012). We will provide more discussions about Assumption 1(i) - the key assumption, and exhibit the value of ρ for
a number of commonly seen constraints (e.g., polyhedral,
quadratic and PSD constraints). To make the presentation
more fluent, we postpone these discussions to Section 6.
The strict feasibility assumption (ii) allows us to explore
the KKT condition of the projection problem shown below.

Assumption (iii) imposes mild Lipschitz continuity conditions on both f (x) and c(x).
Traditional projected gradient descent methods need
to solve the following projection at each iteration
ΠΩ [x] = arg minc(u)≤0 ku − xk2 . Conditional gradient methods (a.k.a. the Frank-Wolfe technique) need to
solve the following linear optimization at each iteration
minu∈Rd ,c(u)≤0 u> ∇f (x). For many constraint functions
(see Section 6), solving the projection problem and the linear optimization could be very expensive.

4. A General Theory of Optimization with
only one projection
In this section, we extend the idea of only one projection
proposed in (Mahdavi et al., 2012) to a general theory, and
then present optimization algorithms with only one projection for non-smooth and smooth optimization, respectively.
To tackle the constraint, we introduce a penalty function
hγ (x) parameterized by γ, which obeys the following certificate: there exist constants C ≥ 0 and λ > G/ρ such
that
hγ (x) ≥ λ[c(x)]+ , ∀x
(4)
hγ (x) ≤ Cγ, ∀x such that c(x) ≤ 0.
From the above condition, it is clear that γ ≥ 0. It is notable that the penalty function hγ (x) will also depend on
λ; however it will be set to a constant value, thus the dependence on λ is omitted. We will construct such a penalty
function hγ (x) for non-smooth and smooth optimization in
next two subsections. We propose to optimize the following augmented objective function
min Fγ (x) = f (x) + hγ (x).

x∈Rd

(5)

We can employ any applicable optimization algorithms to
optimize Fγ (x) pretending that there is no constraint, and
bT that is not necessarily feasifinally obtain a solution x
ble. In order to obtain a feasible solution, we perform one
eT = ΠΩ (b
projection to get x
xT ). The following theorem
bT for Fγ (x) to
allows us to convert the convergence of x
eT for f (x).
that of x
Theorem 1. Let A be any iterative optimization algorithm
applied to minx Fγ (x) with T iterations, which starts with
bT as the final solution, such that the folx1 and returns x
bT holds for any x ∈ Rd
lowing convergence of x
Fγ (b
xT ) − Fγ (x) ≤ BT (γ; x, x1 ),

(6)

where BT (γ; x, x1 ) → 0 when T → ∞. Suppose that
Assumption 1 hold, then
f (e
xT ) − f (x∗ ) ≤

λρ
(Cγ + BT (γ; x∗ , x1 )), (7)
λρ − G

eT = ΠΩ [b
where x
xT ] and x∗ is an optimal solution to (1).

Convex Constrained Optimization with Reduced Projections and Improved Rates

Remark: It is worth mentioning that we omit some constant factors in the convergence bound BT (γ; x, x1 ) that
are irrelevant to our discussions. The notation BT (γ; x, x1 )
emphasizes that it is a function of γ and depends on x1
and a target solution x and it will be referred to as BT . In
the next several subsections, we will see that by carefully
choosing the penalty function hγ (x) we are able to provide
nice convergence for smooth and non-smooth optimization
with only one projection. In the above theorem, we assume
the optimization algorithm A is deterministic. However, a
similar result can be easily extended to a stochastic optimization algorithm A.
Proof. First, we consider c(b
xT ) ≤ 0, which implies that
bT = x
eT . Due to the certificate of hγ (x), Fγ (e
x
xT ) ≥
f (e
xT ) and Fγ (x∗ ) ≤ f (x∗ ) + Cγ. Hence f (e
xT ) ≤
Fγ (b
xT ) ≤ Fγ (x∗ ) + BT (γ; x1 , x∗ ) ≤ f (x∗ ) + Cγ +
BT (γ; x1 , x∗ ). Then (7) follows due to λρ/(λρ − G) ≥ 1.
Next, we assume c(b
xT ) > 0. Inequality (6) implies that
f (b
xT ) + λ[c(b
xT )]+ ≤ f (x∗ ) + Cγ + BT (γ; x∗ , x1 ). (8)
eT k.
By Assumption 1(i), we have [c(b
xT )]+ ≥ ρkb
xT − x
Combined with (8) we have
eT k ≤ f (x∗ ) − f (b
λρkb
xT − x
xT ) + Cγ + BT (γ; x∗ , x1 )
eT k + Cγ + BT (γ; x∗ , x1 ),
≤ Gkb
xT − x
where the last inequality follows that fact f (x∗ )−f (b
xT ) ≤
eT k because
f (x∗ ) − f (e
xT ) + f (e
xT ) − f (b
xT ) ≤ Gkb
xT − x
the Lipschitz property and f (x∗ ) ≤ f (e
xT ). Therefore we
have
Cγ + BT (γ; x∗ , x1 , )
eT k ≤
.
kb
xT − x
λρ − G
Finally, we obtain
f (e
xT ) − f (x∗ ) ≤ f (e
xT ) − f (b
xT ) + f (b
xT ) − f (x∗ )
eT k + Cγ + BT (γ; x∗ , x1 )
≤ Gkb
xT − x
λρ
≤
(Cγ + BT (γ; x∗ , x1 )).
λρ − G

the stochastic subgradient method). The update of subgradient descent method is given by the following
xt+1 = xt − ηt ∂F (xt ),

t = 1, . . . , T,

where ηt is an appropriate step size. If f (x) is µ-strongly
convex, the step size can be set as ηt = 1/(µt) and
the final solution P
can be computed by the α-suffix averT
1
bT = αT
aging x
t=(1−α)T +1 xt with α > 0 (Rakhlin
et al., 2012), or by the polynomial decay averaging with
bt = (1 − s+1
x
xt−1 + s+1
s+t )b
s+t xt and s ≥ 1 (Shamir & Zhang,
2013). Both schemes can attain BT = O(1/(µT )) for
the convergence of F (x) when f (x) is µ-strongly convex.
Combining this with Theorem 1, we have the following
convergence result with the proof omitted due to its simplicity.
Corollary 2. Suppose that Assumption 1 holds and f (x) is
µ-strongly convex. Set F (x) = f (x) + λ[c(x)]+ with λ ≥
bT
G/ρ. Let (9) run for T iterations with ηt = 1/(µt). Let x
be computed by α-suffix averaging or the polynomial decay
eT = ΠΩ (b
averaging. Then with only one projection x
xT ),
we achieve
f (e
xT ) − f∗ ≤

λρ (G + λGc )2 O(1)
.
λρ − G
µT

Remark: We note that the O(1/(µT )) is also achieved for
strongly convex optimization in (Zhang et al., 2013; Chen
et al., 2016) but with a logarithmic number of projections.
In contrast, Corollary 2 implies only one projection is sufficient to achieve the optimal convergence for strongly convex optimization.
4.2. Smooth Optimization
For smooth optimization, we consider both f (x) and c(x)
to be smooth 4 . Let the smoothness parameter of f (x) and
c(x) be Lf and Lc , respectively. In order to ensure the
augmented function Fγ (x) to be still a smooth function,
we construct the following penalty function
hγ (x) = γ ln (1 + exp (λc(x)/γ)) .

4.1. Non-smooth Optimization
Since an optimal convergence rate for general non-smooth
optimization with only one projection has been attained
in (Mahdavi et al., 2012), in this subsection we present an
optimal convergence result for strongly convex problems.
For non-smooth optimization, we can choose
h(x) = λ[c(x)]+ ,
and hence γ = 0. We will use deterministic subgradient descent as an example to demonstrate the convergence
for f (x), though many other optimization algorithms designed for non-smooth optimization are applicable (e.g.,

(9)

(10)

The following proposition shows that hγ (x) is a smooth
function and obeys the condition in (4).
Proposition 1. Suppose c(x) is Lc -smooth and Gc Lipschitz continuous. The penalty function in (10) is a
λ2 G2
(λLc + 4γ c )-smooth function and satisfies (i) hγ (x) ≥
λ[c(x)]+ and (ii) hγ (x) ≤ γ ln 2, ∀x such that c(x) ≤ 0.
Then Fγ (x) is a smooth function and its smoothness parameter is given by LF = Lf + λLc +

λ2 G2c
4γ .

Next, we will

4
it can be extended to when f (x) is non-smooth but its proximal mapping can be easily solved.

Convex Constrained Optimization with Reduced Projections and Improved Rates

establish the convergence for f (x) using Nesterov’s optimal accelerated gradient (NAG) methods. The update of
one variant of NAG can be written as follows
xt+1 = yt − ∇Fγ (yt )/LF

(11)

yt+1 = xt+1 + βt+1 (xt+1 − xt ),

where the value of βt can be set to different values depending on whether f (x) is strongly convex or not (see
Corollary 3). Previous work have established the converbT = xT for Fγ (x), in particular BT = O( LTF2 )
gence of x
forsmooth non-strongly
 convex optimization and BT =
q
O LF exp(−T LµF ) for smooth and strongly convex
optimization. By combining these results with Theorem 1
and appropriately setting γ, we can achieve the following
eT for f (x).
convergence of x
Corollary 3. Suppose that Assumption 1 holds,
dist(y0 , Ω∗ ) ≤ D, f (x) is Lf -smooth and c(x) is
Lc -smooth. Set Fγ (x) = f (x) + hγ (x) with λ > G/ρ
and hγ (x) being (10). Let (11) run for T iterations and
eT = ΠΩ (xT ).
x
λG√
cD
• If f (x) is convex, we can set γ = (T +1)
, βt =
2 ln 2
√
2
1+ 1+4τt−1
τt−1 −1
, where τt =
with τ0 = 1, and achieve
τt
" 2
#
√
λρ
λGc D 2 ln 2 (Lf + λLc )D2
f (e
xT )−f∗ ≤
+
λρ − G
T +1
(T + 1)2

• If f (x) is µ-strongly convex, we can set γ =
α ∈ (1/2, 1) and βt =

√
√
L − µ
√ F √ ,
LF + µ

1
T 2α

with

and achieve


1
1
+
,
f (e
xT ) − f∗ ≤ O
T 2α
T 4α

 1
1
Lf +λLc +λ2 G2c /4 2(1−α)
as long as T ≥
(4α ln T ) 1−α .
µ

Remark: The convergence results above indicate an
O(1/) iteration complexity for smooth optimization and
O(1/1/(2α) ) with α ∈ (1/2, 1) for smooth and strongly
convex optimization with only one projection. All omitted
proofs can be found in (Yang et al., 2017).

5. Improved Convergence for Non-strongly
Convex Optimization
In this section, we will develop improved convergence for
non-strongly convex optimization at a price of a logarithmic number of projections by considering an additional
condition on the target problem. To facilitate the presentation, we first introduce some notations. The -sublevel
set S and -level set L of the problem (1) are denoted by
S = {x ∈ Ω : f (x) ≤ f∗ + }, and L = {x ∈ Ω :
f (x) = f∗ + }, respectively. Let x† denote the closest
point in the -sublevel set S to x ∈ Ω, i.e.,
x† = arg min ku − xk2 ,
u∈Ω

s.t.

f (u) ≤ f∗ + .

(12)

Let x∗ denote the closest optimal solution in Ω∗ to x, i.e.,
x∗ = arg minu∈Ω∗ ku − xk2 .
In this section, we will make the following additional assumption about the problem (1).
Assumption 2. For a convex minimization problem (1), we
assume (i) there exist x0 ∈ Ω and 0 ≥ 0 such that f (x0 )−
minx∈Ω f (x) ≤ 0 ; (ii) Ω∗ is a non-empty convex compact
set; (iii) the optimization problem (1) satisfies a local error
bound condition, i.e., there exist θ ∈ (0, 1] and σ > 0 such
that for any x ∈ S we have dist(x, Ω∗ ) ≤ σ(f (x) −
f∗ )θ where Ω∗ denotes the optimal set and f∗ denotes the
optimal value.
Remark: we would like to remark that the new assumption
only imposes mild conditions on the problem. In particular,
Assumption 2 (i) supposes there is a lower bound of the optimal value f∗ , which usually holds in machine learning applications where the objective function if non-negative; Assumption 2 (ii) ensures that S is also bounded (Rockafellar, 1970), therefore the σ in the local error bound is finite,
which can be easily satisfied for a norm regularized or constraint problems; the local error bound condition holds for
a broad family of functions (e.g., semi-algebraic functions
or real subanalytic functions (Jerome Bolte, 2015; Yang &
Lin, 2016)). In Section 7, we will also demonstrate several
applications of the improved algorithms proposed in this
section by establishing the local error bound condition.
Although the local error bound condition is much weaker
than the strong convexity assumption, below we will propose novel algorithms leveraging this condition with faster
convergence and only a logarithmic number of projections.
5.1. Non-smooth Optimization
To establish an improved convergence for non-smooth optimization, we develop a new algorithm shown in Algorithm
1 based on subgradient descent (GD) method, to which
we refer as LoPGD. The algorithm runs for K epochs and
each epoch employs GD for minimizing F (x) = f (x) +
λ[c(x)]+ with a feasible solution xk−1 ∈ Ω as a starting
point and t iterations of updates. At the end of each epoch,
bk is projected into the constrained
the averaged solution x
domain Ω and the solution xk will be used as the starting
point for next epoch. The step size ηk is decreased by half
every epoch starting from a given value η1 . The theorem
below establishes the iteration complexity of LoPGD and
also exhibits the values of K, t and η1 . To simplify notaλρ
and Ḡ = G + λGc .
tions, we let p = λρ−G
Theorem 4. Suppose Assumptions 1 and 2 hold. Let η1 =
2 2 2
p Ḡ
0
, K = dlog2 (0 /)e and t = 4σ
in Algorithm 1,
2(1−θ)
2pḠ2
where θ and σ are constants appearing in the local error
bound condition. Then f (xK ) − f∗ ≤ 2.
Remark: Since the projection is only conducted at the
end of each epoch and the total number of epochs is at

Convex Constrained Optimization with Reduced Projections and Improved Rates

Algorithm 1 LoPGD
1: INPUT: K ∈ N+ , t ∈ N+ , η1
2: Initialization: x0 ∈ Ω, 0
3: for k = 1, 2, . . . , K do
4:
Let xk1 = xk−1
5:
for s = 1, 2, . . . , t − 1 do
6:
Update xks+1 = xks − ηk ∂F (xks )
7:
end for P
bk = ts=1 xks /t
8:
Let x
9:
Let xk = ΠΩ [b
xk ] and ηk+1 = ηk /2
10: end for
Algorithm 2 LoPNAG
1: INPUT: K ∈ N+ , t1 , . . . , tK ∈ N+ , γ1
2: Initialization: x0 ∈ Ω, 0
3: for k = 1, 2, . . . , K do
4:
Let y0k = xk−1
5:
for s = 0, 1, 2, . . . , tk − 1 do
6:
Update xks+1 = ysk − L1k ∇Fγk (xks )
k
7:
Update ys+1
= xks+1 + βs+1 (xks+1 − xks )
8:
end for
bk = xktk , xk = ΠΩ [b
9:
Let x
xk ] and γk+1 = γk /2
10: end for
most K = dlog2 (0 /)e, so the total number of projections is only a logarithmic number K. The iteration com2(1−θ)
e
plexity in Theorem 4 is O(1/
) that improves the
2
standard result of O(1/ ) without strong convexity. With
e
θ = 1/2, we can achieve O(1/)
iteration complexity with
only O(log(1/)) projections.
5.2. Smooth Optimization
Similar to non-smooth optimization, we also develop a new
algorithm based on NAG shown in Algorithm 2, where
Fγ (x) is defined using hγ (x) in (10), Lk = LFγk is the
−1
smoothness parameter of Fγk and βs = τs−1
, s = 1, . . . ,
τs
is a sequence with τs updated as in Corollary 3. We refer
to this algorithm as LoPNAG. The key idea is to use to a
sequence of reducing values for γk instead of using a small
value as in Corollary 3, and solve each augmented unconstrained problem Fγk (x) approximately with one projection. The theorem below exhibits the iteration complexity
of LoPNAG and reveals the values of K, γ1 and t1 , . . . , tK .
To simplify notations, we let L̄ = Lf + λLc .
Theorem 5. Suppose Assumptions 1 and 2 hold
and f (x) is Lf -smooth and c(x) is Lc -smooth.
0
, K = dlog2 (0 /)e and tk =
Let γ1 = 6pln
√2
p
σ
max{λG
p
18
ln 2, 12(Lf + λLc )0 /2k−1 }
c
1−θ
in Algorithm 2, where θ and σ are constants appearing in
the local error bound condition. Then f (xK ) − f∗ ≤ 2.
Remark: It is not difficult to show that the total number
1−θ
e
of iterations is bounded by O(1/
), which improves the
one in Corollary 3 without strong convexity. If f (x) is a

simple non-smooth function whose proximal mapping can
be easily computed (e.g., `1 norm), we can replace step
6 in Algorithm 2 by a proximal mapping to handle f (x),
which gives the same convergence result in Theorem 5. An
example is presented in Section 7 for compressive sensing
with θ = 1/2.

6. Discussion of Assumption 1 (i)
One might note that a key condition for developing the theory with reduced projections is Assumption 1 (i). Although
Mahdavi et al. (2012) has briefly mentioned that the condition can be satisfied for a PSD cone or a Polytope (a
bounded polyhedron), their discussion lacks of details in
particular on the value of ρ in (2) or (3). Below, we discuss
the condition in details about three types of constraints.
Polyhedral constraints. First, we show that when c(x)
is a polyhedral function, i.e., its epigraph is a polyhedron
(not necessarily bounded), the inequality (3) is satisfied. To
this end, we explore the polyhedral error bound (PEB) condition (Gilpin et al., 2012; Yang & Lin, 2016). In particular, if we consider an optimization problem, minx∈Rd h(x),
where the epigraph of h(x) is polyhedron. Let H∗ denote
the optimal set and h∗ denote the optimal value of the problem above. The PEB says that there exists ρ > 0 such that
for any x ∈ Rd
dist(x, H∗ ) ≤ (h(x) − h∗ )/ρ.

(13)

To show that the inequality (3) holds for a polyhedral
function c(·), we can consider the optimization problem
minx∈Rd [c(x)]+ . The optimal set of the above problem
is given by H∗ = {x ∈ Rd : c(x) ≤ 0}. For any x
such that c(x) > 0, let x\ = arg minc(u)≤0 ku − xk2
be the closest point in the optimal set to x. Therefore if c(·) is a polyhedral function so does [c(x)]+ , by
the PEB condition (13) there exists a ρ > 0 such that
kx − x\ k ≤ ([c(x)]+ − minx [c(x)]+ )/ρ = [c(x)]+ /ρ.
Let us consider a concrete example, where the problem has
a set of affine inequalities c>
i x − bi ≤ 0, i = 1, . . . , m.
There are two methods to encode this into a single constraint function c(x) ≤ 0. The first method is to use
c(x) = max1≤i≤m c>
i x − bi , which is a polyhedral function and therefore satisfies (3). The second method is to
use c(x) = k[Cx − b]+ k, where [a]+ = max(0, a) and
C = (c1 , . . . , cm )> . Thus [c(x)]+ = k[Cx − b]+ k. The
inequality (3) is then guaranteed by Hoffman’s bound and
the parameter ρ is given by the minimum non-zero eigenvalue of C > C (Wang & Lin, 2014). Note that the projection onto a polyhedron is a linear constrained quadratic
programming problem, and the linear optimization over a
polyhedron is a linear programming problem. Both have
polynomial time complexity that would be high if m and d
are large (Karmarkar, 1984; Kozlov et al., 1980).

Convex Constrained Optimization with Reduced Projections and Improved Rates

Quadratic constraint. A quadratic constraint can take
the form of kAx − yk2 ≤ τ , where A ∈ Rm×d and
y ∈ Rm . Such a constraint appears in compressive sensing (Candès & Wakin, 2008)5 , where the goal is to reconstruct a sparse high-dimensional vector x from a small
number of noisy measurements y = Ax + ε ∈ Rm with
m  d. The corresponding optimization problem is
minx∈Rd

kxk1 ,

s.t. kAx − yk2 ≤ τ.

(14)

where τ ≥ kεk2 is an upper bound on the magnitude of the
noise. To check the Assumption 1(i), we note that c(x) =
kAx−yk2 −τ and ∇c(x) = A> (Ax−y). Let us consider
that A has a full row rank 6 and denote by v = Ax
√ − y,
then on thep
boundary c(x) = 0 we have kvk = τ and
kA> vk ≥ τ λmin (AA> ), where λmin (AA> ) > 0 is the
minimum eigenvalue of AA> ∈ Rm×m
p . Therefore the
Assumption 1(i) is satisfied with ρ = τ λmin (AA> ). It
is notable that the projection and the linear optimization
under the quadratic constraint require solving a quadratic
programming problem and therefore could be expensive.
PSD constraint. A PSD constraint X  0 for X ∈ Rd×d
can be written as an inequality constraint −λmin (X) ≤ 0,
where λmin (X) denotes the minimum eigen-value of X.
The subgradient of c(X) = −λmin (X) when λmin (X) =
0 is given by Conv{−uu> |kuk = 1, Xu = 0}, i.e., the
convex hull of the outer products of normalized vectors in
the null space of the matrix X. In (Yang et al., 2017), we
show that if the dimension of the null space of X is r with
1 ≤ r ≤ d, the norm of the subgradient of c(X) on the
boundary c(X) = 0 is lower bounded by ρ = √1r ≥ √1d .
Finally, we note that computing a subgradient of [c(X)]+
only needs to compute one eigen-vector corresponding to
the smallest eigen-value. In contrast, both projection and
linear optimization under a PSD constraint could be very
expensive for high-dimensional problems. In particular, the
projection onto a PSD domain needs to conduct a singular
value decomposition. The linear optimization over a PSD
cone is ill-posed due to that PSD cone is not compact (the
solution is either 0 or infinity). One may add an artificial
constraint on the upper bound of the eigen-values. According to (Jaggi, 2013), the time complexity for solving this
linear optimization problem approximately up to an accu2.5
racy level 0 is O(N d1.5 /0 ) with N being the number
of non-zeros in the gradient and 0 decreasing iteratively
required in the Frank-Wolfe method, which could be much
more expensive especially for high-dimensional problems
and in later iterations than computing the first eigen-pairs
at each iteration in our methods.
5

Here we use the square constraint to make it a smooth function so that the proposed algorithms for smooth optimization are
applicable by using proximal gradient mapping to handle the `1
norm.
6
which is reasonable because m  d.

7. Applications
7.1. Compressive Sensing
We first consider a compressive sensing problem in (14).
Becker et al. (2011) proposed an optimization algorithm
based on the Nesterov’s smoothing and the Nesterov’s optimal method for the smoothed problem, known as NESTA.
It needs to perform the projection into the domain kAx −
yk2 ≤ τ at every iteration and has an iteration complexity
of O(1/). In contrast, the presented algorithm with only
one projection in Section 4.2 using Nesterov’s accelerated
proximal gradient method (Beck & Teboulle, 2009) to solve
the unconstrained problem enjoys an iteration complexity
of O(1/). Moreover, we present a theorem below showing
that the problem (14) satisfies the local error bound condition with θ = 1/2, and hence the presented LoPNAG
e √) iteration complexity with only a logaenjoys an O(1/
rithmic number of projections.
Theorem 6. Let f (x) = kxk1 , c(x) = kAx − yk2 − τ ,
Ω∗ denote the optimal set and f∗ be the optimal solution
to (14). Assume that there exists x0 such that kAx0 −yk2 <
τ and 0 6∈ Ω∗ . Then for any  > 0, x ∈ Rd such that
c(x) ≤ 0 and f (x) ≤ f∗ + , there exists 0 < σ < ∞
such that dist(x, Ω∗ ) ≤ σ(f (x) − f∗ )1/2 . Hence, LoPe √) with
NAG can have an iteration complexity of O(1/
only O(log(1/)) projections.
Next, we demonstrate the effectiveness of the LoPNAG for
solving the compressive sensing problem in (14) by comparing with NESTA. We generate a synthetic data for testing. In particular, we generate a random measurement matrix A ∈ Rm×d with m = 1000 and d = 5000. The entries of the matrix A are generated independently with the
uniform distribution over the interval [−1, +1]. The vector
x∗ ∈ Rd is generated with the same distribution at 100 randomly chosen coordinates. The noise ε ∈ Rm is a dense
vector with independent random entries with the uniform
distribution over the interval [−ζ, ζ], where ζ is the noise
magnitude and is set to 0.01. Finally the vector y was obtained as y = Ax∗ + ε.
We use the Matlab package of NESTA 7 . For fair comparison, we also use the projection code in the NESTA package
for conducting projection. To handle the unknown smoothness parameter in the proposed algorithm, we use the backtracking technique (Beck & Teboulle, 2009). The parameter γ is initially set to 0.001 and decreased by half every
5000 iterations after a projection and the target smoothing
parameter in NESTA is set to 10−5 . For the value of λ in
LoPNAG, we tune it from its theoretical value to several
smaller values and choose the one that yields the fastest
convergence. We report the results in Table 7.1, which
include different number of iterations, the corresponding
7

http://statweb.stanford.edu/˜candes/
nesta/

Convex Constrained Optimization with Reduced Projections and Improved Rates
Table 1. LoPNAG vs. NESTA for solving the compressive sensing problem.
Iters - Projs
5000 - 1
10000 - 2
15000 - 3
20000 - 4
25000 - 5

LoPNAG
Rec. Err.
Objective
0.018017 52.042878
0.018038 52.042418
0.018043 52.042358
0.018043 52.042358
0.018043 52.042358

Time (s)
18.04
35.88
53.09
70.24
87.32

Iters - Projs
1000 - 2000
3000 - 6000
5000 - 10000
8000 - 16000
10000 - 20000

NESTA
Rec. Err.
Objective
0.137798 52.703275
0.018669 52.050051
0.018659 52.050046
0.018657 52.050045
0.018657 52.050044

Time (s)
48.49
93.84
245.23
404.72
501.65

Table 2. LoPGD vs. OPGD and PGD for solving the considered distance metric learning problem.
Iters - Projs
1000 - 1
2000 - 2
4000 - 4
6000 - 6
8000 - 8

LoPGD
Objective
0.0953
0.0695
0.0494
0.0428
0.0405

Time (h)
0.22
0.43
0.87
1.33
1.89

Iters - Projs
1000 - 1
2000 - 1
4000 - 1
6000 - 1
8000 - 1

OPGD
Objective
0.1707
0.1583
0.1469
0.1398
0.1343

number of projections, the recovery error of the found solution compared to the underlying true sparse solution, the
objective value (i.e., the `1 norm of the found solution) and
the running time. Note that each iteration of NESTA requires two projections because it maintains two extra sequence of solutions. From the results, we can see that LoPNAG converges significantly faster than NESTA. Even with
only one projection, we are able to obtain a better solution
than that of NESTA after running 10000 iterations.
7.2. High-dimensional Distance Metric Learning
Consider the following distance metric learning problem:
1 X
min
(1 − yij − kxi − xj k2A )2 + τ kAkoff
1 , (15)
A0 2|E|
(i,j)∈E

where E denotes all pairs of training examples, yij = 1
indicates xi , xj belong to the same class and yij = −1
indicates they belong
to different classes, kzk2A = z> Az
P
=
|A
and kAkoff
ij |. We note that such a formu1
i6=j
lation is useful for high dimensional problems due to the
`1 regularizer. A similar formulation with different forms
of loss function has been adopted in literature (Qi et al.,
2009). We consider the square loss because it gives us
faster convergence with a logarithmic number of projections by LoPGD. Due to the presence of the non-smooth
PSD constraint and the `1 regularizer, Nesterov’s accelerated proximal gradient methods can not be applied efficiently to solving (15) and the augmented unconstrained
problem. Nevertheless, we can apply the proposed LoPGD
method for solving the problem with a logarithmic number
of projections. Regarding the constant θ in the local error
bound condition for (15), it still remains an open problem.
Nonetheless, a local error bound condition with θ = 0.5
might be established under certain regularity condition of
the problem (Zhou & So, 2015; Cui et al., 2017). For example, Cui et al. (2017) provided a direct analysis of a local error bound condition with θ = 0.5 for a class of constrained convex symmetric matrix optimization problems

Time (h)
0.20
0.40
0.80
1.22
1.64

Iters - Projs
1000 - 1000
2000 - 2000
4000 - 4000
6000 - 6000
8000 - 8000

PGD
Objective
0.1491
0.1278
0.1072
0.0957
0.0879

Time (h)
7.97
15.46
29.39
43.36
57.43

regularized by nonsmooth spectral functions (including the
indicator function of a PSD constraint). They established
sufficient conditions (Theorem 16) for a local error bound
condition with θ = 0.5 to hold, which reduces to a regularity condition for (15) depending on the optimal solutions of
the problem. A thorough analysis of the regularity condition is much more involved and left as an open problem.
Next, we demonstrate the empirical performance of
LoPGD for solving (15). We use the colon-cancer data
available on libsvm web portal, which has 2000 features
and 62 examples. Fourty examples are used as training
examples to generate 780 pairs to learn the distance metric. The regularization parameter is set to τ = 0.001. We
compare LoPGD, gradient descent method with only one
projection (referred to as OPGD), and standard projected
GD (referred√to PGD). The step size in PGD and OPGD
is set to η0 / t, where t is the iteration index. We use the
same tuned initial step size for all algorithms. The number
of iterations per-epoch in LoPGD is set to 1000. The penalization parameter λ in both OPGD and LoPGD is tuned
and set to 10. In Table 2, we report the objective values, the
#of iterations/projections, and running time across the first
8000 iterations. We can see that LoPGD converges dramatically faster than PGD and also much faster than OPGD.

8. Conclusion
We have developed a general theory of optimization with
only one projection for a family of inequality constrained
convex optimization problems. It yields an improved iteration complexity for smooth optimization compared with
non-smooth optimization. By exploring the local error
bound condition, we further develop new algorithms with a
logarithmic number of projections and achieve better convergence for both smooth and non-smooth optimization
without strong convexity assumption. Applications in compressive sensing and distance metric learning demonstrate
the effectiveness of the proposed improved algorithms.

Convex Constrained Optimization with Reduced Projections and Improved Rates

Acknowledgements
We are grateful to all anonymous reviewers for their helpful comments. T. Yang is partially supported by National
Science Foundation (IIS-1463988, IIS-1545995). L. Zhang
thanks the support from NSFC (61603177) and JiangsuSF
(BK20160658).

References
Beck, Amir and Teboulle, Marc. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM J. Img. Sci., 2:183–202, 2009.
Becker, Stephen, Bobin, Jérôme, and Candès, Emmanuel J.
Nesta: A fast and accurate first-order method for sparse
recovery. SIAM J. Img. Sci., 4:1–39, 2011. ISSN 19364954.
Bertsekas, Dimitri P. Constrained Optimization and Lagrange Multiplier Methods (Optimization and Neural
Computation Series). Athena Scientific, 1 edition, 1996.
ISBN 1886529043.
Candès, Emmanuel J. and Wakin, Michael B. An introduction to compressive sampling. IEEE Signal Processing
Magazine, 25(2):21 –30, 2008.
Chen, Jianhui, Yang, Tianbao, Lin, Qihang, Zhang, Lijun,
and Chang, Yi. Optimal stochastic strongly convex optimization with a logarithmic number of projections. In
Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence (UAI), 2016.
Cotter, Andrew, Gupta, Maya R., and Pfeifer, Jan. A light
touch for heavily constrained SGD. In Proceedings of
the 29th Conference on Learning Theory (COLT), pp.
729–771, 2016.
Cui, Ying, Ding, Chao, and Zhao, Xinyuan. Quadratic
growth conditions for convex matrix optimization problems associated with spectral functions. CoRR, 2017.
Frank, Marguerite and Wolfe, Philip. An algorithm
for quadratic programming. Naval Research Logistics
(NRL), 3:149–154, 1956.

Hazan, Elad and Kale, Satyen. Projection-free online learning. In Proceedings of the International Conference on
Machine Learning (ICML), 2012.
Hou, Ke, Zhou, Zirui, So, Anthony Man-Cho, and Luo,
Zhi-Quan. On the linear convergence of the proximal gradient method for trace norm regularization. In
Advances in Neural Information Processing Systems
(NIPS), pp. 710–718, 2013.
Jaggi, Martin. Revisiting frank-wolfe: Projection-free
sparse convex optimization. In Proceedings of the International Conference on Machine Learning (ICML), pp.
427–435, 2013.
Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet
Bruce Suter. From error bounds to the complexity of
first-order descent methods for convex functions. CoRR,
abs/1510.08234, 2015.
Karmarkar, N. A new polynomial-time algorithm for linear
programming. In Proceedings of the Sixteenth Annual
ACM Symposium on Theory of Computing, pp. 302–311,
1984.
Kozlov, M.K., Tarasov, S.P., and Khachiyan, L.G. Polynomiale Loesbarkeit der konvexen quadratischen Programmierung. Zh. Vychisl. Mat. Mat. Fiz., 20:1319–1323,
1980. ISSN 0044-4669.
Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark, and
Pletscher, Patrick. Block-coordinate frank-wolfe optimization for structural svms. In Proceedings of the International Conference on Machine Learning (ICML), pp.
53–61, 2013.
Mahdavi, M., Yang, T., Jin, R., and Zhu, S. Stochastic
gradient descent with only one projection. In Advances
in Neural Information Processing Systems (NIPS), pp.
503–511, 2012.
Nesterov, Yurii. Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization.
Kluwer Academic Publishers, 2004.

Garber, Dan and Hazan, Elad. Faster rates for the frankwolfe method over strongly-convex sets. In Proceedings
of the 32nd International Conference on Machine Learning (ICML), pp. 541–549, 2015.

Qi, Guo-Jun, Tang, Jinhui, Zha, Zheng-Jun, Chua, TatSeng, and Zhang, Hong-Jiang. An efficient sparse metric
learning in high-dimensional space via l1 -penalized logdeterminant regularization. In Proceedings of the 26th
international conference on Machine learning (ICML),
pp. 106, 2009.

Gilpin, Andrew, Peña, Javier, and Sandholm, Tuomas.
First-order algorithm with log(1/epsilon) convergence
for epsilon-equilibrium in two-person zero-sum games.
Math. Program., 133(1-2):279–298, 2012.

Rakhlin, Alexander, Shamir, Ohad, and Sridharan, Karthik.
Making gradient descent optimal for strongly convex
stochastic optimization. In Proceedings of the 29th international conference on Machine learning (ICML), 2012.

Convex Constrained Optimization with Reduced Projections and Improved Rates

Rockafellar, R.T. Convex Analysis. Princeton mathematical
series. Princeton University Press, 1970.
Shamir, Ohad and Zhang, Tong. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of
the 30th International Conference on Machine Learning
(ICML), pp. 71–79, 2013.
So, Anthony Man-Cho. Non-asymptotic convergence analysis of inexact gradient methods for machine learning
without strong convexity. CoRR, abs/1309.0113, 2013.
Wang, Po-Wei and Lin, Chih-Jen. Iteration complexity of
feasible descent methods for convex optimization. Journal of Machine Learning Research, 15(1):1523–1548,
2014.
Weinberger, Kilian Q., Blitzer, John, and Saul,
Lawrence K.
Distance metric learning for large
margin nearest neighbor classification. In Advances
in Neural Information Processing Systems (NIPS), pp.
1473–1480, 2006.
Xing, E., Ng, A., Jordan, M., and Russell, S. Distance
metric learning with application to clustering with sideinformation. In Advances in Neural Information Processing Systems (NIPS), volume 15, pp. 505–512, 2003.
Xu, Yi, Yan, Yan, Lin, Qihang, and Yang, Tianbao. Homotopy smoothing for non-smooth problems with lower
complexity than o(1/). In Advances in Neural Information Processing Systems (NIPS), pp. 1208–1216, 2016.
Yang, Tianbao and Lin, Qihang. Rsg: Beating subgradient method without smoothness and strong convexity.
CoRR, abs/1512.03107, 2016.
Yang, Tianbao, Lin, Qihang, and Zhang, Lijun. A richer
theory of convex constrained optimization with reduced
projections and improved rates (the long version of icml
paper). CoRR, arXiv:1608.03487, 2017.
Zhang, Lijun, Yang, Tianbao, Jin, Rong, and He, Xiaofei.
O(logt) projections for stochastic optimization of smooth
and strongly convex functions. In Proceedings of the
International Conference on Machine Learning (ICML),
pp. 1121–1129, 2013.
Zhou, Zirui and So, Anthony Man-Cho. A unified approach
to error bounds for structured convex optimization problems. arXiv:1512.03518, 2015.
Zhou, Zirui, Zhang, Qi, and So, Anthony Man-Cho. L1pnorm regularization: Error bounds and convergence rate
analysis of first-order methods. In Proceedings of the
32nd International Conference on Machine Learning,
(ICML), pp. 1501–1510, 2015.

