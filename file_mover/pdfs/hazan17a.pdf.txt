Efficient Regret Minimization in Non-Convex Games

Elad Hazan 1 Karan Singh 1 Cyril Zhang 1

Abstract
We consider regret minimization in repeated
games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We
give gradient-based methods that achieve optimal
regret, which in turn guarantee convergence to
equilibrium in this framework.

1. Introduction
Repeated games with non-convex utility functions serve to
model many natural settings, such as multiplayer games
with risk-averse players and adversarial (e.g. GAN) training. However, standard regret minimization and equilibria
computation with general non-convex losses are computationally hard. This paper studies computationally tractable
notions of regret minimization and equilibria in non-convex
repeated games.
Efficient online learning algorithms are intimately connected to convexity. This connection is natural, since in
a very broad sense1 , convexity captures efficient computation in continuous mathematical optimization.
The recent success of non-convex learning models, notably deep neural networks, motivates the need for efficient non-convex optimization. Since the latter is NP-hard
in general, efficient optimization methods for non-convex
optimization are designed to find local minima of varied
quality. Stochastic gradient-based methods for non-convex
optimization are currently state-of-the-art in training nonconvex machines.
1
Computer Science, Princeton University.
Correspondence to:
Elad Hazan <ehazan@princeton.edu>,
Karan Singh <karans@princeton.edu>,
Cyril Zhang
<cyril.zhang@princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
Though by no means exclusively, PCA and other spectral
methods being a notable exception.

In this paper we investigate the generalization of the nonconvex statistical, or batch, learning model to online learning. The main question we ask is what kind of guarantees can be obtained efficiently in an adversarial non-convex
scenario.
After briefly discussing why standard regret is not a suitable metric of performance, we introduce and motivate local regret, a surrogate for regret to the non-convex world.
We then proceed to give efficient algorithms for nonconvex online learning with optimal guarantees for this new
objective. In analogy with the convex setting, we discuss
the way our framework captures the offline and stochastic
cases. In the final section, we describe a game-theoretic solution concept which is intuitively appealing, and, in contrast to Nash equilibria, efficiently attainable by simple algorithms.
1.1. Related work
The field of online learning is by now rich with a diverse
set of algorithms for extremely general scenarios, see e.g.
(Cesa-Bianchi & Lugosi, 2006). For bounded cost functions over a bounded domain, it is well known that versions
of the multiplicative weights method gives near-optimal regret bounds (Cover, 1991; Vovk, 1990; Arora et al., 2012).
Despite the tremendous generality in terms of prediction,
the multiplicative weights method in its various forms
yields only exponential-time algorithms for these general
scenarios. This is inevitable, since regret minimization implies optimization, and general non-convex optimization is
NP-hard. Convex forms of regret minimization have dominated the learning literature in recent years due to the fact
that they allow for efficient optimization, see e.g. (Hazan,
2016; Shalev-Shwartz, 2011).
Non-convex mathematical optimization algorithms typically find a local optimum. For smooth optimization,
gradient-based methods are known to find a point with gradient of squared norm at most ε in O( 1ε ) iterations (Nesterov, 2004).2 A rate of O( ε12 ) is known for stochastic
2
We note here that we measure the squared norm of the
gradient, since it is more compatible with convex optimization.
The mathematical optimization literature sometimes measures the
norm of the gradient without squaring it.

Efficient Regret Minimization in Non-Convex Games

gradient descent (Ghadimi & Lan, 2013). Further accelerations in terms of the dimension are possible via adaptive
regularization (Duchi et al., 2011).

bounded, local information may provide no information
about the location of a stationary point. This motivates us
to refine our search criteria.

Recently, stochastic second-order methods have been considered, which enable even better guarantees for nonconvex optimization: not only is the gradient at the point
returned small, but the Hessian is also guaranteed to be
close to positive semidefinite (i.e. the objective function
is locally almost-convex), see e.g. (Erdogdu & Montanari,
2015; Carmon et al., 2016; Agarwal et al., 2016a;b).

Consider, for example, the function sketched in Figure 1.
In this construction, defined on the hypercube in Rn , the
unique point with a vanishing gradient is a hidden valley,
and gradients outside this valley are all identical. Clearly,
it is hopeless in an information-theoretic sense to find this
point efficiently: the number of value or gradient evaluations of this function must be exp(Ω(n)) to discover the
valley.

The relationship between regret minimization and learning
in games has been considered in both the machine learning
literature, starting with (Freund & Schapire, 1997), and the
game theory literature by (Hart & Mas-Colell, 2000). Motivated by (Hart & Mas-Colell, 2000), (Blum & Mansour,
2005) study reductions from internal to external regret, and
(Hazan & Kale, 2007) relate the computational efficiency
of these reductions to fixed point computations.

Figure 1. A difficult “needle in a haystack” case for constrained
non-convex optimization. Left: A function with a hidden valley, with small gradients shown in yellow. Right: Regions with
small projected gradient for the same function. For smaller η,
only points near the valley and bottom-left corner have small projected gradient.

2. Setting
We begin by introducing the setting of online non-convex
optimization, which is modeled as a game between a
learner and an adversary. During each iteration t, the
learner is tasked with predicting xt from K ⊆ Rn , a convex decision set. Concurrently, the adversary chooses a loss
function ft : K → R; the learner then observes ft (x) (via
access to a first-order oracle) and suffers a loss of ft (xt ).
This procedure of play is repeated across T rounds.
The performance of the learner is measured through its regret, which is defined as a function of the loss sequence
f1 , . . . , fT and the sequence of online decisions x1 , . . . , xT
made by the learner. We discuss our choice of regret measure at length in Section 2.2.
Throughout this paper, we assume the following standard
regularity conditions:
Assumption 2.1. We assume the following is true for each
loss function ft :

To circumvent such inherently difficult and degenerate
cases, we relax our conditions, and try to find a vanishing projected gradient. In this section, we introduce this
notion formally, and motivate it as a natural quantity of interest to capture the search for local minima in constrained
non-convex optimization.
Definition 2.2 (Projected gradient). Let f : K → R be
a differentiable function on a closed (but not necessarily
bounded) convex set K ⊆ Rn . Let η > 0. We define
∇K,η f : K → Rn , the (K, η)-projected gradient of f , by
def

∇K,η f (x) =

1
(x − ΠK [x − η∇f (x)]) ,
η

where ΠK [·] denotes the orthogonal projection onto K.
(i) ft is bounded: |ft (x)| ≤ M.
(ii) ft is L-Lipschitz: |ft (x) − ft (y)| ≤ Lkx − yk.
(iii) ft is β-smooth (has a β-Lipschitz gradient):
k∇ft (x) − ∇ft (y)k ≤ βkx − yk.
2.1. Projected Gradients and Constrained Non-Convex
Optimization
In constrained non-convex optimization, minimizing the
gradient presents difficult computational challenges. In
general, even when objective functions are smooth and

This can be viewed as a surrogate for the gradient which
ensures that the gradient descent step always lies within K,
by transforming it into a projected gradient descent step.
Indeed, one can verify by definition that
x − η∇K,η (x) = ΠK [x − η∇f (x)] .
In particular, when K = Rn ,
∇K,η f (x) =

1
(x − x + η∇f (x)) = ∇f (x),
η

and we retrieve the usual gradient at all x.

Efficient Regret Minimization in Non-Convex Games

We first note that there always exists a point with vanishing
projected gradient.
Proposition 2.3. Let K be a compact convex set, and suppose f : K → R satisfies Assumption 2.1. Then, there
exists some point x∗ ∈ K for which
∇K,η f (x∗ ) = 0.
Proof. Consider the map g : K → K, defined by
def

g(x) = x − η∇K,η f (x) = ΠK [x − η∇f (x)] .
This is a composition of continuous functions (noting that
the smoothness assumption implies that ∇f is continuous),
and is therefore continuous. Thus g satisfies the conditions
for Brouwer’s fixed point theorem, implying that there exists some x∗ ∈ K for which g(x∗ ) = x∗ . At this point, the
projected gradient vanishes.
In the limit where ηk∇f (x)k is infinitesimally small, the
projected gradient is equal to the gradient in the interior of
K; on the boundary of K, it is the gradient with its outwardfacing component removed. This exactly captures the firstorder condition for a local minimum.
The final property that we note here is that an approximate
local minimum, as measured by a small projected gradient,
is robust with respect to small perturbations.
Proposition 2.4. Let x be any point in K ⊆ Rn , and let f, g
be differentiable functions K → R. Then, for any η > 0,
k∇K,η [f + g](x)k ≤ k∇K,η f (x)k + k∇g(x)k.
Proof. Let u = x + η∇f (x), and v = u + η∇g(x). Define
their respective projections u0 = ΠK [u] , v 0 = ΠK [v], so
that u0 = x − η∇K,η f (x) and v 0 = x − η∇K,η [f + g](x).
We first show that ku0 − v 0 k ≤ ku − vk.
By the generalized Pythagorean theorem for convex sets,
we have both hu0 − v 0 , v − v 0 i ≤ 0 and hv 0 − u0 , u − u0 i ≤
0. Summing these, we get
hu0 − v 0 , u0 − v 0 − (u − v)i ≤ 0
0

0 2

0

0

=⇒ ku − v k ≤ hu − v , u − vi
≤ ku0 − v 0 k · ku − vk,

In particular, this fact
k∇K,η f (x)k ≤ k∇f (x)k.

≤ k∇K,η [f + g](x) − ∇K,η f (x)k
1
= ku0 − v 0 k
η
1
≤ ku − vk = k∇g(x)k,
η
as required.

implies

that

As we demonstrate later, looking for a small projected gradient becomes a feasible task. In Figure 1 above, such a
point exists on the boundary of K, even when there is no
“hidden valley” at all.
2.2. A Local Regret Measure
In the well-established framework of online convex optimization, numerous algorithms can efficiently achieve optimal regret, in the sense of converging in terms of average
loss towards the best fixed decision in hindsight. That is,
for any u ∈ K, one can play iterates x1 , . . . , xT such that
T
1X
[ft (xt ) − ft (u)] = o(1).
T i=1

Unfortunately, even in the offline case, it is too ambitious
to converge towards a global minimizer in hindsight. In
the existing literature, it is usual to state convergence guarantees towards an ε-approximate stationary point – that is,
there exists some iterate xt for which k∇f (xt )k2 ≤ ε. As
discussed in the previous section, the projected gradient is
a natural analogue for the constrained case.
In light of the computational intractability of direct analogues of convex regret, we introduce local regret, a new
notion of regret which quantifies the objective of predicting points with small gradients on average. The remainder
of this paper discusses the motivating roles of this quantity.
Throughout this paper, for convenience, we will use the
following notation to denote the sliding-window time average of functions f , parametrized by some window size
1 ≤ w ≤ T:
def

Ft,w (x) =

w−1
1 X
ft−i (x).
w i=0

For simplicity of notation, we define ft (x) to be identically
zero for all t ≤ 0. We define local regret below:
Definition 2.5 (Local regret). Fix some η > 0. Define the
w-local regret of an online algorithm as

as claimed. Finally, by the triangle inequality, we have
k∇K,η [f + g](x)k − k∇K,η f (x)k

immediately

def

Rw (T ) =

T
X

k∇K,η Ft,w (xt )k2 ,

t=1

When the window size w is understood by context, we omit
the parameter, writing simply local regret as well as Ft (x).
We turn to the first motivating perspective on local regret.
When an algorithm incurs local regret sublinear in T , a randomly selected iterate has a small time-averaged gradient
in expectation:

Efficient Regret Minimization in Non-Convex Games

Proposition 2.6. Let x1 , . . . , xT be the iterates produced
by an algorithm for online non-convex optimization which
incurs a local regret of Rw (T ). Then,

 Rw (T )
.
Et∼Unif([T ]) k∇K,η Ft,w (xt )k2 ≤
T
This generalizes typical convergence results for the gradient in offline non-convex optimization; we discuss concrete
reductions in Section 4.
2.3. Why Smoothing is Necessary
In this section, we show that for any online algorithm, an
adversarial sequence of loss functions can force the local
regret incurred to scale with T as Ω wT2 . This demonstrates the need for a time-smoothed performance measure
in our setting, and justifies our choice of larger values of
the window size w in the sections that follow.
Theorem 2.7. Define K = [−1, 1]. For any T ≥ 1,
1 ≤ w ≤ T , and η ≤ 1, there exists a distribution D on
0-smooth, 1-bounded cost functions f1 , . . . , fT on K such
that for any online algorithm, when run on this sequence of
functions,
 
1
T
ED [Rw (T )] ≥
.
4w 2w
Proof. We begin by partitioning the T rounds of play into
T
b 2w
c repeated segments, each of length 2w.
For the first half of the first segment (t = 1, . . . , w), the
adversary declares that

ft (x), a bound on local regret truly captures a guarantee of
playing points with small gradients.

3. An Efficient Non-Convex Regret
Minimization Algorithm
Our approach, as given in Algorithm 1, is to play followthe-leader iterates, approximated to a suitable tolerance
using projected gradient descent. We show that this
method efficiently
achieves an optimal local regret bound

of O wT2 , taking O (T w) iterations.
Algorithm 1 Time-smoothed online gradient descent
1: Input: window size w ≥ 1, learning rate 0 < η <
n

tolerance δ > 0, a convex body K ⊆ R .

2: Set x1 ∈ K arbitrarily.
3: for t = 1, . . . , T do
4:
Predict xt . Observe the cost function ft : K → R.
5:
Initialize xt+1 := xt .
6:
while k∇K,η Ft,w (xt+1 )k > δ/w do
7:
Update xt+1 := xt+1 − η∇K,η Ft,w (xt+1 ).
8:
end while
9: end for

Theorem 3.1. Let f1 , . . . , fT be the sequence of loss functions presented to Algorithm 1, satisfying Assumption 2.1.
Then:
(i) The w-local regret incurred satisfies
2

• For odd t, select ft (x) i.i.d. as follows:
(
−x, with probability
ft (x) :=
x,
with probability

Rw (T ) ≤ (δ + 2L)
1
2
1
2

• For even t, ft (x) := −ft−1 (x).
During the second half (t = w + 1, . . . , 2w), the adverT
sary sets all ft (x) = 0. This construction is repeated b 2w
c
times, padding the final T mod 2w costs arbitrarily with
ft (x) = 0.
By this construction, at each round t at which ft (x) is
drawn randomly, we have Ft,w (x) = ft (x)/w. Furthermore, for any xt played by the algorithm,
|ft (xt )| = 1 with

probability at least 21 . so that E k∇K,η Ft,w (xt )k2 ≥ 2w1 2 .
The claim now follows from the fact that there
 T are at least
w
of
these
rounds
per
segment,
and
exactly
2
2w segments
in total.
We further note that the notion of time-smoothing captures non-convex online optimization under limited concept drift: in online learning problems where Ft,w (x) ≈

β
2,

T
.
w2

(ii) The total number of gradient steps τ taken by Algorithm 1 satisfies
τ≤

M

δ2 η −

βη 2
2


 · 2T w + w2 .

Proof of (i). We note that Algorithm 1 will only play an
iterate xt if k∇K,η Ft−1,w k ≤ δ/w. (Note that at t = 1,
Ft−1,w is zero.) Let ht (x) = w1 (ft (x) − ft−w (x)), which
is 2L
w -Lipschitz. Then, for each 1 ≤ t ≤ T we have a
bound on each cost
k∇K,η Ft,w (xt )k2 = k∇K,η [Ft,w−1 + ht (x)] (xt )k2
2

≤ (k∇K,η Ft,w−1 k + k∇ht (xt )k)

2
δ
2L
(δ + 2L)2
,
≤
+
=
w
w
w2

where the first inequality follows from Proposition 2.4.
Summing over all t gives the desired result.

Efficient Regret Minimization in Non-Convex Games

Proof of (ii). First, we require an additional property of the
projected gradient.
n

Lemma 3.2. Let K ∈ R be a closed convex set, and let
η > 0. Suppose f : K → R is differentiable. Then, for any
x ∈ R,
h∇f (x), ∇K,η f (x)i ≥ k∇K,η f (x)k2 .
Proof. Let u = x − η∇f (x) and u0 = ΠK [u]. Then,

Using Lemma 3.3, we have

FT (xT ) ≤

0

= hu − u , u − xi ≥ 0,

βη 2
2



w2

T
δ2 X
·
τt ,
t=1

whence
τ=

T
X

τt ≤

t=1

h∇f (x), ∇K,η f (x)i k∇K,η f (x)k2
−
η2
η2
= hu − x, u0 − xi − hu0 − x, u0 − xi
0

2M T
−
w

η−

≤

w2

δ2 η −


βη 2
2

M


δ2 η −

βη 2

·


2M T
− FT (xT )
w


 · 2T w + w2 ,

2

as claimed.

where the last inequality follows by the generalized
Pythagorean theorem.
For 2 ≤ t ≤ T , let τt be the number of gradient steps taken
in the outer loop at iteration t − 1, in order to compute the
iterate xt . For convenience, define τ1 = 0. We establish a
progress lemma during each gradient descent epoch:

Setting η = 1/β and δ = L gives the asymptotically optimal local regret bound, with O(T w) time-averaged gradient steps (and thus O(T w2 ) individual gradient oracle
calls). We further note that in the case where K = Rn ,
one can replace the gradient descent subroutine (the inner
loop) with non-convex SVRG (Allen-Zhu & Hazan, 2016),
achieving a complexity of O(T w5/3 ) gradient oracle calls.

Lemma 3.3. For any 2 ≤ t ≤ T ,


βη 2 δ 2
Ft−1 (xt ) − Ft−1 (xt−1 ) ≤ −τt η −
.
2
w2
Proof. Consider a single iterate z of the inner loop, and
the next iterate z 0 := z − η∇K,η Ft−1 (z). We have, by
β-smoothness of Ft−1 ,
Ft−1 (z 0 ) − Ft−1 (z) ≤ h∇Ft−1 (z), z 0 − zi +
= −η h∇Ft−1 (z), ∇K,η Ft−1 (z)i +

β 0
kz − zk2
2

βη 2
k∇K,η Ft−1 (z)k2 .
2



Ft−1 (z 0 ) − Ft−1 (z) ≤ − η −

βη
2


2

k∇K,η Ft−1 (z)k2 .

The algorithm only takes projected gradient steps when
k∇K,η Ft−1 (z)k ≥ δ/w. Summing across all τt consecutive iterations in the epoch yields the claim.
To complete the proof of the theorem, we write the telescopic sum (understanding F0 (x0 ) = 0):
T
X

=

T
X
t=2

Corollary 4.1. Let f : K → R satisfy Assumption 2.1.
When online algorithm A is run on a sequence of T identical loss functions f (x), it holds that for any 1 ≤ w < T ,
Et∼D[w,T ] k∇K,η f (xt )k2 ≤

Ft−1 (xt ) − Ft−1 (xt−1 ) + ft (xt ) − ft−w (xt )

t=1

≤

For offline optimization on a fixed non-convex function
f : K → R, we demonstrate that a bound on local regret translates to convergence. In particular, using Algorithm 1 one finds apoint x ∈ K with k∇K,η f (x)k2 ≤ ε
while making O 1ε calls to the gradient oracle, matching
the best known result for the convergence of gradient-based
methods.

Ft (xt ) − Ft−1 (xt−1 )

t=1
T
X

In this section, we discuss the ways in which our online
framework generalizes the offline and stochastic versions
of non-convex optimization – that any algorithm achieving
a small value of Rw (T ) efficiently finds a point with small
gradient in these settings. For convenience, for 1 ≤ t ≤
t0 ≤ T , we denote by D[t,t0 ] the uniform distribution on
time steps t through t0 inclusive.
4.1. Offline non-convex optimization

Thus, by Lemma 3.2,

FT (xT ) =

4. Implications for Offline and Stochastic
Non-Convex Optimization

[Ft−1 (xt ) − Ft−1 (xt−1 )] +

2M T
.
w

Rw (A)
.
T −w

In particular, Algorithm 1, with parameter
choices T =
q
1
2
2w, η = β , δ = L, and w = (δ + 2L) ε , yields
Et∼Dw,T k∇K,η f (xt )k2 ≤ ε.

Efficient Regret Minimization in Non-Convex Games

Furthermore, the algorithm makes O
dient oracle in total.

1
ε



calls to the gra-

When an online algorithm incurs small local regret in expectation, it has a convergence guarantee in offline stochastic non-convex optimization:

Proof. Since ft (x) = f (x) for all t, it follows that
Ft,w (x) = f (x) for all t ≥ w. As a consequence, we
have

Proposition 4.3. Let 1 ≤ w < T . Suppose that online algorithm A is run on a sequence of T identical
loss functions f (x) satisfying Assumption 2.1, with identical stochastic gradient oracles satisfying Assumption 4.2.
Sample t ∼ D[w,T ] . Then, over the randomness of t and
the oracles,

Et∼Dw,t k∇K,η f (xt )k2 ≤
≤

T
1 X
k∇K,η f (xt )k2
T − w t=1

Rw (A)
.
T −w

With the stated choice of parameters, Theorem 3.1 guarantees that
Et∼Dw,t k∇K,η f (xt )k2 ≤

T
ε
·
= ε.
2 T −w

Also, since the loss functions are identical, the execution
of line 7 of Algorithm 1 requires exactly one call to the
gradient oracle at each iteration. This entails that the total
number of gradient oracle calls made in the execution is
O(T w + w2 ) = O( 1ε ).
4.2. Stochastic non-convex optimization
We examine the way in which our online framework captures stochastic non-convex optimization of a fixed function f : Rn → R, in which an algorithm has access to a
f (x). We note that the
noisy stochastic gradient oracle ∇f
reduction here will only apply in the unconstrained case;
it becomes challenging to reason about the projected gradient under noisy information. From a local regret bound,
werecover
a stochastic algorithm with oracle complexity

σ4
O ε2 . We note that this black-box reduction recovers an
optimal convergence rate in terms of ε, but not σ 2 .
In the setting, the algorithm must operate on the noisy estimates of the gradient as the feedback. In particular, for
any ft that the adversary chooses, the learning algorithm
is supplied with a stochastic gradient oracle for ft . The
discussion in the preceding sections may be viewed as a
special case of this setting with σ = 0. We list the assumptions we make on the stochastic gradient oracle, which are
standard:
Assumption 4.2. We assume that each call to the stochasf (x)
tic gradient oracle yields an i.i.d. random vector ∇f
with the following properties:
h
i
f (x) = ∇f (x).
(i) Unbiased: E ∇f
h
i
f (x) − ∇f (x)k2 ≤ σ 2 .
(ii) Bounded variance: E k∇f


 E [Rw (A)]
.
E k∇f (xt )k2 ≤
T −w
Proof. Observe that


Et∼D[w,T ] k∇f (xt )k2 ≤

PT

k∇f (xt )k2
Rw (A)
≤
.
T −w
T −w

t=1

The claim follows by taking the expectation of both sides,
over the randomness of the oracles.
For a concrete online-to-stochastic reduction, we consider
Algorithm 2, which exhibits such a bound on expected local regret.
Algorithm 2 Time-smoothed online gradient descent with
stochastic gradient oracles
1: Input: learning rate η > 0, window size w ≥ 1.
2: Set x1 = 0 ∈ Rn arbitrarily.
3: for t = 1, . . . , T do
4:
Predict xt . Observe the cost function ft : Rn → R.
Pw−1 f
5:
Update xt+1 := xt − wη i=0 ∇f
t−i (xt ).
6: end for
Theorem 4.4. Let f1 , . . . , ft satisfy Assumption 2.1. Then,
Algorithm 2, with access to stochastic gradient oracles
f (xt )} satisfying Assumption 4.2, and a choice of η =
{∇f
t
1
,
guarantees
β
E [Rw (T )] ≤ 8βM + σ 2

T
.
w

Furthermore, Algorithm 2 makes a total of O(T w) calls to
the stochastic gradient oracles.
Using this expected local regret bound in Proposition 4.3,
we obtain the reduction claimed at the beginning of the section:
Corollary 4.5. Algorithm 2, with parameter choices w =
12M β+2σ 2
, T = 2w, and η = β1 , yields
ε


E k∇f (xt )k2 ≤ ε.
 4
Furthermore, the algorithm makes O σε2 stochastic gradient oracle calls in total.

Efficient Regret Minimization in Non-Convex Games

5. An Efficient Algorithm with Second-Order
Guarantees
We note that by modifying Algorithm 1 to exploit secondorder information, our online algorithm can be improved to
play approximate first-order critical points which are also
locally almost convex. This entails replacing the gradient
descent epochs with a cubic-regularized Newton method
(Nesterov & Polyak, 2006; Agarwal et al., 2016a).
In this setting, we assume that we have access to each ft
through a value, gradient, and Hessian oracle. That is,
once we have observed ft , we can obtain ft (x), ∇ft (x),
and ∇2 ft (x) for any x. Let MinEig(A) be the minimum
(eigenvalue, eigenvector) pair for matrix A. As is standard
for offline second-order algorithms, we must add the following additional smoothness restriction:
Assumption 5.1. ft is twice differentiable and has an L2 Lipschitz Hessian:
k∇2 f (x) − ∇2 f (y)k ≤ L2 kx − yk.
Additionally, we consider only the unconstrained case
where K = Rn ; the second-order optimality condition is
irrelevant when the gradient does not vanish at the boundary of K.
The second-order Algorithm 3 uses the same approach as
in Algorithm 1, but terminates each epoch under a stronger
approximate-optimality condition. We define


4β
Φt (x) := max k∇Ft (x)k2 , − 2 · λmin (∇2 Ft (x))3 ,
3L2
PT
so that the quantity
t=1 Φt (xt ) is termwise lower
bounded by the costs in Rw (T ), but penalizes local concavity.
We characterize the convergence and oracle complexity
properties of this algorithm:
Theorem 5.2. Let f1 , . . . , fT be the sequence of loss functions presented to Algorithm 3, satisfying Assumptions 2.1
and 5.1. Choose δ = β. Then, for some constants C1 , C2
in terms of M, L, β, L2 :
(i) The iterates {xt } produced by Algorithm 3 satisfy
T
X

Φt (xt ) ≤ C1 ·

t=1

T
.
w2

(ii) The total number of iterations τ of the inner loop taken
by Algorithm 3 satisfies
τ ≤ C2 · T w 2 .

Algorithm 3 Time-smoothed online Newton method
1: Input: window size w ≥ 1, tolerance δ > 0.
2: Set x1 ∈ K arbitrarily.
3: for t = 1, . . . , T do
4:
Predict xt . Observe the cost function ft : Rn → R.
5:
Initialize xt+1 := xt .
6:
while Φt (xt+1 ) > δ 3 /w3 do
7:
Update xt+1 := xt+1 − β1 ∇Ft,w (xt+1 ).

8:
Let (λ, v) := MinEig ∇2 Ft,w (xt+1 ) .
9:
if λ < 0 then
10:
Flip the sign of v so that hv, ∇Ft,w (xt+1 )i ≤ 0.
2λ
v.
11:
Compute yt+1 := xt + L
2
12:
if Ft,w (yt+1 ) < Ft,w (xt+1 ) then
13:
Set xt+1 := yt+1 .
14:
end if
15:
end if
16:
end while
17: end for
Proof of (i). For each 1 ≤ t ≤ T , we have
Φt−1 (xt ) ≤

δ3
.
w3

Let ht (x) := w1 (ft (x) − ft−w (x)). Then, since ht (x) is
2β
2L
w -Lipschitz and w -smooth,
n
Φt (xt ) = max k∇Ft−1 (xt ) + ∇ht (xt )k2 ,
o
4β
2
2
3
·
λ
(∇
F
(x
)
+
∇
h
(x
))
−
min
t t
t t
3L22
(


3 )
2
δ 3/2
2L
δ
2β
4β
≤ max
+
+
,
·
,
w
3L22
w
w
w3/2
which is bounded by C1 /w2 , for some C1 (M, L, β, L22 ).
The claim follows by summing this inequality across all
1 ≤ t ≤ T.
Proof of (ii). We first show the following progress lemma:
Lemma 5.3. Let z, z 0 be two consecutive iterates of the
inner loop in Algorithm 3 during round t. Then,
Ft (z 0 ) − Ft (z) ≤ −

Φt (z)
.
2β

Proof. Let u denote the step z 0 − z. Let g := ∇Ft (z),
H := ∇2 Ft (z), and (λ, v) := MinEig(H).
Suppose that at time t, the algorithm takes a gradient step,
so that u = g/β. Then, by second-order smoothness of Ft ,
we have
Ft (z 0 ) − Ft (z) ≤ hg, ui +

β
1
kuk2 = − kgk2 .
2
2β

Efficient Regret Minimization in Non-Convex Games

Supposing instead that the algorithm takes a second-order
2λ
v (whichever sign makes hg, ui ≤ 0),
step, so that u = ± L
2
the third-order smoothness of Ft implies
1
L2
Ft (z 0 ) − Ft (z) ≤ hg, ui + uT Ht u +
kuk3
2
6
L2
λ
kuk3
= hg, ui + kuk2 +
2
6
1 4βλ3
2λ3
=
·
.
≤
2
3L2
2β 3L22
The lemma follows due to the fact that the algorithm takes
the step that gives a smaller value of Ft (z 0 ).
The rest of the proof follows the same structure as that for
part (ii) of Theorem 3.1: summing Lemma 5.3 implies a
statement analogous to Lemma 3.3, which we telescope
over all epochs. For sake of completeness, we give the
proof in the appendix.

6. A Solution Concept for Non-Convex Games
Finally, we discuss an application of our regret minimization framework to learning in k-player T -round iterated
games with smooth, non-convex payoff functions. Suppose
that each player i ∈ [k] has a fixed decision set Ki ⊂ Rn ,
and a fixed payoff function fi : K → R satisfies Assumption 2.1 as before. Here, K denotes the Cartesian product
of the decision sets Ki : each payoff function is defined in
terms of the choices made by every player.
In such a game, it is natural to consider the setting where
players will only consider small local deviations from their
strategies. This is a natural setting, which models risk aversion. This setting lends itself to the notion of a local equilibrium, to replace the stronger condition of Nash equilibrium: a joint strategy in which no player encounters a large
gradient on her utility. However, finding an approximate
local equilibrium in this sense remains computationally intractable when the utility functions are non-convex.
Using the idea time-smoothing, we formulate a tractable relaxed notion of local equilibrium, defined over some time
window w. Intuitively, this definition captures a state of
an iterated game in which each player examines the past
w actions played, and no player can make small deviations
to improve the average performance of her play against her
opponents’ historical play. We formulate this solution concept as follows:
Definition 6.1 (Smoothed local equilibrium). Fix some

	k
η > 0, w ≥ 1. Let fi (x1 , . . . , xk ) : K → R i=1 be
the payoff functions for a k-player iterated game. A
joint strategy (x1t , . . . , xkt ) is an ε-approximate (η, w)smoothed local equilibrium with respect to past iterates



	w−1
(x1t−j , . . . , xkt−j ) j=0 if, for every player i ∈ [k],


#
" Pw−1


˜i,t−j
f


j=0
(xit ) ≤ ε,
∇K,η


w

where
def
i+1
k
f˜i,t0 (x) = fi (x1t0 , . . . , xi−1
t0 , x, xt0 , . . . , xt0 ).

To achieve such an equilibrium efficiently, we use Algorithm 4, which runs k copies of any online algorithm that
achieves a w-local regret bound for some η > 0.
Algorithm 4 Time-smoothed game simulation
1: Input: convex decision sets K1 , . . . , Kk ⊆ Rn , payoff
functions fi : (K1 , . . . , Kk ) → R, online algorithm A,
window size 1 ≤ w < T .
2: Initialize k copies (A1 , . . . , Ak ) of A with window
size w, where each Ai plays on decision set Ki .
3: for t = 1, . . . , T do
4:
Each Ai outputs xit .
5:
Show each Ai the online loss function
k
fi,t (x) := −fi (x1t , . . . , xi−1
, x, xi+1
t
t , . . . , xt ).

6: end for

We show this meta-algorithm yields a subsequence of iterates that satisfy our solution concept, with error parameter
dependent on the local regret guarantees of each player:
Theorem 6.2. For some t such that w ≤ t ≤ T , the
joint strategy (x1t , . . . , xkt ) produced by Algorithm 4 is an
ε-approximate (η, w)-smoothed local equilibrium with re
	t−1
spect to (x1t−j , . . . , xkt−j ) j=0 , where
v
u k
uX Rw,A (T )
i
ε=t
.
T
−
w
i=1
The proof, which we give in the appendix, follows from
the same method as for the reductions in Section 4, after
summing the regret bounds from each Ai .

7. Concluding Remarks
We have described how to extend the theory of online learning to non-convex loss functions, while permitting efficient
algorithms. Our definitions give rise to efficient online and
stochastic non-convex optimization algorithms that converge to local optima of first and second order. We give
a game theoretic solution concept which we call local equilibrium, which, in contrast to existing solution concepts
such as Nash equilibrium, is efficiently attainable in any
non-convex game.

Efficient Regret Minimization in Non-Convex Games

Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. 1523815. The authors
gratefully acknowledge Naman Agarwal, Brian Bullins,
Matt Weinberg, and Yi Zhang for helpful discussions.

References
Agarwal, Naman, Allen-Zhu, Zeyuan, Bullins, Brian,
Hazan, Elad, and Ma, Tengyu. Finding approximate local minima for nonconvex optimization in linear time.
arXiv preprint arXiv:1611.01146, 2016a.
Agarwal, Naman, Bullins, Brian, and Hazan, Elad. Second order stochastic optimization for machine learning
in linear time. arXiv preprint arXiv:1602.03943, 2016b.
Allen-Zhu, Zeyuan and Hazan, Elad. Variance reduction
for faster non-convex optimization. In Proceedings of
The 33rd International Conference on Machine Learning, pp. 699–707, 2016.
Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative weights update method: a meta-algorithm
and applications.
Theory of Computing, 8(6):
121–164, 2012.
doi: 10.4086/toc.2012.v008a006.
URL http://www.theoryofcomputing.org/
articles/v008a006.
Blum, A. and Mansour, Y. From external to internal regret.
In COLT, pp. 621–636, 2005.
Carmon, Yair, Duchi, John C., Hinder, Oliver, and Sidford,
Aaron. Accelerated methods for non-convex optimization. arXiv preprint 1611.00756, 2016.
Cesa-Bianchi, Nicolò and Lugosi, Gábor. Prediction,
Learning, and Games. Cambridge University Press,
2006.
Cover, Thomas. Universal portfolios. Math. Finance, 1(1):
1–19, 1991.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.
Erdogdu, Murat A and Montanari, Andrea. Convergence
rates of sub-sampled newton methods. In Advances in
Neural Information Processing Systems, pp. 3034–3042,
2015.
Freund, Yoav and Schapire, Robert E. A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Comput. Syst. Sci., 55(1):119–139, August
1997.

Ghadimi, Saeed and Lan, Guanghui. Stochastic first-and
zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368,
2013.
Hart, Sergiu and Mas-Colell, Andreu. A simple adaptive
procedure leading to correlated equilibrium. Econometrica, 68(5):1127–1150, 2000.
Hazan, Elad. Introduction to online convex optimization.
Foundations and Trends in Optimization, 2
(3-4):157–325, 2016. ISSN 2167-3888. doi: 10.
1561/2400000013.
URL http://dx.doi.org/
10.1561/2400000013.
Hazan, Elad and Kale, Satyen. Computational equivalence
of fixed points and no regret algorithms, and convergence to equilibria. In Platt, J.c., Koller, D., Singer, Y.,
and Roweis, S. (eds.), Advances in Neural Information
Processing Systems 20, pp. 625–632. MIT Press, Cambridge, MA, 2007. URL http://books.nips.cc/
papers/files/nips20/NIPS2007_0695.pdf.
Nesterov, Yurii. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media,
2004.
Nesterov, Yurii and Polyak, Boris T. Cubic regularization
of newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006.
Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2011.
Vovk, Volodimir G. Aggregating strategies. In Proceedings
of the Third Annual Workshop on Computational Learning Theory, COLT ’90, pp. 371–386, 1990.

