Stochastic Convex Optimization:
Faster Local Growth Implies Faster Global Convergence

Yi Xu 1 Qihang Lin 2 Tianbao Yang 1

Abstract
In this paper, a new theory is developed for firstorder stochastic convex optimization, showing
that the global convergence rate is sufficiently
quantified by a local growth rate of the objective function in a neighborhood of the optimal
solutions. In particular, if the objective function F (w) in the -sublevel set grows as fast as
1/θ
kw − w∗ k2 , where w∗ represents the closest
optimal solution to w and θ ∈ (0, 1] quantifies
the local growth rate, the iteration complexity
of first-order stochastic optimization for achiev2(1−θ)
e
ing an -optimal solution can be O(1/
),
which is optimal at most up to a logarithmic factor. To achieve the faster global convergence,
we develop two different accelerated stochastic subgradient methods by iteratively solving
the original problem approximately in a local region around a historical solution with the size
of the local region gradually decreasing as the
solution approaches the optimal set. Besides
the theoretical improvements, this work also include new contributions towards making the proposed algorithms practical: (i) we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge
of multiplicative growth constant and even the
growth rate θ; (ii) we consider a broad family
of problems in machine learning to demonstrate
that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient
method. For example, when applied to the `1
regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed
stochastic methods have a logarithmic iteration
complexity.
1
Department of Computer Science, The University of Iowa,
Iowa City, IA 52242, USA 2 Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USA. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1. Introduction
In this paper, we are interested in solving the following
stochastic optimization problem:
min F (w) , Eξ [f (w; ξ)],

w∈K

(1)

where ξ is a random variable, f (w; ξ) is a convex function
of w, Eξ [·] is the expectation over ξ and K is a convex domain. We denote by ∂f (w; ξ) a subgradient of f (w; ξ).
Let K∗ denote the optimal set of (1) and F∗ denote the optimal value.
Traditional stochastic subgradient (SSG) method updates
the solution according to
wt+1 = ΠK [wt − ηt ∂f (wt ; ξt )],

(2)

for t = 1, . . . , T , where ξt is a sampled value of ξ at t-th
iteration, ηt is a step size and ΠK [w] = arg minv∈K kw −
vk22 is a projection operator that projects a point into K.
Previous studies have shown that under the following assumptions i) k∂f (w; ξ)k2 ≤ G, ii) there exists w∗ ∈ K∗
such that kwt − w∗ k2 ≤ B for t = 1, . . . , T 1 , and by set√ in (2), with a high probability
ting the step size ηt = GB
T
1 − δ we have

p
√ 
b T ) − F∗ ≤ O GB(1 + log(1/δ))/ T , (3)
F (w
PT
bT =
where w
t=1 wt /T . The above convergence implies that in order to obtain an -optimal solution by
SSG, i.e., finding a w such that F (w) − F∗ ≤ 
with a high probability
1 − δ, one needs at least T =
p
O(G2 B 2 (1 + log(1/δ))2 /2 ) in the worst-case.
It is commonly known that the slow convergence of SSG
is due to the variance in the stochastic subgradient and
the non-smoothness nature of the problem as well, which
therefore requires a decreasing step size or a very small step
size. Recently, there emerges a stream of studies on various
variance reduction techniques to accelerate stochastic gradient method (Roux et al., 2012; Zhang et al., 2013; Johnson & Zhang, 2013; Xiao & Zhang, 2014; Defazio et al.,
1

This holds if we assume the domain K is bounded such that
maxw,v∈K kw − vk2 ≤ B or if assume dist(w1 , K∗ ) ≤ B/2
and project every solution wt into K ∩ B(w1 , B/2).

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

2014). However, they all hinge on the smoothness assumption. The proposed algorithms in this work tackle the issue
of variance of stochastic subgradient without the smoothness assumption from another pespective.
The main motivation for addressing this problem is from
a key observation: a high probability analysis of the SSG
method shows that the variance term of the stochastic subgradient is accompanied by an upper bound of distance of
intermediate solutions to the target solution. This observation has also been leveraged in previous analysis to design faster convergence for stochastic convex optimization
that use a strong or uniform convexity condition (Hazan &
Kale, 2011; Juditsky & Nesterov, 2014) or a global growth
condition (Ramdas & Singh, 2013) to control the distance
of intermediate solutions to the optimal solution by their
functional residuals. However, we find these global assumptions are completely unnecessary, which may not only
restrict their applications to a broad family of problems but
also worsen the convergence rate due to the larger multiplicative growth constant that could be domain-size dependent. In contrast, we develop a new theory only relying on
the local growth condition to control the distance of intermediate solutions to the -optimal solution by their functional residuals but achieving a fast global convergence.
Besides the fundamental difference, the present work also
possesses several unique algorithmic contributions compared with previous similar work on stochastic optimization: (i) we have two different ways to control the distance
of intermediate solutions to the -optimal solution, one by
explicitly imposing a bounded ball constraint and another
one by implicitly regularizing the intermediate solutions,
where the later one could be more efficient if the projection into the intersection of a bounded ball and the problem domain is complicated; (ii) we develop more practical
variants that can be run without knowing the multiplicative
growth constant though under a slightly stringent condition; (iii) for problems whose local growth rate is unknown
we still develop an improved convergence result of the proposed algorithms comparing with the SSG method. In addition, the present work will demonstrate the improved results and practicability of the proposed algorithms for many
problems in machine learning, which is lacking in similar
previous work.

2. Related Work
The most similar work to the present one is (Ramdas &
Singh, 2013), which studied stochastic convex optimization
under a global growth condition, which they called Tsybakov noise condition. One major difference from their result is that we achieve the same order of iteration complexity up to a logarithmic factor under only a local growth condition. As observed later on, the multiplicative growth con-

stant in local growth condition is domain-size independent
that is smaller than that in global growth condition, which
could be domain-size dependent. Besides, the stochastic
optimization algorithm in (Ramdas & Singh, 2013) assume
the optimization domain K is bounded, which is removed
in this work. In addition, they do not address the issue
when the multiplicative constant is unknown and lack study
of applicability for machine learning problems. Juditsky
& Nesterov (2014) presented primal-dual subgradient and
stochastic subgradient methods for solving problems under
the uniform convexity assumption (see the definition under
Observation 1). As exhibited shortly, the uniform convexity condition covers only a smaller family of problems than
the considered local growth condition. However, when the
problem is uniform convex, the iteration complexity obtained in this work resembles that in (Juditsky & Nesterov,
2014).
Recently, there emerge a wave of studies that attempt to
improve the convergence of existing algorithms under no
strong convexity assumption by considering certain weaker
conditions than strong convexity (Necoara et al., 2015; Liu
et al., 2015; Zhang & Yin, 2013; Liu & Wright, 2015; Gong
& Ye, 2014; Karimi et al., 2016; Zhang, 2016; Qu et al.,
2016; Wang & Lin, 2014). Several recent works (Necoara
et al., 2015; Karimi et al., 2016; Zhang, 2016) have unified
many of these conditions, implying that they are a kind of
global growth condition with θ = 1/2. Unlike the present
work, most of these developments require certain smoothness assumption except (Qu et al., 2016).
Luo & Tseng (1992a;b; 1993) pioneered the idea of using local error bound condition to show faster convergence of gradient descent, proximal gradient descent, and
many other methods for a family of structured composite problems (e.g., the LASSO problem). Many follow-up
works (Hou et al., 2013; Zhou et al., 2015; Zhou & So,
2015) have considered different regularizers (e.g., `1,2 regularizer, nuclear norm regularizer). However, these works
only obtained asymptotically faster (i.e., linear) convergence and they hinge on the smoothness on some parts of
the problem. Yang & Lin (2016); Xu et al. (2016) have
considered the same local growth condition (aka local error bound condition in their work) for developing faster deterministic algorithms for non-smooth optimization. However, they did not address the problem of stochastic convex
optimization, which restricts their applicability to largescale problems in machine learning.
Finally, we note that the improved iteration complexity in
this paper does not contradict to the lower bound in (Nemirovsky A.S. & Yudin, 1983; Nesterov, 2004). The bad
examples constructed to derive the lower bound for general non-smooth optimization do not satisfy the assumptions made in this work (in particular Assumption 1(b)).

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

3. Preliminaries
Recall the notations K∗ and F∗ that denote the optimal set
of (1) and the optimal value, respectively. For the optimization problem in (1), we make the following assumption throughout the paper.
Assumption 1. For a stochastic optimization problem (1),
we assume
(a) there exist w0 ∈ K and 0 ≥ 0 such that F (w0 ) −
F∗ ≤ 0 ;
(b) K∗ is a non-empty compact set;
(c) There exists a constant G such that k∂f (w; ξ)k2 ≤ G.
Remark: (a) essentially assumes the availability of a lower
bound of the optimal objective value, which usually holds
for machine learning problems (due to non-negativeness
of the objective function). (b) simply assumes the optimal set is closed and bounded. This is a relaxed condition
in contrast with most previous work that assume the domain K is bounded. Even if K is unbounded, as long as the
function is a proper lower-semicontinuous convex and coercive function defined on a finite dimensional space, K∗ is
nonempty and compact (Bolte et al., 2015). Note that any
norm-regularized loss function minimization problem on
a finite dimensional space in machine learning satisfy this
property. (c) is a standard assumption also made in many
previous stochastic gradient-based methods. By Jensen’s
inequality, we also have k∂F (w)k2 ≤ G.
For any w ∈ K, let w∗ denote the closest optimal solution
in K∗ to w, i.e., w∗ = arg minv∈K∗ kv − wk22 , which is
unique. We denote by L the -level set of F (w) and by S
the -sublevel set of F (w), respectively, i.e., L = {w ∈
K : F (w) = F∗ + }, S = {w ∈ K : F (w) ≤ F∗ + }.
Given K∗ is bounded, it follows from (Rockafellar, 1970,
Corollary 8.7.1) that the sublevel set S is bounded for any
 ≥ 0 and so as the level set L . Let w† denote the closest
point in the -sublevel set to w, i.e.,
w† = arg min kv − wk22 .
v∈S

(4)

It is easy to show that w† ∈ L when w ∈
/ S (using the
KKT condition). Let B(w, r) = {u ∈ Rd : ku−wk2 ≤ r}
denote an Euclidean ball centered at w with a radius r.
Denote by dist(w, K∗ ) = minv∈K∗ kw − vk2 the distance between w and the set K∗ , by ∂ 0 F (w) the projection of 0 onto the nonempty closed convex set ∂F (w), i.e.,
k∂ 0 F (w)k2 = minv∈∂F (w) kvk2 .
3.1. Functional Local Growth Rate
We quantify the functional local growth rate by measuring
how fast the functional value increase when moving a point
away from the optimal solution in the -sublevel set. In
particular, a function F (w) has a local growth rate θ ∈

(0, 1] in the -sublevel set (  1) if there exists a constant
λ > 0 such that:
1/θ

λkw − w∗ k2

≤ F (w) − F∗ ,

∀w ∈ S ,

(5)

where w∗ is the closest solution in the optimal set K∗
to w. Note that the local growth rate θ is at most 1.
This is due to that F (w) is G-Lipschitz continuous and
limw→w∗ kw − w∗ k1−α
= 0 if α < 1. The inequality
2
in (5) can be equivalently written as
kw − w∗ k2 ≤ c(F (w) − F∗ )θ ,

∀w ∈ S ,

(6)

where c = 1/λθ , which is called as local error bound condition in (Yang & Lin, 2016). In this work, to avoid confusion with earlier work by Luo & Tseng (1992a;b; 1993)
who also explored a related but different local error bound
condition, we refer to the inequality in (5) or (6) as local
growth condition (LGC). If the function F (x) is assumed
to satisfy (5) for all w ∈ K, it is referred to as global
growth condition (GGC). Note that since we do not assume
a bounded K, the GGC might be ill posed. In the following
discussions, when compared with GGC we simply assume
the domain is bounded.
Below, we present several observations mostly from existing work to clarify the relationship between the LGC (6)
and previous conditions, and also justify our choice of LGC
that covers a much broader family of functions than previous conditions and induces a smaller multiplicative growth
constant c than that induced by GGC.
Observation 1. Strong convexity or uniform convexity condition implies LGC with θ = 1/2, but not vice versa.
F (w) is said to satisfy a uniform convexity condition on K
with convexity parameters p ≥ 2 and µ if:
F (u) ≥ F (v) + ∂F (v)> (u − v) +

µku − vkp2
, ∀u, v ∈ K.
2

If we let u = w, v = w∗ , and ∂F (v) = 0, we have (5)
with θ = 1/p ∈ (0, 1/2]. Clearly LGC covers a broader
family of functions than uniform convexity.
Observation 2. The weak strong convexity (Necoara et al.,
2015), essential strong convexity (Liu et al., 2015), restricted strong convexity (Zhang & Yin, 2013), optimal
strong convexity (Liu & Wright, 2015), semi-strong convexity (Gong & Ye, 2014) and other error bound conditions
considered in several recent work (Karimi et al., 2016;
Zhang, 2016) imply a GGC on the entire optimization domain K with θ = 1/2 for a convex function.
Some of these conditions are also equivalent to the GGC
with θ = 1/2. We refer the reader to (Necoara et al.,
2015), (Karimi et al., 2016) and (Zhang, 2016) for more
discussions of these conditions.
The third observation shows that LGC could imply faster
convergence than that induced by GGC.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Observation 3. The LGC could induce a smaller constant
c in (6) that is domain-size independent than that induced
by the GGC on the entire optimization domain K.

Lemma 1. For any w ∈ K and  > 0, we have

To illustrate this, we consider a function f (x) = x2 if |x| ≤
1 and f (x) = |x| if 1 < |x| ≤ s, where s specifies the size
of the domain. In the -sublevel set ( < 1), the LGC (6)
holds with θ = 1/2 and c = 1. In order to make the inequality |x| ≤ cf (x)1/2 hold for all x ∈ [−s, s], we can
p
√
|x|
|x| = s.
see that c = max|x|≤s f (x)
1/2 = max|x|≤s
As a result, GGC induces a larger c that depends on the
domain size.

where w† ∈ S is the closest point in the -sublevel set to
w as defined in (4).
Remark: In view of LGC, we can see that kw − w† k2 ≤
c
(F (w) − F (w† )) for any w ∈ K. Yang & Lin (2016)
1−θ
have leveraged this relationship to improve the convergence
of the standard subgradient method. In the sequel, we will
build on this relationship to further develop novel stochastic optimization algorithms with faster convergence in high
probability.

The next observation shows that Luo-Tseng’s local error
bound condition is closely related to the LGC with θ =
1/2. To this end, we first give the definition of Luo-Tseng’s
local error bound condition. Let F (w) = h(w) + P (w),
where h(w) is a proper closed function with an open domain containing K and is continuously differentiable with
a locally Lipschitz continuous gradient on any compact set
within dom(h) and P (w) is a proper closed convex function. Such a function F (w) is said to satisfy Luo-Tseng’s
local error bound if for any ζ > 0, there exists c, ε > 0 so
that kw−w∗ k2 ≤ ckproxP (w−∇h(w))−wk2 , whenever
kproxP (w − ∇h(w)) − wk2 ≤ ε and F (w) − F∗ ≤ ζ,
where proxP (w) = arg minu∈K 12 ku − wk22 + P (w).
Observation 4. If F (w) = h(w) + P (w) is defined above
and satisfies the Luo-Tseng’s local error bound condition,
it then implies that there exists a sufficiently small 0 > 0
and C > 0 such that kw − w∗ k2 ≤ C(F (w) − F∗ )1/2 for
any w ∈ B(w∗ , 0 ).
This observation was established in (Li & Pong, 2016, Theorem 4.1). Note that the LGC condition with  = G0 and
θ = 1/2 also implies that kw − w∗ k2 ≤ C(F (w) − F∗ )1/2
for any w ∈ B(w∗ , 0 ). Nonetheless, Luo-Tseng’s local error bound imposes some smoothness assumption on h(w).
The last observation is that the LGC is equivalent to a
Kurdyka - Łojasiewicz inequality (KL), which was proved
in (Bolte et al., 2015, Theorem 5).
Observation 5. If F (w) satisfies a KL inequality, i.e.,
ϕ0 (F (w) − F∗ )k∂ 0 F (w)k2 ≥ 1 for w ∈ {x ∈ K, F (x) −
F∗ < } with ϕ(s) = csθ , then LGC (6) holds, and vice
versa.
The above KL inequality has been established for continuous semi-algebraic and subanalytic functions (Attouch
et al., 2013; Bolte et al., 2006; 2015), which cover a broad
family of functions therefore justifying the generality of the
LGC.
Finally, we present a key lemma that can leverage the LGC
to control the distance of intermediate solutions to an optimal solution.

kw − w† k2 ≤

dist(w† , K∗ )
(F (w) − F (w† )),


4. Main Results
In this section, we will present the proposed accelerated
stochastic subgradient (ASSG) methods and establish their
improved iteration complexity with a high probability. The
key to our development is to control the distance of intermediate solutions to the -optimal solution by their functional residuals that are decreasing as the solutions approach the optimal set. It is this decreasing factor that help
mitigate the non-vanishing variance issue in the stochastic subgradient. To formally illustrate this, we consider the
following stochastic subgradient update:
wτ +1 = ΠK∩B(w1 ,D) [wτ − η∇f (wτ ; ξτ )].

(7)

Lemma 2. Given w1 ∈ K, apply t-iterations of (7). For
any fixed w ∈ K ∩ B(w1 , D) and δ ∈ (0, 1), with a probability at least 1 − δ, the following inequality holds
q
2
2
3 log( 1δ )
4GD
kw1 − wk2
ηG
√
b t ) − F (w) ≤
F (w
+
+
,
2
2ηt
t
P
b t = tτ =1 wt /t.
where w
Remark: The proof of the above lemma follows similarly
as that of Lemma 10 in (Hazan & Kale, 2011). We note that
the last term is due to the variance of the stochastic subgradients. In fact, due to the non-smoothness nature of the
problem the variance of the stochastic subgradients cannot
be reduced, we therefore propose to address this issue by
reducing D in light of the inequality in Lemma 1.
The updates in (7) can be also understood as approximately
solving the original problem in the neighborhood of w1 . In
light of this, we will also develop a regularized variant of
the proposed method. In the sequel, all omitted proofs can
be found in the supplement.
4.1. Accelerated Stochastic Subgradient Method: the
Constrained variant (ASSG-c)
In this subsection, we present the constrained variant of
ASSG that iteratively solves the original problem approx-

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

imately in an explicitly constructed local neighborhood of
the recent historical solution. The detailed steps are presented in Algorithm 1. We refer to this variant as ASSG-c.
The algorithm runs in stages and each stage runs t iterations
of updates similar to (7). Thanks to Lemma 1, we gradually decrease the radius Dk in a stage-wise manner. The
step size keeps the same during each stage and geometrically decreases between stages. We notice that ASSG-c is
similar to the Epoch-GD method by Hazan & Kale (2011)
and the (multi-stage) AC-SA method with domain shrinkage by Ghadimi & Lan (2013) for stochastic strongly convex optimization. However, the difference between ASSG
and Epoch-GD/AC-SA lies at the initial radius D1 and the
number of iterations per-stage, which is due to difference
between the strong convexity assumption and Lemma 1.
The convergence of ASSG-c is presented in the theorem
below.
Theorem 1. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given δ ∈ (0, 1), let δ̃ = δ/K, K =
0
dlog2 ( 0 )e, D1 ≥ c
1−θ and t be the smallest integer
G2 D 2

such that t ≥ max{9, 1728 log(1/δ̃)} 2 1 . Then ASSG-c
0
guarantees that, with a probability 1 − δ, F (wK ) − F∗ ≤
2. As a result, the iteration complexity of ASSG-c for
achieving an 2-optimal solution with a high probability 1 − δ is O(c2 G2 dlog2 ( 0 )e log(1/δ)/2(1−θ) ) provided
c0
D1 = O( (1−θ)
).
Remark: It is notable that the faster local growth rate θ
implies the faster global convergence, i.e., lower iteration
complexity. In light of the lower bound presented in (Ramdas & Singh, 2013) under a GGC, our iteration complexity under the LGC is optimal up to at most a logarithmic
factor. It is worth mentioning that unlike traditional highprobability analysis of SSG that usually requires the domain to be bounded, the convergence analysis of ASSG
does not rely on such a condition. Furthermore, the iteration complexity of ASSG has a better dependence on
the quality of the initial solution or the size of domain if
it is bounded. In particular, if we let 0 = GB assuming
dist(w0 , K∗ ) ≤ B, though this is not necessary in practice,
then the iteration complexity of ASSG has only a logarithmic dependence on the distance of the initial solution to the
optimal set, while that of SSG has a quadratic dependence
on this distance. The above theorem requires a target precision  in order to set D1 . In subsection 4.3, we alleviate
this requirement to make the algorithm more practical.
4.2. Accelerated Stochastic Subgradient Method: the
Regularized variant (ASSG-r)
One potential issue of ASSG-c is that the projection into the
intersection of the problem domain and an Euclidean ball
might increase the computational cost per-iteration depending on the problem domain K. To address this issue, we

Algorithm 1 ASSG-c(w0 , K, t, D1 , 0 )
0
1: Input: w0 ∈ K, K, t, 0 and D1 ≥ c
1−θ
2
2: Set η1 = 0 /(3G )
3: for k = 1, . . . , K do
4:
Let w1k = wk−1
5:
for τ = 1, . . . , t − 1 do
6:
wτk+1 = ΠK∩B(wk−1 ,Dk ) [wτk − ηk ∂f (wτk ; ξτk )]
7:
end for
Pt
8:
Let wk = 1t τ =1 wτk
9:
Let ηk+1 = ηk /2 and Dk+1 = Dk /2.
10: end for
11: Output: wK
present a regularized variant of ASSG. Before delving into
the details of ASSG-r, we first present a common strategy
that solves the non-strongly convex problem (1) by stochastic strongly convex optimization. The basic idea is from the
classical deterministic proximal point algorithm (Rockafellar, 1976) which adds a strongly convex regularizer to the
original problem and solve the resulting proximal problem.
In particular, we construct a new problem
1
kw − w1 k22 ,
min Fb(w) = F (w) +
2β

w∈K

(8)

where w1 ∈ K is called the regularization reference point.
b ∗ denote the optimal solution to the above problem
Let w
given w1 . It is easy to know Fb(w) is a β1 -strongly convex
function on K. We can employ the stochastic subgradient
method suited for strongly convex problems to solve the
above problem. The update is given by

2
0
0
 ,
(9)
wt+1 = ΠK [wt+1
] = arg min w − wt+1
2
w∈K

0
where wt+1
= wt − ηt (∂f (wt ; ξt ) + β1 (wt − w1 )), and
2
b∗ −
ηt = 2β
. We present a lemma below to bound kw
t
wt k2 and kwt − w1 k2 by the above update, which will be

used in the proof of convergence of ASSG-r for solving (1).
b ∗ − wt k2 ≤ 3βG
Lemma 3. For any t ≥ 1, we have kw
and kwt − w1 k2 ≤ 2βG.
Remark: The lemma implies that the regularization term
implicitly imposes a constraint on the intermediate solutions to center around the regularization reference point,
which achieves a similar effect as the ball constraint in Algorithm 1.
Recall that the main iteration of the proximal point algorithm (Rockafellar, 1976) is
wk ≈ arg min F (w) +
w∈K

1
kw − wk−1 k22 ,
2βk

(10)

where wk approximately solves the minimization problem
above with βk changing with k. With the same idea, our
2
The factor 2 in the step size is used for proving the high probability convergence.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Algorithm 2 the ASSG-r algorithm for solving (1)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:

2c2 0
2(1−θ)

Input: w0 ∈ K, K, t, 0 and β1 ≥
for k = 1, . . . , K do
Let w1k = wk−1
for τ = 1, . . . , t − 1 do
k k
Let wτ0 +1 = 1 − τ2 wτk + τ2 w1k − 2β
τ ∂f (wτ ; ξτ )
k
0
Let wτ +1 = ΠK (wτ +1 )
end for
Pt
Let wk = 1t τ =1 wτk , and βk+1 = βk /2
end for
Output: wK

regularized variant of ASSG generates wk from stage k by
solving the minimization problem (10) approximately using (9). The detailed steps are presented in Algorithm 2,
which starts from a relatively large value of the parameter
β = β1 and gradually decreases β by a constant factor after running a number of t iterations (9) using the solution
from the previous stage as the new regularization reference
point. Despite of its similarity to the proximal point algorithm, ASSG-r incorporates the LGC into the choices of
βk and the number of iterations per-stage and obtains new
iteration complexity described below.
Theorem 2. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given δ ∈ (0, 1/e), let δ̃ = δ/K, K =
2
0
dlog2 ( 0 )e, β1 ≥ 2c
2(1−θ) and t be the smallest inte2

log t/δ̃)+log t)
}.
ger such that t ≥ max{3, 136β1 G (1+log(4
0
Then ASSG-r guarantees that, with a probability 1 − δ,
F (wK ) − F∗ ≤ 2. As a result, the iteration complexity
of ASSG-r for achieving an 2-optimal solution with a high
probability 1 − δ is O(c2 G2 log(0 /) log(1/δ)/2(1−θ) )
2
0
provided β1 = O( 2c
2(1−θ) ).

4.3. More Practical Variants of ASSG
Readers may have noticed that the presented algorithms require appropriately setting up the initial values of D1 or β1
that depend on unknown c and potentially unknown θ. This
subsection is devoted to more practical variants of ASSG.
For ease of presentation, we focus on the constrained variant of ASSG.
When c is known, we present the details of a restarting variant of ASSG in Algorithm 3, to which we refer as RASSG.
The key idea is to use an increasing sequence of t and another level of restarting for ASSG.
Theorem 3 (RASSG with unknown c). Let  ≤ 0 /4,
ω = 1, and K = dlog2 ( 0 )e in Algorithm 3. Suppose
(1)
D1 is sufficiently large so that there exists ˆ1 ∈ [, 0 /2],
with which F (·) satisfies a LGC (6) on Sˆ1 with θ ∈ (0, 1)
(1)
δ
0
and the constant c, and D1 = ˆc
1−θ . Let δ̂ = K(K+1) ,
1

2
(1)
and t1 = max{9, 1728 log(1/δ̂)} GD1 /0 . Then

Algorithm 3 ASSG with Restarting: RASSG
(1)

1: Input: w(0) , K, D1 , t1 , 0 and ω ∈ (0, 1]
(1)
0

2: Set
= 0 , η1 = 0 /(3G2 )
3: for s = 1, 2, . . . , S do
(s) (s)
4:
Let w(s) =ASSG-c(w(s−1) , K, ts , D1 , 0 )
(s+1)

Let ts+1 = ts 22(1−θ) , D1
(s+1)
(s)
0
= ω0
6: end for
7: Output: w(S)
5:

(s)

= D1 21−θ , and

with at most S = dlog2 (ˆ
1 /)e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S) ) − F∗ ≤
2. The total number of iterations of RASSG for obtaining 2-optimal solution is upper bounded by TS =
O(dlog2 ( 0 )e log(1/δ)/2(1−θ) ).
Remark: The above theorem requires a slightly stringent
LGC condition on Sˆ1 that is induced by the initial value
of D1 . If the problem satisfies the LGC with θ = 1, we
can give a slightly smaller value for θ in order to run Algorithm 3. If the target precision  is not specified, we can
give it a sufficiently small value 0 (e.g., the machine precision) that only affects K marginally. The corresponding
iteration complexity for achieving an -optimal solution is
given by O(dlog2 ( 00 )e log(1/δ)/2(1−θ) ). The parameter
ω ∈ (0, 1] is introduced to increase the practical performance of RASSG, which accounts for decrease of the objective gap of the initial solutions for each call of ASSG-c.
When θ is unknown, we can set θ = 0 and c = Bε with ε ≥
 in the LGC (6), where Bε = maxw∈Lε minv∈K∗ kw −
vk2 is the maximum distance between the points in the εlevel set Lε and the optimal set K∗ . The following theorem
states the convergence result.
Theorem 4 (RASSG with unknown θ). Let θ = 0,  ≤
0 /4 , ω = 1, and K = dlog2 ( 0 )e in Algorithm 3. Assume
(1)
D1 is sufficiently large so that there exists ˆ1 ∈ [, 0 /2]
B 
(1)
δ
rendering that D1 = ˆˆ11 0 . Let δ̂ = K(K+1)
, and
2

(1)
t1 = max{9, 1728 log(1/δ̂)} GD1 /0 . Then with
at most S = dlog2 (ˆ
1 /)e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S) ) − F∗ ≤
2. The total number of iterations of RASSG for obtaining 2-optimal solution is upper bounded by TS =
O(dlog2 ( 0 )e log(1/δ)

G2 Bˆ2
1
2

).

Remark: The Lemma 6 in the supplement shows that B
is a monotonically decreasing function in terms of , which
guarantees the existence of ˆ1 given a sufficiently large
(1)
D1 . The iteration complexity of RASSG could be still
better with a smaller factor Bˆ1 than the B in the iteration
complexity of SSG (see (3)), where B is the domain size or
the distance of initial solution to the optimal set.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

5. Applications in Risk Minimization
In this section, we present some applications of the proposed ASSG to risk minimization in machine learning. Let
(xi , yi ), i = 1, . . . , n denote a set of pairs of feature vectors
and labels that follow a distribution P, where xi ∈ X ⊂ Rd
and yi ∈ Y. Many machine learning problems end up solving the regularized empirical risk minimization problem:
n
1X
`(w> xi , yi ) + λR(w),
(11)
min F (w) =
n i=1
w∈Rd
where R(w) is a regularizer, λ is the regularization parameter and `(z, y) is a loss function. Below we will present
several examples in machine learning that enjoy faster convergence by the proposed ASSG than by SSG.
5.1. Piecewise Linear Minimization
First, we consider some examples of non-smooth and nonstrongly convex problems such that ASSG can achieve
linear convergence. In particular, we consider the problem (11) with a piecewise linear loss and `1 , `∞ or `1,∞
regularizers.
Piecewise linear loss includes hinge loss, generalized hinge
loss, absolute loss, and -insensitive loss. For particular
forms of these loss functions, please refer to (Yang et al.,
2014). The epigraph of F (w) defined by sum of a piecewise linear loss function and an `1 , `∞ or `1,∞ norm regularizer is a polyhedron. According to the polyhedral error
bound condition (Yang & Lin, 2016), for any  > 0 there
exists a constant 0 < c < ∞ such that dist(w, K∗ ) ≤
c(F (w) − F∗ ) for any w ∈ S , meaning that the proposed
ASSG has an O(log(0 /)) iteration complexity for solving such family of problems. Formally, we state the result
in the following corollary.
Corollary 5. Assume the loss function `(z, y) is piecewise
linear, then the problem in (11) with `1 , `∞ or `1,∞ norm
regularizer satisfy the LGC in (6) with θ = 1. Hence ASSG
can have an iteration complexity of O(log(1/δ) log(0 /))
with a high probability 1 − δ.
5.2. Piecewise Convex Quadratic Minimization
Next, we consider some examples of piecewise quadratic
minimization problems in machine learning and show that
e 1 . We first
ASSG enjoys an iteration complexity of O

give an definition of piecewise convex quadratic functions,
which is from (Li, 2013). A function g(w) is a real
polynomial if there exists k ∈ N+ such that g(w) =
P
Qd
αji
j
+
0≤|αj |≤k λj
i=1 wi , where λj ∈ R and αi ∈ N ∪
P
d
{0}, αj = (α1j , . . . , αdj ), and |αj | = i=1 αij . The constant k is called the degree of g. A continuous function
F (w) is said to be a piecewise convex polynomial if there
exist finitely many polyhedra P1 , . . . , Pm with ∪m
j=1 Pj =
Rd such that the restriction of F on each Pj is a convex

polynomial. Let Fj be the restriction of F on Pj . The degree of a piecewise convex polynomial function F is the
maximum of the degree of each Fj . If the degree is 2,
the function is referred to as a piecewise convex quadratic
function. Note that a piecewise convex quadratic function
is not necessarily a smooth function nor a convex function (Li, 2013).
For examples of piecewise convex quadratic problems
in machine learning, one can consider the problem (11)
with a huber loss, squared hinge loss or square loss, and
`1 , `∞ , `1,∞ , or huber norm regularizer (Zadorozhnyi
et
2016). The Huber function is defined as `γ (z) =
 al.,
1 2
z
if |z| ≤ γ,
2
, which is a piecewise conγ(|z| − 12 γ) otherwise,
vex quadratic function. The huber loss function `(z, y) =
`γ (z − y) has been used for robust regression. A Huber
Pd
regularizer is defined as R(w) = i=1 `γ (wi ).
It has been shown that (Li, 2013), if F (w) is convex and
piecewise convex quadratic, then it satisfies the LGC (6)
with θ = 1/2. The corollary below summarizes the iteration complexity of ASSG for solving these problems.
Corollary 6. Assume the loss function `(z, y) is a convex
and piecewise convex quadratic, then the problem in (11)
with `1 , `∞ , `1,∞ or huber norm regularizer satisfy the
LGC in (6) with θ = 1/2. Hence ASSG can have an ite log(1/δ) ) with a high probability
eration complexity of O(

1 − δ.
5.3. Structured composite non-smooth problems
Next, we present a corollary of our main result regarding
the following structured problem:
min F (w) , h(Xw) + P (w).

w∈Rd

(12)

Corollary 7. Assume h(u) is a strongly convex function on
any compact set and P (w) is polyhedral, then the problem
in (12) satisfies the LGC in (6) with θ = 1/2. Hence ASSG
e log(1/δ) ) with a high
can have an iteration complexity of O(

probability 1 − δ.
The proof of the first part of Corollary 7 can be found
in (Yang & Lin, 2016). One example
Pnof h(u) is p-norm
error (p ∈ (0, 1)), where h(u) = n1 i=1 |ui − yi |p . The
local strong convexity of the p-norm error (p ∈ (1, 2)) is
shown in (Goebel & Rockafellar, 2007).
Finally, we give an example that satisfies the LGC with
intermediate values θ ∈ (0, 1/2). We can consider an `1
constrained `p norm regression (Nyquist, 1983):
n

min F (w) ,

kwk1 ≤s

1X >
(x w − yi )p ,
n i=1 i

p ∈ 2N+ .

Liu & Yang (2016) have shown that the problem above satisfies the LGC in (6) with θ = p1 .

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

-6
-6.5
-7
-7.5
-8
0

2

4

6

8

number of iterations

SSG
ASSG(t=106 )
RASSG(t1 =106 )

-3
-4
-5
-6
0

2

4

6

8

number of iterations

-3.5
-4
-4.5
-5
0

10
7
×10

2

4

6

8

number of iterations

SSG
ASSG(t=106 )
RASSG(t1 =106 )

-3.5
-4
-4.5
-5
-5.5
-6
-6.5
-7
-7.5

10
7
×10

0

2

4

6

8

number of iterations

robust + ℓ1 norm, E2006-tfidf

log10 (objective gap)

log10 (objective gap)

0

-2

-3

10
7
×10

robust + ℓ1 norm, million songs
-1

-2.5

-3

SSG
ASSG(t=104 )
RASSG(t1 =104 )

-2
-3
-4
-5

10
7
×10

squared hinge + ℓ1 norm, url
0.22

0
-1

SSG
SAGA
SVRG++
RASSG

0.2
0.18
0.16
0.14
0.12
0.1

-6

0.06
0

2

4

6

8

number of iterations

10
5
×10

1

SSG
ASSG(t=104 )
RASSG(t1 =104 )

0
-1
-2
-3
-4
-5
-6
-7
-8
0

2

4

6

8

number of iterations

10
5
×10

huber loss + ℓ1 norm, E2006-log1p
0.16

SSG
SAGA
SVRG++
RASSG

0.14
0.12
0.1
0.08
0.06
0.04

0.08

-7

huber loss + ℓ1 norm, E2006-tfidf

log10 (objective gap)

-5
-5.5

-2

huber loss + ℓ1 norm, million songs

objective

-4.5

SSG
ASSG(t=106 )
RASSG(t1 =106 )

log10 (objective gap)

-4

hinge loss + ℓ1 norm, real-sim
-1
-1.5

objective

SSG
ASSG(t=106 )
RASSG(t1 =106 )

log10 (objective gap)

log10 (objective gap)

hinge loss + ℓ1 norm, covtype
-3
-3.5

0.02
0

0.5

1

1.5

cpu time (s)

2
×10

5

0

0.5

1

1.5

cpu time (s)

2
5
×10

Figure 1. Comparison of different algorithms for solving different problems on different datasets.

6. Experiments
In this section, we perform some experiments to demonstrate effectiveness of proposed algorithms. We use very
large-scale datasets from libsvm website in experiments,
including covtype.binary (n = 581012), real-sim (n =
72309), url (n = 2396130) for classification, million songs
(n = 463715), E2006-tfidf (n = 16087), E2006-log1p
(n = 16087) for regression. The detailed statistics of these
datasets are shown in the supplement.
We first compare ASSG with SSG on three tasks: `1 norm
regularized hinge loss minimization for linear classification, `1 norm regularized Huber loss minimization for linear regression, and `1 norm regularized p-norm robust regression with a loss function `(w> xi , yi ) = |w> xi − yi |p .
The regularization parameter λ is set to be 10−4 in all tasks
(We also perform the experiments with λ = 10−2 and include the results in the supplement). We set γ = 1 in Huber
loss and p = 1.5 in robust regression. In all experiments,
we use the constrained variant of ASSG, i.e., ASSG-c. For
fairness, we use the same initial solution with all zero entries for all algorithms.
We use a decreasing step size pro√
portional to 1/ τ (τ is the iteration index) in SSG. The
initial step size of SSG is tuned in a wide range to obtain
the fastest convergence. The step size of ASSG in the first
stage is also tuned around the best initial step size of SSG.
The value of D1 in both ASSG and RASSG is set to 100
for all problems. In implementing the RASSG, we restart
every 5 stages with t increased by a factor of 1.15, 2 and
2 respectively for hinge loss, Huber loss and robust regression. We tune the parameter ω among {0.3, 0.6, 0.9, 1}.
We report the results of ASSG with a fixed number of iterations per-stage t and RASSG with an increasing sequence
of t. The results are plotted in Figure 1 (first 6 figures),
in which we plot the log difference between the objective
value and the smallest obtained objective value (to which
we refer as objective gap) versus number of iterations. The

figures show that (i) ASSG can quickly converge to a certain level set determined implicitly by t; (ii) RASSG converges much faster than SSG to more accurate solutions;
(iii) RASSG can gradually decrease the objective value.
Finally, we compare RASSG with state-of-art stochastic
optimization algorithms for solving a finite-sum problem
with a smooth piecewise quadratic loss (e.g., squared hinge
loss, huber loss) and an `1 norm regularization. In particular, we compare with SAGA (Defazio et al., 2014) and
SVRG++ (Allen-Zhu & Yuan, 2016). We conduct experiments on two high-dimensional datasets url and E2006log1p and fix the regularization parameter λ = 10−4 (We
also include the results for λ = 10−2 in the supplement).
We use δ = 1 in Huber loss. For RASSG, we start from
D1 = 100 and t1 = 103 , then restart it every 5 stages with
t increased by a factor of 2. We tune the initial step sizes for
all algorithms in a wide range and set the values of parameters in SVRG++ followed by (Allen-Zhu & Yuan, 2016).
We plot the objective versus the CPU time (second) in Figure 1 (last 2 figures). The results show that RASSG converges faster than other three algorithms for the two tasks.
This is not surprising considering that RASSG, SAGA and
e
SVRG++ suffer from an iteration complexity of O(1/),
O(n/), and O(n log(1/) + 1/), respectively.

7. Conclusion
In this paper, we have proposed accelerated stochastic subgradient methods for solving general non-strongly convex
stochastic optimization under the functional local growth
condition. The proposed methods enjoy a lower iteration
complexity than vanilla stochastic subgradient method and
also a logarithmic dependence on the impact of the initial
solution. We have also made an extension by developing
a more practical variant. Applications in machine learning
have demonstrated the faster convergence of the proposed
methods.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Acknowledgement
We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). T.
Yang would like to thank Lijun Zhang for pointing out
(Kakade & Tewari, 2008) for his attention.

References
Allen-Zhu, Zeyuan and Yuan, Yang. Improved svrg for
non-strongly-convex or sum-of-non-convex objectives.
In ICML, pp. 1080–1089, 2016.
Attouch, Hedy, Bolte, Jérôme, and Svaiter, Benar Fux.
Convergence of descent methods for semi-algebraic and
tame problems: proximal algorithms, forward-backward
splitting, and regularized gauss-seidel methods. Math.
Program., 137(1-2):91–129, 2013.
Bolte, Jérôme, Daniilidis, Aris, and Lewis, Adrian. The
łojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM J. on Optimization, 17:1205–1223, 2006.
Bolte, Jérôme, Nguyen, Trong Phong, Peypouquet, Juan,
and Suter, Bruce. From error bounds to the complexity of
first-order descent methods for convex functions. CoRR,
abs/1510.08234, 2015.
Defazio, Aaron, Bach, Francis R., and Lacoste-Julien, Simon. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In
NIPS, pp. 1646–1654, 2014.
Ghadimi, Saeed and Lan, Guanghui. Optimal stochastic
approximation algorithms for strongly convex stochastic composite optimization, ii: Shrinking procedures and
optimal algorithms. SIAM Journal on Optimization, 23
(4):20612089, 2013.
Goebel, R. and Rockafellar, R. T. Local strong convexity
and local lipschitz continuity of the gradient of convex
functions. Journal of Convex Analysis, 2007.
Gong, Pinghua and Ye, Jieping. Linear convergence of
variance-reduced projected stochastic gradient without
strong convexity. CoRR, abs/1406.1102, 2014.
Hazan, Elad and Kale, Satyen. Beyond the regret minimization barrier: an optimal algorithm for stochastic
strongly-convex optimization. In COLT, pp. 421–436,
2011.
Hou, Ke, Zhou, Zirui, So, Anthony Man-Cho, and Luo,
Zhi-Quan. On the linear convergence of the proximal
gradient method for trace norm regularization. In NIPS,
pp. 710–718, 2013.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
NIPS, pp. 315–323, 2013.
Juditsky, Anatoli and Nesterov, Yuri. Deterministic and
stochastic primal-dual subgradient algorithms for uniformly convex minimization. Stoch. Syst., 4:44–80,
2014.
Kakade, Sham M. and Tewari, Ambuj. On the generalization ability of online strongly convex programming
algorithms. In NIPS, pp. 801–808, 2008.
Karimi, Hamed, Nutini, Julie, and Schmidt, Mark W. Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. In ECMLPKDD, pp. 795–811, 2016.
Li, Guoyin. Global error bounds for piecewise convex
polynomials. Math. Program., 137(1-2):37–64, 2013.
Li, Guoyin and Pong, Ting Kei. Calculus of the exponent of kurdyka- łojasiewicz inequality and its applications to linear convergence of first-order methods. CoRR,
abs/1602.02915, 2016.
Liu, Ji and Wright, Stephen J. Asynchronous stochastic
coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25:351–376, 2015.
Liu, Ji, Wright, Stephen J., Ré, Christopher, Bittorf, Victor, and Sridhar, Srikrishna. An asynchronous parallel
stochastic coordinate descent algorithm. J. Mach. Learn.
Res., 16:285–322, 2015. ISSN 1532-4435.
Liu, Mingrui and Yang, Tianbao. Adaptive accelerated gradient converging methods under holderian error bound
condition. CoRR, abs/1611.07609, 2016.
Luo, Zhi-Quan and Tseng, Paul. On the convergence
of coordinate descent method for convex differentiable
minization. Journal of Optimization Theory and Applications, 72(1):7–35, 1992a.
Luo, Zhi-Quan and Tseng, Paul. On the linear convergence
of descent methods for convex essenially smooth minization. SIAM Journal on Control and Optimization, 30(2):
408–425, 1992b.
Luo, Zhi-Quan and Tseng, Paul. Error bounds and convergence analysis of feasible descent methods: a general
approach. Annals of Operations Research, 46:157–178,
1993.
Necoara, I., Nesterov, Yu., and Glineur, F. Linear convergence of first order methods for non-strongly convex optimization. CoRR, abs/1504.06298, 2015.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Nemirovsky A.S., Arkadii Semenovich. and Yudin, D. B.
Problem complexity and method efficiency in optimization. Wiley-Interscience series in discrete mathematics.
Wiley, Chichester, New York, 1983. ISBN 0-471-103454. A Wiley-Interscience publication.
Nesterov, Yurii. Introductory lectures on convex optimization : a basic course. Applied optimization. Kluwer
Academic Publ., 2004. ISBN 1-4020-7553-7.
Nyquist, H. The optimal lp norm estimator in linear regression models. Communications in Statistics - Theory and
Methods, 12(21):2511–2524, 1983.
Qu, Chao, Xu, Huan, and Ong, Chong Jin. Fast rate analysis of some stochastic optimization algorithms. In ICML,
pp. 662–670, 2016.
Ramdas, Aaditya and Singh, Aarti. Optimal rates for
stochastic convex optimization under tsybakov noise
condition. In ICML, pp. 365–373, 2013.
Rockafellar, R. Tyrrell. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14:877–898, 1976.
Rockafellar, R.T. Convex Analysis. Princeton mathematical
series. Princeton University Press, 1970.
Roux, Nicolas Le, Schmidt, Mark W., and Bach, Francis.
A stochastic gradient method with an exponential convergence rate for finite training sets. In NIPS, pp. 2672–
2680, 2012.
Wang, Po-Wei and Lin, Chih-Jen. Iteration complexity of
feasible descent methods for convex optimization. Journal of Machine Learning Research, 15(1):1523–1548,
2014.
Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057–2075, 2014.
Xu, Yi, Yan, Yan, Lin, Qihang, and Yang, Tianbao. Homotopy smoothing for non-smooth problems with lower
complexity than O(1/). In NIPS, pp. 1208–1216, 2016.
Yang, Tianbao and Lin, Qihang. Rsg: Beating sgd
without smoothness and/or strong convexity. CoRR,
abs/1512.03107, 2016.
Yang, Tianbao, Mahdavi, Mehrdad, Jin, Rong, and Zhu,
Shenghuo. An efficient primal-dual prox method for
non-smooth optimization. Machine Learning, 2014.
Zadorozhnyi, Oleksandr, Benecke, Gunthard, Mandt,
Stephan, Scheffer, Tobias, and Kloft, Marius. Hubernorm regularization for linear prediction models. In
ECML-PKDD, pp. 714–730, 2016.

Zhang, Hui. New analysis of linear convergence of
gradient-type methods via unifying error bound conditions. CoRR, abs/1606.00269, 2016.
Zhang, Hui and Yin, Wotao. Gradient methods for convex minimization: better rates under weaker conditions.
CoRR, abs/1303.4645, 2013.
Zhang, Lijun, Mahdavi, Mehrdad, and Jin, Rong. Linear
convergence with condition number independent access
of full gradients. In NIPS, pp. 980–988, 2013.
Zhou, Zirui and So, Anthony Man-Cho. A unified approach
to error bounds for structured convex optimization problems. CoRR, abs/1512.03518, 2015.
Zhou, Zirui, Zhang, Qi, and So, Anthony Man-Cho. L1pnorm regularization: Error bounds and convergence rate
analysis of first-order methods. In ICML, pp. 1501–
1510, 2015.

