Minimax Regret Bounds for Reinforcement Learning

Mohammad Gheshlaghi Azar 1 Ian Osband 1 Rémi Munos 1

Abstract
We consider the problem of provably optimal
exploration in reinforcement learning for finite
horizon MDPs. We show that an optimistic
modification√to value iteration achieves
√ a regret
e HSAT +H 2 S 2 A+H T ) where
bound of O(
H is the time horizon, S the number of states, A
the number of actions and T the number of timesteps. This result improves
√ over the best previe
AT ) achieved by the
ous known bound O(HS
UCRL2 algorithm of Jaksch et al. (2010). The
key significance of our new results is that when
3 3
T ≥
√ H S A and SA ≥ H, it leads to a regret of
e
O( HSAT√
) that matches the established lower
bound of Ω( HSAT ) up to a logarithmic factor.
Our analysis contains two key insights. We use
careful application of concentration inequalities
to the optimal value function as a whole, rather
than to the transitions probabilities (to improve
scaling in S), and we define Bernstein-based "exploration bonuses" that use the empirical variance of the estimated values at the next states (to
improve scaling in H).

1. Introduction
We consider the reinforcement learning (RL) problem of an
agent interacting with an environment in order to maximize
its cumulative rewards through time (Burnetas & Katehakis, 1997; Sutton & Barto, 1998). We model the environment as a Markov decision process (MDP) whose transition dynamics are unknown from the agent. As the agent
interacts with the environment it observes the states, actions and rewards generated by the system dynamics. This
leads to a fundamental trade off: should the agent explore
poorly-understood states and actions to gain information
and improve future performance, or exploit its knowledge
to optimize short-run rewards.
1
DeepMind, London, UK. Correspondence to: Mohammad
Gheshlaghi Azar <mazar@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

The most common approach to this learning problem is
to separate the process of estimation and optimization.
In this paradigm, point estimates of the unknown quantities are used in place of the unknown parameters and a
plan is made with respect to these estimates. Naive optimization with respect to these point estimates can lead
to premature exploitation and so may never learn the optimal policy. Dithering approaches to exploration (e.g.,
-greedy) address this failing through random action selection. However, as this exploration is not directed the
resultant algorithms may take exponentially long to learn
(Kearns & Singh, 2002). In order to learn efficiently it is
necessary that the agent prioritizes potentially informative
states and actions. To do this, it is important that the agent
maintains some notion of its own uncertainty. In some
sense, given any prior belief, the optimal solution to this
exploration/exploitation dilemma is given by the dynamic
programming in the extended Bayesian belief state (Bertsekas, 2007). However, the computational demands of this
method become intractable for even small problems (Guez
et al., 2013) while finite approximations can be arbitrarily
poor (Munos, 2014).
To combat these failings, the majority of provably efficient
learning algorithms employ a heuristic principle known as
optimism in the face of uncertainty (OFU). In these algorithms, each state and action is afforded some “optimism”
such that its imagined value is as high as statistically plausible. The agent then chooses a policy under this optimistic view of the world. This allows for efficient exploration since poorly-understood states and actions are afforded higher optimistic bonus. As the agent resolves its
uncertainty, the effects of optimism will reduce and the
agent’s policy will approach optimality. Almost all reinforcement learning algorithms with polynomial bounds on
sample complexity employ optimism to guide exploration
(Kearns & Singh, 2002; Brafman & Tennenholtz, 2002;
Strehl et al., 2006; Dann et al., 2017).
An alternative principle motivated by the Thompson sampling (Thompson, 1933) has emerged as a practical competitor to optimism. The algorithm posterior sampling reinforcement learning (PSRL) maintains a posterior distribution for MDPs and, at each episode of interaction, follows a policy which is optimal for a single random sample
(Strens, 2000). Previous works have argue for the potential

Minimax Regret Bounds for Reinforcement Learning

benefits of such PSRL methods over existing optimistic approaches (Osband et al., 2013; Osband & Van Roy, 2016b)
but they come with guarantees on the Bayesian regret only.
However a very recent work (Agrawal & Jia, 2017) have
shown that an optimistic version of posterior sampling (using a max over several
samples) achieves a frequentist re√
e
SAT ) (for large T ) in the more general
gret bound O(H
setting of weakly communicating MDPs.

At a high level, this work addresses the noted shortcomings
of existing RL algorithms (Bartlett & Tewari, 2009; Jaksch
et al., 2010; Osband & Van Roy, 2016b), in terms of dependency on S and H. We demonstrates that it is possible
to design a simple and computationally efficient optimistic
algorithm that simultaneously address both the loose scaling in √
S and H to obtain the first regret bounds that match
the Ω( HSAT ) lower bounds as T becomes large.

In this paper we present a conceptually simple and computationally efficient approach to optimistic reinforcement
learning in finite-horizon MDPs and report results for the
frequentist regret. Our algorithm, upper confidence bound
value iteration (UCBVI) is similar to model-based interval estimation (MBIE-EB) (Strehl & Littman, 2005) with a
delicate alteration to the form of the “exploration bonus”.
In particular UCBVI replaces the universal scalar of the
bonus in MBIE-EB with the empirical variance of the nextstate value function of each state-action pair. This altere
ation √
is essential to improve the regret bound from O(H)
e H).
to O(

We should be careful to mention the current limitations of
our work, each of which may provide fruitful ground for future research. First, we study the setting of episodic, finite
horizon MDPs and not the more general setting of weakly
communicating systems (Bartlett & Tewari, 2009; Jaksch
et al., 2010). Also we assume that the horizon length H
is known to the learner. Further,
improve
√ our bounds only
e
AT ) for T > H 3 S 3 A.
over previous scaling O(HS

Our key contribution
is to establish√a high probability regret
√
e HSAT +H 2 S 2 A+H T ) where S is the numbound O(
ber of states, A is the number of actions, H is the episode
e
length and T is the total number of time-steps (and where O
3 3
ignores logarithmic factors). Importantly,
for T > H S A
√
e HSAT ), which matches
and SA ≥ H this bound is O(
the established lower bound for this problem, up to logarithmic factors (Osband & Van Roy, 2016a).1 This positive
result is the first of its kind and helps to address an ongoing
question about where the fundamental lower bounds lie for
reinforcement learning in finite horizon MDPs (Bartlett &
Tewari, 2009; Dann & Brunskill, 2015; Osband & Van Roy,
2016a). Our refined analysis contains two key ingredients:
• We use careful application of Bernstein and Freedman inequalities (Bernstein, 1927; Freedman, 1975)
to the concentration of the optimal value function directly, rather than building confidence sets for the
transitions probabilities and rewards, like in UCRL2
(Jaksch et al., 2010) and UCFH (Dann & Brunskill,
2015).
• We use empirical-variance exploration bonuses based
on Bernstein’s inequality, which together with a recursive Bellman-type Law of Total Variance (LTV) provide tight bounds on the expected sum of the variances
of the value estimates, in a similar spirit to the analysis
from (Azar et al., 2013; Lattimore & Hutter, 2012).
1

In fact the lower bound of (Jaksch et al., 2010) is for the more
general setting of the weakly communicating MDPs and it doesn’t
directly apply to our setting. But a similar approach can be used
to prove a lower bound of same order for the finite-horizon MDPs,
as it is already used in (Osband & Van Roy, 2016a).

We hope that this work will serve to elucidate several of the
existing shortcomings of exploration in the tabular setting
and help further the direction of research towards provably
optimal exploration in reinforcement learning.

2. Problem formulation
In this section, we briefly review some notation, as well as
some standard concepts and definitions from the theory of
Markov decision processes (MDPs).
Markov Decision Problems We consider the problem of
undiscounted episodic reinforcement learning (RL) (Bertsekas & Tsitsiklis, 1996), where an RL agent interacts
with a stochastic environment and this interaction is modeled as a discrete-time MDP. An MDP is a quintuple
hS, A, P, R, Hi, where S and A are the set of states and
actions, P is the state transition distribution, The function
R : S ×A → < is a real-valued function on the state-action
space and the horizon H is the length of episode. We denote by P (·|x, a) and R(x, a) the probability distribution
over the next state and the immediate reward of taking action a at state x, respectively. The agent interacts with the
environment in a sequence of episodes. The interaction between the agent and environment at every episode2 k ∈ [K]
is as follows: starting from xk,1 ∈ S which is chosen by
the environment, the agent interacts with the environment
for H steps by following a sequence of actions chosen in
A and observes a sequence of next-states and rewards until the end of episode. The initial state xk,1 may change
arbitrarily from one episode to the next. We also use the
notation k · k1 for the `1 norm throughout this paper.
Assumption 1 (MDP Regularity). We assume S and A are
finite sets with cardinalities S, A, respectively. We also
assume that the immediate reward R(x, a) is deterministic
2

We write [n] for {i ∈ N | 1 ≤ i ≤ n}.

Minimax Regret Bounds for Reinforcement Learning

and belongs to the interval [0, 1].3
In this paper we focus on the setting where the reward function R is known, but extending our algorithm to unknown
stochastic rewards poses no real difficulty.
The policy during an episode is expressed as a mapping
π : S × [H] → A. The value Vhπ : S → R denotes the value function at every step h = 1, 2, . . . , H
and state x ∈ S such that Vhπ (x) corresponds to the expected sum of H − h rewards received under policy π,
starting from xh = x ∈ S. Under Assumption 1 there
exists always a policy π ∗ which attains the best possible
def
values, and we define the optimal value function Vh∗ (x) =
supπ Vhπ (x) for all x ∈ S and h ≥ 1. The policy π
at every step h defines the state transition kernel Phπ and
the reward function

rhπ

as

def
Phπ (y|x) =

P (y|x, π(x, h))

def

and rhπ (x) = R(x, π(x, h)) for all x ∈ S. For every
V : S → R the right-linear operators P · and Phπ · are
def P
also defined as (P V )(x, a) =
y∈S P (y|x, a)V (y) for
def P
π
all (x, a) ∈ S × A and (Ph V )(x) = y∈S Phπ (y|s)V (y)
for all x ∈ S, respectively. The Bellman operator for
the policy π, at every step h > 0 and x ∈ S, is defined
def
as (Thπ V )(x) = rhπ (x) + (Phπ V )(x). We also define
the state-action Bellman operator for all (x, a) ∈ S × A
def

as (T V )(x, a) = R(x, a) + (P V )(x, a) and the optidef

mality Bellman operator for all x ∈ S as (T ∗ V )(x) =
maxa∈A (T V )(x, a). For ease of exposition, we remove
the dependence on x and (x, a), e.g., writing P V for
(P V )(x, a) and V for V (x), when there is no possible confusion.
We measure the performance of the learner over T = KH
steps4 by the regret Regret(K), defined as
def

Regret(K) =

K
X

V1∗ (xk,1 ) − V1πk (xk,1 ),

k=1

where πk is the control policy followed by the learner at
episode k. Thus the regret measures the expected loss of
following the policy produced by the learner instead of the
optimal policy. So the goal of learner is to follow a sequence of policies π1 , π2 , . . . , πK such that Regret(K) is
as small as possible.

3. Upper confidence bound value iteration
In this section we introduce two variants of the algorithm
that we investigate in this paper. We call the algorithm upper confidence bound value iteration (UCBVI). UCBVI is
3

For rewards in [Rmin , Rmax ] simply rescale these bounds.
In this paper we will often substitute T =KH to highlight
various dependencies relative to the existing literature. This
equivalence should be kept in mind by the reader.
4

an extension of value iteration which guarantees that the
resultant value function is a (high-probability) upper confidence bound (UCB) on the optimal value function. This
algorithm is related to the model based interval estimation
(MBIE-EB) algorithm (Strehl & Littman, 2008). Our key
contribution is the precise design of the upper confidence
sets, and the analysis which lead to tight regret bounds.
UCBVI, described in Algorithm 1, calls UCB-Q-values
(Algorithm 2) which returns UCBs on the Q-values computed by value iteration using an empirical Bellman operator to which is added a confidence bonus bonus. We consider two variants of UCBVI depending on the structure of
bonus, which we present in Algorithms 3 and 4.
Algorithm 1 UCBVI
Initialize data H = ∅
for episode k = 1, 2, . . . , K do
Qk,h = UCB − Q − values(H)
for step h = 1, . . . , H do
Take action ak,h = arg maxa Qk,h (xk,h , a)
Update H = H ∪ (xk,h , ak,h , xk,h+1 )
end for
end for
Algorithm 2 UCB-Q-values
Require: Bonus algorithm bonus, Data H
Compute, for P
all (x,a,y) ∈ S × A × S,
Nk (x,a,y) = (x0 ,a0 ,y0 )∈H I(x0 = x,a0 = a,y 0 = y)
P
Nk (x,a) = P
y∈S Nk (x,a,y)
0
(x,a) = (xi,h ,ai,h ,xi,h+1 )∈H I(xi,h = x,ai,h = a)
Nk,h
Let K = {(x,a) ∈ S × A, Nk (x,a) > 0}
Estimate Pbk (y|x,a) = NNkk(x,a,y)
(x,a) for all (x,a) ∈ K
Initialize Vk,H+1 (x) = 0 for all (x,a) ∈ S × A
for h = H,H − 1,...,1 do
for (x,a) ∈ S × A do
if (x,a) ∈ K then
0
bk,h (x,a) = bonus(Pbk ,Vk,h+1 ,Nk ,Nk,h
)
Qk,h (x,a) = min Qk−1,h (x,a),H,

R(x,a) + (Pbk Vk,h+1 )(x,a) + bk,h (x,a)
else
Qk,h (x,a) = H
end if
Vk,h (x) = maxa∈A Qk,h (x,a)
end for
end for
return Q-values Qk,h

The first of these UCBVI-CH is based upon ChernoffHoeffding’s concentration inequality, considers UCBVI
with bonus = bonus_1. bonus_1 is a very simple bound
which only assumes that values are bounded in [0, H]. We

Minimax Regret Bounds for Reinforcement Learning

will see in Theorem 1 that this very simple
√ algorithm can
e
already achieve a regret bound of O(H
SAT ), thus improving√the best previously known regret bounds from a
S to a S dependence. The intuition for this improved
S-dependence is that our algorithm (as well as our analysis) does not consider confidence sets on the transition
dynamics P (y|x, a) like UCRL2 and UCFH do, but instead directly maintains confidence intervals on the optimal value function. This is crucial as, for any given (x, a),
the transition dynamics are S-dimensional whereas the Qvalue function is one-dimensional.
Algorithm 3 bonus_1
Require: Pbk (x, a),
qNk (x, a)
1
b(x, a) = 7HL Nk (x,a)
where L = ln(5SAT /δ),
return b
However, the loose form of UCB given by UCBVI-CH
does not look at the value function of the next state, and
just consider it as being bounded in [0, H]. However,
much better bounds can be obtained by looking at the variance of the next state values. Our main result relies upon
UCBVI with bonus = bonus_2, which we refer to as
UCBVI-BF as it relies on Bernstein-Freedman’s concentration inequalities to build the confidence set. UCBVI-BF
builds upon the intuition for UCBVI-CH but also incorporates a variance-dependent exploration bonus. This leads to
tighter√exploration bonuses and an improved regret bound
e HSAT ).
of O(
Algorithm 4 bonus_2
Require: Pbk (x, a), Vk,h+1 , Nk , N 0

k,h

s
b(x,a)=

+

8LVarY ∼Pbk (·|x,a) (Vk,h+1 (Y ))

14HL
+
Nk (x,a)
3Nk (x,a)
v
h

i
u P
u8
bk (y|x,a) min 10020H 3 S 2 AL2 ,H 2
P
y
t
N
(y)
k,h+1

Nk (x,a)

where L=ln(5SAT /δ)
return b
Compared to UCBVI-BF here we use a bonus built from
the empirical variance of the estimated next values. The
idea is that if we had knowledge of the optimal value V ∗ ,
we could build tight confidence bounds using the variance
of the optimal value function at the next state in place of the
loose bound of H. Since however V ∗ is unknown, here we
use as a surrogate the empirical variance of the estimated
values. As more data is gathered, this variance estimate

will converge to the variance of V ∗ . Now we need to make
sure our estimates Vk,h are optimistic (i.e., that they upper
bound Vh∗ ) at all times. This is achieved by adding an additional bonus (last term in b(x, a)), which guarantees that
we upper bound the variance of V ∗ . Now, using an iterative -Bellman-type- Law of Total Variance, we have (see
proof) that the sum of the next-state variances of V ∗ (over
H time steps) (which is related to the sum of the exploration bonuses over H steps) is bounded by the variance
of the H-steps return. Thus the size of the bonuses built
by UCBVI-BF are constrained over the H steps. And we
prove that the√sum of those bonuses do not grow linearly
in H but in H only. √This is the key for our improved
dependence from H to H.

4. Main results
In this section we present the main results of the paper,
which are upper bounds on the regret of UCBVI-CH and
UCBVI-BF algorithms. We assume Assumption 1 holds.
Theorem 1 (Regret bound for UCBVI-CH ).
Consider a parameter δ > 0. Then the regret of
UCBVI-CH is bounded w.p. at least 1 − δ, by
√
Regret(K) ≤ 20H 3/2 L SAK + 250H 2 S 2 AL2 ,
where L = ln(5HSAT /δ).
For T ≥ HS 3 A and√SA ≥ H this bound translates to a
e
regret bound of O(H
SAT ), where T = KH is the total
number of time-steps at the end of episode K.
Theorem 1 is significant in that,√for large T , it improves
the regret dependence from S to S, compared to the best
known bound of (Jaksch et al., 2010). The main intuition
for this improved S-dependence is that we bound the estimation error of the next-state value function directly, instead of the transition probabilities. More precisely, instead of bounding the estimation error (Pbkπk − P πk )Vk,h+1
by kPbkπk − P πk k1 kVk,h+1 k∞ (as is done in (Jaksch et al.,
∗
2010) for example), we bound (Pbkπk − P πk )Vh+1
instead
(for which a bound with no dependence on S can be
achieved since V ∗ is deterministic) and handle carefully
∗
the correction term (Pbkπk − P πk )(Vk,h+1 − Vh+1
).
Our second result, Theorem 2, demonstrates that we can
improve upon the H-dependence by using a more refined,
Bernstein-Friedman-type, exploration bonus.
Theorem 2 (Regret bound for UCBVI-BF ).
Consider a parameter δ > 0. Then the regret of
UCBVI-BF is bounded w.p. 1 − δ, by
√
Regret(K) ≤ 30HL SAK + 2500H 2 S 2 AL2
√
+4H 3/2 KL,
where L = ln(5HSAT /δ).

Minimax Regret Bounds for Reinforcement Learning

We note that for T ≥ H 3 S 3 A and
√ SA ≥ H this bound
e HSAT ). This result
translates to a regret bound of O(
is particularly significant since,√for T large enough (i.e.,
e HSAT ) which matches
T ≥ H 3 S 3 A), our bound is O(
√
the established lower bound Ω( HSAT ) of (Jaksch et al.,
2010; Osband & Van Roy, 2016a) up to logarithmic factors.
The key insight is to apply concentration inequalities to
bound the estimation errors and the exploration bonuses in
terms of the variance of V ∗ at the next state. We then use
the fact that the sum of these variances is bounded by the
variance of the return (see e.g., Munos & Moore, 1999;
Azar et al., 2013; Lattimore & Hutter, 2012),
√ which shows
that the estimation errors accumulate as H instead of linearly in H, thus implying the improved H-dependence.

simplify notations in this sketch of proof we will not make
the numerical constants explicit, and instead we will denote by  a numerical constant which can vary from line
to line. The exact values of these constants are provided
in the full proof. We will also make use of simplified notations, such as using L to represent the logarithmic term
L = ln(HSAT /δ).
def

The cumulative regret at episode K is Regret(K) =
P
def
πk
∗
^
=
k,1 ). Define Regret(K)
P1≤k≤K V1 (xk,1 ) − V1 π(x
k
Under Ω we have
1≤k≤K Vk,1 (xk,1 ) − V1 (xk,1 ).
^
^
Regret(K) ≤ Regret(K),
so we now bound Regret(K).
def
def
π
e k,h = Vk,h − V πk . Thus
Define ∆k,h = V ∗ − V k and ∆
h

e k,h =
≤ ∆

∆k,h
Computational efficiency
Theorems 1 and 2 guarantee the statistical efficiency of
UCBVI. In addition, both UCBVI-CH and UCBVI-BF
are computationally tractable. Each episode both algorithms perform an optimistic value iteration with computational cost of the same order as solving a known MDP. In
fact, the computational cost of these algorithms can be further reduced by only selectively recomputing UCBVI after sufficiently many observations. This technique is common to the literature (Jaksch et al., 2010; Dann & Brunskill,
2015) and would not affect the Õ statistical efficiency. The
computational cost of this variant of UCBVI then amounts
e
to O(SA
min(SA, T ) min(T, S)) as it only needs to upe
date the model O(SA)
times (Jaksch et al., 2010).
Weakly communicating MDPs
In this short paper we focus on the setting of finite horizon
MDPs. By comparison, previous optimistic approaches to
exploration, such as UCRL2, provide bounds for the more
general setting of weakly communicating MDPs (Jaksch
et al., 2010; Bartlett & Tewari, 2009). However, we believe
that much of the insight from the UCBVI algorithm (and
its analysis) will carry over to this more general setting using existing techniques such as ‘the doubling trick‘ (Jaksch
et al., 2010).

5. Proof sketch

5.1. Sketch Proof of Theorem 1
Vh∗ , ∀k, h}

Let Ω = {Vk,h ≥
be the event under which
all computed Vk,h values are upper bounds on the optimal
value function. Using backward induction on h (and standard concentration inequalities) one can prove that Ω holds
with high probability (see Lem. 18 in the appendix). To

(Pbkπk −

=

h

πk
Pbkπk Vk,h+1 + bk,h − P πk Vh+1
e k,h+1 + bk,h .
P πk )Vk,h+1 + P πk ∆

The difficulty in bounding (Pbkπk − P πk )Vk,h+1 is that both
Vk,h+1 and Pbkπk are random variables and are not independent (the value function Vk,h+1 computed at h + 1
may depend on the samples collected from state xh,k ),
thus a straightforward application of Chernoff-Hoeffding
(CH) inequality does not work here. In (Jaksch et al.,
2010), this issue is addressed by bounding it √
by kPbkπk −
P πk k1 kVk,h+1 k∞ at the price of an additional S.
√
e
The main contribution
of our O(H
SAT ) bound (which
√
removes a S factor compared to the previous bound of
(Jaksch et al., 2010)) is to handle this term more properly.
Instead of directly bounding (Pbkπk −P πk )Vk,h+1 , we bound
∗
(Pbkπk −P πk )Vh+1
, using straightforward application of CH
√
∗
(which removes the S factor since Vh+1
is deterministic),
π
and deal with the correction term (Pbk k − P πk )(Vk,h+1 −
∗
Vh+1
). We have
e k,h
∆

=

∗
(Pbkπk − P πk )(Vk,h+1 − Vh+1
)
πk e
+P ∆k,h+1 + bk,h + ek,h ,

def
∗
where ek,h = (Pbkπk − P πk )Vh+1
(xk,h ) is the estimation
error of the optimal value function at the next state. Defindef e
ing δek,h = ∆
k,h (xk,h ), we have

δek,h

Here we provide the sketch proof of our results. The full
proof is deferred to the appendix.

h

≤ (Pbkπk − P πk )∆k,h+1 (xk,h ) + δek,h+1
+k,h + bk,h + ek,h ,
def

where k,h = P πk ∆k,h+1 (xk,h ) − ∆k,h+1 (xk,h+1 ).
Step 1: bound on the correction term (Pbkπk −
P πk )∆k,h+1 (xk,h ). Using Bernstein’s inequality (B),
this term is bounded by
s
X
L
SHL
πk
∆k,h+1 (y) +
,
P (y|xk,h )
π
k
P (y|xk,h )nk,h
nk,h
y

Minimax Regret Bounds for Reinforcement Learning
def

where nk,h = Nk (xk,h , πk (xk,h )). Now considering only
the y such that P πk (y|xk,h )nk,h ≥ H 2 L, and since
e k,h+1 , then (Pbπk − P πk )∆k,h+1 (xk,h ) is
0 ≤ ∆k,h+1 ≤ ∆
k
bounded by

Step 3: Bounding the exploration bonuses
Using the pigeon-hole principle, we have
s
X
X
1
bk,h = HL
nk,h
k,h

s
¯k,h +

L
SHL
δek,h+1 +
P πk (xk,h+1 |xk,h )nk,h
nk,h

where ¯k,h =
√

q

L
nk,h



ek,h+1
δ
P πk (xk,h+1 |xk,h )

P

y

P

πk

=
≤

s
y

(y|xk,h ) √

P

πk

(y|xk,h )

−

.

P πk (y|xk,h )nk,h L
∆k,h+1 (y) ≤
n2k,h

SH 2 L
.
nk,h

Neglecting this term (and the smaller order term
SHL/nk,h ) for now (by the pigeon-hole principle we
can prove that these terms contribute to the final regret by
a constant at most S 2 AH 2 L2 ), we have
δek,h

≤
≤


1 e
1+
δk,h+1 + k,h + ¯k,h + bk,h + ek,h
H
H−1


1 H X 
k,i + ¯k,i + bk,i + ek,i .
1+
|
{zH } i=h
≤e

(k,h + ¯k,h + bk,h + ek,h ).

P
Step
2: Bounding the martingales
k,h k,h and
P

¯
.
Using
Azuma’s
inequality
we
deduce
k,h
k,h

k,h

k,h

(Az)

¯k,h ≤

√
T L.

(2)

P

The proof of Theorem 1 relied on proving by a straightforward induction over h that Ω = {Vk,h ≥ Vh∗ , ∀k, h} hold
with high probability. In the case of exploration bonuses
defined by:

s


LVY ∼Pbπk (·|x,a) Vk,h+1 (Y )
k

bk,h (x, a) =

(1)

We
those 4 terms. It is easy to check that
P now boundP

and
¯k,h are sums of martingale differk,h k,h
k,h 
ences, which are bounded
√ using Azuma’s inequality, and
e
lead to a regret of O(H
T ) without dependence on the
size of state and action space. The leading terms in the regret
P bound comes from the sum of the
Pexploration bonuses
b
and
the
estimation
errors
k,h k,h
k,h ek,h .

X

(3)

5.2. Sketch Proof of Theorem 2

+

+

HL
Nk (x, a)
}

Nk (x, a)
{z
empirical Bernstein
v


u
u min H 3 S 2 AL2 P Pbk0 (y|x,a) , H 2
y N
t
(y)
k,h+1

|

(Az)
√
k,h ≤ H T L,

HL SAT .

Putting everything together: Plugging (2) and (3) into
(1) (and adding the smaller order term) we deduce
√

3
^
Regret(K) ≤ Regret(K)
≤  H 2 L SAK + H 2 S 2 AL2 .

k,h

X

n=1

1
n

Using CH, w.h.p. we have ek,h = (Pbkπk −
q
L
. Thus this bound on the estimation errors are
H nk,h
of the same order as the exploration bonuses (which is the
reason we choose those bonuses...).

The regret is thus bounded by
X

√

r

k,h ek,h .
(CH)
∗
P πk )Vh+1
≤

|

^
Regret(K)
≤

(x,a)
X NKX

Step 4: Bounding on the estimation errors

The sum over the neglected y such that P πk (y|xk,h )nk,h <
H 2 L contributes to an additional term
X

HL

x,a

e k,h+1 (y)
∆

k,h bk,h :

k,h

1
SHL
≤ ¯k,h + δek,h+1 +
,
H
nk,h
def

P

,
}

Nk (x, a)
{z
additional bonus

(4)
the backward induction over h is not straightforward. In∗
deed, if the Vk,h+1 are upper bounds on Vh+1
, it is not necessarily the case that the empirical variance of Vk,h+1 are
∗
upper bound on the empirical variance of Vh+1
. However
we can prove by (backward) induction over h that Vk,h+1 is
∗
sufficiently close to Vh+1
to guarantee that the variance of
those terms are sufficiently close to each other so that the
additional bonus (additional bonus in (4)) will make sure
that Vk,h is still an upper-bound on Vh∗ . More precisely,
define the set of indices:
[k, h]hist

def

=

{(i, j), s.t.(1 ≤ i ≤ k ∧ 1 ≤ j ≤ H)
∨(i = k ∧ h < j ≤ H)},

Minimax Regret Bounds for Reinforcement Learning
def

and the event Ωk,h = {Vi,j ≥ Vh∗ , (i, j) ∈ [k, h]hist }.
Our induction is the following:
• Assume that Ωk,h holds. q Then we prove that
∗
(Vk,h+1 − Vh+1
)(y) ≤ H N 0 SAL(y) .

πk at any episode k) of the next-state values (for that policy), and then using recursively the Law of Total Variance
to conclude that this quantity is nothing but the variance of
the returns. This step is detailed now. For simplicity of the
exposition of this sketch we neglect second order terms.

k,h+1

• We

deduce

H 3 S 2 AL2

that

P

VY ∼Pbk (·|x,a) Vk,h+1 (Y )

bk (y|x,a)
P
0
y Nk,h+1
(y)

≥ VY ∼Pbk (·|x,a)



+

∗
Vh+1
(Y ) ,

so the additional bonus compensates for the possible
variance difference. Thus Vk,h ≥ Vh∗ and Ωk,h−1
holds.
So in order to prove that all values computed by the
algorithm are upper bounding V ∗ , we just need to
∗
prove that under Ωk,h , we have (Vk,h+1 − Vh+1
)(y) ≤
q
A
,
H),
which
is
obtained
by demin(H 1.5 SL N 0
(y)
k,h+1

riving the following regret bound on
ek,h (y)
R

def

=

X

(Vi,h+1 −

πi
Vh+1
)(xi,h+1 )I{xi,h+1

= y}

i≤k

≤

HL

q

0
SANk,h+1
(y).

(5)

Indeed, since {Vi,h }i is a decreasing sequence in i, we have
(Vk,h+1 −

∗
Vh+1
)(y)

≤

0
ek,h+1 (y)/Nk,h+1
R
(y)

≤ HL

q

0
SA/Nk,h+1
(y).

Once we have proven that w.h.p., all computed values are
upper bounds on V ∗ (i.e. event Ω), then we prove that under
Ω, the following regret bound holds:
√
^
Regret(K) ≤ Regret(K)
≤ (HL SAK + H 2 S 2 AL2 ).
(6)
The proof of (5) relies on the same derivations as those
used for proving (6). The only two differences being that
0
(i) HK is replaced by Nk,h+1
(y), the number of times a
state y was reached
at
time
h
+
1, up to episode k, and (ii)
√
the additional H factor which comes from the fact that
0
at any episode, Nk,h+1
(y) can only tick once, whereas the
total number of transitions from y during any episode can
be as large as H. The full proof of (5) will be given in
details in the appendix. We now give a proof sketch of (6)
under Ω.
Similar steps used for proving Theorem 1 apply. The main
difference compared to Theorem 1 is the bound on the sum
of the exploration bonuses and the estimation errors (which
we consider √
in Steps 3’ and 4’ below). This is where we can
remove the H factor. The use of the Bernstein inequality makes it possible to bound both of those terms in terms
of the expected sum of variances (under the current policy

Step 3’: Bounding the sum of exploration bonuses bk,h .
We have
s

X
√ X VY ∼Pbπk (·|xk,h ) Vk,h+1 (Y )
h
bk,h =  L
nk,h
k,h
k,h
|
{z
}
main term
v


u
P Pbk (y|x,a)
2
u min H 3 S 2 AL2
,
H
0
X L
y Nk,h+1 (y)
t
.
+
+
Nk (x, a)
Nk (x, a)
k,h
{z
}
|
second order term

By Cauchy-Schwarz, the main term is bounded by
P
1/2
P
def
1
b
b k,h+1
, where V
=
k,h Vk,h+1
k,h nk,h

P
1
π
≤
VY ∼Pb k (·|xk,h ) Vk,h+1 (Y ) .
Since
k,h nk,h
h
SA ln(T ) by the pigeon-hole principle, we now focus on
P b
the term k,h V
k,h+1 .
def
b k,h+1 is close to Vπk
We now prove that V
=
k,h+1

π
k
π
VY ∼P k (·|xk,h ) Vh+1 (Y ) by bounding the following
h
quantity:

b k,h+1 − Vπk
V
k,h+1
=

2
Pbπk Vk,h+1
− (Pbπk Vk,h+1 )2
πk 2
πk 2
−P πk (Vh+1
) + (P πk Vh+1
)

(i)

≤
(ii)

≤

πk 2
2
∗
Pbπk Vk,h+1
− P πk (Vh+1
) + 2H(P πk − Pbπk )Vh+1
πk 2
2
2
+ P πk (Vk,h+1
− (Vh+1
) )
(Pbπk − P πk )Vk,h+1
{z
} |
|
{z
}
(ak,h )

s
+H 2

L
,
nk,h

(a0k,h )

(7)

where (i) holds since under Ωk,h , Vk,h ≥ Vh∗ ≥ Vhπk and
(ii) holds due to Chernoff Hoeffding.
P b
πk
Step 3’-a: bounding k,h V
k,h+1 −Vk,h+1 . Using similar argument as those used in (Jaksch et al., 2010), we have
that
q
ak,h ≤ H 2 kPbπk − P πk k1 ≤ H 2 SL/nk,h ,
def

(where nk,h = Nk (xk,h , πk (xk,h ))). Thus
from the
√
P
pigeon-hole principle, k,h ak,h ≤ H 2 S AT L.

Minimax Regret Bounds for Reinforcement Learning

X

Now a0k,h is bounded as
a0k,h

k,h

≤ 2HP πk (Vk,h − Vhπk )
b k,h .
2HP πk ∆

=

Thus using Azuma’s inequality,
X

(Az)

a0k,h

X

√
δbk,h+1 + H 2 T L

≤

2H

≤

√
2H U + H 2 T L,

k,h

k,h

2

where U is defined as an upper-bound on the pseudo regret:
√
def P
U = k,h (bk,h + ek,h ) + H T (an upper bound on the
r.h.s. of (1)).
Step 3’-b: bounding

P

k,h

k
Vπk,h+1

(F r)

≤

k,h

i
X hX π
√
k
Vk,h+1
E
|Hk + H 2 T L
h

k

√
T H + H 2 T L.

≤

p
From (1) √
we see that U ≤ L (T H + H 2 U )SA thus
U ≤ (L HSAT + H 2 SAL2 ). This implies (6).
√
So the reason we are able to remove the H factor from
the regret bound comes from the fact that the sum, over H
steps, of the variances of the next state values (which define the amplitude of the confidence intervals) is at most
bounded by the variance of the return. Intuitively this
means that the size of the confidence √
intervals do not add
up linearly over H steps but grows as H only. Although
the sequence of estimation errors are not independent over
time, we are able to demonstrate a concentration of measure phenomenon that shows that those estimation errors
concentrate as if they were independent.

k
Vπk,h+1
. (Dominant term)

P
k
For any episode k, E[ h Vπk,h+1
|Hk ] is the expected sum
of variances of the value function Vkπ (y) at the next state
y ∼ P πk (·|xk,h ) under the true transition model for the
current policy. A recursive application of the law of total variance (see e.g., Munos & Moore, 1999; Azar et al.,
2013; Lattimore & Hutter, 2012) shows that this quantity is
nothing else than the variance
 H reP of the return (sum of
wards) under policy πk : V
h r(xk,h , πk (xk,h )) , which
is thus bounded byP
H 2 . Finally, using Freedman’s (Fr) ink
equality to bound k,h Vπk,h+1
by its expectation (see the
exact derivation in the appendix), we deduce
X

p
ek,h ≤ L (T H + H 2 U )SA.

(8)

P
P
Thus, using (8), (7) and the bounds on ak,h and a0k,h ,
we deduce that
X
p
bk,h ≤ L (T H + H 2 U )SA.

6. Conclusion
In this paper we refine the familiar concept of optimism
in the face of uncertainty. Our key contribution is the design and analysis of the algorithm UCBVI-BF , which addresses two key shortcomings in existing algorithms for optimistic exploration in finite MDPs. First we apply a concentration to the value as a whole, rather than √
the transition
estimates, this leads to a reduction from S to S. Next we
apply a recursive law of total variance to couple estimates
across an episode, rather than at each time
√ step individually, this leads to a reduction from H to H.
Theorem 2 provides the first regret bounds which, for sufficiently
large T , match the lower bounds for the problem
√
e HSAT ) up to logarithmic factors. It remains an open
O(
problem whether we can match the lower bound using this
approach for small T . We believe that the higher order term
2
e 2 S 2 A) to O(HS
e
can be improved from O(H
A) by a more
careful analysis, i.e., a more extensive use of FreedmanBernstein
√ inequalities. The same applies
√ to the term of order H T which can be improved to HT .

k,h

Step
4’: Bounding the sum of estimation errors
P
e
k,h k,h . We now use Bernstein inequality to bound the
estimation errors
X
X π
∗
ek,h =
(Pbk k − P πk )Vh+1
(xk,h )
k,h

k,h

s
≤

X
k,h



V∗k,h+1
nk,h

+

HL
,
nk,h


def
∗
where V∗k,h+1 = VY ∼P πk (·|xk,h ) Vh+1
(Y ) . Now, in a
very similar way as in Step 3’ above, we relate V∗k,h+1
k
to Vπk,h+1
and use the Law of total variance to bound
P
πk
V
k,h k,h+1 by HT and deduce that

These results are particularly significant because they help
to estabilish the information-theoretic
lower bound of rein√
forcement learning at Ω( HSAT ) (Osband & Van Roy,
2016a), whereas it was suggested in
√ some previous work
that lower-bound should be of Ω(H SAT ). Moving from
this big-picture insight to an analytically rigorous bound is
non-trivial. Although we push many of the technical details
to the appendix, our paper also makes several contributions
in terms of analytical tools that may be useful in subsequent
work. In particular we believe that the way we construct the
exploration bonus and confidence intervals in UCBVI-CH
is novel to the literature of RL. Also the constructive approach in the proof of UCBVI-CH, which bootstraps the
regret bounds to prove that Vk,h s are ucbs, is another analytical contribution of this paper.

Minimax Regret Bounds for Reinforcement Learning

Acknowledgements

Dann, Christoph and Brunskill, Emma. Sample complexity
of episodic fixed-horizon reinforcement learning. In Advances in Neural Information Processing Systems, 2015.

The authors would like to thank Marc Bellemare and all the
other wonderful colleagues at DeepMind for many hours of
discussion and insight leading to this research. We are also
grateful for the anonymous reviewers for their helpful comments and for fixing several mistakes in an earlier version
of this paper.

Dann, Christoph, Lattimore, Tor, and Brunskill, Emma.
Ubev-a more practical algorithm for episodic rl with
near-optimal pac and regret guarantees. arXiv preprint
arXiv:1703.07710, 2017.

References

Freedman, David A. On tail probabilities for martingales.
the Annals of Probability, pp. 100–118, 1975.

Agrawal, Shipra and Jia, Randy. Posterior sampling for
reinforcement learning: worst-case regret bounds. arXiv
preprint arXiv:1705.07041, 2017.
Azar, Mohammad Gheshlaghi, Munos, Rémi, and Kappen,
Hilbert J. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model.
Machine learning, 91(3):325–349, 2013.
Bartlett, Peter L. and Tewari, Ambuj. REGAL: A regularization based algorithm for reinforcement learning in
weakly communicating MDPs. In Proceedings of the
25th Conference on Uncertainty in Artificial Intelligence
(UAI2009), pp. 35–42, June 2009.
Bernstein, S. Theory of probability, 1927.
Bertsekas, D. P. Dynamic Programming and Optimal
Control, volume I. Athena Scientific, Belmount, Massachusetts, third edition, 2007.

Guez, Arthur, Silver, David, and Dayan, Peter. Scalable and efficient bayes-adaptive reinforcement learning
based on monte-carlo tree search. Journal of Artificial
Intelligence Research, pp. 841–883, 2013.
Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret
bounds for reinforcement learning. Journal of Machine
Learning Research, 11:1563–1600, 2010.
Kearns, Michael J. and Singh, Satinder P. Near-optimal
reinforcement learning in polynomial time. Machine
Learning, 49(2-3):209–232, 2002.
Lattimore, Tor and Hutter, Marcus. PAC bounds for discounted MDPs. CoRR, abs/1202.3890, 2012.
Maurer, Andreas and Pontil, Massimiliano. Empirical
bernstein bounds and sample variance penalization. stat,
1050:21, 2009.

Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Programming. Athena Scientific, Belmont, Massachusetts,
1996.

Munos, R. and Moore, A. Influence and variance of a
Markov chain : Application to adaptive discretizations in
optimal control. In Proceedings of the 38th IEEE Conference on Decision and Control, 1999.

Brafman, Ronen I. and Tennenholtz, Moshe. R-max - a
general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213–231, 2002.

Munos, Rémi. From bandits to Monte-Carlo Tree Search:
The optimistic principle applied to optimization and
R in Machine Learnplanning. Foundations and Trends
ing, 7(1):1–129, 2014.

Bubeck, Sébastien and Cesa-Bianchi, Nicolò. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. CoRR, abs/1204.5721, 2012. URL http:
//arxiv.org/abs/1204.5721.

Osband, Ian and Van Roy, Benjamin. On lower bounds for
regret in reinforcement learning. stat, 1050:9, 2016a.

Bubeck, Sébastien, Munos, Rémi, Stoltz, Gilles, and
Szepesvári, Csaba. X-armed bandits. Journal of Machine Learning Research, 12:1587–1627, 2011.
Burnetas, Apostolos N and Katehakis, Michael N. Optimal
adaptive policies for markov decision processes. Mathematics of Operations Research, 22(1):222–255, 1997.
Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and
Games. Cambridge University Press, New York, NY,
USA, 2006.

Osband, Ian and Van Roy, Benjamin. Why is posterior
sampling better than optimism for reinforcement learning. arXiv preprint arXiv:1607.00215, 2016b.
Osband, Ian, Russo, Dan, and Van Roy, Benjamin. (more)
efficient reinforcement learning via posterior sampling.
In Advances in Neural Information Processing Systems,
pp. 3003–3011, 2013.
Strehl, Alexander L and Littman, Michael L. A theoretical
analysis of model-based interval estimation. In Proceedings of the 22nd international conference on Machine
learning, pp. 856–863. ACM, 2005.

Minimax Regret Bounds for Reinforcement Learning

Strehl, Alexander L and Littman, Michael L. An analysis
of model-based interval estimation for markov decision
processes. Journal of Computer and System Sciences, 74
(8):1309–1331, 2008.
Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Langford, John, and Littman, Michael L. PAC model-free
reinforcement learning. In ICML, pp. 881–888, 2006.
Strens, Malcolm J. A. A Bayesian framework for reinforcement learning. In ICML, pp. 943–950, 2000.
Sutton, Richard and Barto, Andrew. Reinforcement Learning: An Introduction. MIT Press, March 1998.
Thompson, W.R. On the likelihood that one unknown probability exceeds another in view of the evidence of two
samples. Biometrika, 25(3/4):285–294, 1933.
Weissman, Tsachy, Ordentlich, Erik, Seroussi, Gadiel,
Verdu, Sergio, and Weinberger, Marcelo J. Inequalities for the l1 deviation of the empirical distribution.
Hewlett-Packard Labs, Tech. Rep, 2003.

