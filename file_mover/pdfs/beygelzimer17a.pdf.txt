√
Efﬁcient Online Bandit Multiclass Learning with Õ( T ) Regret
Alina Beygelzimer 1 Francesco Orabona 2 Chicheng Zhang 3

Abstract
We present
√ an efﬁcient second-order algorithm
with Õ( η1 T )1 regret for the bandit online multiclass problem. The regret bound holds simultaneously with respect to a family of loss functions
parameterized by η, for a range of η restricted
by the norm of the competitor. The family of
loss functions ranges from hinge loss (η = 0)
to squared hinge loss (η = 1). This provides a
solution to the open problem of (Abernethy, J.
and √
Rakhlin, A. An efﬁcient bandit algorithm
for T -regret in online multiclass prediction?
In COLT, 2009). We test our algorithm experimentally, showing that it also performs favorably
against earlier algorithms.

1. Introduction
In the online multiclass classiﬁcation problem, the learner
must repeatedly classify examples into one of k classes. At
each step t, the learner observes an example xt ∈ Rd and
predicts its label ỹt ∈ [k]. In the full-information case,
the learner observes the true label yt ∈ [k] and incurs loss
1[ỹt �= yt ]. In the bandit version of this problem, ﬁrst considered in (Kakade et al., 2008), the learner only observes
its incurred loss 1[ỹt �= yt ], i.e., whether or not its prediction was correct. Bandit multiclass learning is a special
case of the general contextual bandit learning (Langford &
Zhang, 2008) where exactly one of the losses is 0 and all
other losses are 1 in every round.
The goal of the learner is to minimize its regret with respect to the best predictor in some reference class of predictors, that is the difference between the total number of
mistakes the learner makes and the total number of mis1
Yahoo Research, New York, NY 2 Stony Brook University,
Stony Brook, NY 3 University of California, San Diego, La Jolla,
CA. Correspondence to: Alina Beygelzimer <beygel@yahooinc.com>, Francesco Orabona <francesco@orabona.com>,
Chicheng Zhang <chichengzhang@ucsd.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
Õ(·) hides logarithmic factors.

takes of the best predictor in the class. Kakade et al. (2008)
proposed a bandit modiﬁcation of the Multiclass Perceptron algorithm (Duda & Hart, 1973), called the Banditron,
that uses a reference class of linear predictors. Note that
even in the full-information setting, it is difﬁcult to provide
a true regret bound. Instead, performance bounds are typically expressed in terms of the total multiclass hinge loss
of the best linear predictor, a tight upper bound on 0-1 loss.
The Banditron, while computationally efﬁcient, achieves
only O(T 2/3 ) expected regret with respect to this loss,
where T is the number of rounds. This is suboptimal as√the
Exp4 algorithm of Auer et al. (2003) can achieve Õ( T )
regret for the 0-1 loss, albeit very inefﬁciently. Abernethy
& Rakhlin (2009) posed an open problem: Is there an efﬁcient bandit multiclass
√ learning algorithm that achieves
expected regret of Õ( T ) with respect to any reasonable
loss function?
The ﬁrst attempt to solve this open problem was by Crammer & Gentile (2013). Using a stochastic assumption about
the mechanism
generating the labels, they were able to
√
show a Õ( T ) regret, with a second-order algorithm.
Later, Hazan & Kale (2011), following a suggestion by
Abernethy & Rakhlin (2009), proposed the use of the logloss coupled with a softmax prediction. The softmax depends on a parameter that controls the smoothing factor.
The value of this parameter determines the exp-concavity
of the loss, allowing Hazan & Kale (2011) to prove
worst-case regret bounds that range between O(log T ) and
2
O(T 3 ), again with a second-order algorithm. However, the
choice of the smoothing factor in the loss becomes critical
in obtaining strong bounds.
The original Banditron algorithm has been also extended
in many ways. Wang et al. (2010) have proposed a variant
based on the exponentiated gradient algorithm (Kivinen &
Warmuth, 1997). Valizadegan et al. (2011) proposed different strategies to adapt the exploration rate to the data in
the Banditron algorithm. However, these algorithms suffer
from the same theoretical shortcomings as the Banditron.
There has been signiﬁcant recent focus on developing efﬁcient algorithms for the general contextual bandit problem (Dudı́k et al., 2011; Agarwal et al., 2014; Rakhlin &
Sridharan, 2016; Syrgkanis et al., 2016a;b). While solving

Efﬁcient Online Bandit Multiclass Learning

a more general problem that does not make assumptions
on the structure of the reward vector or the policy class,
these results assume that contexts or context/reward pairs
are generated i.i.d., or the contexts to arrive are known beforehand, which we do not assume here.
In this paper, we follow a different route. Instead of designing an ad-hoc loss function that allows us to prove strong
guarantees, we propose an algorithm that simultaneously
satisﬁes a regret bound with respect to all the loss functions
in a family of functions that are tight upper bounds to the 01 loss. The algorithm, named Second Order Banditron Algorithm (SOBA), is efﬁcient and based on the second-order
Perceptron algorithm (Cesa-Bianchi
√ et al., 2005). The regret bound is of the order of Õ( T ), providing a solution
to the open problem of Abernethy & Rakhlin (2009).

2. Deﬁnitions and Settings
We ﬁrst introduce our notation. Denote the rows of a matrix
V ∈ Rk×d by v1 , v2 , . . . , vk . The vectorization of V is
deﬁned as vec(V ) = [v1 , v2 , . . . , vk ]T , which is a vector
in Rkd . We deﬁne the reverse operation of reshaping a kd×
1 vector into a k × d matrix by mat(V ), using a row-major
order. To simplify notation, we will use V and vec(V )
interchangeably throughout the paper. For matrices A and
B, denote by A⊗B their Kronecker product. For matrices
X
� and Y of the same dimension, denote by �X, Y � =
i,j Xi,j Yi,j their inner product. We use � · � to denote
the �2 norm of a vector, and � · �F to denote the Frobenius
norm of a�
matrix. For a positive deﬁnite matrix A, we use
�x�A = �x , Ax� to denote the Mahalanobis norm of x
with respect to A. We use 1k to denote the vector in Rk
whose entries are all 1s.
We use Et−1 [·] to denote the conditional expectation given
the observations up to time t − 1 and xt , yt , that is, x1 , y1 ,
ỹ1 , . . . , xt−1 , yt−1 , ỹt−1 , xt , yt .
Let [k] denote {1, . . . , k}, the set of possible labels. In our
setting, learning proceeds in rounds:
For t = 1, 2, . . . , T :
1. The adversary presents an example xt ∈ Rd to the
learner, and commits to a hidden label yt ∈ [k].

2. The learner predicts a label ỹt ∼ pt , where pt ∈
Δk−1 is a probability distribution over [k].
3. The learner receives the bandit feedback 1[ỹt �= yt ].

The goal of the learner
�T is to minimize the total number of
mistakes, MT = t=1 1[ỹt �= yt ].

We will use linear predictors speciﬁed by a matrix
W ∈ Rk×d . The prediction is given by W (x) =
arg maxi∈[k] (W x)i , where (W x)i is the ith element of

the vector W x, corresponding to class i.
A useful notion to measure the performance of a competitor
U ∈ Rk×d is the multiclass hinge loss
�(U , (x, y)) := max[1 − (U x)y + (U x)i ]+ ,
i�=y

(1)

where [·]+ = max(·, 0).

3. A History of Loss Functions
As outlined in the introduction, a critical choice in obtaining strong theoretical guarantees is the choice of the loss
function. In this section we introduce and motivate a family of multiclass loss functions.
In the full information setting, strong binary and multiclass
mistake bounds are obtained through the use of the Perceptron algorithm (Rosenblatt, 1958). A common misunderstanding of the Perceptron algorithm is that it corresponds
to a gradient descent procedure with respect to the (binary
or multiclass) hinge loss. However, it is well known that
the Perceptron simultaneously satisﬁes mistake bounds that
depend on the cumulative hinge loss and also on the cumulative squared hinge loss, see for example Mohri & Rostamizadeh (2013). Note also that the squared hinge loss is
not dominated by the hinge loss, so, depending on the data,
one loss can be better than the other.
We show that the Perceptron algorithm satisﬁes an even
stronger mistake bound with respect to the cumulative loss
of any power of the multiclass hinge loss between 1 and 2.
Theorem 1. On any sequence (x1 , y1 ), . . . , (xT , yT ) with
�xt � ≤ X for all t ∈ [T ], and any linear predictor U ∈
Rk×d , the total number of mistakes MT of the multiclass
Perceptron satisﬁes, for any q ∈ [1, 2],
√ �
1− 1 q1
(U ) + �U �F X 2 MT ,
MT ≤ MT q LMH,q
�T
where LMH,q (U ) = t=1 �(W , (xt , yt ))q . In particular,
it simultaneously satisﬁes the following:
√ �
MT ≤ LMH,1 (U ) + 2X 2 �U �2F + X�U �F 2 LMH,1 (U )
√ �
MT ≤ LMH,2 (U )+2X 2 �U �2F +X�U �F 2 2 LMH,2 (U ) .
For the proof, see Appendix B.

A similar observation was done by Orabona et al. (2012)
who proved a logarithmic mistake bound with respect to
all loss functions in a similar family of functions smoothly
interpolating between the hinge loss to the squared hinge
loss. In particular, Orabona et al. (2012) introduced the
following family of binary loss functions
�
η
2
1 − 2−η
x + 2−η
x2 , x ≤ 1
(2)
�η (x) :=
0,
x > 1.

Efﬁcient Online Bandit Multiclass Learning

Figure 1. Plot of the loss functions in �η for different values of η.

Algorithm 1 Second Order Banditron Algorithm (SOBA)
Input: Regularization parameter a > 0, exploration parameter γ ∈ [0, 1].
1: Initialization: W1 = 0, A0 = aI, θ0 = 0
2: for t = 1, 2, . . . , T do
3:
Receive instance xt ∈ Rd
4:
ŷt = arg maxi∈[k] (Wt xt )i
5:
Deﬁne pt = (1 − γ)eŷt + γk 1k
6:
Randomly sample ỹt according to pt
7:
Receive bandit feedback 1[ỹt �= yt ]
8:
Initialize update indicator nt = 0
9:
if ỹt = yt then
10:
ȳt = arg maxi∈[k]\{yt } (Wt xt )i
1
11:
gt = pt,y
(eȳt − eyt ) ⊗ xt
√ t
12:
zt = pt,yt gt
13:

where 0 ≤ η ≤ 1. Note that η = 0 recovers the binary hinge loss, and η = 1 recovers the squared hinge
loss. Meanwhile, for any 0 ≤ η ≤ 1, �η (x) ≤
max{�0 (x), �1 (x)}, and �η is an upper bound on 0-1 loss:
1[x < 0] ≤ �η (x). See Figure 1 for a plot of the different
functions in the family.
Here, we deﬁne a multiclass version of the loss in (2) as
�
�
�η (U , (x, y)) := �η (U x)y − max(U x)i . (3)
i�=y

Hence, �0 (U , (x, y)) = �(U , (x, y)) is the classical multiclass hinge loss and �1 (U , (x, y)) = �2 (U , (x, y)) is the
squared multiclass hinge loss.
√
Our algorithm has a Õ( η1 T ) regret bound that holds simultaneously for all loss functions in this family, with η in
a range that ensure that (U x)i − (U x)j ≤ 2−η
η , i, j ∈ [k].
We also show that there exists a setting of the parameters of the algorithm
that gives a mistake upper bound of
√
Õ((L∗ T )1/3 + T ), where L∗ is the cumulative hinge loss
of the competitor, which is never worse that the best bounds
in Kakade et al. (2008).

4. Second Order Banditron Algorithm
This section introduces our algorithm for bandit multiclass
online learning, called Second Order Banditron Algorithm
(SOBA), described in Algorithm 1.
SOBA makes a prediction using the γ-greedy strategy: At
each iteration t, with probability 1 − γ, it predicts ŷt =
arg maxi∈[k] (Wt xt )i ; with the remaining probability γ, it
selects a random action in [k]. As discussed in Kakade
et al. (2008), randomization is essential for designing bandit multiclass learning algorithms. If we deterministically
output a label and make a mistake, then it is hard to make an
update since we do not know the identity of yt . However,

�Wt , zt �2 +2�Wt , gt �
1+z T A−1 zt
�t−1t t−1
mt + s=1 ns ms ≥ 0 then

mt =

14:
if
15:
Turn on update indicator nt = 1
16:
end if
17:
end if
18:
Update At = At−1 + nt zt ztT
19:
Update θt = θt−1 − nt gt
20:
Set Wt+1 = mat(A−1
t θt )
21: end for

Remark: matrix At is of dimension kd × kd, and vector
θt is of dimension kd; in line 20, the matrix multiplication
results in a kd dimensional vector, which is reshaped to
matrix Wt+1 of dimension k × d.
if randomization is used, we can estimate yt and perform
online stochastic mirror descent type updates (Bubeck &
Cesa-Bianchi, 2012).
SOBA keeps track of two model �
parameters: cumulative
t
Perceptron-style updates θt = − s=1 ns gs ∈ Rkd and
�t
corrected covariance matrix At = aI + s=1 ns zs zsT ∈
Rkd×kd . The classiﬁer Wt is computed by matricizing over
kd
the matrix-vector product A−1
t−1 θt−1 ∈ R . The weight
vector θt is standard in designing online mirror descent
type algorithms (Shalev-Shwartz, 2011; Bubeck & CesaBianchi, 2012). The matrix At is standard in designing online learning algorithms with adaptive regularization (CesaBianchi et al., 2005; Crammer et al., 2009; McMahan &
Streeter, 2010; Duchi et al., 2011; Orabona et al., 2015).
The algorithm updates its model (nt = 1) only when the
following conditions hold simultaneously: (1) the predicted
label is correct (ỹt = y�
t ), and (2) the “cumulative regulart−1
ized negative margin” ( s=1 ns ms + mt ) is positive if this
update were performed. Note that when the predicted label
is correct we know the identity of the true label.
As we shall see, the set of iterations where nt = 1 includes all iterations where ỹt = yt �= ŷt . This fact is cru-

Efﬁcient Online Bandit Multiclass Learning

cial to the mistake bound analysis. Furthermore, there are
some iterations where ỹt = yt = ŷt but we still make an
update. This idea is related to “online passive-aggressive
algorithms” (Crammer et al., 2006; 2009) in the full information setting, where the algorithm makes an update even
when it predicts correctly but the margin is too small.
Let’s now describe our algorithm more in details. Throughout, suppose all the examples are �2 -bounded: �xt �2 ≤ X.

As outlined above, we associate a time-varying
�t regularizer
Rt (W ) = 12 �W �2At , where At = aI + s=1 ns zs zsT is
a kd × kd matrix and
zt =

√

pt,yt gt = √

1
(eȳt − eyt ) ⊗ xt .
pt,yt

Note that this time-varying regularizer is constructed by
scaled versions of the updates gt . This is critical, because
in expectation this becomes the correct regularizer. Indeed,
it is easy to verify that, for any U ∈ Rk×d ,
Et−1 [1[yt = ỹt ] gt ] = (eȳt − eyt ) ⊗ xt ,
2

2

Et−1 [1[yt = ỹt ] �U , zt � ] = �U , (eȳt − eyt ) ⊗ xt � .

In words, this means that in expectation the regularizer contains the outer products of the updates, that in turn promote
the correct class and demotes the wrong one. We stress
that it is impossible to get the same result with the estimator proposed in Kakade et al. (2008). Also, the analysis
is substantially different from the Online Newton Step approach (Hazan et al., 2007) used in Hazan & Kale (2011).
In reality, we do not make an update in all iterations in
which ỹt = y�
t , since the algorithm need to maintain the
t
invariant that s=1 ms ns ≥ 0, which is crucial to the
proof of Lemma 2. Instead, we prove a technical lemma
that gives an explicit form on the expected update nt gt and
expected regularization nt zt ztT . Deﬁne
� t−1
�
�
n s m s + mt ≥ 0 ,
qt := 1
s=1

ht := 1[ŷt �= yt ] + qt 1[ŷt = yt ] .

Lemma 1. For any U ∈ Rkd ,
Et−1 [nt �U , gt �] = ht �U , (eyt − eȳt ) ⊗ xt � ,
�
�
2
2
Et−1 nt �U , zt � = ht �U , (eyt − eȳt ) ⊗ xt � .
The proof of Lemma 1 is deferred to the end of Subsection 4.1.
Our last contribution is to show how our second order algorithm satisﬁes a mistake bound for an entire family of

loss functions. Finally, we relate the performance of the
algorithm that predicts ŷt to the γ-greedy algorithm.
Putting all together, we have our expected mistake bound
for SOBA. 2
Theorem 2. SOBA has the following expected upper bound
on the number of mistakes, MT , for any U ∈ Rk×d and any
2
),
0 < η ≤ min(1, 2 maxi �u
i �X+1
aη
�U �2F
2−η
T
�
�
�
k
+
E ztT A−1
t zt + γT,
γη(2 − η) t=1

E [MT ] ≤ Lη (U ) +

�T
where Lη (U ) := t=1 �η (U , (xt , yt )) is the cumulative
η-loss of the linear predictor U , and {ui }ki=1 are rows of
U.
�
2
In particular, setting γ = O( k dTln T ) and a = X 2 , we
have
�
�
k√
dT ln T .
E [MT ] ≤ Lη (U ) + O X 2 �U �2F +
η
Note that, differently from previous analyses (Kakade
et al., 2008; Crammer & Gentile, 2013; Hazan & Kale,
2011), we do not need to assume a bound on the norm of
the competitor, as in the full information Perceptron and
Second Order Perceptron algorithms. In Appendix A, we
also present an adaptive variant of SOBA that sets exploration rate γt dynamically, which achieves a regret bound
within a constant factor of that using optimal tuning of γ.
We prove Theorem 2 in the next Subsection, while in Subsection 4.2 we prove a mistake bound with respect to the
hinge loss, that is not fully covered by Theorem 2.
4.1. Proof of Theorem 2
Throughout the proofs, U , Wt , gt , and zt ’s should be
thought of as kd × 1 vectors. We ﬁrst show the following lemma. Note that this is a statement over any sequence
and no expectation is taken.
Lemma 2. For any U ∈ Rkd , with the notation of Algorithm 1, we have:
T
�
t=1

�
�
2
nt 2 �U , −gt � − �U , zt �
≤ a�U �2F +

T
�

nt gtT A−1
t gt .

t=1

2
Throughout the paper, expectations are taken with respect to
the randomization of the algorithm.

Efﬁcient Online Bandit Multiclass Learning

Proof. First, from line 14 of Algorithm 1, it can be seen
(by induction) that SOBA maintains the invariant that
t
�
s=1

ns ms ≥ 0.

Proof. Using Lemma 2 with ηU , we get that
T
�

(4)

We next reduce the proof to the regret analysis of online
least squares problem. For iterations where nt = 1, deﬁne αt = √p1t,y so that gt = αt zt . From the algorithm,
�tt
At = aI + s=1 ns zs zsT , and Wt is the ridge regression solution based
on data collected in time
1 to t − 1, i.e.
�t−1
�t−1
−1
Wt = A−1
t−1 (−
s=1 ns gs ) = At−1 (−
s=1 ns αs zs ).

t=1

≤ aη 2 �U �2F +

t

[T ] such that nt = 1,

t−1 t

T

�
1� �
nt (�Wt , zt � + αt )2 kt − (�U , zt � + αt )2
2 t=1
≤

1
a
1
�U �2A0 − �U − WT +1 �2AT ≤ �U �2F .
2
2
2

We also have by deﬁnition of mt ,
(�Wt , zt � + αt )2 kt − (�U , zt � + αt )2
2

= mt − 2 �U , gt � − �U , zt � − αt2 ztT A−1
t zt .
�T
Putting all together and using the fact that t=1 nt mt ≥ 0,
we have the stated bound.

We can now prove the following
�Tmistake bound for the prediction ŷt , deﬁned as M̂T := t=1 1[ŷt �= yt ].

Theorem 3. For any U ∈ Rk×d , and any 0 < η ≤
2
min(1, 2 maxi �u
), the expected number of mistakes
i �X+1
committed by ŷt can be bounded as
�T
�
�
k t=1 E[nt ztT A−1
aη�U �2F
t zt ]
+
E M̂T ≤ Lη (U ) +
2−η
γη(2 − η)
�
�
2T X 2
2
dk
ln
1
+
2
adk
aη�U �F
+
,
≤ Lη (U ) +
2−η
γη(2 − η)
�T
where Lη (U ) := t=1 �η (U , (xt , yt )) is the η-loss of the
linear predictor U .

nt gtT A−1
t gt .

t=1

t

0 ≤ −E
+E

1
1
2
(�Wt , zt � + αt )2 (1 − ztT A−1
t zt ) − (�U , zt � + αt )
2
2
1
1
≤ �U − Wt �2At−1 − �U − Wt+1 �2At .
2
2

Denoting by kt = 1 − ztT A−1
t zt , by Sherman-Morrison
1
formula, kt = 1+zT A
.
Summing
over all rounds t ∈
−1
z

T
�

Taking expectations, using Lemma 1 and the fact that
1
k
pt,y ≤ γ and that At is positive deﬁnite, we have

By per-step analysis in online least squares, (see, e.g.,
Orabona et al., 2012)(See Lemma 6 for a proof), we have
that if an update is made at iteration t, i.e. nt = 1, then

Otherwise nt = 0, in which case we have Wt+1 = Wt
and At+1 = At .

�
�
2
nt 2η �U , −gt � − η 2 �U , zt �

�

�

T
�
t=1

T
�
t=1

ht · 2η �U , (eyt − eȳt ) ⊗ xt �
2

ht · η (�U , (eyt − eȳt ) ⊗ xt �)

+ aη 2 �U �2F +
Add the terms η(2 − η)E

�

2

�

T
k�
E[nt ztT A−1
t zt ] .
γ t=1

��

T
t=1

�
ht to both sides and di-

vide both sides by η(2 − η), to have
E

�

T
�
t=1

+

�

ht ≤ E

�

T
�
t=1

(5)

ht f (�U , (eyt − eȳt ) ⊗ xt �)

�

T

�
k
aη
�U �2F +
E[nt ztT A−1
t zt ],
2−η
γη(2 − η) t=1

η
2
z + 2−η
z 2 . Taking a close look
where f (z) := 1 − 2−η
at the function f , we observe that the two roots of the
quadratic function are 1 and 2−η
η , respectively. Setting
η ≤ 1, the function is negative in (1, 2−η
η ] and positive in
(−∞, 1]. Additionally, if 0 < η ≤ 2 maxi �u2 i �2 X+1 , then

for all i, j ∈ [k], �U , (ei − ej ) ⊗ xt � ≤
we have that

2−η
η .

Therefore,

f (�U , (eyt − eȳt ) ⊗ xt �)

= f ((U xt )yt − (U xt )ȳt )

≤ �η ((U xt )yt − (U xt )ȳt )
�
�
≤ �η (U xt )yt − max(U xt )r = �η (U , (xt , yt )) .
r�=yt

where the ﬁrst equality is from algebra, the ﬁrst inequality is from that f (·) ≤ �η (·) in (−∞, 2−η
η ], the second
inequality is from that �η (·) is monotonically decreasing.
Putting �
together the two constraints on η, and noting that
T
M̂T ≤ t=1 ht , we have the ﬁrst bound.
The second statement follows from Lemma 3 below.

Efﬁcient Online Bandit Multiclass Learning

Lemma 3. If d ≥ 1, k ≥ 2, T ≥ 2, then
�
�
T
�
2X 2 T
T −1
.
E[nt zt At zt ] ≤ dk ln 1 +
adk
t=1

• yt = ỹt �= ŷt . In this case, ȳt = ŷt , therefore
�Wt , gt � ≥ 0, making mt ≥ 0. This implies that
�t−1
s=1 ns ms + mt ≥ 0, guaranteeing nt = 1.

• yt = ỹt = ŷ�
t . In this case, nt is set to 1 if and only if
t−1
qt = 1, i.e. s=1 ns ms + mt ≥ 0.

Speciﬁcally, if a = X 2 , the right hand side is ≤ dk ln T .
Proof. Observe that

This gives that nt = Gt + Ht .

T
�

|AT |
nt ztT A−1
t zt ≤ ln
|A0 |
t=1


�T
t =yt ]
2X 2 t=1 1[ỹpt,y
t
,
≤ d k ln 1 +
adk

Second, we have the following two equalities:

where the ﬁrst inequality is a well-known fact from linear
algebra (e.g. Hazan et al., 2007, Lemma 11). Given that the
AT is kd × kd, the second inequality comes from the fact
that |AT | is maximized when all its eigenvalues are equal
�T

n �z �

2X 2

2

�T

1[ỹt =yt ]
t=1
pt,y
t

T)
to tr(A
= a + t=1d kt t ≤ a +
dk
dk
Finally, using Jensen’s inequality, we have that,
�
�
T
�
2X 2 T
T −1
.
E[nt zt At zt ] ≤ d k ln 1 +
adk
t=1

.

Proof of Theorem 2. Observe that by triangle inequality,
1[ỹt �= yt ] ≤ 1[ỹt �= ŷt ] + 1[yt �= ŷt ]. Summing over
t, taking expectation on both sides, we conclude that
(6)

The ﬁrst statement follows from combining the above inequality with Theorem 3.
For the second statement, ﬁrst note that from Theorem 3,
and Equation (6), we have
�
�
2T X 2
2
dk
ln
1
+
2
adk
aη�U �F
+
+ γT
E[MT ] ≤ Lη (U ) +
2−η
γη(2 − η)
2d k 2 ln T
+ γT,
≤ Lη (U ) + X 2 �U �2F +
γη
where the second inequality is from that η ≤ 1, and
Lemma 3 with a = �
X 2 . The statement is concluded by

the setting of γ = O(

k2 d ln T
T

= 1[ŷt �= yt ] �U , (eyt − eȳt ) ⊗ xt � ,

Et−1 [Gt �U , gt �]
�
�
1[ỹt = yt ]
= Et−1
1[ŷt = yt ]qt �U , (eyt − eȳt ) ⊗ xt �
pt,yt
= 1[ŷt �= yt ]qt �U , (eyt − eȳt ) ⊗ xt � .

If a = X 2 , then the right hand side is d k ln(1+ d2Tk ), which
is at most dk ln T under the conditions on d, k, T .

E[MT ] ≤ E[M̂T ] + γT .

Et−1 [Ht �U , gt �]
�
�
1[ỹt = yt ]
= Et−1
1[ŷt �= yt ] �U , (eyt − eȳt ) ⊗ xt �
pt,yt

).

The ﬁrst statement follows from adding up the two equalities above.
The proof for the second statement is identical, except replacing �U , (eyt − eȳt ) ⊗ xt � with
2
�U , (eyt − eȳt ) ⊗ xt � .
4.2. Fall-Back Analysis
The loss function �η is an interpolation between the hinge
and the squared hinge losses. Yet, the bound becomes vacuous for η = 0. Hence, in this section √
we show that SOBA
also guarantees a Õ((L0 (U )T )1/3 + T ) mistake bound
w.r.t. L0 (U ), the multiclass hinge loss of the competitor,
assuming L0 (U ) is known. Thus the algorithm achieves
a mistake guarantee no worse than the sharpest bound implicit in Kakade et al. (2008).
Theorem 4. Set a = X 2 and denote by MT the number
of mistakes done by SOBA. Then SOBA has the following
guarantees:3
√
1. If L0 (U ) ≥ (�U �2F + 1) dk 2 X 2 T ln T , then with
2
2
parameter setting γ = min(1, ( dk X LT02(U ) ln T )1/3 ),
one has the following expected mistake bound:

Proof of Lemma 1. We show the lemma in two steps. Let
� ŷt ].
Gt := qt · 1[yt = ỹt = ŷt ], and Ht := 1[yt = ỹt =

First, we show that nt = Gt + �
Ht . Recall that SOBA
t−1
maintains the invariant (4), hence s=1 ns ms ≥ 0. From
line 14 of SOBA, we see that nt = 1 only if ỹt = yt . Now
consider two cases:

3

E[MT ] ≤ L0 (U )
�
�
+ O �U �F (d k 2 X 2 L0 (U )T ln T )1/3 .

Assuming the knowledge of �U �F it would be possible to
reduce the dependency on �U �F in both bounds. However such
assumption is extremely unrealistic and we prefer not to pursue it.

Efﬁcient Online Bandit Multiclass Learning

√
2. If L0 (U ) < (�U �2F + 1) dk 2 X 2 T ln T , then with
2
2
parameter setting γ = min(1, ( d k XT ln T )1/2 ), one
has the following expected mistake bound:
�
�
√
E[MT ] ≤ L0 (U ) + O k(�U �2F + 1)X dT ln T .
�T

where L0 (U ) := t=1 �0 (U , (xt , yt )) is the hinge loss of
the linear classiﬁer U .
Proof. Recall that M̂T the mistakes made by ŷt , that is
�
T
1[ŷ �= yt ]. Adding to both sides of (5) the term
t=1
�T t
ηE[ t=1 ht ] and dividing both sides by η, and plugging
a = X 2 , we get that for all η > 0,
�
� T
� T
�
�
ht ≤ E
ht · (1 − �U , (eyt − eȳt ) ⊗ xt �)
E
t=1

t=1

T
�

η
2
ht · �U , (eȳt − eyt ) ⊗ xt �
+
2
t=1

�

d k2
ηX 2
�U �2F +
ln T
2
2γ η
� T
� T
�
�
�
�
1
2
2
≤E
�0 (U , (xt , yt )) +
ht +
· η�U �F X
2
t=1
t=1
+

+

d k2
ln T .
2γ η

where the ﬁrst inequality uses Lemma 3, the
second inequality is from Cauchy-Schwarz that
�U , (e
√yt − eȳt ) ⊗ xt � ≤ �U �F · �(eyt − eȳt ) ⊗ xt � ≤
�U �F 2X and that (1 − �U , (eyt − eȳt ) ⊗ xt �) ≤
�(U , (xt , yt )).
Taking η =
E

�

T
�
t=1

�

dk2
2γ

�U �2F (E[

�

ln T
,
ht ]+ 12 )X 2

t

we have

ht ≤ L0 (U )

�
�
� � T
�
�
�
�
d k2 X 2
1
ln T
+ ��U �2F E
ht +
2
2γ
t=1
�
�
� � T
�
�
�
�
1
d k2 X 2
ln T ,
≤ L0 (U ) + ��U �2F E
ht +
2
γ
t=1

where the √last inequality
due to the elementary
√ is √
c + d, and the setting of
inequality c + d ≤
a = X 2 . Solving the inequality
�Tand using the fact that
E[MT ] ≤ E[M̂T ] + γT ≤ E[ t=1 ht ] + γT , we have
E[MT ] ≤ L0 (U ) + γT


�
2
2
2
2 �U �2 X 2 ln T
d
k
d
k
�U
�
X
ln
T
F
F
.
+ L0 (U )
+O
γ
γ

The theorem follows from Lemma 5 in Appendix B, taking
U = �U �2F , H = d k 2 X 2 ln T , L = L0 (U ).

5. Empirical Results
We tested SOBA to empirically validate the theoretical
ﬁndings. We used three different datasets from Kakade
et al. (2008): SynSep, SynNonSep, Reuters4.
The ﬁrst two are synthetic, with 106 samples in R400 and
9 classes. SynSep is constructed to be linearly separable, while SynNonSep is the same dataset with 5% random label noise. Reuters4 is generated from the RCV1
dataset (Lewis et al., 2004), extracting the 665,265 examples that have exactly one label from the set {CCAT,
ECAT, GCAT, MCAT}. It contains 47,236 features. We
also report the performance on Covtype from LibSVM
repository.4 We report averages over 10 different runs.
SOBA, as the Newtron algorithm, has a quadratic complexity in the dimension of the data, while the Banditron and
the Perceptron algorithm are linear. Following the long tradition of similar algorithms (Crammer et al., 2009; Duchi
et al., 2011; Hazan & Kale, 2011; Crammer & Gentile,
2013), to be able to run the algorithm on large datasets,
we have implemented an approximated diagonal version of
SOBA, named SOBAdiag. It keeps in memory just the diagonal of the matrix At . Following Hazan & Kale (2011),
we have tested only algorithms designed to work in the
fully adversarial setting. Hence, we tested the Banditron
and the PNewtron, the diagonal version of the Newtron algorithm in Hazan & Kale (2011). The multiclass Perceptron algorithm was used as a full-information baseline.
In the experiments, we only changed the exploration rate γ,
leaving ﬁxed all the other parameters the algorithms might
have. In particular, for the PNewtron we set α = 10,
β = 0.01, and D = 1, as in Hazan & Kale (2011). In
SOBA, a is ﬁxed to 1 in all the experiments. We explore
the effect of the exploration rate γ in the ﬁrst row of Figure
5. We see that the PNewtron algorithm,5 thanks to the exploration based on the softmax prediction, can achieve very
good performance for a wide range of γ.
It is important to note that SOBAdiag has good performance on all four datasets for a value of γ close to 1%. For
bigger values, the performance degrades because the best
possible error rate is lower bounded by k−1
k γ due to explo4

https://www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets/
5
We were unable to make the PNewtron work on Reuters4.
For any setting of γ the error rate is never better than 57%. The
reason might be that the dataset RCV1 has 47,236 features, while
the one reported in Kakade et al. (2008); Hazan & Kale (2011)
has 346,810, hence the optimal setting of the 3 other parameters
of PNewtron might be different. For this reason we prefer not to
report the performance of PNewtron on Reuters4.

Efﬁcient Online Bandit Multiclass Learning

0.4

0.4

0.6

error rate

0.2

0.3
0.25
0.2

0.5

10 -2

0.05

10 -1

SynSep

10 -4

10 -3

10 -2

10 -4
10 -5
10 2

0.35
10 -4

10 -1

Banditron,
PNewtron,

=2 -6
=2 -13

SOBAdiag,
Perceptron

=2 -7

=2
=2

-13

SOBAdiag,
Perceptron

=2

-7

10 3

0
10 -4

10 -1

10 -3

Covtype
Banditron,
PNewtron,

=2 -5
=2 -9

SOBAdiag,
Perceptron

=2 -6

10 -2

10 -1

Reuters4

10 0

0.7

error rate
10

10 -2

Banditron,
SOBAdiag,
Perceptron

=2
=2

-5
-7

0.6

error rate
-13

Banditron,
PNewtron,

10 -3

0.8

SynNonSep

10 -1

10 -3

0.2

0.1

10 0

10 -2

0.3

0.4

error rate

10 -3

10 0

error rate

0.55

0.45

0.1

10 -4

Banditron
SOBAdiag
Perceptron

0.4

0.15

0.1

0

0.5

Banditron
PNewtron
SOBAdiag
Perceptron

0.65

0.35

0.3

Reuters4

Covtype

0.7

Banditron
PNewtron
SOBAdiag
Perceptron

0.45

error rate

error rate

SynNonSep

Banditron
PNewtron
SOBAdiag
Perceptron

error rate

SynSep

0.5

0.5

10 -1

-1

0.4

10 4

number of examples

10 5

10 6

10 2

10 3

10 4

10 5

10 6

number of examples

10 2

10 3

10 4

number of examples

10 5

10 2

10 3

10 4

10 5

number of examples

Figure 2. Error rates vs. the value of the exploration rate γ (top row) and vs. the number examples (bottom row). The x-axis is
logarithmic in all the plots, while the y-axis is logarithmic in the plots in the second row. Figure best viewed in colors.

ration. For smaller values of exploration, the performance
degrades because the algorithm does not update enough. In
fact, SOBA updates only when ỹt = yt , so when γ is too
small the algorithms does not explore enough and remains
stuck around the initial solution. Also, SOBA requires an
initial number
�of updates to accumulate enough negative
terms in the t nt mt in order to start updating also when
ŷt is correct but the margin is too small.
The optimal setting of γ for each algorithm was then used
to generate the plots in the second row of Figure 5, where
we report the error rate over time. With the respective optimal setting of γ, we note that the performance of PNewtron
does not seem better than the one of the Multiclass Perceptron algorithm, and on par or worse to the Banditron’s one.
On the other hand, SOBAdiag has the best performance
among the bandits algorithms on 3 datasets out of 4.

The ﬁrst dataset, SynSep, is separable and with their optimal setting of γ, all the algorithms converge with a rate of
roughly O( T1 ), as can be seen from the log-log plot, but the
bandit algorithms will not converge to zero error rate, but
to k−1
k γ. However, SOBA has an initial phase in which the
error rate is high, due to the effect mentioned above.
On the second dataset, SynNonSep, SOBAdiag outperforms all the other algorithms (including the fullinformation Perceptron), achieving an error rate close to
the noise level of 5%. This is due to SOBA being a secondorder algorithm, while the Perceptron is a ﬁrst-order algorithm. A similar situation is observed on Covtype. On the
last dataset, Reuters4, SOBAdiag achieves performance
better than the Banditron.

6. Discussion and Future Work
In this paper, we study the problem of online multiclass
learning with bandit feedback. We propose
SOBA, an algo√
rithm that achieves a regret of Õ( η1 T ) with respect to ηloss of the competitor. This answers a COLT open problem
posed by (Abernethy & Rakhlin, 2009). Its key ideas are to
apply a novel adaptive regularizer in a second order online
learning algorithm, coupled with updates only when the
predictions are correct. SOBA is shown to have competitive performance compared to its precedents in synthetic
and real datasets, in some cases even better than the fullinformation Perceptron algorithm. There are several open
questions we wish to explore:
1. Is it possible to design efﬁcient algorithms with mistake bounds that depend�
on the loss of the competitor, i.e.
E[MT ] ≤ Lη (U ) + Õ( kdLη (U ) + kd)? This type of
bound occurs naturally in the full information multiclass
online learning setting, (see e.g. Theorem 1), or in multiarmed bandit setting, e.g. (Neu, 2015).
2. Are there efﬁcient algorithms that have a ﬁnite mistake
bound in the separable case? (Kakade et al., 2008) provides an algorithm that performs enumeration and plurality
vote to achieve a ﬁnite mistake bound in the ﬁnite dimensional setting, but unfortunately the algorithm is impractical. Notice that it is easy to show that in SOBA ŷt makes a
logarithmic number of mistakes in the separable case, with
a constant rate of exploration, yet it is not clear how to decrease the exploration over time in order to get a logarithmic number of mistakes for ỹt .
Acknowledgments. We thank Claudio Gentile for suggesting the original plan of attack for this problem, and
thank the anonymous reviewers for thoughtful comments.

Efﬁcient Online Bandit Multiclass Learning

References
Abernethy,
√ J. and Rakhlin, A. An efﬁcient bandit algorithm
for T -regret in online multiclass prediction? In COLT,
2009.
Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and
Schapire, R. E. Taming the monster: a fast and simple
algorithm for contextual bandits. ICML, 2014.
Auer, P., Cesa-Bianchi, N., and Gentile, C. Adaptive and
self-conﬁdent on-line learning algorithms. J. Comput.
Syst. Sci., 64(1):48–75, 2002.
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
The nonstochastic multiarmed bandit problem. SIAM J.
Comput., 32(1):48–77, January 2003.
Bubeck, S. and Cesa-Bianchi, N. Regret analysis of
stochastic and nonstochastic multi-armed bandit problems. FnTML, 5(1):1–122, 2012.
Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and
games. Cambridge University Press, 2006.
Cesa-Bianchi, N., Conconi, A., and Gentile, C. A secondorder Perceptron algorithm. SIAM Journal on Computing, 34(3):640–668, 2005.
Crammer, K. and Gentile, C. Multiclass classiﬁcation with
bandit feedback using adaptive regularization. Machine
learning, 90(3):347–383, 2013.
Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S.,
and Singer, Y. Online passive-aggressive algorithms.
JMLR, 7(Mar):551–585, 2006.

Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Efﬁcient bandit algorithms for online multiclass prediction.
In ICML, pp. 440–447. ACM, 2008.
Kivinen, J. and Warmuth, M. Exponentiated gradient versus gradient descent for linear predictors. Information
and Computation, 132(1):1–63, January 1997.
Langford, J. and Zhang, T. The epoch-greedy algorithm for
multi-armed bandits with side information. In NIPS 20,
pp. 817–824. 2008.
Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. RCV1:
A new benchmark collection for text categorization research. JMLR, 5(Apr):361–397, 2004.
McMahan, H Brendan and Streeter, Matthew. Adaptive bound optimization for online convex optimization.
COLT, 2010.
Mohri, M. and Rostamizadeh, A. Perceptron mistake
bounds. arXiv preprint arXiv:1305.0208, 2013.
Neu, G. First-order regret bounds for combinatorial semibandits. In COLT, pp. 1360–1375, 2015.
Orabona, F., Cesa-Bianchi, N., and Gentile, C. Beyond
logarithmic bounds in online learning. In AISTATS, pp.
823–831, 2012.
Orabona, F., Crammer, K., and Cesa-Bianchi, N. A generalized online mirror descent with applications to classiﬁcation and regression. Machine Learning, 99(3):411–
435, 2015.

Crammer, K., Kulesza, A., and Dredze, M. Adaptive regularization of weight vectors. In NIPS, pp. 414–422, 2009.

Rakhlin, A. and Sridharan, K. BISTRO: An efﬁcient
relaxation-based method for contextual bandits. In
ICML, 2016.

Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient
methods for online learning and stochastic optimization.
JMLR, 12(Jul):2121–2159, 2011.

Rosenblatt, F. The Perceptron: A probabilistic model for
information storage and organization in the brain. Psychological review, 65(6):386–407, 1958.

Duda, R. O. and Hart, P. E. Pattern classiﬁcation and scene
analysis. John Wiley, 1973.

Shalev-Shwartz, S. Online learning and online convex optimization. FnTML, 4(2):107–194, 2011.

Dudı́k, M., Hsu, D. J., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., and Zhang, T. Efﬁcient optimal
learning for contextual bandits. In UAI 2011, pp. 169–
178, 2011.

Syrgkanis, Vasilis, Krishnamurthy, Akshay, and Schapire,
Robert. Efﬁcient algorithms for adversarial contextual
learning. In ICML, pp. 2159–2168, 2016a.

Hazan, E. and Kale, S. Newtron: an efﬁcient bandit algorithm for online multiclass prediction. In NIPS, pp.
891–899, 2011.

Syrgkanis, Vasilis, Luo, Haipeng, Krishnamurthy, Akshay,
and Schapire, Robert E. Improved regret bounds for
oracle-based adversarial contextual bandits. In NIPS, pp.
3135–3143, 2016b.

Hazan, E., Agarwal, A., and Kale, S. Logarithmic regret algorithms for online convex optimization. Machine
Learning, 69(2-3):169–192, 2007.

Valizadegan, H., Jin, R., and Wang, S. Learning to trade off
between exploration and exploitation in multiclass bandit prediction. In KDD, pp. 204–212. ACM, 2011.

Efﬁcient Online Bandit Multiclass Learning

Wang, S., Jin, R., and Valizadegan, H. A potential-based
framework for online multi-class learning with partial
feedback. In AISTATS, pp. 900–907, 2010.

