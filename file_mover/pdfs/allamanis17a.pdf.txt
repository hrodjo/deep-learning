Learning Continuous Semantic Representations of Symbolic Expressions

Miltiadis Allamanis 1 Pankajan Chanthirasegaran 2 Pushmeet Kohli 3 Charles Sutton 2 4

Abstract
Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of
representation learning. As a step in this direction, we propose a new architecture, called neural
equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are
trained to represent semantic equivalence, even
of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at
the same time, small changes in syntax can lead
to very large changes in semantics, which can be
difficult for continuous neural architectures. We
perform an exhaustive evaluation on the task of
checking equivalence on a highly diverse class of
symbolic algebraic and boolean expression types,
showing that our model significantly outperforms
existing architectures.

1. Introduction
Combining abstract, symbolic reasoning with continuous
neural reasoning is a grand challenge of representation learning. This is particularly important while dealing with exponentially large domains such as source code and logical
expressions. Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very
different. Although symbolic reasoning is very powerful,
it also tends to be hard. For example, problems such as
the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse. This raises the
exciting opportunity of using pattern recognition within
symbolic reasoning, that is, to learn patterns from datasets
of symbolic expressions that approximately represent seWork started when M. Allamanis was at Edinburgh. This work
was done while P. Kohli was at Microsoft. 1 Microsoft Research,
Cambridge, UK 2 University of Edinburgh, UK 3 DeepMind, London, UK 4 The Alan Turing Institute, London, UK. Correspondence
to: Miltiadis Allamanis <t-mialla@microsoft.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

mantic relationships. However, apart from some notable
exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba
et al., 2014), this area has received relatively little attention
in machine learning. In this work, we explore the direction
of learning continuous semantic representations of symbolic
expressions. The goal is for expressions with similar semantics to have similar continuous representations, even if their
syntactic representation is very different. Such representations have the potential to allow a new class of symbolic
reasoning methods based on heuristics that depend on the
continuous representations, for example, by guiding a search
procedure in a symbolic solver based on a distance metric
in the continuous space.
In this paper, we make a first essential step of addressing
the problem of learning continuous semantic representations (S EM V ECs) for symbolic expressions. Our aim is,
given access to a training set of pairs of expressions for
which semantic equivalence is known, to assign continuous
vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are
assigned to identical (or highly similar) continuous vectors.
This is an important but hard problem; learning composable
S EM V ECs of symbolic expressions requires that we learn
about the semantics of symbolic elements and operators
and how they map to the continuous representation space,
thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature. As we show
in our evaluation, relatively simple logical and polynomial
expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural
network architectures.
Our work in similar in spirit to the work of Zaremba et al.
(2014), who focus on learning expression representations to
aid the search for computationally efficient identities. They
use recursive neural networks (T REE NN)1 (Socher et al.,
2012) for modeling homogenous, single-variable polynomial expressions. While they present impressive results, we
find that the T REE NN model fails when applied to more
complex symbolic polynomial and boolean expressions. In
particular, in our experiments we find that T REE NNs tend
to assign similar representations to syntactically similar expressions, even when they are semantically very different.
The underlying conceptual problem is how to develop a con1

To avoid confusion, we use T REE NN for recursive neural
networks and RNN for recurrent neural networks.

Learning Continuous Semantic Representations of Symbolic Expressions

tinuous representation that follows syntax but not too much,
that respects compositionality while also representing the
fact that a small syntactic change can be a large semantic
one.
To tackle this problem, we propose a new architecture, called
neural equivalence networks (E Q N ET). E Q N ETs learn how
syntactic composition recursively composes S EM V ECs, like
a T REE NN, but are also designed to model large changes
in semantics as the network progresses up the syntax tree.
As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than
pairwise decisions. The network architecture is based on
composing residual-like multi-layer networks, which allows
more flexibility in modeling the semantic mapping up the
syntax tree. To encourage representations within an equivalence class to be tightly clustered, we also introduce a
training method that we call subexpression autoencoding,
which uses an autoencoder to force the representation of
each subexpression to be predictable and reversible from its
syntactic neighbors. Experimental evaluation on a highly
diverse class of symbolic algebraic and boolean expression
types shows that E Q N ETs dramatically outperform existing
architectures like T REE NNs and RNNs.
To summarize, the main contributions of our work are: (a)
We formulate the problem of learning continuous semantic
representations (S EM V ECs) of symbolic expressions and
develop benchmarks for this task. (b) We present neural
equivalence networks (E Q N ETs), a neural network architecture that learns to represent expression semantics onto a
continuous semantic representation space and how to perform symbolic operations in this space. (c) We provide
an extensive evaluation on boolean and polynomial expressions, showing that E Q N ETs perform dramatically better
than state-of-the-art alternatives. Code and data are available at groups.inf.ed.ac.uk/cup/semvec.

2. Model
In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which
we call S EM V ECs, and in learning to generate identical representations for expressions that are semantically equivalent,
i.e. they belong to the same equivalence class. Equivalence
is a stronger property than similarity, which has been the
focus of previous work in neural network learning (Chopra
et al., 2005), since equivalence is additionally a transitive
relationship.
Problem Hardness. Finding the equivalence of arbitrary
symbolic expressions is a NP-hard problem or worse. For
example, if we focus on boolean expressions, reducing an
expression to the representation of the false equivalence
class amounts to proving its non-satisfiability — an NPcomplete problem. Of course, we do not expect to circum-

vent an NP-complete problem with neural networks. A
network for solving boolean equivalence would require an
exponential number of nodes in the size of the expression if
P 6= N P . Instead, our goal is to develop architectures that
efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions
in a given training set. The supplementary material shows
a sample of such expressions that illustrate the hardness of
this problem.
Notation and Framework. To allow our representations
to be compositional, we employ the general framework of
recursive neural networks (T REE NN) (Socher et al., 2012;
2013), in our case operating on tree structures of the syntactic parse of a formula. Given a tree T , T REE NNs learn
distributed representations for each node in the tree by recursively combining the representations of its subtrees using a
neural network. We denote the children of a node n as ch(n)
which is a (possibly empty) ordered tuple of nodes. We also
use par(n) to refer to the parent node of n. Each node in
our tree has a type, e.g. a terminal node could be of type “a”
referring to the variable a or of type “and” referring to a
node of the logical A ND (∧) operation. We refer to the type
of a node n as τn . In pseudocode, T REE NNs retrieve the
representation of a tree T rooted at node ρ, by invoking the
function T REE NN(ρ) that returns a vector representation
rρ ∈ RD , i.e., a S EM V EC. The function is defined as
T REE NN (current node n)
if n is not a leaf then
rn ← C OMBINE(T REE NN(c0 ), . . . , T REE NN(ck ), τn ),
where (c0 , . . . , ck ) = ch(n)
else
rn ← L OOKUP L EAF E MBEDDING(τn )
return rn
The general framework of T REE NN allows two points
of variation, the implementation of L OOKUP L EAF E M BEDDING and C OMBINE . Traditional T REE NNs (Socher
et al., 2013) define L OOKUP L EAF E MBEDDING as a simple
lookup operation within a matrix of embeddings and C OM BINE as a single-layer neural network. As discussed next,
these will both prove to be serious limitations in our setting.
To train these networks to learn S EM V ECs, we will use a
supervised objective based on a set of known equivalence
relations (see Section 2.2).
2.1. Neural Equivalence Networks
Our domain requires that the network learns to abstract
away syntax, assigning identical representations to expressions that may be syntactically different but semantically
equivalent, and also assigning different representations to
expressions that may be syntactically very similar but nonequivalent. In this work, we find that standard neural architectures do not handle well this challenge. To represent semantics from syntax, we need to learn to recursively

Learning Continuous Semantic Representations of Symbolic Expressions

rp

SemVec

/ k·k2

¯lout

rc1

a
a

rc2

c

r̃c1 –rc1 , r̃c2 –rc2 , r̃p –rp

rp

¯l1

rc1

¯l0

rc2

r̃c1
x̃

r̃p
r̃c2

Combine

Combine

SubexpAe

(a) Architectural diagram of E Q N ETs. Example parse tree shown is of the boolean expression (a ∨ c) ∧ a.

C OMBINE (rc0 , . . . , rck , τp )
¯l0 ← [rc , . . . , rc ]
0
k
¯l1 ← σ Wi,τ · ¯l0
p
¯lout ← Wo0,τ · ¯l0 + Wo1,τ · ¯l1
p
p 
return ¯lout / ¯lout 
2

S UBEXPA E (rc0 , . . . , rck , rp , τp )
x ← [rc0 , . . . , rck ]

x̃ ← tanh Wd · tanh We,τp · [rp , x] · n
x̃ ← x̃ · kxk2 / kx̃k2
r̃p ← C OMBINE(x̃, τp ) 
return − x̃> x + r̃>
p rp

(b) C OMBINE of E Q N ET.

(c) Loss function used for subexpression autoencoder
Figure 1. E Q N ET architecture.

compose and decompose semantic representations and remove syntactic “noise”. Any syntactic operation may significantly change semantics (e.g. negation, or appending
∧FALSE) while we may reach the same semantic state
through many possible operations. This necessitates using high-curvature operations over the semantic representation space. Furthermore, some operations are semantically
reversible and thus we need to learn reversible semantic
representations (e.g. ¬¬A and A should have an identical
S EM V ECs). Based on these, we define neural equivalence
networks (E Q N ET), which learn to compose representations
of equivalence classes into new equivalence classes (Figure 1a). Our network follows the T REE NN architecture,
i.e. is implemented using T REE NN to model the compositional nature of symbolic expressions but is adapted based
on the domain requirements. The extensions we introduce
have two aims: first, to improve the network training; and
second, and more interestingly, to encourage the learned
representations to abstract away surface level information
while retaining semantic content.
The first extension that we introduce is to the network structure at each layer in the tree. Traditional T REE NNs (Socher
et al., 2013) use a single-layer neural network at each tree
node. During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform
the input S EM V ECs to the output S EM V EC and maintain
semantic equivalences, requiring high-curvature operations.
Part of the problem stems from the fact that within the
Euclidean space of S EM V ECs some operations need to be
non-linear. For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic
representation space. Instead, we turn to multi-layer neural

networks. In particular, we define the network as shown
in the function C OMBINE in Figure 1b. This uses a twolayer MLP with a residual-like connection to compute the
S EM V EC of each parent node in that syntax tree given that
of its children. Each node type τn , e.g., each logical operator, has a different set of weights. We experimented with
deeper networks but this did not yield any improvements.
However, as T REE NNs become deeper, they suffer from
optimization issues, such as diminishing and exploding gradients. This is essentially because of the highly compositional nature of tree structures, where the same network
(i.e. the C OMBINE non-linear function) is used recursively,
causing it to “echo” its own errors and producing unstable
feedback loops. We observe this problem even with only
two-layer MLPs, as the overall network can become quite
deep when using two layers for each node in the syntax
tree. We resolve this issue in the training procedure by
constraining each S EM V EC to have unit norm. That is, we
set L OOKUP L EAF E MBEDDING(τn ) = Cτn / kCτn k2 , and
we normalize the output of the final layer of C OMBINE in
Figure 1b. The normalization step of ¯lout and Cτn is somewhat similar to weight normalization (Salimans & Kingma,
2016) and vaguely resembles layer normalization (Ba et al.,
2016). Normalizing the S EM V ECs partially resolves issues
with diminishing and exploding gradients, and removes a
spurious degree of freedom in the semantic representation.
As simple as this modification may seem, we found it vital
for obtaining good performance, and all of our multi-layer
T REE NNs converged to low-performing settings without it.
Although these modifications seem to improve the representation capacity of the network and its ability to be trained,
we found that they were not on their own sufficient for good

Learning Continuous Semantic Representations of Symbolic Expressions

performance. In our early experiments, we noticed that the
networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous
space were primarily ones that were syntactically similar.
At the same time, we observed that the networks did not
learn to unify representations of the same equivalence class,
observing multiple syntactically distinct but semantically
equivalent expressions to have distant S EM V ECs.
Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.
We add a regularization term on the S EM V ECs that we call
a subexpression autoencoder (S UBEXPA E). We design this
regularization to encourage the S EM V ECs to have two properties: abstraction and reversibility. Because abstraction
arguably means removing irrelevant information, a network
with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic
information rather than semantic information. To achieve
this, we introduce a component that aims to encourage reversibility, which we explain by an example. Observe that
given the semantic representation of any two of the three
nodes of a subexpression (by which we mean the parent, left
child, right child of an expression tree) it is often possible to
completely determine or at least place strong constraints on
the semantics of the third. For example, consider a boolean
formula F (a, b) = F1 (a, b) ∨ F2 (a, b) where F1 and F2
are arbitrary propositional formulae over the variables a, b.
Then clearly if we know that F implies that a is true but F1
does not, then F2 must imply that a is true. More generally,
if F belongs to some equivalence class e0 and F1 belongs
to a different class e1 , we want the continuous representation of F2 to reflect that there are strong constraints on the
equivalence class of F2 .
Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child
representations together, to encourage dependence in the
representations of parents and children. More specifically,
given any node p in the tree with children c0 . . . ck , we can
define a parent-children tuple [rc0 , . . . , rck , rp ] containing
the (computed) S EM V ECs of the children and parent nodes.
What S UBEXPA E does is to autoencode this representation
tuple into a low-dimensional space with a denoising autoencoder. We then seek to minimize the reconstruction error of
the child representations (r̃c0 , . . . , r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from
the reconstructed children. More formally, we minimize
the return value of S UBEXPA E in Figure 1c where n is a
binary noise vector with κ percent of its elements set to
zero. Note that the encoder is specific to the parent node
type τp . Although our S UBEXPA E may seem similar to the
recursive autoencoders of Socher et al. (2011), it differs

in two major ways. First, S UBEXPA E autoencodes on the
entire parent-children representation tuple, rather than the
child representations alone. Second, the encoding is not
used to compute the parent representation, but only serves
as a regularizer.
Subexpression autoencoding has several desirable effects.
First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions. Second, because
the denoising autoencoder is reconstructing parent and child
representations together, this encourages child representations to be predictable from parents and siblings. Putting
these two together, the goal is that the information discarded
by the autoencoder bottleneck will be more syntactic than
semantic, assuming that the semantics of child node is more
predictable from its parent and sibling than its syntactic
realization. The goal is to nudge the network to learn consistent, reversible semantics. Additionally, subexpression
autoencoding has the potential to gradually unify distant
representations that belong to the same equivalence class.
To illustrate this point, imagine two semantically equivalent c00 and c000 child nodes 
of different expressions that
have distant S EM V ECs, i.e. rc00 − rc000 2   although
C OMBINE(rc00 , . . . ) ≈ C OMBINE(rc000 , . . . ). In some cases
due to the autoencoder noise, the differences between the input tuple x0 , x00 that contain rc00 and rc000 will be non-existent
and the decoder will predict a single location r̃c0 (possibly
different from rc00 and rc000 ). Then, when minimizing the
reconstruction error, both rc00 and rc000 will be attracted to
r̃c0 and eventually should merge.
2.2. Training
We train E Q N ETs from a dataset of expressions whose
semantic equivalence is known. Given a training set
T = {T1 . . . TN } of parse trees of expressions, we assume
that the training set is partitioned into equivalence classes
E = {e1 . . . eJ }. We use a supervised objective similar
to classification; the difference between classification and
our setting is that whereas standard classification problems
consider a fixed set of class labels, in our setting the number
of equivalence classes in the training set will vary with N .
Given an expression tree T that belongs to the equivalence
class ei ∈ E, we compute the probability

exp T REE NN(T )> qei + bi
 (1)
P (ei |T ) = P
>
j exp T REE NN(T ) qej + bj
where qei are model parameters that we can interpret as
representations of each equivalence class that appears in the
training class, and bi are scalar bias terms. Note that in this
work, we only use information about the equivalence class
of the whole expression T , ignoring available information
about subexpressions. This is without loss of generality,
because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to

Learning Continuous Semantic Representations of Symbolic Expressions

the training set. To train the model, we use a max-margin
objective that maximizes classification accuracy, i.e.
!
P (ej |T )
LACC (T, ei ) = max 0, arg max log
+m
P (ei |T )
ej 6=ei ,ej ∈E
(2)
where m > 0 is a scalar margin. And therefore the optimized loss function for a single expression tree T that
belongs to equivalence class ei ∈ E is
µ X
L(T, ei ) = LACC (T, ei ) +
S UBEXPA E(ch(n), n)
|Q|
n∈Q

(3)
where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight. We found
that subexpression autoencoding is counterproductive early
in training, before the S EM V ECs begin to represent aspects
of semantics. So, for each epoch t, we set µ = 1 − 10−νt
with ν ≥ 0. Instead of the supervised objective that we
propose, an alternative option for training E Q N ET would be
a Siamese objective (Chopra et al., 2005) that learns about
similarities (rather than equivalence) between expressions.
In practice, we found the optimization to be very unstable,
yielding suboptimal performance. We believe that this has
to do with the compositional and recursive nature of the task
that creates unstable dynamics and the fact that equivalence
is a stronger property than similarity.

3. Evaluation
Datasets. We generate datasets of expressions grouped
into equivalence classes from two domains. The datasets
from the B OOL domain contain boolean expressions and
the P OLY datasets contain polynomial expressions. In both
domains, an expression is either a variable, a binary operator
that combines two expressions, or a unary operator applied
to a single expression. When defining equivalence, we interpret distinct variables as referring to different entities in
the domain, so that, e.g., the polynomials c · (a · a + b) and
f ·(d·d+e) are not equivalent. For each domain, we generate
“simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more
complex operators. We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.
All expressions are symbolically simplified into a canonical
from in order to determine their equivalence class and are
grouped accordingly. Table 1 shows the datasets we generated. In the supplementary material we present some sample
expressions. For the polynomial domain, we also generated
O NE V-P OLY datasets, which are polynomials over a single
variable, since they are similar to the setting considered by
Zaremba et al. (2014) — although O NE V-P OLY is still a little more general because it is not restricted to homogeneous
polynomials. Learning S EM V ECs for boolean expressions

is already
a hard problem; with n boolean variables, there
n
are 22 equivalence classes (i.e. one for each possible truth
table). We split the datasets into training, validation and test
sets. We create two test sets, one to measure generalization
performance on equivalence classes that were seen in the
training data (S EEN E Q C LASS), and one to measure generalization to unseen equivalence classes (U NSEEN E Q C LASS).
It is easiest to describe U NSEEN E Q C LASS first. To create the U NSEEN E Q C LASS, we randomly select 20% of all
the equivalence classes, and place all of their expressions
in the test set. We select equivalence classes only if they
contain at least two expressions but less than three times
the average number of expressions per equivalence class.
We thus avoid selecting very common (and hence trivial
to learn) equivalence classes in the testset. Then, to create
S EEN E Q C LASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each
class into training, validation, S EEN E Q C LASS test in the
proportions 60%–15%–25%. We provide the datasets online
at groups.inf.ed.ac.uk/cup/semvec.

Baselines. To compare the performance of our model, we
train the following baselines. T F -I DF: learns a representation given the expression tokens (variables, operators and
parentheses). This captures topical/declarative knowledge
but is unable to capture procedural knowledge. G RU refers
to the token-level gated recurrent unit encoder of Bahdanau
et al. (2015) that encodes the token-sequence of an expression into a distributed representation. Stack-augmented
R NN refers to the work of Joulin & Mikolov (2015) which
was used to learn algorithmic patterns and uses a stack as
a memory and operates on the expression tokens. We also
include two recursive neural networks (T REE NN). The 1layer T REE NN which is the original T REE NN also used by
Zaremba et al. (2014). We also include a 2-layer T REE NN,
where C OMBINE is a classic two-layer MLP without residual connections. This shows the effect of S EM V EC normalization and subexpression autoencoder.

Hyperparameters. We tune the hyperparameters of all
models using Bayesian optimization (Snoek et al., 2012)
on a boolean dataset with 5 variables and maximum tree
size of 7 (not shown in Table 1) using the average k-NN
(k = 1, . . . , 15) statistics (described next). The selected
hyperparameters are detailed in the supplementary material.
3.1. Quantitative Evaluation
Metrics. To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of
each expression (using cosine similarity) that belong to the
same equivalence class. More formally, given a test query
expression q in an equivalence class c we find the k nearest
neighbors Nk (q) of q across all expressions, and define the

Learning Continuous Semantic Representations of Symbolic Expressions
Table 1. Dataset statistics and results. S IMP datasets contain simple operators (“∧, ∨, ¬” for B OOL and “+, −” for P OLY) while the rest
contain all operators (i.e. “∧, ∨, ¬, ⊕, ⇒” for B OOL and “+, −, ·” for P OLY). ⊕ is the X OR operator. The number in the dataset name
indicates its expressions’ maximum tree size. L refers to a “larger” number of 10 variables. H is the entropy of equivalence classes.
Dataset
# # Equiv
#
H
score5 (%) in U NSEEN E Q C LASS
Vars Classes
Exprs
tf-idf GRU StackRNN 1L T REE NN 2L T REE NN E Q N ET
S IMP B OOL 8
S IMP B OOL 10S
B OOL 5
B OOL 8
B OOL 10S
S IMP B OOL L5
B OOL L5
S IMP P OLY 5
S IMP P OLY 8
S IMP P OLY 10
ONE V-P OLY 10
ONE V-P OLY 13
P OLY 5
P OLY 8
S

3
3
3
3
10
10
10
3
3
3
1
1
3
3

120
191
95
232
256
1,342
7,312
47
104
195
83
677
150
1,102

39,048
26,304
1,239
257,784
51,299
10,050
36,050
237
3,477
57,909
1,291
107,725
516
11,451

5.6
7.2
5.6
6.2
8.0
9.9
11.8
5.0
5.8
6.3
5.4
7.1
6.7
9.0

17.4
6.2
34.9
10.7
5.0
53.1
31.1
21.9
36.1
25.9
43.5
3.2
37.8
13.9

30.9
11.0
35.8
17.2
5.1
40.2
20.7
6.3
14.6
11.0
10.9
4.7
34.1
5.7

26.7
7.6
12.4
16.0
3.9
50.5
11.5
1.0
5.8
6.6
5.3
2.2
2.2
2.4

27.4
25.0
16.4
15.7
10.8
3.48
0.1
40.6
12.5
19.9
10.9
10.0
46.8
10.4

25.5
93.4
26.0
15.4
20.2
19.9
0.5
27.1
13.1
7.1
8.5
56.2
59.1
14.8

97.4
99.1
65.8
58.1
71.4
85.0
75.2
65.6
98.9
99.3
81.3
90.4
55.3
86.2

dataset contains all equivalence classes but at most 200 uniformly sampled (without replacement) expressions per equivalence class.

score as
scorek (q) =

|Nk (q) ∩ c|
.
min(k, |c|)

(4)

To report results for a given testset, we simply average
scorek (q) for all expressions q in the testset. We also report
the precision-recall curves for the problem of clustering the
S EM V ECs into their appropriate equivalence classes.
Evaluation. Figure 2 presents the average per-model
precision-recall curves across the datasets. Table 1 shows
score5 of U NSEEN E Q C LASS. Detailed plots are found in
the supplementary material. E Q N ET performs better for all
datasets, by a large margin. The only exception is P OLY 5,
where the 2-L T REE NN performs better. However, this may
have to do with the small size of the dataset. The reader
may observe that the simple datasets (containing fewer operations and variables) are easier to learn. Understandably,
introducing more variables increases the size of the represented space reducing performance. The tf-idf method
performs better in settings with more variables, because
it captures well the variables and operations used. Similar observations can be made for sequence models. The
one and two layer T REE NNs have mixed performance; we
believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional
nature of T REE NNs. Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the
O NE V-P OLY datasets with a traditional T REE NN architecture. Our evaluation suggests that E Q N ETs perform much
better within the O NE V-P OLY setting.
Evaluation of Compositionality. We evaluate whether
E Q N ETs successfully learn to compute compositional representations, rather than overfitting to expression trees of

a small size. To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more
complex ones; for example, training on the training set of
B OOL 5 but testing on the testset of B OOL 8. We average
over 11 different train-test pairs (full list in supplementary
material) and show the results in Figure 3a and Figure 3b.
These graphs again show that E Q N ETs are better than any
of the other methods, and indeed, performance is only a bit
worse than in the non-transfer setting.
Impact of E Q N ET Components E Q N ETs differ from
traditional T REE NNs in two major ways, which we analyze here. First, S UBEXPA E improves performance. When
training the network with and without S UBEXPA E, on average, the area under the curve (AUC) of scorek decreases
by 16.8% on the S EEN E Q C LASS and 19.7% on the U N SEEN E Q C LASS . This difference is smaller in the transfer
setting, where AUC decreases by 8.8% on average. However, even in this setting we observe that S UBEXPA E helps
more in large and diverse datasets. The second key difference to traditional T REE NNs is the output normalization
and the residual connections. Comparing our model to the
one-layer and two-layer T REE NNs again, we find that output normalization results in important improvements (the
two-layer T REE NNs have on average 60.9% smaller AUC).
We note that only the combination of the residual connections and the output normalization improve the performance,
whereas when used separately, there are no significant improvements over the two-layer T REE NNs.
3.2. Qualitative Evaluation
Table 2 shows expressions whose S EM V EC nearest neighbor
is of an expression of another equivalence class. Manually
inspecting boolean expressions, we find that E Q N ET confusions happen more when a X OR or implication operator is

Learning Continuous Semantic Representations of Symbolic Expressions
1.0

1.0

tf-idf
GRU
StackRNN

tf-idf
GRU
StackRNN

0.8

0.6

Precision

Precision

0.8

TreeNN-1Layer
TreeNN-2Layer
EqNet

0.4

0.2

TreeNN-1Layer
TreeNN-2Layer
EqNet

0.6

0.4

0.2

0.0
0.0

0.2

0.4

0.6

0.8

0.0
0.0

1.0

0.2

0.4

Recall

0.6

0.8

1.0

Recall

(a) S EEN E Q C LASS

(b) U NSEEN E Q C LASS

Figure 2. Precision-Recall Curves averaged across datasets.
Table 2. Non semantically equivalent first nearest-neighbors from B OOL 8 and P OLY 8. A checkmark indicates that the method correctly
results in the nearest neighbor being from the same equivalence class.
Expr

a ∧ (a ∧ (a ∧ (¬c)))

a ∧ (a ∧ (c ⇒ (¬c)))

(a ∧ a) ∧ (c ⇒ (¬c))

a + (c · (a + c))

((a + c) · c) + a

(b · b) − b

tfidf
G RU
1L-T REE NN
E Q N ET

c ∧ ((a ∧ a) ∧ (¬a))
X
a ∧ (a ∧ (a ∧ (¬b)))
X

c ⇒ (¬((c ∧ a) ∧ a))
a ∧ (a ∧ (c ∧ (¬c)))
a ∧ (a ∧ (c ⇒ (¬b)))
X

c ⇒ (¬((c ∧ a) ∧ a))
(a ∧ a) ∧ (c ⇒ (¬c))
(a ∧ a) ∧ (c ⇒ (¬b))
(¬(b ⇒ (b ∨ c))) ∧ a

a + (c + a) · c
b + (c · (a + c))
a + (c · (b + c))
X

(c · a) + (a + c)
((b + c) · c) + a
((b + c) · c) + a
X

b · (b − b)
(b + b) · b − b
(a − c) · b − b
(b · b) · b − b

100

100

¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b))) ((c ∨ (¬b)) ⇒ a) ∧ (a ⇒ a)

a − ((a + b) · a)

scorek

scorek

((b ⊕ (¬c)) ∧ b) ⊕ (a ∨ b) ((b · a) − a) · b

10−1

10−1

5

k

10

GRU

StackRNN

k

10

(b) U NSEEN E Q C LASS
TreeNN-1Layer

b + ((b · b) · b)

Figure 4. Visualization of score5 for all expression nodes for three
B OOL 10 and four P OLY 8 test sample expressions using E Q N ET.
The darker the color, the lower the score, i.e. white implies a score
of 1 and dark red a score of 0.
5

(a) S EEN E Q C LASS
tf-idf

((c · b) · c) · a

TreeNN-2Layer

EqNet

Figure 3. Evaluation of compositionality; training set simpler than
test set. Average scorek (y-axis in log-scale). Markers are shown
every three ticks for clarity. T REE NN refers to Socher et al. (2012).

involved. In fact, we fail to find any confused expressions
for E Q N ET not involving these operations in B OOL 5 and
in the top 100 expressions in B OOL 10. As expected, tf-idf
confuses expressions with others that contain the same operators and variables ignoring order. In contrast, G RU and
T REE NN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply
nested variables or operators. In contrast, E Q N ET tends
to confuse fewer expressions (as we previously showed)
and the confused expressions tend to be more syntactically
diverse and semantically related.
Figure 4 shows a visualization of score5 for each node in
the expression tree. One may see that as E Q N ET knows how

to compose expressions that achieve good score, even if the
subexpressions achieve a worse score. This suggests that
for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without
merging the equivalence classes or affecting the upstream
performance of the network. Larger scale interactive t-SNE
visualizations can be found online.
Figure 5 presents two PCA visualizations of the S EM V ECs
of simple expressions and their negations/negatives. It can
be discerned that the black dots and their negations (in
red) are discriminated in the semantic representation space.
Figure 5b shows this property in a clear manner: left-right
discriminates between polynomials with 1 and −a, topbottom between polynomials with −b and b and the diagonal
parellelt to y = −x between c and −c. We observe a similar
behavior in Figure 5a for boolean expressions.

4. Related Work
Researchers have proposed compilation schemes that can
transform any given program or expression to an equivalent
neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-

Learning Continuous Semantic Representations of Symbolic Expressions

¬b¬(b ∧ c)
¬(a ∧ b)

a∨c
a ∨ (b ∨ c)
ab ∨ c
c
a ∨ (ba∧∨c)b
a a∧∧(bc∨ c)

a − (b − c)
¬(a ∧ (b ∧ c))

c − (a + b)

a −(a
b + c) − (c + b)

¬(a ∨ b)

¬c
¬(b¬(a
∨ c)∧ c)
¬(a
¬(a∧∨(b
(b∨∧c))
c))

¬a
¬(a ∨ (b ∨ c))
¬(a ∨ c)

b∧c
a ∧ (ba∧∧c)
b b

(a) Negation in B OOL expressions

c−a
(b − b) − (a − c)

a − (b + c)

a−c

(b − a) + c

(a + b) − (b + c)

(c − c) − (a − b)
a − (c − b)

b−a
b − (a + c)

(b) Negatives in P OLY expressions

Figure 5. A PCA visualization of some simple non-equivalent boolean and polynomial expressions (black-square) and their negations
(red-circle). The lines connect the negated expressions.

mann, 1994). One can consider a serialized version of the
resulting neural network as a representation of the expression. However, it is not clear how we could compare the
serialized representations corresponding to two expressions
and whether this mapping preserves semantic distances.
Recursive neural networks (T REE NN) (Socher et al., 2012;
2013) have been successfully used in NLP with multiple
applications. Socher et al. (2012) show that T REE NNs can
learn to compute the values of some simple propositional
statements. E Q N ET’s S UBEXPA E may resemble recursive
autoencoders (Socher et al., 2011) but differs in form and
function, encoding the whole parent-children tuple to force
a clustering behavior. In addition, when encoding each
expression our architecture does not use a pooling layer but
directly produces a single representation for the expression.
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks. Although they learn
representations of the student tasks, these representations
capture task-specific syntactic features rather than code semantics. Piech et al. (2015) also learn distributed matrix
representations of student code submissions. However, to
learn the representations, they use input and output program
states and do not test for program equivalence. Additionally,
these representations do not necessarily represent program
equivalence, since they do not learn the representations over
all possible input-outputs. Allamanis et al. (2016) learn
variable-sized representations of source code snippets to
summarize them with a short function-like name but aim
learn summarization features in code rather than representations of symbolic expression equivalence.
More closely related is the work of Zaremba et al. (2014)
who use a T REE NN to guide the search for more efficient
mathematical identities, limited to homogeneous singlevariable polynomial expressions. In contrast, E Q N ETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned S EM V ECs to better

represent equivalence, and do not use search when looking
for equivalent expressions. Alemi et al. (2016) use RNNs
and convolutional neural networks to detect features within
mathematical expressions to speed the search for premise
selection in automated theorem proving but do not explicitly
account for semantic equivalence. In the future, S EM V ECs
may be useful within this area.
Our work is also related to recent work on neural network
architectures that learn controllers/programs (Gruau et al.,
1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas,
2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).
In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network
architectures but to learn continuous semantic representations (S EM V ECs) of expression semantics irrespectively of
how they are syntactically expressed or evaluated.

5. Discussion & Conclusions
In this work, we presented E Q N ETs, a first step in learning
continuous semantic representations (S EM V ECs) of procedural knowledge. S EM V ECs have the potential of bridging
continuous representations with symbolic representations,
useful in multiple applications in artificial intelligence, machine learning and programming languages.
We show that E Q N ETs perform significantly better than
state-of-the-art alternatives. But further improvements are
needed, especially for more robust training of compositional
models. In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic
space to be represented. Fixed-sized S EM V ECs, like the
ones used in E Q N ET, eventually limit the capacity that is
available to represent procedural knowledge. In the future,
to represent more complex procedures, variable-sized representations would seem to be required.

Learning Continuous Semantic Representations of Symbolic Expressions

Acknowledgments
This work was supported by Microsoft Research through
its PhD Scholarship Programme and the Engineering
and Physical Sciences Research Council [grant number
EP/K024043/1]. We thank the University of Edinburgh Data
Science EPSRC Centre for Doctoral Training for providing
additional computational resources.

References
Alemi, Alex A, Chollet, Francois, Irving, Geoffrey, Szegedy,
Christian, and Urban, Josef. DeepMath – Deep sequence models for premise selection. arXiv preprint
arXiv:1606.04442, 2016.
Allamanis, Miltiadis, Peng, Hao, and Sutton, Charles. A
convolutional attention network for extreme summarization of source code. In ICML, 2016.
Ba, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Geoffrey E.
Layer normalization. arXiv preprint arXiv:1607.06450,
2016.
Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.
Neural machine translation by jointly learning to align
and translate. In ICLR, 2015.
Chopra, Sumit, Hadsell, Raia, and LeCun, Yann. Learning
a similarity metric discriminatively, with application to
face verification. In CVPR, 2005.
Dyer, Chris, Ballesteros, Miguel, Ling, Wang, Matthews,
Austin, and Smith, Noah A. Transition-based dependency
parsing with stack long short-term memory. In ACL,
2015.
Graves, Alex, Wayne, Greg, and Danihelka, Ivo. Neural
Turing machines. arXiv preprint arXiv:1410.5401, 2014.
Grefenstette, Edward, Hermann, Karl Moritz, Suleyman,
Mustafa, and Blunsom, Phil. Learning to transduce with
unbounded memory. In NIPS, 2015.
Gruau, Frédéric, Ratajszczak, Jean-Yves, and Wiber, Gilles.
A neural compiler. Theoretical Computer Science, 1995.
Joulin, Armand and Mikolov, Tomas. Inferring algorithmic
patterns with stack-augmented recurrent nets. In NIPS,
2015.
Kaiser, Łukasz and Sutskever, Ilya. Neural GPUs learn
algorithms. In ICLR, 2016.
Loos, Sarah, Irving, Geoffrey, Szegedy, Christian, and
Kaliszyk, Cezary. Deep network guided proof search.
arXiv preprint arXiv:1701.06972, 2017.
Mou, Lili, Li, Ge, Zhang, Lu, Wang, Tao, and Jin, Zhi.
Convolutional neural networks over tree structures for
programming language processing. In AAAI, 2016.

Neelakantan, Arvind, Le, Quoc V, and Sutskever, Ilya. Neural programmer: Inducing latent programs with gradient
descent. In ICLR, 2015.
Neto, João Pedro, Siegelmann, Hava T, and Costa, J Félix.
Symbolic processing in neural networks. Journal of the
Brazilian Computer Society, 8(3):58–70, 2003.
Piech, Chris, Huang, Jonathan, Nguyen, Andy, Phulsuksombati, Mike, Sahami, Mehran, and Guibas, Leonidas J.
Learning program embeddings to propagate feedback on
student code. In ICML, 2015.
Reed, Scott and de Freitas, Nando. Neural programmerinterpreters. ICLR, 2016.
Salimans, Tim and Kingma, Diederik P. Weight normalization: A simple reparameterization to accelerate training of
deep neural networks. In Advances in Neural Information
Processing Systems, 2016.
Siegelmann, Hava T. Neural programming language. In
Proceedings of the 12th National Conference on Artificial
Intelligence, 1994.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practical Bayesian optimization of machine learning algorithms.
In NIPS, 2012.
Socher, Richard, Pennington, Jeffrey, Huang, Eric H, Ng,
Andrew Y, and Manning, Christopher D. Semi-supervised
recursive autoencoders for predicting sentiment distributions. In EMNLP, 2011.
Socher, Richard, Huval, Brody, Manning, Christopher D,
and Ng, Andrew Y. Semantic compositionality through
recursive matrix-vector spaces. In EMNLP, 2012.
Socher, Richard, Perelygin, Alex, Wu, Jean Y, Chuang, Jason, Manning, Christopher D, Ng, Andrew Y, and Potts,
Christopher. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.
Zaremba, Wojciech, Kurach, Karol, and Fergus, Rob. Learning to discover efficient mathematical identities. In Advances in Neural Information Processing Systems, pp.
1278–1286, 2014.

