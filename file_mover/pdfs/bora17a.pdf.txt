Compressed Sensing using Generative Models
Ashish Bora 1 Ajil Jalal 2 Eric Price 1 Alexandros G. Dimakis 2

Abstract
The goal of compressed sensing is to estimate a
vector from an underdetermined system of noisy
linear measurements, by making use of prior
knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity
in a well-chosen basis. We show how to achieve
guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range
of a generative model G : Rk ! Rn . Our main
theorem is that, if G is L-Lipschitz, then roughly
O(k log L) random Gaussian measurements suffice for an `2 /`2 recovery guarantee. We demonstrate our results using generative models from
published variational autoencoder and generative
adversarial networks. Our method can use 5-10x
fewer measurements than Lasso for the same accuracy.

1. Introduction
Compressive or compressed sensing is the problem of reconstructing an unknown vector x⇤ 2 Rn after observing
m < n linear measurements of its entries, possibly with
added noise:
y = Ax⇤ + ⌘,
where A 2 Rm⇥n is called the measurement matrix and
⌘ 2 Rm is noise. Even without noise, this is an underdetermined system of linear equations, so recovery is impossible unless we make an assumption on the structure
Code for experiments in the paper can be found at:
https://github.com/AshishBora/csgm 1 University
of Texas at Austin, Department of Computer Science
2
University of Texas at Austin, Department of Electrical
and Computer Engineering. Correspondence to: Ashish Bora
<ashish.bora@utexas.edu>, Ajil Jalal <ajiljalal@utexas.edu>,
Eric Price <ecprice@cs.utexas.edu>, Alexandros G. Dimakis
<dimakis@austin.utexas.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

of the unknown vector x⇤ . We need to assume that the unknown vector is “natural,” or “simple,” in some applicationdependent way.
The most common structural assumption is that the vector x⇤ is k-sparse in some known basis (or approximately
k-sparse). Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x⇤ if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted
Eigenvalue Condition (REC) (Tibshirani, 1996; Candes
et al., 2006; Donoho, 2006; Bickel et al., 2009). The problem is also called high-dimensional sparse linear regression
and there is vast literature on establishing conditions for
different recovery algorithms, different assumptions on the
design of A and generalizations of RIP and REC for other
structures, see e.g. (Bickel et al., 2009; Negahban et al.,
2009; Agarwal et al., 2010; Loh & Wainwright, 2011; Bach
et al., 2012).
This significant interest is justified since a large number of
applications can be expressed as recovering an unknown
vector from noisy linear measurements. For example,
many tomography problems can be expressed in this framework: x⇤ is the unknown true tomographic image and the
linear measurements are obtained by x-ray or other physical sensing system that produces sums or more general linear projections of the unknown pixels. Compressed sensing has been studied extensively for medical applications
including computed tomography (CT) (Chen et al., 2008),
rapid MRI (Lustig et al., 2007) and neuronal spike train
recovery (Hegde et al., 2009). Another impressive application is the “single pixel camera” (Duarte et al., 2008),
where digital micro-mirrors provide linear combinations to
a single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image. These results have been extended by combining sparsity with additional structural assumptions (Baraniuk et al., 2010; Hegde
et al., 2015), and by generalizations such as translating
sparse vectors into low-rank matrices (Negahban et al.,
2009; Bach et al., 2012; Foygel & Mackey, 2014). These
results can improve performance when the structural assumptions fit the sensed signals. Other works perform “dictionary learning,” seeking overcomplete bases where the
data is more sparse (see (Chen & Needell, 2016) and refer-

Compressed Sensing using Generative Models

ences therein).
In this paper instead of relying on sparsity, we use structure from a generative model. Recently, several neural
network based generative models such as variational autoencoders (VAEs) (Kingma & Welling, 2013) and generative adversarial networks (GANs) (Goodfellow et al., 2014)
have found success at modeling data distributions. In these
models, the generative part learns a mapping from a low
dimensional representation space z 2 Rk to the high dimensional sample space G(z) 2 Rn . While training, this
mapping is encouraged to produce vectors that resemble
the vectors in the training dataset. We can therefore use
any pre-trained generator to approximately capture the notion of a vector being “natural” in our domain: the generator defines a probability distribution over vectors in sample
space and tries to assign higher probability to more likely
vectors, for the dataset it has been trained on. We expect
that vectors “natural” to our domain will be close to some
point in the support of this distribution, i.e., in the range of
G.
Our Contributions: We present an algorithm that uses
generative models for compressed sensing. Our algorithm
simply uses gradient descent to optimize the representation
z 2 Rk such that the corresponding image G(z) has small
measurement error kAG(z) yk22 . While this is a nonconvex objective to optimize, we empirically find that gradient
descent works well, and the results can significantly outperform Lasso with relatively few measurements.
We obtain theoretical results showing that, as long as gradient descent finds a good approximate solution to our objective, our output G(z) will be almost as close to the true
x⇤ as the closest possible point in the range of G.
The proof is based on a generalization of the Restricted Eigenvalue Condition (REC) that we call the SetRestricted Eigenvalue Condition (S-REC). Our main theorem is that if a measurement matrix satisfies the S-REC
for the range of a given generator G, then the measurement error minimization optimum is close to the true x⇤ .
Furthermore, we show that random Gaussian measurement
matrices satisfy the S-REC condition with high probability for large classes of generators. Specifically, for d-layer
neural networks such as VAEs and GANs, we show that
O(kd log n) Gaussian measurements suffice to guarantee
good reconstruction with high probability. One result, for
ReLU-based networks, is the following:
Theorem 1.1. Let G : Rk ! Rn be a generative
model from a d-layer neural network using ReLU activations. Let A 2 Rm⇥n be a random Gaussian matrix for
m = O(kd log n), scaled so Ai,j ⇠ N (0, 1/m). For any
x⇤ 2 Rn and any observation y = Ax⇤ + ⌘, let zb minimize
ky AG(z)k2 to within additive ✏ of the optimum. Then

with 1
kG(b
z)

e

⌦(m)

probability,

x⇤ k2  6 min kG(z ⇤ )
z ⇤ 2Rk

x⇤ k2 + 3k⌘k2 + 2✏.

In the error bound above, the first two terms are the minimum possible error of any vector in the range of the generator and the norm of the noise; these are necessary for such a
technique, and have direct analogs in standard compressed
sensing guarantees. The third term ✏ comes from gradient
descent not necessarily converging to the global optimum;
empirically, ✏ does seem to converge to zero, and one can
check post-observation that this is small by computing the
upper bound ky AG(b
z )k2 .
While the above is restricted to ReLU-based neural networks, we also show similar results for arbitrary LLipschitz generative models, for m ⇡ O(k log L). Typical neural networks have poly(n)-bounded weights in each
layer, so L  nO(d) , giving for any activation, the same
O(kd log n) sample complexity as for ReLU networks.

Theorem 1.2. Let G : Rk ! Rn be an L-Lipschitz function. Let A 2 Rm⇥n be a random Gaussian matrix for
m = O(k log Lr ), scaled so Ai,j ⇠ N (0, 1/m). For any
x⇤ 2 Rn and any observation y = Ax⇤ + ⌘, let zb minimize
ky AG(z)k2 to within additive ✏ of the optimum over
vectors with kb
z k2  r. Then with 1 e ⌦(m) probability,
kG(b
z ) x⇤ k2  6 min kG(z ⇤ ) x⇤ k2 +3k⌘k2 +2✏+2 .
z ⇤ 2Rk
kz ⇤ k2 r

The downside is two minor technical conditions: we only
optimize over representations z with kzk bounded by r, and
our error gains an additive term. Since the dependence
on these parameters is log(rL/ ), and L is something like
nO(d) , we may set r = nO(d) and = 1/nO(d) while only
losing constant factors, making these conditions very mild.
In fact, generative models normally have the coordinates
p of
z be independent uniform or Gaussian, so kzk ⇡ k ⌧
nd , and apconstant signal-to-noise ratio would have k⌘k2 ⇡
kx⇤ k ⇡ n
1/nd .
We remark that, while these theorems are stated in terms
of Gaussian matrices, the proofs only involve the distributional Johnson-Lindenstrauss property of such matrices.
Hence the same results hold for matrices with subgaussian
entries or fast-JL matrices (Ailon & Chazelle, 2009).

2. Our Algorithm
All norms are 2-norms unless specified otherwise.
Let x⇤ 2 Rn be the vector we wish to sense. Let A 2
Rm⇥n be the measurement matrix and ⌘ 2 Rm be the noise
vector. We observe the measurements y = Ax⇤ + ⌘. Given
y and A, our task is to find a reconstruction x̂ close to x⇤ .

Compressed Sensing using Generative Models

A generative model is given by a deterministic function
G : Rk ! Rn , and a distribution PZ over z 2 Rk . To
generate a sample from the generator, we can draw z ⇠ PZ
and the sample then is G(z). Typically, we have k ⌧ n,
i.e. the generative model maps from a low dimensional representation space to a high dimensional sample space.

We also take note of the related work of (Gilbert et al.,
2017) that connects model-based compressed sensing
with the invertibility of Convolutional Neural Networks,
Bayesian compressed sensing (Ji et al., 2008) and compressive sensing using Gaussian mixture models (Yang et al.,
2014).

Our approach is to find a vector in representation space
such that the corresponding vector in the sample space
matches the observed measurements. We thus define the
objective to be

A related result appears in (Baraniuk & Wakin, 2009),
which studies the measurement complexity of an RIP condition for smooth manifolds. This is analogous to our
S-REC for the range of G, but the range of G is neither smooth (because of ReLUs) nor a manifold (because
of self-intersection). Their recovery result was extended
in (Hegde & Baraniuk, 2012) to unions of two manifolds.

loss(z) = kAG(z)

yk2

(1)

By using any optimization procedure, we can minimize
loss(z) with respect to z. In particular, if the generative
model G is differentiable, we can evaluate the gradients
of the loss with respect to z using backpropagation and use
standard gradient based optimizers. If the optimization procedure terminates at ẑ, our reconstruction for x⇤ is G(ẑ).
We define the measurement error to be kAG(ẑ) yk2 and
the reconstruction error to be kG(ẑ) x⇤ k2 .

3. Related Work
Several recent lines of work explore generative models for
reconstruction. The first line of work attempts to project
an image on to the representation space of the generator. These works assume full knowledge of the image,
and are special cases of the linear measurements framework where the measurement matrix A is identity. Excellent reconstruction results with SGD in the representation
space to find an image in the generator range have been
reported by (Lipton & Tripathi, 2017) with stochastic clipping and (Creswell & Bharath, 2016) with logistic measurement loss. A different approach is introduced in (Dumoulin et al., 2016) and (Donahue et al., 2016). In their
method, a recognition network that maps from the sample space vector x to the representation space vector z is
learned jointly with the generator in an adversarial setting.
A second line of work explores reconstruction with structured partial observations. The inpainting problem consists
of predicting the values of missing pixels given a part of
the image. This is a special case of linear measurements
where each measurement corresponds to an observed pixel.
The use of generative models for this task has been studied in (Yeh et al., 2016), where the objective is taken to be
a sum of L1 error in measurements and a perceptual loss
term given by the discriminator. Super-resolution is a related task that attempts to increase the resolution of an image. We can view the observations as local spatial averages
of the unknown higher resolution image and hence cast this
as another special case of linear measurements. For prior
work on super-resolution see e.g. (Yang et al., 2010; Dong
et al., 2016; Kim et al., 2016) and references therein.

4. Theoretical Results
We begin with a brief review of the Restricted Eigenvalue
Condition (REC) in standard compressed sensing. The
REC is a sufficient condition on A for robust recovery to
be possible. The REC essentially requires that all “approximately sparse” vectors are far from the nullspace of the
matrix A. More specifically, A satisfies REC for a constant
> 0 if for all approximately sparse vectors x,
kAxk

(2)

kxk.

It can be shown that this condition is sufficient for recovery
of sparse vectors using Lasso. If one examines the structure of Lasso recovery proofs, a key property that is used is
that the difference of any two sparse vectors is also approximately sparse (for sparsity up to 2k). This is a coincidence
that is particular to sparsity. By contrast, the difference of
two vectors “natural” to our domain may not itself be natural. The condition we need is that the difference of any two
natural vectors is far from the nullspace of A.
We propose a generalized version of the REC for a set
S ✓ Rn of vectors, the Set-Restricted Eigenvalue Condition (S-REC):
Definition 1. Let S ✓ Rn . For some parameters
>
0,
0, a matrix A 2 Rm⇥n is said to satisfy the
S-REC(S, , ) if 8 x1 , x2 2 S,
kA(x1

x2 )k

kx1

x2 k

.

There are two main differences between the S-REC and the
standard REC in compressed sensing. First, the condition
applies to differences of vectors in an arbitrary set S of
“natural” vectors, rather than just the set of approximately
k-sparse vectors in some basis. This will let us apply the
definition to S being the range of a generative model.
Second, we allow an additive slack term . This is necessary for us to achieve the S-REC when S is the output of
general Lipschitz functions. Without it, the S-REC depends

Compressed Sensing using Generative Models

on the behavior of S at arbitrarily small scales. Since there
are arbitrarily many such local regions, one cannot guarantee the existence of an A that works for all these local
regions. Fortunately, as we shall see, poor behavior at a
small scale will only increase our error by O( ).

piecewise linear with at most two pieces. The popular
ReLU or LeakyReLU non-linearities satisfy this assumption. We do not make any other assumption, and in particular, the magnitude of the weights in the network do not
affect our guarantee.

But how can we find such a matrix? To answer this, we
present two lemmas showing that random Gaussian matrices of relatively few measurements m satisfy the S-REC for
the outputs of large and practically useful classes of generative models G : Rk ! Rn .

for some ↵ < 1.
Then a random matrix A 2
1
Rm⇥n with IID entries Aij ⇠ N (0, m
) satisfies the
k
⌦(↵2 m)
S-REC(G(R ), 1 ↵, 0) with 1 e
probability.

The S-REC definition requires that for any two vectors in
S, if they are significantly different (so the right hand side
is large), then the corresponding measurements should also
be significantly different (left hand side). Hence we can
hope to approximate the unknown vector from the measurements, if the measurement matrix satisfies the S-REC.

In the first lemma, we assume that the generative model
G(·) is L-Lipschitz, i.e., 8 z1 , z2 2 Rk , we have
kG(z1 )

G(z2 )k  Lkz1

z2 k.

Note that state of the art neural network architectures
with linear layers, (transposed) convolutions, max-pooling,
residual connections, and all popular non-linearities satisfy
this assumption. In Lemma 8.5 in the Appendix we give a
simple bound on L in terms of parameters of the network;
for typical networks this is nO(d) . We also require the input
z to the generator to have bounded norm. Since generative
models such as VAEs and GANs typically assume their input z is drawn with independent uniform or Gaussian inputs, this only prunes an exponentially unlikely fraction of
the possible outputs.
Lemma 4.1. Let G : R ! R be L-Lipschitz. Let
k

n

k

Lemma 4.2. Let G : Rk ! Rn be a d-layer neural network, where each layer is a linear transformation followed
by a pointwise non-linearity. Suppose there are at most c
nodes per layer, and the non-linearities are piecewise linear with at most two pieces, and let
✓
◆
1
m=⌦
kd log c
↵2

To show Theorems 1.1 and 1.2, we just need to show that
the S-REC implies good recovery. In order to make our
error guarantee relative to `2 error in the image space Rn ,
rather than in the measurement space Rm , we also need
that A preserves norms with high probability (Cohen et al.,
2009). Fortunately, Gaussian matrices (or other distributional JL matrices) satisfy this property.
Lemma 4.3. Let A 2 Rm⇥n by drawn from a distribution
that (1) satisfies the S-REC(S, , ) with probability 1 p
and (2) has for every fixed x 2 Rn , kAxk  2kxk with
probability 1 p.
For any x⇤ 2 Rn and noise ⌘, let y = Ax⇤ + ⌘. Let x
b
approximately minimize ky Axk over x 2 S, i.e.,
ky

be an L2 -norm ball in R . For ↵ < 1, if
◆
✓
k
Lr
m=⌦
log
,
↵2
k

then a random matrix A 2 R
with IID entries such that
1
Aij ⇠ N 0, m
satisfies the S-REC(G(B k (r)), 1 ↵, )
2
with 1 e ⌦(↵ m) probability.
m⇥n

x2S

Axk + ✏.

Then,

k

B (r) = {z | z 2 R , kzk  r}

Ab
xk  min ky

kb
x

x⇤ k 

✓

4

◆
+ 1 min kx⇤

with probability 1

x2S

xk +

1

(2k⌘k + ✏ + )

2p.

Combining Lemma 4.1, Lemma 4.2, and Lemma 4.3 gives
Theorems 1.1 and 1.2. In our setting, S is the range of the
generator, and x
b in the theorem above is the reconstruction
G(b
z ) returned by our algorithm.

All proofs, including this one, are deferred to Appendix A.

5. Models

Note that even though we proved the lemma for an L2 ball,
the same technique works for any compact set.

In this section we describe the generative models used in
our experiments. We used two image datasets and two different generative model types (a VAE and a GAN). This
provides some evidence that our approach can work with
many types of models and datasets.

For our second lemma, we assume that the generative
model is a neural network such that each layer is a composition of a linear transformation followed by a pointwise
non-linearity. Many common generative models have such
architectures. We also assume that all non-linearities are

In our experiments, we found that it was helpful to add a
regularization term L(z) to the objective to encourage the

750

300
400
500

200

100

50

0.00

25

0.02

1umEer Rf meaVurementV

0.15
0.10
0.05
0.00

5000
7500
10000

0.04

DCGA1+5eg

0.20

2500

0.06

DCGA1

0.25

1000

0.08

LDssR (WDveleW)

500

VA(+5eg

LDssR (DC7)

0.30

200

VA(

100

0.10

0.35

50

LaVVR

20

5ecRnsWrucWLRn errRr (per pLxel)

0.12

10

5ecRnVtructLRn errRr (per pLxel)

Compressed Sensing using Generative Models

1umber Rf meDsuremenWs

(a) Results on MNIST

(b) Results on celebA

Figure 1. We compare the performance of our algorithm with baselines. We show a plot of per pixel reconstruction error as we vary the
number of measurements. The vertical bars indicate 95% confidence intervals.

optimization to explore more in the regions that are preferred by the respective generative models (see comparison to unregularized versions in Fig. 1). Thus the objective
function we use for minimization is
kAG(z)

yk2 + L(z).

Both VAE and GAN typically imposes an isotropic Gaussian prior on z. Thus kzk2 is proportional to the negative
log-likelihood under this prior. Accordingly, we use the
following regularizer:
L(z) = kzk2 ,

(3)

where measures the relative importance of the prior as
compared to the measurement error.
5.1. MNIST with VAE
The MNIST dataset consists of about 60, 000 images of
handwritten digits, where each image is of size 28⇥28 (LeCun et al., 1998). Each pixel value is either 0 (background)
or 1 (foreground). No pre-processing was performed. We
trained VAE on this dataset. The input to the VAE is a vectorized binary image of input dimension 784. We set the
size of the representation space k = 20. The recognition
network is a fully connected 784 500 500 20 network.
The generator is also fully connected with the architecture
20 500 500 784. We train the VAE using the Adam
optimizer (Kingma & Ba, 2014) with a mini-batch size 100
and a learning rate of 0.001. We use = 0.1 in Eqn. (3).
The digit images are reasonably sparse in the pixel space.
Thus, as a baseline, we use the pixel values directly for
sparse recovery using Lasso. We set shrinkage parameter
to be 0.1 for all the experiments.

5.2. CelebA with DCGAN
CelebA is a dataset of more than 200, 000 face images
of celebrities (Liu et al., 2015). The input images were
cropped to a 64 ⇥ 64 RGB image, giving 64 ⇥ 64 ⇥ 3 =
12288 inputs per image. Each pixel value was scaled so
that all values are between [ 1, 1]. We trained a DCGAN
(Radford et al., 2015; Kim, 2017) on this dataset. We set
the input dimension k = 100 and use a standard normal distribution. The architecture follows that of (Radford et al.,
2015). The model was trained by one update to the discriminator and two updates to the generator per cycle. Each update used the Adam optimizer (Kingma & Ba, 2014) with
minibatch size 64, learning rate 0.0002 and 1 = 0.5. We
use = 0.001 in Eqn. (3).
For baselines, we perform sparse recovery using Lasso on
the images in two domains: (a) 2D Discrete Cosine Transform (2D-DCT) and (b) 2D Daubechies-1 Wavelet Transform (2D-DB1). While we provide Gaussian measurements of the original pixel values, the L1 penalty is on either the DCT coefficients or the DB1 coefficients of each
color channel of an image. For all experiments, we set the
shrinkage parameter to be 0.1 and 0.00001 respectively for
2D-DCT, and 2D-DB1.

6. Experiments and Results
6.1. Reconstruction from Gaussian measurements
We take A to be a random matrix with IID Gaussian entries
with zero mean and standard deviation of 1/m. Each entry
of noise vector ⌘ is also an IID Gaussian random variable.
We compare performance of different sensing algorithms
qualitatively and quantitatively. For quantitative comparison, we use the reconstruction error = kx̂ x⇤ k2 , where x̂

Compressed Sensing using Generative Models

is an estimate of x⇤ returned by the algorithm. In all cases,
we report the results on a held out test set, unseen by the
generative model at training time.

measurement matrix may not satisfy the S-REC condition
(with good constants and ), and consequently, our theorems may not be applicable.

MNIST: The p
standard deviation of the noise vector is
set such that
E[k⌘k2 ] = 0.1. We use Adam optimizer (Kingma & Ba, 2014), with a learning rate of 0.01.
We do 10 random restarts with 1000 steps per restart and
pick the reconstruction with best measurement error.

MNIST: We construct a low resolution image by spatial
2 ⇥ 2 pooling with a stride of 2 to produce a 14 ⇥ 14 image.
These measurements are used to reconstruct the original
28 ⇥ 28 image. Fig. 2b shows reconstructions produced
by our algorithm on images from a held out test set. We
observe sharp reconstructions which closely match the fine
structure in the ground truth.

In Fig. 1a, we show the reconstruction error as we change
the number of measurements both for Lasso and our algorithm. We observe that our algorithm is able to get low
errors with far fewer measurements. For example, our
algorithm’s performance with 25 measurements matches
Lasso’s performance with 400 measurements. Fig. 2a
shows sample reconstructions by Lasso and our algorithm.
However, our algorithm is limited since its output is constrained to be in the range of the generator. After 100
measurements, our algorithm’s performance saturates, and
additional measurements give no additional performance.
Since Lasso has no such limitation, it eventually surpasses
our algorithm, but this takes more than 500 measurements
of the 784-dimensional vector. We expect that a more
powerful generative model with representation dimension
k > 20 can make better use of additional measurements.
celebA: The p
standard deviation of the noise vector is
set such that E[k⌘k2 ] = 0.01. We use Adam optimizer (Kingma & Ba, 2014), with a learning rate of 0.1.
We do 2 random restarts with 500 update steps per restart
and pick the reconstruction with best measurement error.
In Fig. 1b, we show the reconstruction error as we change
the number of measurements both for Lasso and our algorithm. In Fig. 3 we show sample reconstructions by Lasso
and our algorithm. We observe that our algorithm is able
to produce reasonable reconstructions with as few as 500
measurements, while the output of the baseline algorithms
is quite blurry. Similar to the results on MNIST, if we continue to give more measurements, our algorithm saturates,
and for more than 5000 measurements, Lasso gets a better
reconstruction. We again expect that a more powerful generative model with k > 100 would perform better in the
high-measurement regime.
6.2. Super-resolution
Super-resolution is the task of constructing a high resolution image from a low resolution version of the same image. This problem can be thought of as special case of
our general framework of linear measurements, where the
measurements correspond to local spatial averages of the
pixel values. Thus, we try to use our recovery algorithm
to perform this task with measurement matrix A tailored
to give only the relevant observations. We note that this

celebA: We construct a low resolution image by spatial 4⇥
4 pooling with a stride of 4 to produce a 16 ⇥ 16 image.
These measurements are used to reconstruct the original
64 ⇥ 64 image. In Fig. 4 we show results on images from a
held out test set. We see that our algorithm is able to fill in
the details to match the original image.
6.3. Understanding sources of error
Although better than baselines, our method still admits
some error. This error can be decomposed into three components: (a) Representation error: the unknown image is
far from the range of the generator (b) Measurement error:
The finite set of random measurements do not contain all
the information about the unknown image (c) Optimization
error: The optimization procedure did not find the best z.
In this section we present some experiments that suggest
that the representation error is the dominant term. In our
first experiment, we ensure that the representation error is
zero, and try to minimize the sum of other two errors. In
this setting, we observe that the reconstructions are almost
perfect. In the second experiment, we ensure that the measurement error is zero, and try to minimize the sum of other
two. Here, we observe that the total error obtained is very
close to the total error in our reconstruction experiments
(Sec. 6.1).
6.3.1. S ENSING IMAGES FROM RANGE OF GENERATOR
Our first approach is to sense an image that is in the range
of the generator. More concretely, we sample a z ⇤ from
PZ . Then we pass it through the generator to get x⇤ =
G(z ⇤ ). Now, we pretend that this is a real image and try to
sense that. This method eliminates the representation error
and allows us to check if our gradient based optimization
procedure is able to find z ⇤ by minimizing the objective.
In Fig. 6a and Fig. 6b, we show the reconstruction error for
images in the range of the generators trained on MNIST
and celebA datasets respectively. We see that we get almost
perfect reconstruction with very few measurements. This
suggests that objective is being properly minimized and we
indeed get ẑ close to z ⇤ . i.e. the sum of optimization error
and the measurement error is small in the absence of the

Compressed Sensing using Generative Models

(a) We show original images (top row) and reconstructions by
Lasso (middle row) and our algorithm (bottom row).

(b) We show original images (top row), low resolution version
of original images (middle row) and reconstructions (last row).

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

Figure 2. Results on MNIST. Reconstruction with 100 measurements (left) and Super-resolution (right)

DCGAN

BOurreG

OriginDO

Figure 3. Reconstruction results on celebA with m = 500 measurements (of n = 12288 dimensional vector). We show original images
(top row), and reconstructions by Lasso with DCT basis (second row), Lasso with wavelet basis (third row), and our algorithm (last row).

DCGAN

OriginDO

Figure 4. Super-resolution results on celebA. Top row has the original images. Second row shows the low resolution (4x smaller) version
of the original image. Last row shows the images produced by our algorithm.

Figure 5. Results on the representation error experiments on celebA. Top row shows original images and the bottom row shows closest
images found in the range of the generator.

750

300
400
500

200

100

50

0.00

25

0.01

0.05

0.00

1umber Rf measurements

2500

0.02

0.10

1000

0.03

500

0.04

)rRm generatRr

0.15

200

0.05

)rRm test set

100

)rRm generatRr

50

0.06

0.20

20

)rRm test set

5eFRnstruFtiRn errRr (per pixel)

0.07

10

5eFRnstruFtiRn errRr (per pixel)

Compressed Sensing using Generative Models

1umber Rf measurements

(a) Results on MNIST

(b) Results on celebA

Figure 6. Reconstruction error for images in the range of the generator. The vertical bars indicate 95% confidence intervals.

Figure 7. Results on the representation error experiments on
MNIST. Top row shows original images and the bottom row
shows closest images found in the range of the generator.

representation error.
6.3.2. Q UANTIFYING REPRESENTATION ERROR
We saw that in absence of the representation error, the overall error is small. However from Fig. 1, we know that the
overall error is still non-zero. So, in this experiment, we
seek to quantify the representation error, i.e., how far are
the real images from the range of the generator?
From the previous experiment, we know that the ẑ recovered by our algorithm is close to z ⇤ , the best possible value,
if the image being sensed is in the range of the generator.
Based on this, we make an assumption that this property is
also true for real images. With this assumption, we get an
estimate to the representation error as follows: We sample
real images from the test set. Then we use the full image in
our algorithm, i.e., our measurement matrix A is identity.
This eliminates the measurement error. Using these measurements, we get the reconstructed image G(ẑ) through
our algorithm. The estimated representation error is then
kG(ẑ) x⇤ k2 . We repeat this procedure several times over
randomly sampled images from our dataset and report average representation error values. The task of finding the
closest image in the range of the generator has been studied in prior work (Creswell & Bharath, 2016; Dumoulin
et al., 2016; Donahue et al., 2016).
On the MNIST dataset, we get average per pixel represen-

tation error of 0.005. The recovered images are shown in
Fig. 7. In contrast, with only 100 Gaussian measurements,
we get a per pixel reconstruction error of about 0.009. On
the celebA dataset, we get average per pixel representation
error of 0.020. The recovered images are shown in Fig. 5.
In contrast, with only 500 Gaussian measurements, we get
a per pixel reconstruction error of about 0.028.
This suggests that the representation error is the major component of the total error, and thus a more flexible generative
model can help reduce it on both datasets.

7. Conclusion
We demonstrate how to perform compressed sensing using generative models from neural nets. These models can
represent data distributions more concisely than standard
sparsity models, while their differentiability allows for fast
signal reconstruction. This will allow compressed sensing
applications to make significantly fewer measurements.
Our theorems and experiments both suggest that, after relatively few measurements, the signal reconstruction gets
close to the optimal within the range of the generator. To
reach the full potential of this technique, one should use
larger generative models as the number of measurements
increase. Whether this can be expressed more concisely
than by training multiple independent generative models of
different sizes is an open question.
Generative models are an active area of research with ongoing rapid improvements. Because our framework applies to
general generative models, this improvement will immediately yield better reconstructions with fewer measurements.
We also believe that one could also use the performance of
generative models for our task as one benchmark for the
quality of different models.

Compressed Sensing using Generative Models

Acknowledgements
We would like to thank Philipp Krähenbühl for helpful discussions. This research has been supported by NSF Grants
CCF 1344364, 1407278, 1422549, 1618689, ARO YIP
W911NF-14-1-0258, and the William Hartwig fellowship.

References
Agarwal, Alekh, Negahban, Sahand, and Wainwright, Martin J. Fast global convergence rates of gradient methods
for high-dimensional statistical recovery. In Advances
in Neural Information Processing Systems, pp. 37–45,
2010.
Ailon, Nir and Chazelle, Bernard. The fast johnson–
lindenstrauss transform and approximate nearest neighbors. SIAM Journal on Computing, 39(1):302–322,
2009.
Bach, Francis, Jenatton, Rodolphe, Mairal, Julien, Obozinski, Guillaume, et al. Optimization with sparsityinducing penalties. Foundations and Trends R in Machine Learning, 4(1):1–106, 2012.
Baraniuk, Richard G and Wakin, Michael B. Random projections of smooth manifolds. Foundations of computational mathematics, 9(1):51–77, 2009.
Baraniuk, Richard G, Cevher, Volkan, Duarte, Marco F,
and Hegde, Chinmay. Model-based compressive sensing. IEEE Transactions on Information Theory, 56(4):
1982–2001, 2010.
Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexandre B. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, pp. 1705–1732, 2009.
Candes, Emmanuel J, Romberg, Justin K, and Tao, Terence. Stable signal recovery from incomplete and inaccurate measurements. Communications on pure and
applied mathematics, 59(8):1207–1223, 2006.
Chen, Guang-Hong, Tang, Jie, and Leng, Shuai. Prior image constrained compressed sensing (piccs): a method
to accurately reconstruct dynamic ct images from highly
undersampled projection data sets. Medical physics, 35
(2):660–663, 2008.
Chen, Guangliang and Needell, Deanna. Compressed sensing and dictionary learning. Proceedings of Symposia in
Applied Mathematics, 73, 2016.
Cohen, A., Dahmen, W., and DeVore, R. Compressed sensing and best k-term approximation. J. Amer. Math. Soc,
22(1):211–231, 2009.

Creswell, Antonia and Bharath, Anil Anthony. Inverting
the generator of a generative adversarial network. arXiv
preprint arXiv:1611.05644, 2016.
Donahue, Jeff, Krähenbühl, Philipp, and Darrell,
Trevor. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Dong, Chao, Loy, Chen Change, He, Kaiming, and Tang,
Xiaoou. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis
and machine intelligence, 38(2):295–307, 2016.
Donoho, David L. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–1306, 2006.
Duarte, Marco F, Davenport, Mark A, Takbar, Dharmpal,
Laska, Jason N, Sun, Ting, Kelly, Kevin F, and Baraniuk,
Richard G. Single-pixel imaging via compressive sampling. IEEE signal processing magazine, 25(2):83–91,
2008.
Dumoulin, Vincent, Belghazi, Ishmael, Poole, Ben, Lamb,
Alex, Arjovsky, Martin, Mastropietro, Olivier, and
Courville, Aaron. Adversarially learned inference. arXiv
preprint arXiv:1606.00704, 2016.
Foygel, Rina and Mackey, Lester. Corrupted sensing:
Novel guarantees for separating structured signals. IEEE
Transactions on Information Theory, 60(2):1223–1247,
2014.
Gilbert, Anna C, Zhang, Yi, Lee, Kibok, Zhang, Yuting,
and Lee, Honglak. Towards understanding the invertibility of convolutional neural networks. arXiv preprint
arXiv:1705.08664, 2017.
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,
Bing, Warde-Farley, David, Ozair, Sherjil, Courville,
Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in neural information processing systems,
pp. 2672–2680, 2014.
Hegde, Chinmay and Baraniuk, Richard G. Signal recovery on incoherent manifolds. IEEE Transactions on Information Theory, 58(12):7204–7214, 2012.
Hegde, Chinmay, Duarte, Marco F, and Cevher, Volkan.
Compressive sensing recovery of spike trains using a
structured sparsity model. In SPARS’09-Signal Processing with Adaptive Sparse Structured Representations,
2009.
Hegde, Chinmay, Indyk, Piotr, and Schmidt, Ludwig. A
nearly-linear time framework for graph-structured sparsity. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 928–937,
2015.

Compressed Sensing using Generative Models

Ji, Shihao, Xue, Ya, and Carin, Lawrence. Bayesian compressive sensing. IEEE Transactions on Signal Processing, 56(6):2346–2356, 2008.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267–288, 1996.

Kim, Jiwon, Kwon Lee, Jung, and Mu Lee, Kyoung. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1646–
1654, 2016.

Vershynin, Roman. Introduction to the non-asymptotic
analysis of random matrices.
arXiv preprint
arXiv:1011.3027, 2010.

Kim, Taehoon. A tensorflow implementation of “deep
convolutional generative adversarial networks”.
https://github.com/carpedm20/DCGAN-tensorflow,
2017.
Kingma, Diederik and Ba, Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam: A
arXiv preprint

Kingma, Diederik P and Welling, Max. Auto-encoding
variational bayes. arXiv preprint arXiv:1312.6114,
2013.
LeCun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.
Lipton, Zachary C and Tripathi, Subarna. Precise recovery
of latent vectors from generative adversarial networks.
arXiv preprint arXiv:1702.04782, 2017.
Liu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou.
Deep learning face attributes in the wild. In Proceedings
of the IEEE International Conference on Computer Vision, pp. 3730–3738, 2015.
Loh, Po-Ling and Wainwright, Martin J. High-dimensional
regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pp. 2726–2734, 2011.
Lustig, Michael, Donoho, David, and Pauly, John M.
Sparse mri: The application of compressed sensing for
rapid mr imaging. Magnetic resonance in medicine, 58
(6):1182–1195, 2007.
Matoušek, Jiřı́. Lectures on discrete geometry, volume 212.
Springer Science & Business Media, 2002.
Negahban, Sahand, Yu, Bin, Wainwright, Martin J, and
Ravikumar, Pradeep K. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information
Processing Systems, pp. 1348–1356, 2009.
Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint
arXiv:1511.06434, 2015.

Yang, Jianbo, Yuan, Xin, Liao, Xuejun, Llull, Patrick,
Brady, David J, Sapiro, Guillermo, and Carin, Lawrence.
Video compressive sensing using gaussian mixture models. IEEE Transactions on Image Processing, 23(11):
4863–4878, 2014.
Yang, Jianchao, Wright, John, Huang, Thomas S, and Ma,
Yi. Image super-resolution via sparse representation.
IEEE transactions on image processing, 19(11):2861–
2873, 2010.
Yeh, Raymond, Chen, Chen, Lim, Teck Yian, HasegawaJohnson, Mark, and Do, Minh N. Semantic image inpainting with perceptual and contextual losses. arXiv
preprint arXiv:1607.07539, 2016.

