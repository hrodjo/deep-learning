Identify the Nash Equilibrium in Static Games with Random Payoffs

Yichi Zhou 1 Jialian Li 1 Jun Zhu 1

Abstract
We study the problem on how to learn the pure
Nash Equilibrium of a two-player zero-sum static
game with random payoffs under unknown distributions via efficient payoff queries. We introduce a multi-armed bandit model to this problem
due to its ability to find the best arm efficiently
among random arms and propose two algorithms
for this problem—LUCB-G based on the confidence bounds and a racing algorithm based on
successive action elimination. We provide an
analysis on the sample complexity lower bound
when the Nash Equilibrium exists.

1. Introduction
We consider the static zero-sum game where two players
are involved with finite pure strategies. From game theory, if both players use only pure strategies and the payoffs
are distinct from each other, at most one pure Nash Equilibrium (NE) exists (Osborne & Rubinstein, 1994). We
concentrate on the setting where all payoffs are random
variables under some unknown distributions. Samples (or
queries) can be obtained by submitting pure strategies of
the two players and receiving the associated payoffs. Our
target is to answer the questions: (1) whether there is a pure
NE; and 2) how to identify it if exists using as few queries
as possible.
Our motivation for this problem comes from the need of
identifying NE in many practical competitive situations.
Since NE is a fundamental concept in game theory and
many other fields, the computational complexity needed for
NE is of much interest. However, in practice we are often
given access to the data generated from some practical phenomena, rather than a clear rule for the payoffs of the game.
Hence, the empirical game-theoretic analysis (Wellman,
2006; Jordan et al., 2008) has received a lot of attention

to estimate the practical games through simulation. In the
empirical modeling, pure-strategy profiles of players are
submitted to the game and we receive the associated payoffs. Fearnley et al. (2015) consider the process managed
in an online manner by algorithms and analyze the complexity of these payoff-query algorithms. The main focus
is on whether the query methods can figure out mixed Nash
Equilibrium with only a fraction of profiles. Extensions
have been made to obtain query complexity on approximate
Nash Equilibrium (Babichenko, 2016), correlated equilibrium (Hart & Nisan, 2016), and well-supported approximate correlated equilibrium (Goldberg & Roth, 2016).
The above work is essentially a revealed-payoff search
model (Jordan et al., 2008), where payoffs are deterministic
and every profile only needs to be queried at most once. We
concentrate on the noisy-payoff model (Jordan et al., 2008),
where the received payoff of a query is a sample of an underlying distribution. This random payoff setting is more
realistic in practice, where randomness naturally arises because of incomplete information, noise, or other stochastic
factors in the world. A simple but well-known example is
the coin flipping game, where two players throw a coin and
guess its landing upper side. Since the physical process of
coin landing can be determined by many noisy factors, the
payoffs to the two players, which depend on the landing
results, are random. This notable discrepancy leads to different algorithms and complexity bounds for the two models, since for noisy-payoff models, more queries are needed
for any profile to get an estimated payoff near its expectation value with high probability and the additional computational cost can take a dominating role in complexity. Previous work has explored different methods for noisy payoff models, such as interleaving the samples (Walsh et al.,
2003) and using regression for payoffs (Vorobeychik et al.,
2007). This paper turns to bandit models, a relatively natural direction from the view of online learning, since query
methods themselves hold a sequential property. Put another
way, we select a strategy to submit based on previous observations at each round of query methods.

1

Dept. of Comp. Sci. & Tech., TNList Lab, State Key Lab
for Intell. Tech. & Systems, CBICR Center, Tsinghua University.
Correspondence to: Jun Zhu <dcszj@tsinghua.edu.cn>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

This task can be viewed as a variant of best arm identification (BAI) in the literature of multi-armed bandits
(Jamieson & Nowak, 2014), where different pure strategy
profiles are regarded as arms. The classical BAI problem
is to identify which arm is the one with the highest mean.

Identify the Nash Equilibrium in Static Games with Random Payoffs

There are two basic settings for BAI problem—fixed budget and fixed confidence (Kaufmann et al., 2015). In this
paper, we focus on the fixed confidence setting, where the
purpose of an algorithm is to identify the best arm with a
fixed probability by as few pulls (queries) as possible.
Contributions: We study both the sample complexity
lower bound and algorithms in the fixed confidence setting
for two-player zero-sum static games with random payoffs.
In Section 3, we discuss the sample complexity lower
bound for the case NE exists. Previous proofs on the lower
bound for BAI all rely on changes of distributions (Audibert & Bubeck, 2010; Kaufmann et al., 2015; Mannor &
Tsitsiklis, 2004)—changes on a single arm can change the
best arm in the bandit model. For our problem, we prove
the lower bound for the arms in the same row or same column with the NE by similar techniques. However, this approach does not work for those arms that are in neither the
same row nor column with the NE, since changing the distribution of such an arm does not change the NE (details
are in Section 3). To get the lower bound on these arms,
we rephrase the arm selection as a hypothesis testing problem and use the minimax techniques for hypothesis testing (Tsybakov, 2009) to get the bound.
There are two types of algorithms for the BAI problem in the fixed confidence setting (Jamieson & Nowak,
2014)—based on either confidence bound (Kalyanakrishnan et al., 2012) or successive eliminations on suboptimal
arms (Maron & Moore, 1997). In Section 4, we propose
two corresponding algorithms to identify NE for our problem in the fixed confidence setting. The first algorithm has
a provable bound on sample complexity, and we show that
the second one will stop in a finite number of time steps
with probability at least 1 − δ.
Related work: Garivier et al. (2016) also study the twoplayer zero-sum game with random payoffs. They consider
the case that each player selects her strategy one-by-one,
while we focus on the case that both players select their
strategies simultaneously. Our setting is suitable for the
case that each player chooses actions independently. Many
games are static, such as the tai sai game where players
independently guess the range of the outputs of three dices.
Much work has been done on Nash Equilibrium.
Daskalakis et al. (2009) shows that it is computationally
hard to recognize exact Nash Equilibrium, even for the
simplest two-person game (Chen et al., 2009). As empirical game-theoretical analysis (Wellman, 2006) is proposed, Fearnley et al. (2015) studies the payoff-query algorithms and considers the query complexity as a criterion for
computational complexity. Following work (Babichenko,
2016; Goldberg & Roth, 2016) has extended this criterion
to some other approximate equilibrium. Query bounds for

NE have also been given on specific games such as twostrategy anonymous games (Goldberg & Turchetta, 2015)
and bimatrix games (Fearnley & Savani, 2016).

2. Preliminaries
We start by presenting the basic settings, notations and assumptions that will be used in the sequel.
2.1. Basic settings
Two-player zero-sum static game with random payoffs:
A static game is a model in which all players choose their
strategies once and simultaneously. A two-player zero-sum
game involves two players 1 and 2, and each player chooses
her own strategy si from a strategy set Si , i ∈ {1, 2}. After decisions are made, player 1 gets payoff rews1 ,s2 and
player 2 gets −rews1 ,s2 . Each player tries to maximize her
payoff. The game can be represented by a m × n-matrix
where m = |S1 |, n = |S2 |. If rewi,j is a deterministic
value for any i, j, it is direct to identify the NE (i∗ , j ∗ )
which has the minimum value in row i∗ and the maximum
value in column j ∗ . We consider the more practical games
with random payoffs, whose distributions are unknown.
This makes the identification of NE difficult and hence we
employ query methods to learn the NE empirically. In each
query, the algorithm generates a pure strategy (s1 , s2 ), and
the environment returns the associated payoff. Our target
is to determine whether the Nash Equilibrium (NE) exists1
and what it is if it does exist with as few queries as possible.
Multi-armed bandits: In a bandit model, an agent is facing a set of actions (or arms), and needs to select one arm
to pull every time. In our case, an arm is specified by s ∈
[m] × [n], where [m] denotes the set {1, · · · , m}. Successive pulls of an arm (i, j) yield a sequence of observations
(or rewards) Y1i,j , Y2i,j , · · · . A policy I = {It : t ∈ N+ }
denotes a sequence of random variables, where the variable
It ∈ [m] × [n] indicates which arm to pull at time step t.
The classical best arm identification (BAI) problem is to
identify which arm is the one with the highest mean. There
are two basic settings for the BAI problem—fixed budget
and fixed confidence (Kaufmann et al., 2015). In this paper,
we focus on the identification of the NE of a static game or
detect its absence in the fixed confidence setting. That is,
we aim to identify the correct NE with probability at least
1 − δ with efficient sampling, where δ ∈ (0, 1) is the confidence parameter. Algorithms satisfying this requirement
are known as δ-PAC algorithms (Kaufmann et al., 2015).
Following Kaufmann et al. (2015), a practical BAI algorithm in the fixed confidence setting typically consists of:
• Policy: given a sequence of past observations, a
policy determines which arms to pull.
• Stopping rule: a stopping rule can be described as
1

We focus on the NE of pure strategy. So NE may not exist.

Identify the Nash Equilibrium in Static Games with Random Payoffs

a series of observation sets Ft , t ∈ N+ . When an
element o ∈ Ft is observed, the policy stops sampling.
• Recommendation rule: a recommendation rule is
usually to recommend the best arm.
As we shall see, in our problem the best arm may not exist,
when the sampling process stops, the recommendation rule
determines whether the NE exists or not, and if exists, it
determines which is the NE.
2.2. Basic assumptions and notations
Let Pi,j ∈ P be the underlying distribution of arm (i, j)
and P is a set of probability measures. For an arm s, we
use µs to denote the expectation of Ps . And let µ̄s (t) denote the empirical mean of s at time step t, we omit t for
simplicity when there is no ambiguity . Here, we consider
pulling an arm once as a time step, that is, time step t means
that we have pulled arms for t times. We use M̄t to denote
the empirical matrix with entry (i, j) representing the empirical mean of Pi,j at time step t. For s = (s1 , s2 ), we
define row(s) = {s0 = (s01 , s02 ) : s01 = s1 }, col(s) =
{s0 = (s01 , s02 ) : s02 = s2 }, nei(s) = row(s) ∪ col(s), and
s[1] = s1 , s[2] = s2 .
Let N E(M) denote the NE of matrix M. Formally,
N E(M) = s if there is an arm s such that µs =
mins0 ∈row(s) µs0 and µs = maxs0 ∈col(s) µs0 ; otherwise if
there is no such arm, we denote N E(M) = none to show
that no Nash Equilibrium exists. Specifically, we use M
to denote the matrix whose entry (i, j) is the expectation
of distribution Pi,j . Our target is to identify N E(M). For
convenience, let s∗ = N E(M). Let Ri , Cj be the sets of
the arms corresponding to the i-th row and the j-th column
of M respectively. Our lower bound mainly focuses on the
case that the NE exists (i.e., N E(M) 6= none) and our algorithms are δ-PAC. We assume that the expectations of the
arms are mutually different.
In the proof of the lower bound, it is natural to make P
abundant enough to include various continuous distributions while ruling out some extreme situations where distributions are not mutually absolutely continuous. So we
assume that P consists of parametric distributions continuously parameterized by their means. This assumption has
been widely used in studying multi-armed bandits (Lai &
Robbins, 1985; Kaufmann et al., 2015).
Assumption 1. For all p, q ∈ P such that p 6= q, for all
α > 0:
• ∃q1 ∈ P, KL(p, q) < KL(p, q1 ) < KL(p, q) + α,
Eq1 > Eq > Ep.
• ∃q2 ∈ P, KL(p, q) < KL(p, q2 ) < KL(p, q) + α,
Eq2 < Eq < Ep.
Here KL(q, p) is the KL-divergence. Many distributions
are included in P, such as the broad class of one-parameter

exponential family distributions.

3. Lower bound
Let Nδ (s) denote the number of pulls on an arm s by a δPAC algorithm. For arm s ∈ nei(s∗ ), we provide a lower
bound of Nδ (s) in Lemma 1, which is obtained by the
classical technique of changes of distributions (Kaufmann
et al., 2015; Lai & Robbins, 1985; Audibert & Bubeck,
2010) and Theorem 1 in Kaufmann et al. (2015).
Theorem 1. (Kaufmann et al., 2015). Let v and v 0 be two
bandit models with K arms, such that for all a ∈ [K],
the distributions Pa and P0a are mutually absolutely continuous. For any almost-surely finite stopping time σ with
respect to Ft , we have
X
E[Nδ (sa )]KL(Pa , P0a ) ≥ sup d(Pv (ε), Pv0 (E)),
E∈(Ft )

a≤K

where d(x, y) = x log(x/y) + (1 − x) log[(1 − x)/(1 − y)].
Lemma 1. Let s0 = arg mins∈nei(s∗ )\s∗ KL(Ps∗ , Ps ),
then the number of pulls on nei(s∗ ) of any δ-PAC algorithm has a lower bound as follows:

X
1
E[Nδ (s)] ≥
KL(P
s∗ , Ps0 )
s∈nei(s∗ )

X
1
1
.
+
log
∗)
KL(P
,
P
2.4δ
s
s
∗
∗
s∈nei(s )\s

Proof. By Assumption 1, for all arms s ∈ nei(s∗ ), there
exists an alternative model, in which the only arm modified
is arm s, and the modified distribution P0s satisfies:
• KL(Ps , Ps∗ ) < KL(Ps , P0s ) < KL(Ps , Ps∗ ) + α,
and EP0s < µs∗ for s ∈ row(s∗ )\s∗
• KL(Ps , Ps∗ ) < KL(Ps , P0s ) < KL(Ps , Ps∗ ) + α,
and EP0s > µs∗ for s ∈ col(s∗ )\s∗
• KL(Ps∗ , Ps0 ) < KL(Ps∗ , P0s∗ ) < KL(Ps∗ , Ps0 ) + α,
and EP0s∗ < µs0 for s0 ∈ col(s∗ )\s∗ or EP0s∗ > µs0
for s0 ∈ row(s∗ )\s∗ .
Denote the original bandit model by v and the modified
one by v 0 . In particular, the NE for v 0 is no longer
s∗ . Consider the event E : the recommendation rule
recommends s∗ as NE. Any δ-PAC algorithm satisfies
Pv (E) > 1 − δ and Pv0 (E) < δ, so by Theorem 1,
1
.
E[Nδ (s)]KL(Ps , P0s ) ≥ d(Pv (E), Pv0 (E)) ≥ log 2.4δ
Hence we have:
( log 1/(2.4δ)
∗
log 1/(2.4δ)
KL(Ps ,Ps∗ )+α , s 6= s
E[Nδ (s)] ≥
≥
log 1/(2.4δ)
∗
KL(Ps , P0s )
KL(Ps∗ ,Ps0 )+α , s = s
Let α → 0 and we complete the proof.
From the proof, we can see that the lower bound relies on the fact that we can change the best arm (i.e.,
N E(M) in our case) by changing the distribution of a single arm. However, this proof technique is not suitable for

Identify the Nash Equilibrium in Static Games with Random Payoffs

s∈
/ nei(s∗ ) because the NE will not change no matter what
the distribution of an arm s ∈
/ nei(s∗ ) is.
In theory, we only need to pull s ∈ nei(s∗ ) to identify NE
because of the same reason (i.e., the distributions of arm
s∈
/ nei(s∗ ) won’t change NE). In practice however a policy does not know which arm is in nei(s∗ ) in advance, so
it may make some pulls on s ∈
/ nei(s∗ ) before making
a sufficient number of pulls on nei(s∗ ). So we can consider the arm selection as a hypothesis testing problem, and
then use the lower bound techniques for the minimax risk
of hypothesis testing (See Chapter 2 in Tsybakov (2009)).
Specifically, our proof is based on the following lemma:
Lemma 2. Let P1 , · · · , PK be probability distributions
supported on some set X , with Pi absolutely continuous
w.r.t P1 . For any measurable function ψ : X → [K], we
have: K
K
X
X
1
KL(P1 , Pk )},
Pk (ψ = k) ≥ exp{−
e

Ht ≥

where Pk (ψ = k) := Pk ({x : ψ(x) = k}) for clarity.

(1)

Proof. With straight-forward computations, we have:

1
Ht ≥ Ht (h1,1 ) + Ht (hi,j ) + Ht (h1,j ) + Ht (hi,1 )
4
1 X
=
1[It0 ∈
/ nei((1, 1)); h1,1 ]
4 0
t ≤t

+ 1[It0 ∈
/ nei((i, j)); hi,j ] + 1[It0 ∈
/ nei((1, j)); h1,j ]

+ 1[It0 ∈
/ nei((i, 1)); hi,1 ]
 X
1X
1[It0 = (i0 , j 0 ); h1,1 ]
≥
4 0
t ≤t
i0 ,j 0 ≥2
X
1[It0 = (1, j 0 ); hi,1 ]
+ 1[It0 = (1, 1); hi,j ] +
j 0 ≥2

k=2

k=1

e−∆ (1 − e−t∆ )
.
4e(1 − e−∆ )

+

X


1[It0 = (i , 1); h1,j ] .
0

i0 ≥2

Proof. This lemma is an extension of Lemma 2.6 in Tsybakov (2009) from two distributions to multiple distributions. We put the proof in Appendix A.
Now we show what the hypotheses to be tested in our problem are and how to apply Lemma 2. Without loss of generality, consider a game M with N E(M) = (1, 1). Let’s
consider a set of hypotheses: new games hi,j constructed
by swapping the i-th row with the first row and the j-th column with the first column of M. Obviously, these games
are essentially the same game up to permutation. We will
show the lower bound on the maximum number of pulls
among these hypotheses by arbitrary policies.
0

0

00

00

00

0

Formally, define fi,j (i , j ) = (i , j ) where i = i if
i0 ∈
/ {1, i} else i00 = 1 + i − i0 and j 00 = j 0 if j 0 ∈
/ {1, j}
else j 00 = 1 + j − j 0 . Let hi,j specify a game such that
the distribution of arm (i0 , j 0 ) is Pfi,j (i0 ,j 0 ) . Let Ht (hi,j )
be the sum of the expected number of pulls on arms s ∈
/
nei((i, j)) until time step t under hypothesis hi,j :
Ht (hi,j ) :=

X

Define Phta,b as the distribution of observations until time
step t under the hypothesis ha,b , and define function


(1, 1) i0 , j 0 ≥ 2,




(1, j) i0 ≥ 2, j 0 = 1,
0 0
g(i , j ) :=
0
0


(i, 1) j ≥ 2, i = 1,


(i, j) i0 = j 0 = 1.
Let Pit0 ,j 0 = Phtg(i0 ,j0 ) . Consider events ev(t, s): policy
selects arm s at time step t. We have:
Ht ≥

1 X X t0
P 0 0 (ev(t0 , (i0 , j 0 )))
4 0 0 0 i ,j
t ≤t i ,j

X
0
1 X
t0
exp{−
KL(P1,1
, Pit0 ,j 0 )}
≥
4e 0
0 0
t ≤t

i ,j

t
1 X
exp{−t∆}
≥
4e 0
t =1

1[It0 ∈
/ nei((i, j)); hi,j ],

t0 ≤t

and let Ht := maxi,j Ht (hi,j ) be the maximum number
of pulls under any hypothesis. Then, theorem 2 shows the
lower bound of Ht .
Theorem 2. For any i, j
≥
2, let ∆
:=
(m − 1)(n − 1)(M u(Pi,j , P1,1 ) + M u(Pi,1 , P1,j ) +
Pj 0 6=j
Pi0 6=i
0
0
0
0
j 0 ≥2 M u(Pi,j , P1,j ) +
i0 ≥2 M u(Pi ,1 , Pi ,j )) +
mM u(Pi,1 , Pi,j )
+
nM u(P1,j , Pi,j )
where
M u(P1 , P2 ) := KL(P1 , P2 ) + KL(P2 , P1 ). Then,
we have the lower bound:

e−∆ (1 − e−t∆ )
=
.
4e(1 − e−∆ )
The second inequality is proven by Lemma 2. The third
is by the fact that let Pi,j (a, b) denote the distribution
of arm (a, b) under hypothesis
hg(a,b) , then we have
P
t
, Pit0 ,j 0 ) =
KL(P
KL(P1,1
0
1,1 (It0 ), Pi0 ,j 0 (It0 ));
t ≤t
0
and
note
that
KL(P
(I
),
Pi0 ,j 0 (It0 ))
≤
1,1 t
P
00 00
00 00
0
0
KL(P
(i
,
j
),
P
(i
,
j
));
summing
over
all
00
00
1,1
i ,j
i ,j
i0 , j 0 , we get the third inequality.
We can get a different lower bound by choosing a different
i, j in Theorem 2, and take the maximum one. Though our

Identify the Nash Equilibrium in Static Games with Random Payoffs

lower bound is not on the expected number of pulls, it intuitively answers why the pulls on s ∈
/ nei(s∗ ) are unavoide−∆ (1−e−t∆ )
e−∆
able. Obviously, 4e(1−e−∆ ) ≤ 4e(1−e
−∆ ) , which suggests that there is a policy which pulls on s ∈
/ nei(s∗ ) for a
bounded number of times. This result inspires us to design
a policy with a bounded number of pulls on s ∈
/ nei(s∗ ),
as shown in Section 4.

4. Algorithms
We now present two δ-PAC algorithms for our problem.
The first one is inspired by LUCB (Kalyanakrishnan et al.,
2012) and UCB1 (Auer et al., 2002), while the second one
follows another line of BAI algorithms which are based on
the successive action eliminations (Even-Dar et al., 2006;
Maron & Moore, 1997).
4.1. LUCB-G
We first present and analyze the LUCB-G (i.e., LUCB for
Game) algorithm, as illustrated in Alg. 1.
4.1.1. A LGORITHM
Informally, our problem can be divided into m + n bandit
tasks—m for identifying s∗r (i) := arg mins∈Ri µs and n
for s∗c (j) := arg maxs∈Cj µs . So in each round2 , LUCBG can be divided into two stages. In the first stage, it selects
two bandit tasks—a row and a column. Note that LUCB-G
tries to identify s∗r and s∗c after each round. If before round
γ, it identified s̄∗r (i) as the arm with minimum mean in Ri ,
or identified s̄∗c (j) 3 as the arm with maximum mean in Cj ,
then LUCB-G will not select row i or column j. That is to
say, we only select bandit models from the following rows
and columns at the γ-th round:
ar(γ) := {i ∈ [m] : s̄∗r (i) not identified until round γ.}
ac(γ) := {j ∈ [n] : s̄∗c (j) not identified until round γ.}
We’ll introduce how to identify these arms later in this
section. In the second stage, we pull arms according
to past observations and some confidence bound function
β : N+ × N+ → (0, ∞), which will be presented soon in
Section 4.1.2. Here we show our first policy in Alg. 1.
We have a clock for each bandit task, and our confidence
bounds rely on them. Define τr (i, t) as the set of all the
time steps t0 that satisfy the two requirements: (1) t0 < t;
(2) at the round when t0 takes place, row i is chosen, and at
least one line from 15 to 17 is executed. Similarly, define
τc (j, t) as the set of all the time steps t0 that satisfy the two
requirements: (1) t0 < t; (2) at the round when t0 takes
place, column j is chosen, and at least one line from 21 to
23 is executed.
The method of identifying s̄∗r (i) and s̄∗c (j) also relies on
2
3

We pull arms for several times in each round.
With a probability, s̄∗r (i) 6= s∗r (i) or s̄∗c (j) 6= s∗c (j).

the confidence bound function β. For an arm s, define
L(s, u, t) = µ̄s (t0 )−β(u, t), U (s, u, t) = µ̄s (t0 )+β(u, t)4 .
And let Ts (τ ) = {t : It = s, t ∈ τ }. Alg. 1 determines
s̄∗r (i) and s̄∗c (j) at time step t as follows:
• If ∃s ∈ Ri , for all s0 ∈ Ri \s, we have
U (s, |Ts (τ )|, |τ |) ≤ L(s0 , |Ts0 (τ )|, |τ |), where
τ := τr (i, t), then Alg. 1 takes s as s̄∗r (i).
• If ∃s ∈ Cj , for all s0 ∈ Cj \s, we have
L(s, |Ts (τ )|, |τ |) ≥ U (s0 , |Ts0 (τ )|, |τ |), where
τ := τc (j, t), then Alg. 1 takes s as s̄∗c (j).
Now we introduce the stopping and recommendation rules
for Alg. 1.
Stopping and recommendation rules: The policy stops
and recommends N E as follows:
• If after round γ, there is an arm s, Alg. 1 takes it
as s̄∗r (s[1]) and s̄∗c (s[2]). Then Alg. 1 stops and
recommends s as the NE.
• Else if after round γ, s̄∗r (i) and s̄∗c (j) have all been
determined. Then the policy stops and the recommendation rule determines that the underlying game does
not have a NE.
4.1.2. δ-PAC
We now show that Alg. 1 is a δ-PAC algorithm. Lemma
3 guarantees that if β(u, t) satisfies the requirements in
InEq. (2), then the probability that there is an arm violating its confidence bounds is less than δ. Our
q choice of the
4

log(mnt /4δ) 5
confidence bound function is β(u, t) =
,
2u
which satisfies this requirement. And Theorem 3 is a simple application of Lemma 3 since if no arm violates its
confidence bounds, the stopping and recommendation rules
won’t make mistakes.

Lemma 3. Let β(u, t) : N+ × N+ → (0, ∞) be a function
such that:
∞ X
t
X
t=1 u=1

exp{−2uβ(u, t)2 } ≤

δ
.
2K

(2)

Consider a bandit model v with K arms, for each arm,
there is a sequence (t1 , u1 ), (t2 , u2 ), · · · such that ti ≥
ui , ti+1 ≥ ti , ui+1 ≥ ui , and a sequence u01 , u02 , · · · such
that u0i ≥ ui , and then the probability that ∃s ∈ v, i such
that
u0i
1 X
| 0
Y s − µs | > β(ui , ti )
ui i=1 i
is less than δ.
4
In this paper, we have t0 ≥ t. Thus in fact L and U are
functions of t0 , but for convenience we omit the notation of t0 .
5
For convenience, let 1/0 = +∞. So if u = 0, β(u, t) =
+∞.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Proof. The proof can be found in Appendix B.
Theorem 3. The probability of making mistakes by the recommendation and stopping rules of LUCB-G is at most δ.
Proof. If all arms don’t violate their confidence bounds,
then it is easy to see that we determine NE correctly. As
our choice of β(u, t) satisfies the condition in InEq. (2),
we can use Lemma 3 to get the result.
4.1.3. S AMPLE COMPLEXITY
We analyze the sample complexity of Alg. 1 in this section. We provide a proof of the sample complexity when
N E(M) 6= none here, and the sample complexity when
N E(M) = none is a straight-forward application of the
result in LUCB (Kalyanakrishnan et al., 2012).
P
1
For convenience, let Hr (i) =
s∈Ri \s∗
)2
r (i) (µs −µs∗
r (i)
P
1
. In the second
and Hc (j) =
s∈Cj \s∗
∗ (j) )2
c (j) (µs −µsc
stage of each round, LUCB-G pulls arms similarly as
LUCB and UCB1. When the NE of the underlying game
exists, the pulls can be divided into three parts:
• part1: Time steps t such that s∗ is the NE of M̄t .
• part2: Time steps t such that s 6= s∗ is the NE of M̄t .

Algorithm 1 LUCB-G
1: Input: distribution matrix M, confidence δ
2: Pull all arms
3: Chr(i) = Chc(j) = 0 for i ∈ [m], j ∈ [n], t = m ∗ n
4: while Not stop do
5:
selr (i) = selc (j) = 0 for i ∈ [m], j ∈ [n]
6:
if s = N E(M̄t ) 6= none then
7:
selr (s[1]) = selc (s[2]) = 1
8:
else
9:
Let î := arg mini∈ar[γ] Chr(i)
10:
Let ĵ := arg minj∈ar[γ] Chc(j)
11:
selr (î) = selc (ĵ) = 1
12:
end if
13:
if ∃î ∈ [m], selr (î) = 1 then
14:
Chr(î) = Chr(î) + 1
15:
Pull s1 := arg mins∈Rî µ̄s (t), t = t + 1
r
|τr (î,t)|/3
,t = t+1
16:
Pull arg mins∈Rî µ̄s − 2 log
|T (τ (î,t))|
s

17:
18:
19:
20:
21:

• part3: Time steps t such that there is no NE of M̄t .
22:

Therefore, the total sample complexity can be decomposed
as the summation of the bounds for the three parts.
We first provide sample complexity bounds for the pulls in
part2 and part3, which appear because of the algorithm’s
misjudgments on which arm is N E(M) during training,
while deferring the bound for part1 to Lemma 6, which is
relatively standard.
Obviously, at time step t, if part2 or part3 happens, then s∗ 6= arg mins0 ∈row(s∗ ) µ̄s0 (t) or s∗ 6=
arg maxs0 ∈col(s∗ ) µ̄s0 (t). Lemma 4 ensures that these
events will not happen with a high probability if Alg. 1 selects row(s∗ ) and col(s∗ ) for a sufficiently large number
of times. The key idea is to use the UCB1 policy (line 20,
26) (Auer et al., 2002): considering column j, when the algorithm chooses it in the first stage, the algorithm pulls an
arm in Cj by UCB1. This ensures that with a high probability, the policy pulls a sufficiently large number of times
on s∗c (j), and then by Hoeffding’s inequality, we get the
bound. A formal statement is in Lemma 4.
Lemma 4. Without loss of generality, consider column j,
let φ(γ) := 1[s∗c (j) 6= arg maxs0 ∈Cj µ̄s0 when
1 sePAlg.
∞
lects column j for the γ-th time], let φ =
φ(γ).
γ=1
Then, the expectation of φ satisfies the inequality:

Pull arg mins∈Rî \s1 L(s, |Ts (τr (î, t))|, |τr (î, t)|), t =
t+1
end if
if ∃ĵ ∈ [n], selc (ĵ) = 1 then
Chc(ĵ) = Chc(ĵ) + 1
Pull s1 := arg maxs∈Cĵ µ̄s (t), t = t + 1
r
|τr (ĵ,t)|/3
Pull arg maxs∈Cĵ µ̄s + 2 log
, t = t+1
|T (τ (ĵ,t))|
s

E[φ] − c1 Hc (j)(log E[φ]) − c2 Hc (j) − c3 ≤ 0,
where c1 , c2 , c3 are positive constants.

(3)

r

Pull arg maxs∈Cĵ \s1 U (s, |Ts (τr (ĵ, t)|), |τr (ĵ, t)|), t =
t+1
24:
end if
25: end while
23:

Proof. Let tγ denote the time step when Alg. 1 selects column j for the γ-th time and ξγ = arg maxs∈Cj µ̄s (tγ ).
Let ∆γ = µs∗c (j) − µξγ . If ξγ 6= s∗c (j), then we have
µ̄s∗c (j) (tγ ) ≤ µs∗c (j) − ∆γ /2 or µ̄ξγ (tγ ) ≥ µξγ + ∆γ /2.
So let Set1(s) = {γ : ξγ = s}, Set2(s) = {γ ∈
Set1(s) : µ̄ξγ (tγ ) ≥ µξγ + ∆γ /2} and Set3(s) = {γ ∈
Set1(s) : µ̄s∗c (j) (tγ ) ≤ µs∗c (j) − ∆γ /2}. With the above
argument, for s ∈ Cj \s∗c (j), we have E[|Set1(s)|] ≤
E(|Set2(s)| + |Set3(s)|).
Due to Alg. 1, the policy pulls s for rounds γ ∈
Set1(s).
So by Hoeffding’s inequality, E|Set2(s)| ≤
P∞
exp{−2γ((µ
− µs )/2)2 } ≤ 2/(µs∗c (j) − µs )2 .
s∗
γ=1
c (j)
Now consider Set3(s), which is computed as:

E|Set3(s)| = E 

X

γ∈Set1(s)
2

r


(µs∗c (j) + µs )
] .
1[µ̄s∗c (j) (tγ ) ≤
2

Let T (t) be the number of pulls on s∗c (j) at line 22 in Alg. 1
at that time step t. By Hoeffding’s inequality and straight-

Identify the Nash Equilibrium in Static Games with Random Payoffs

forward computations, we have
h X
γ
E|Set3(s)| ≤E
1[T (tγ ) ≤ ]
2

Then, the following inequality holds:
X
X
E|Sw | ≤O (
Λ(Hr (i)) +
Λ(Hc (j)))

γ∈Set1(s)

i

(µs∗c (j) + µs )
γ i
; T (tγ ) ≥ ]
+1[µ̄s∗c (j) (tγ ) ≤
2
2
h X
γ
≤E
1[T (tγ ) ≤ ]
2
γ∈Set1(s)
µs∗ (j) − µs 2 i
+ exp{−γ( c
) }
2


X
4
γ
.
≤E 
1[T (tγ ) ≤ ] +
2
(µs∗c (j) − µs )2
γ∈Set1(s)

Note that Line 22 is the UCB1 policy proposed by Auer
et al. (2002). So by Theorem 1 in Auer et al. (2002) (A
slightly modification on this theorem, see Appendix C),
we have E[γ 0 − T (tγ 0 )] ≤ O(Hc (j) log γ 0 ). Then with
Markov inequality, we can get P [γ 0 − T (tγ 0 ) ≥ γ 0 /2] ≤
log γ 0 )
O(Hc (j) log γ 0 )
, that is, P [T (tγ 0 ) ≤ γ 0 /2] ≤ O(Hc (j)
.
γ0
γ0
So let Set3 = ∪s∈Cj \s∗c (j) Set3(s), we have:



E[|Set3|] ≤O(Hc (j)) + E 

X

1[T (tγ ) ≤

γ∈Set1



γ 
]
2


X O(Hc (j) log γ)

γ
γ∈Set1


|Set1|
X O(Hc (j)logγ)

=O(Hc (j)) + E 
γ
γ=1


≤O(Hc (j)) + E O(Hc (j)(log |Set1|)2 )
≤O(Hc (j)) + E 

≤O(Hc (j) + Hc (j)(log E|Set1|)2 )
where Set1 = ∪s∈Cj \s∗c (j) Set1(s). The third inequality is
by simple integration, and the last inequality holds because
f (x) = (log x)2 is a concave function for x > e. Note that
φ = |Set1|. With E|Set1(s)| ≤ E(|Set2(s)| + |Set3(s)|),
we complete the proof.
It is noteworthy that although we do not have an analytical solution of E[φ] from InEq. (3), it is obvious that the
solution is bounded, that is, it will not diverge as δ → 0.
Then, we can get the sample complexity on part 2 and 3, as
in Lemma 5.
Lemma 5. Suppose s∗ = N E(M) 6= none, let Sw =
{Rounds γ such that Rs∗ [1] or Cs∗ [2] is not chosen by Alg. 1}.
Let Λ(a) be the maximum value among all solutions that
satisfy the following inequality (constants c1 , c2 , c3 are the
same as in Lemma 4):
x − c1 a(log x)2 − c2 a − c3 ≤ 0.

j


+(m + n)(Λ(Hr (s [1])) + Λ(Hc (s∗ [2]))) .
∗

Proof. We put the proof in Appendix D.
The bound on part1 is based on the result of LUCB, as in
Lemma 6, which has almost the same result on the sample
complexity as policy LUCB.
Lemma 6. Without loss of generality, considering column
j, suppose s̄∗c (j) is identified by Alg. 1 after being selected
for γc (j) rounds, then

Hc (j)
) .
E[γc (j)] = O Hc (j) log(
δ


Proof. The proof is the same as that of Theorem 6 in
(Kalyanakrishnan et al., 2012), except slightly changes on
description and constants. See Appendix E.
With the above results, we are ready to get our major result
on the sample complexity of LUCB-G, as in Theorem 4.
Theorem 4. When N E(M) 6= none, the sample complexity of LUCB-G is:


O Hr (s∗ [1]) log

+ E[|Sw |] ,

Hr (s∗ [1])
Hc (s∗ [2])
+ Hc (s∗ [2]) log
δ
δ

where E[|Sw |] is bounded as in Lemma 5.
When
N E(M) = none, the sample complexity of LUCB-G is:


X
X
H
(i)
H
(j)
r
c
.
O
Hr (i) log
+
Hc (j) log
δ
δ
i
j

Proof. By Lemma 5 and Lemma 6, with straight-forward
computations, we get the complexity.
Note that the sample complexity of LUCB-G is optimal
within a constant gap if N E(M) 6= none. This is because
that E[|sw |] is bounded and for some family P, P1 , P2 ∈ P,
the KL-divergence KL(P1 , P2 ) has the same order as the
squared mean-difference (EP1 − EP2 )2 (e.g., normal distributions with unit variances). Therefore, we can replace
the KL-divergence terms in Lemma 1 by the corresponding
squared mean-difference terms.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Algorithm 2 Racing
1: Input: distribution matrix M, confidence δ
2: γ = 1.
3: while Not stop do
4:
Pull all arms except those have been eliminated.
5:
For an active arm s, if ∃s0 ∈ row(s) : 2β1 (γ) <
µ̄s − µ̄s0 and ∃s0 ∈ col(s) : µ̄s0 − µ̄s > 2β1 (γ), then
we eliminate s.
6:
For all sequences of arms S = {s1 , s2 , · · · , s2k }, if
S satisfies ∀i ∈ N:
• s2i+1 ∈ row(s2i ), s2i+2 ∈ col(s2i+1 ).
•

all arms are eliminated in row(s2i ), col(s2i+1 )
except s ∈ S.

•

µ̄s2i+1 − µ̄s2i > 2β1 (γ).

•

µ̄s2i+1 − µ̄s2i+2 > 2β1 (γ),

where sj := s(j−1)%(2k)+1 , eliminate all arms in S.
7:
γ = γ + 1.
8: end while

Pulls

Pulls

5

ALL
Racing

4

LUCB-G

5

ALL

4

Racing

3
3
−1 −3 −5 −7

δ

LUCB-G
2
δ
−1 −3 −5 −7

(a) first-game

(b) first-game

Pulls
5
4.5

ALL
Racing
LUCB-G

4
−1 −3 −5 −7

δ

(c) second-game

4.2. A racing algorithm
Finally, we present another algorithm, along the line of racing algorithms for BAI (Even-Dar et al., 2006; Maron &
Moore, 1997). A racing algorithm maintains a set of active
arms, and during each round it samples all the active arms
and then eliminates some arms according to certain rules.
However, we cannot eliminate an arm when the algorithm
“knows” it cannot be the NE immediately. Consider a 2 × 2
game. Suppose an algorithm determines (1, 1) 6= s̄r (1),
and eliminates it immediately. Then we cannot determine
whether (2, 1) is NE or not. Therefore, our racing algorithm eliminates arm s only if Alg. 2 determines that
∗
∗
∗
s ∈
/ {s̄∗r (s[1],
q s̄c (s[2]))} or s̄r (s[1]) 6= s̄c (s[2]). Let
2

log(cmnγ /δ)
. Our racing algorithm is shown
β1 (γ) =
2γ
in Alg. 2, whose stopping and recommendation rules are:
• If only an arm is not eliminated after round γ, then
recommend this arm as NE.

• If all arms are eliminated after round γ, then the algorithm determines N E = none.
As shown in Theorem 5, this algorithm is δ-PAC and it will
terminate in finite time step with probability at least 1 − δ.
Theorem 5. Alg. 2 is δ-PAC and will terminate in finite
time with probability at least 1 − δ.
Proof. We put the proof in Appendix F.

5. Experiments
We now empirically verify the sample complexity of our
algorithms. We choose a simple algorithm as our baseline
(denoted by ALL), which pulls all arms at each round until stopping. The stopping and recommendation rules are

Figure 1. The results on two simulated games.

the same as LUCB-G, and the confidence bound for this
baseline is slightly different, see Appendix G for details.
We evaluate on synthetic 5×5 games, where the payoffs are
all random Bernoulli variables. The first game has a NE,
while the second game has no NE. The results are shown
in Fig. 1, where both axes are in log-scale with base 10.
The number of pulls needed for both games are shown in
Fig. 1(a) and Fig. 1(c) separately and our algorithms outperform the baseline (i.e., ALL). Fig. 1(b) shows the number of pulls on s ∈
/ nei(s∗ ) in the first game and we can see
that this number is bounded, agreeing with our analysis.

6. Conclusions and Discussions
We analyze the two-player zero-sum static game with random payoffs via efficient sampling and give a lower bound
of the sample complexity in the case that the Nash Equilibrium (NE) exists. We then present two δ-PAC algorithms
to identify the NE. They follow two lines of algorithms for
the best arm identification problem in the fixed confidence
setting. The sample complexity of the first algorithm is optimal within a constant gap if NE exists.
As we cannot give an explicit form for the expectation
number of pulls wasting on arms in neither the row nor the
column of the NE, our lower bound can be loose to some
extent. It is worth of having a further study for tighter lower
bounds. Moreover, an analysis of the sample complexity in
the case that NE does not exist is still an open problem, and
we expect better work on it in the future.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Acknowledgements
This work is supported by the National Basic Research
(973) Program of China (No. 2013CB329403), NSFC
Projects (Nos. 61620106010 and 61621136008), and the
Youth Top-notch Talent Support Program.

References
Audibert, Jean-Yves and Bubeck, Sébastien. Best arm
identification in multi-armed bandits. In COLT-23th
Conference on Learning Theory-2010, pp. 13–p, 2010.
Auer, Peter, Cesa-Bianchi, Nicolo, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235–256, 2002.
Babichenko, Yakov. Query complexity of approximate
nash equilibria. Journal of the ACM (JACM), 63(4):36,
2016.
Chen, Xi, Deng, Xiaotie, and Teng, Shang-Hua. Settling
the complexity of computing two-player nash equilibria.
Journal of the ACM (JACM), 56(3):14, 2009.
Daskalakis, Constantinos, Goldberg, Paul W, and Papadimitriou, Christos H. The complexity of computing a nash
equilibrium. SIAM Journal on Computing, 39(1):195–
259, 2009.
Even-Dar, Eyal, Mannor, Shie, and Mansour, Yishay.
Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(Jun):
1079–1105, 2006.
Fearnley, John and Savani, Rahul. Finding approximate
nash equilibria of bimatrix games via payoff queries.
ACM Transactions on Economics and Computation
(TEAC), 4(4):25, 2016.
Fearnley, John, Gairing, Martin, Goldberg, Paul W, and
Savani, Rahul. Learning equilibria of games via payoff queries. Journal of Machine Learning Research, 16:
1305–1344, 2015.
Garivier, Aurélien, Kaufmann, Emilie, and Koolen,
Wouter M. Maximin action identification: A new bandit framework for games. In 29th Annual Conference on
Learning Theory, pp. 1028–1050, 2016.
Goldberg, Paul W and Roth, Aaron. Bounds for the query
complexity of approximate equilibria. ACM Transactions on Economics and Computation (TEAC), 4(4):24,
2016.

Goldberg, Paul W and Turchetta, Stefano. Query complexity of approximate equilibria in anonymous games. In International Conference on Web and Internet Economics,
pp. 357–369. Springer, 2015.
Hart, Sergiu and Nisan, Noam. The query complexity of
correlated equilibria. Games and Economic Behavior,
2016.
Jamieson, Kevin and Nowak, Robert. Best-arm identification algorithms for multi-armed bandits in the fixed
confidence setting. In Information Sciences and Systems
(CISS), 2014 48th Annual Conference on, pp. 1–6. IEEE,
2014.
Jordan, Patrick R, Vorobeychik, Yevgeniy, and Wellman,
Michael P. Searching for approximate equilibria in empirical games. In Proceedings of the 7th international
joint conference on Autonomous agents and multiagent
systems-Volume 2, pp. 1063–1070. International Foundation for Autonomous Agents and Multiagent Systems,
2008.
Kalyanakrishnan, Shivaram, Tewari, Ambuj, Auer, Peter,
and Stone, Peter. Pac subset selection in stochastic multiarmed bandits. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12), pp. 655–
662, 2012.
Kaufmann, Emilie, Cappé, Olivier, and Garivier, Aurélien.
On the complexity of best arm identification in multiarmed bandit models. The Journal of Machine Learning
Research, 2015.
Lai, Tze Leung and Robbins, Herbert. Asymptotically efficient adaptive allocation rules. Advances in applied
mathematics, 6(1):4–22, 1985.
Mannor, Shie and Tsitsiklis, John N. The sample complexity of exploration in the multi-armed bandit problem. Journal of Machine Learning Research, 5(Jun):
623–648, 2004.
Maron, Oded and Moore, Andrew W. The racing algorithm: Model selection for lazy learners. In Lazy learning, pp. 193–225. Springer, 1997.
Osborne, Martin J and Rubinstein, Ariel. A course in game
theory. MIT press, 1994.
Tsybakov, Alexandre B. Introduction to nonparametric estimation. Springer, 2009.
Vorobeychik, Yevgeniy, Wellman, Michael P, and Singh,
Satinder. Learning payoff functions in infinite games.
Machine Learning, 67(1-2):145–168, 2007.

Identify the Nash Equilibrium in Static Games with Random Payoffs

Walsh, William E, Parkes, David C, and Das, Rajarshi.
Choosing samples to compute heuristic-strategy nash
equilibrium. In International Workshop on AgentMediated Electronic Commerce, pp. 109–123. Springer,
2003.
Wellman, Michael P. Methods for empirical game-theoretic
analysis. In Proceedings of the National Conference on
Artificial Intelligence, volume 21, pp. 1552. Menlo Park,
CA; Cambridge, MA; London; AAAI Press; MIT Press;
1999, 2006.

