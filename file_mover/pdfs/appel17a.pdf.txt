A Simple Multi-Class Boosting Framework
with Theoretical Guarantees and Empirical Proficiency

Ron Appel 1 Pietro Perona 1

Abstract
There is a need for simple yet accurate white-box
learning systems that train quickly and with little data. To this end, we showcase REBEL, a
multi-class boosting method, and present a novel
family of weak learners called localized similarities. Our framework provably minimizes the
training error of any dataset at an exponential
rate. We carry out experiments on a variety of
synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI
datasets against other state-of-the-art methods,
showing the empirical proficiency of our method.

1. Motivation
The past couple of years have seen vast improvements in
the performance of machine learning algorithms. Deep
Nets of varying architectures reach almost (if not better
than) human performance in many domains (LeCun et al.,
2015). A key strength of these systems is their ability to
transform the data using complex feature representations
to facilitate classification. However, there are several considerable drawbacks to employing such networks.
A first drawback is that validating through many architectures, each of which may have millions of parameters, requires a lot of data and time. In many fields (e.g. pathology
of not-so-common diseases, expert curation of esoteric subjects, etc.), gathering large amounts of data is expensive
or even impossible (Yu et al., 2015). Autonomous robots
that need to learn on the fly may not be able to afford the
large amount of processing power or time required to properly train more complex networks simply due to their hardware constraints. Moreover, most potential users (e.g. nonmachine-learning scientists, small business owners, hobby1

Caltech, Pasadena, USA. Correspondence to: Ron
Appel <appel@vision.caltech.edu>, Pietro Perona <perona@vision.caltech.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

(a) Old: Decision Stumps

(b) New: Localized Similarities

Figure 1. (a) The typical decision stumps commonly used in
boosting lead to classification boundaries that are axis aligned
and not representative of the data. Although these methods can
achieve perfect training accuracy, it is apparent that they heavily
overfit. (b) Our method uses localized similarities, a novel family
of simple weak learners (see Sec. 5.1). Paired with a procedure
that provably guarantees exponential loss minimization, our classifiers focus on smooth, well-generalizing boundaries.

ists, etc.) may not have the expertise or artistry required to
hypothesize a set of appropriate models.
A second drawback is that the complex representations
achieved by these networks are difficult to interpret and to
analyze. For many riskier applications (e.g. self-driving
cars, robotic surgeries, military drones, etc.), a machine
should only run autonomously if it is able to explain its every decision and action. Further, when used towards the scientific analysis of phenomena (e.g. understanding animal
behavior, weather patterns, financial market trends, etc.),
the goal is to extract a causal interpretation of the system in
question; hence, to be useful, a machine should be able to
provide a clear explanation of its internal logic.
For these reasons, it is desirable to have a simple white-box
machine learning system that is able to train quickly and
with little data. With these constraints in mind, we showcase a multi-class boosting algorithm called REBEL and
a novel family of weak learners called similarity stumps,
leading to much better generalization than decision stumps,
as shown in Fig. 1. Our proposed framework is simple, efficient, and is able to perfectly train on any dataset (i.e. fully
minimize the training error in a finite number of iterations).

A Simple Multi-class Boosting Framework

The main contributions of our work are as follows:
1. a simple multi-class boosting framework using localized similarities as weak learners (see Sec. 3)
2. a proof that the training error is fully minimized within
a finite number of iterations (see Sec. 5)
3. a procedure for selecting an adequate learner at each
iteration (see Sec. 5.2)
4. empirical demonstrations of state-of-the-art results on
a range of datasets (see Sec. 7)

2. Background
Boosting is a fairly mature method, originally formulated
for binary classification (e.g. AdaBoost and similar variants) (Schapire, 1990; Freund, 1995; Freund & Schapire,
1996). Multi-class classification is more complex than its
binary counterpart, however, many advances have been
made in both performance and theory in the context of
boosting. Since weak learners come in two flavors, binary and multi-class, two corresponding families of boosting methods have been explored.
The clever combination of multiple binary weak learners can result in a multi-class prediction. AdaBoost.MH
reduces the K-class problem into a single binary problem with a K-fold augmented dataset (Schapire & Singer,
1999). AdaBoost.MO and similar methods reduce the
K-class problem into C one-versus-all binary problems
using Error-Correcting Output Codes to select the final
hypothesized class (Allwein et al., 2001; Sun et al., 2005;
Li, 2006). More recently, CD-MCBoost and CW-Boost
return a K-dimensional vector of class scores, focusing
each iteration on a (binary) problem of improving the
margin of one class at a time (Saberian & Vasconcelos,
2011; Shen & Hao, 2011). REBEL also returns a vector of
class scores, increasing the margin between dynamicallyselected binary groupings of the K classes at each iteration
(Appel et al., 2016).
When multi-class weak learners are acceptable (and available), a reduction to binary problems is unnecessary. AdaBoost.M1 is a straightforward extension of its binary
counterpart (Freund & Schapire, 1996). AdaBoost.M2 and
AdaBoost.MR make use of a K-fold augmented dataset
to estimate output label probabilities or rankings for a
given input (Freund & Schapire, 1996; Schapire & Singer,
1999). More recent methods such as SAMME, AOSOLogitBoost, and GD-MCBoost are based on linear combinations of a fixed set of codewords, outputting Kdimensional score vectors (Zhu et al., 2009; Sun et al.,
2011; Saberian & Vasconcelos, 2011).
In the noteworthy paper “A Theory of Multiclass Boosting”
(Mukherjee & Schapire, 2010), many of the existing boosting methods were shown to be inadequate at training; either

because they require their weak learners to be too strong,
or because their loss functions are unable to deal with
some training data configurations. (Mukherjee & Schapire,
2010) outline the appropriate Weak Learning Condition
that a boosting algorithm must require of its weak learners in order to guarantee training convergence. However,
no method is prescribed with which to find an adequate set
of weak learners.
The goal of our work is to propose a multi-class boosting
framework with a simple family of binary weak learners
that guarantee training convergence and are easily interpretable. Using REBEL (Appel et al., 2016) as the multiclass boosting method, our framework is meant to be as
straightforward as possible so that it is accessible and practical to more users; outlining it in Sec. 3 below.

3. Our Framework
In this section, we define our notation, introduce our boosting framework, and describe our training procedure.
Notation
scalars (regular), vectors (bold):
constant vectors:
indicator vector:
logical indicator function:
inner product:
element-wise multiplication:
element-wise function:

x, x ≡ [x1 , x2 , ...]
0 ≡ [0,0, ...], 1 ≡ [1,1, ...]
δk (0 with a 1 in the kth entry)
1(LOGICAL EXPRESSION) ∈ {0,1}
hx, vi
x⊙ v
F[x] ≡ [F(x1 ), F(x2 ), ...]

In the multi-class classification setting, a datapoint is represented as a feature vector x and is associated with a class
label y. Each point is comprised of d features and belongs
to one of K classes: x ∈ X ⊆ Rd, y ∈ Y ≡ {1,2, ...,K}
A good classifier reduces the training error while generalizing well to potentially-unseen data. We use REBEL
(Appel et al., 2016) due to its support for binary weak learners, its mathematical simplicity (i.e. closed-form solution to loss minimization), and its strong empirical performance. REBEL returns a vector-valued output H, the sum
of T {weak learner f, accumulation vector a} pairs, where
K
ft : X → {±1} and at ∈ R :
H(x) ≡

T
X

ft(x) at

t=1

The hypothesized class is simply the index of the maximal
entry in H:
F(x) ≡ arg max{hH(x), δy i}
y∈Y

The average misclassification error ε can be expressed as:
N

ε ≡

1X
1(F(xn )6= yn )
N n=1

(1)

A Simple Multi-class Boosting Framework

REBEL uses an exponential loss function to upper-bound
the average training misclassification error:
N
1 X
hexp[yn ⊙ H(xn )], 1i
ε ≤ L ≡
2N n=1

(2)

where: yn ≡ 1−2 δyn (i.e. all +1s with a −1 in the ynth index)
Being a greedy, additive model, all previously-trained parameters are fixed and each iteration amounts to jointly optimizing a new weak learner f and accumulation vector a.
To this end, the loss at iteration I+1 can be expressed as:
LI+1 =

N
1 X
hwn , exp[f(xn ) yn ⊙ a]i
N n=1

(3)

1
where: wn ≡ exp[yn ⊙ HI(xn )]
2

4. Binarizing Multi-Class Data
At each iteration, the first step in determining an adequate
weak learner is binarizing the data, i.e. assigning a temporary binary label to each data point by placing it into one of
two groups. The following manipulations result in a procedure for binarizing datapoints given their boosting weights.
Eq. 5 can be upper-bounded as follows:

By expanding sfT ± sfF , U is expressed as a squared norm:

U =

Given a weak learner f, we define true and false (i.e. correct
and incorrect) multi-class weight sums (sfT and sfF ) as:
N

1X
sf ≡
1[f(xn)yn < 0]⊙ wn ,
N n=1
T

N

1X
thus: sfT +sfF =
wn ,
N n=1

*

h P
N
1
N

F

N

1X
sfT −sfF =
f(xn) wn ⊙ yn
N n=1

f(xn ) wn ⊙ yn

n=1

h

1
N


1
ln[sfT ] − ln[sfF ]
2

p
∗
∴ Lf = 2h sfT ⊙ sfF , 1i

,1

i

+

N
X
2


= 
f(xn ) un 
n=1

(7)

n=1

Eq. 7 can be written as a product of matrices by stacking
all of the un as column vectors of a K ×N matrix U and
defining f as a row vector with elements f(xn ):
⊤

⊤

U = f [U U] f
(4)

In this form, it is easily shown that with the optimal accumulation vector a∗, the loss has an explicit expression:
a∗ =

wn

n=1

Using these weight sums, the loss can be simplified to:
LI+1 ≡ Lf ≡ hsfT , exp[−a]i + hsfF , exp[a]i

N
P

i2

1 wn ⊙ yn
where: un ≡ √ r N
P
N
wn

N

1X
sf ≡
1[f(xn)yn > 0]⊙ wn
N n=1

U

}|
{
z
pT F
∗
1 D [sfT −sfF ]2 E
T
F
,1
Lf = 2h sf ⊙ sf , 1i ≤ hsf +sf , 1i −
2 [sfT +sfF ]
(6)
p
1  1 2
sT
since: x(1−x) ≤ − −x ∀x, using: x = T F
2
2
s +s

⊤

Note that the trace of U U can be lower-bounded:

⊤

tr(U U) =

N
X

n=1

(5)

At each iteration, growing decision trees requires an exhaustive search through a pool of decision stumps (which
is tractable but time-consuming), storing the binary learner
that best reduces the multi-class loss in Eq. 5. In some situations, axis-aligned trees are simply unable to reduce the
loss any further, thereby stalling the training procedure.
Our proposed framework circumvents such situations. At
each iteration, instead of exhaustively searching for an adequate learner, we first determine an appropriate “binarization” of the multi-class data (i.e. a separation of the K-class
data into two distinct groups) and then find a weak learner
with a guaranteed reduction in loss, foregoing the need for
an exhaustive search.

2

kun k =

N
* P [wn ]2
n=1
hP
N

N

wn

n=1
N
X

since by Jensen’s inequality:

n=1

+

i, 1 ≥

x2n ≥

N
1 X
hwn , 1i
N 2 n=1

N
1 X 2
xn
N n=1

⊤

Furthermore, U U has N (not-necessarily unique) nonnegative eigenvalues, each associated with an independent
eigenvector. Let v̂n be the eigenvector corresponding to the
nth largest eigenvalue λn . Hence, f can be decomposed as:
f = hf ,v̂1 i v̂1 +
∴ U = λ1 hf ,v̂1 i2 +

N
X

n=2

N
X

hf ,v̂n i v̂n

n=2

λn hf ,v̂n i2 ≥ λ1 hf ,v̂1 i2

(8)

A Simple Multi-class Boosting Framework

Since the trace of a matrix is equal to the sum of its eigen⊤
values and U U has at most K non-zero eigenvalues (λ1
being the largest), hence:
λ1 ≥

⊤
L0
1
tr(U U) ≥
K
KN

(9)

N

∴ T ≥ T0 ⇒

1X
hwn , 1i = L0
since:
N n=1
Based on this formulation, binarization is achieved by setting the binarized class bn of each sample n as the sign of
its corresponding element in v̂1 : bn ≡ sign(hv̂1 , δn i)
Accordingly, if b is the vector with elements bn , then:
hb,v̂1 i2 = hsign[v̂1 ],v̂1 i2 = h|v̂1 |, 1i2 ≥ 1

(10)

Finally, by combining Eq. 6, Eq. 9, and Eq. 10, with perfect
binarized classification (i.e. when the binary weak learner
perfectly classifies the binarized data), the loss ratio at any
iteration is bounded by:
Lf ∗
1
≤ 1−
L0
2KN
In general, there is no guarantee that any weak learner can
achieve perfect binarized classification. In the following
section, we show that with the ability to isolate any single
point in space (i.e. to classify an inner point as +1 and all
outer points as −1), the loss decreases exponentially.

5. Isolating Points
Assume that we have a weak learner fi that can isolate a
single point xi in the input space X. Accordingly, denote
fi = 2 δi −1 as a vector of −1s with a +1 in the ith entry,
corresponding to classification using the isolating learner
fi(xn ). If N ≥ 4, then for any unit vector v̂ ∈ RN :
4
max{hfi ,v̂i } ≥
i
N

K
2 T
1
1−
<
2
KN 2
N

⇒ ε=0

Although this bound is too weak to be of practical use, it
is a bound nonetheless (and can likely be improved). In
the following section, we specify a suitable family of weak
learners with the ability to isolate single points.
5.1. One/Two-Point Localized Similarities

(please refer to supplement for proof)

2

in Eq. 2), our framework attains minimal training error on
any1 training set after a finite number of iterations:
' 
&

KN 2 KN 
ln(2/KN )

≈
ln
define: T0 ≡
2
2
2
ln 1− KN
2

(11)

(please refer to supplement for proof)
Combining Eq. 6, Eq. 9, and Eq. 11, the loss ratio at each
iteration is upper-bounded as follows:
mini {Lfi }
2
≤ 1−
L0
KN 2
Before the first iteration, the initial loss L0 = K/2. Each
iteration decreases the loss exponentially. Since the training error is discrete and is upper bounded by the loss (as

Classical decision stumps compare a single feature to a
threshold, outputting +1 or −1. Instead, our proposed family of weak learners (called localized similarities) compare
points in the input space using a similarity measure. Due
to its simplicity and effectiveness, we use negative squared
Euclidean distance −kxi−xj k2 as a measure of similarity
between points xi and xj . A localized similarity has two
modes of operation:
1. In one-point mode, given an anchor xi and a threshold
τ , the input space is classified as positive if it is more
similar to xi than τ , and negative otherwise; ranging
between +1 and −1:
fi(x) ≡

τ − kxi −xk2
τ + kxi −xk2

2. In two-point mode, given supports xi and xj , the input
space is classified as positive if it is more similar to
xi than to xj (and vice-versa), with maximal absolute
activations around xi and xj ; falling off radially away
from the midpoint m:
fij(x) ≡
where: d ≡

hd, x−mi
4kdk4 + kx−mk4

1
1
[xi −xj ] and: m ≡ [xi +xj ]
2
2

One-point mode enables the isolation of any single datapoint, guaranteeing a baseline reduction in loss. However,
it essentially leads to pure memorization of the training
data; mimicking a nearest-neighbor classifier. Two-point
mode adds the capability to generalize better by providing margin-style functionality. The combination of these
1
There may be situations in which multiple samples belonging to different classes are coincident in the input space. These
cases can be dealt with (before or during training) either by assigning all such points as a special “mixed” class (to be dealt with
at a later stage), or by setting the class labels of all coincident
points to the single label that minimizes the error.

A Simple Multi-class Boosting Framework

two modes enables the flexibility to tackle a wide range of
classification problems. Furthermore, in either mode, the
functionality of a localized similarity is easily interpretable:
“which of these fixed training points is a given query point
more similar to?”
5.2. Finding Adequate Localized Similarities
Given a dataset with N samples, there are about N 2 possible localized similarities. The following procedure efficiently selects an adequate localized similarity:

2

xj = arg min {kxi −xj k }
bj =−bi

5. Calculate the loss achieved using the two-point localized similarity fij . If it outperforms the previous best,
store the newer learner and update the best-so-far loss.
6. Find all points that are similar enough to xj and remove
them from consideration for the remainder of the current iteration. In our implementation, we remove all xn
for which:
fij(xn ) ≤ fij(xj )/2
If all points have been removed, return the best-so-far
localized similarity; otherwise, loop back to step 4.
Upon completion of this procedure, the best-so-far localized similarity is guaranteed to lead to an adequate reduction in loss, based on the derivation in Sec. 4 above.

6. Generalization Experiments
Our boosting method provably reduces the loss well after
the training error is minimized. In this section, we demonstrate that the continual reduction in loss serves only to improve the decision boundaries and not to overfit the data.
We generated 2-dimensional synthetic datasets in order to
better visualize and get an intuition for what the classifiers
2
“most similar” need not be exact; approximate nearestneighbors also works with negligible differences.

Figure 2. A 500-point 2-dimensional synthetic dataset with a
(2/3, 1/3) split of train data (left plot) to test data (right plot).
Background shading corresponds to the hypothesized class using
our framework.

Training Loss

1
0.8
0.6
0.4

log
0.2
0
0
10

10 -10
10 -20
10

Test Error [%]

Relative Error, Training Loss

0. Using Eq. 5, calculate the base loss L1 for the homogeneous stump f1 (i.e. the one-point stump with any xi
and τ ≡ ∞, classifying all points as +1).
1. Compute the eigenvector v̂1 (as in Eq. 8); label the
points based on their binarized class labels bn .
2. Find the optimal isolating localized similarity fi (i.e.
with xi and appropriate τ , classifying point i as +1 and
all other points as −1).
3. Using Eq. 5, calculate the corresponding loss Li . Of the
two stumps f1 and fi , store the one with smaller loss as
best-so-far.
4. Find point xj most similar2 to xi among points of the
opposite binarized class:

10

-30

40
0.6

10 2

0.4
0.2
0

Loss [-20.6]

Train Error [0.0%]
Test Error [0.0%]
10

1

10

2

10

3

Iteration [max = 1000]

Figure 3. A plot of training loss, training error, and test error as
a classifier is trained for 1000 iterations. Note that the test error
does not increase even after the training error drops to zero. The
lower inset is a zoomed-in plot of the train and test error, the upper
inset is a plot of training loss using a log-scaled y-axis; both inset
plots are congruous with the original x-axis.

are doing. The results shown in this chapter are based on
a dataset composed of 500 points belonging to one of three
classes in a spiral formation, with a (2/3, 1/3) train/test
split. Fig. 2 shows the hypothesized class using a classifier
trained for 1000 iterations.
Our classifier achieves perfect training (left) and test
classification (right), producing a visually simple wellgeneralizing contour around the points. Training curves
are given in Fig. 3, tracking the loss and classification errors per training iteration. Note that the test error does not
increase even after the training error drops to zero.
The following experiments explore the functionality of our
framework (i.e. REBEL using localized similarities) in two
scenarios that commonly arise in practice: (1) varying sparsity of training data, and, (2) varying amounts of mislabeled
training data.

A Simple Multi-class Boosting Framework

10

-50

10 2

50
3

0.6

10 3

2
1

0.4

0

log
0.2

10

Loss [-44.2]

Train Error [0.0%]
Test Error [1.2%]

0
0
10

10

1

10

2

10

(c1 ) ∼1% mislabeled data (4 points)

(b1 ) Test Data

3

1

Training Loss

0.8

(a1 ) Train Data

0.8
0.6
0.4

log
0.2

10

-20

10

-30

0.4

log

10

(a2 ) Train Data

3
2
1
0

Loss [-22.5]

Train Error [0.0%]
Test Error [1.8%]

0
10 0

10 1

10 2

10 3

0
0
10

10

1

10

1
0.8
0.6
0.4

log
0.2

0.4

log
0.2

10

-150

10

10 2

30
10

10 3

5

0

Loss [-116.5]

10 1

10 2

10 3

10

-40

10

-60

10 2

(a4 ) Train Data

10 3

Test Error [%]
Test Error [%]

50
20

0.6

15
10
5

0.4

0

log
0.2
0
0
10

10

Loss [ -58.9]

Train Error [ 0.0%]
Test Error [14.0%]
10

1

10

2

5
0

Loss [-15.0]

0
10 0

10 1

10 2

1
0.8
0.6
0.4

log
0.2

10

-10

10

-20

10

-30

10 3

10

10 2

10 3

10 2

10 3

70
8
6
4
2
0

Loss [-22.7]

Train Error [0.0%]
Test Error [5.4%]

0
10 0

10 1

10

3

Iteration [max = 2000]

Figure 4. Classification boundaries (a,b), and training curves (c)
when a classifier is trained on varying amounts of data. Stars are
correctly-classified, circles are misclassified. In all cases, the test
error is fairly stable once reaching its minimum.

(c4 ) ∼30% mislabeled data (97 points)

(b4 ) Test Data

1
0.8
0.6
0.4

log
0.2
0
0
10

10

10

Test Error [%]

-20

10

10 2

10

Training Loss

0.8

10

Relative Error, Training Loss

Training Loss

1

-20

40
15

Iteration [max = 2000]

(c1 ) Training on 1/5 of the data (67 points)
Relative Error, Training Loss

(b1 ) Test Data

3

Train Error [0.0%]
Test Error [3.6%]

Iteration [max = 5000]

(a1 ) Train Data

10

(c3 ) ∼10% mislabeled data (32 points)

(b3 ) Test Data

Train Error [ 0.0%]
Test Error [ 2.0%]

0
10 0

-10

10

Training Loss

-100

10

Test Error [%]

0.6

(a3 ) Train Data

-50

10

Test Error [%]

0.8

10

Relative Error, Training Loss

Training Loss

1

2

Iteration [max = 1000]

(c2 ) Training on 2/5 of the data (133 points)
Relative Error, Training Loss

(b2 ) Test Data

0

Loss [-96.8]

Iteration [max = 1000]

(a2 ) Train Data

1

(c2 ) ∼3% mislabeled data (10 points)

(b2 ) Test Data

10 2

40
4

2

Training Loss

10

10 3

3

Test Error [%]

0.6

0.2

-10

Test Error [%]

0.8

10

Relative Error, Training Loss

Training Loss

1

10 2

40
4

Iteration [max = 5000]

(c3 ) Training on 3/5 of the data (200 points)
Relative Error, Training Loss

(b3 ) Test Data

-50

-100

Train Error [0.0%]
Test Error [0.0%]

Iteration [max = 2000]

(a3 ) Train Data

10

10

Test Error [%]

Training Loss

1

Relative Error, Training Loss

(c4 ) Training on 4/5 of the data (267 points)

Test Error
Error [%]
[%]
Test

(b4 ) Test Data

Relative Error, Training Loss

(a4 ) Train Data

10

-50

-100

200
40

10 3

30
20
10
0

Loss [ -67.6]

Train Error [ 0.0%]
Test Error [31.1%]
10

1

10

2

10

3

Iteration [max = 10000]

Figure 5. Classification boundaries (a,b), and training curves (c)
when a classifier is trained on varying fractions of mislabeled data.
In all cases, the test error is fairly stable once reaching its minimum. Even with 30% mislabeled data, the classification boundaries are reasonable given the training labels.

6.1. Sparse Training Data
In this section of experiments, classifiers were trained using
varying amounts of data, from 4/5 to 1/5 of the total training set. Fig. 4 shows the classification boundaries learned
by the classifier (ai ,bi ), and the training curves (ci ). In all
cases, the boundaries seem to aptly fit (and not overfit) the
training data (i.e. being satisfied with isolated patches without overzealously trying to connect points of the same class
together). This is more rigorously observed from the training curves; the test error does not increase after reaching its
minimum (for hundreds of iterations).
6.2. Mislabeled Training Data
In this section of experiments, classifiers were trained with
varying fractions of mislabeled data; from 1% to 30% of
the training set. Fig. 5 shows the classification boundaries
(ai ,bi ) and the training curves (ci ). All classifiers seem to
degenerate gracefully, isolating rogue points and otherwise
maintaining smooth boundaries. Even the classifier trained
on 30% mislabeled data (which we would consider to be
unreasonably noisy) is able to maintain smooth boundaries.

In all cases, the training curves still show that the test error
is fairly stable once reaching its minimum value. Moreover,
test errors approximately equal the fraction of mislabeled
data, further validating the generalization of our method.
6.3. Real Data
Although the above observations are promising, they could
result from the fact that the synthetic datasets are 2dimensional. In order to rule out this possibility, we
performed similar experiments on several UCI datasets
(Bache & Lichman, 2013) of varying input dimensionalities (from 9 to 617). From the training curves in Fig. 6,
we observe that once the test errors saturate, they no longer
increase, even after hundreds of iterations.
In Fig. 7, we plot the training losses on a log-scaled y-axis.
The linear trend signifies an exponential decrease in loss
per iteration. Our proven bound predicts a much slower (exponential) rate than the actual trend observed during training. Note that within the initial ∼ 10% of the iterations, the
loss drops at an even faster rate, after which it settles down

10

4

A Simple Multi-class Boosting Framework

0.4

log
0.2

10

200
30

10 3

25

~
0

Loss [-222.7]

Train Error [ 0.0%]
Test Error [27.0%]

0
0
10

10

1

10

2

10

3

10

4

0.6
0.4

log
0.2

10

10

log

10

100
6

10 3

4
2
0

Loss [-34.7]

10 2

Iteration [max = 10000]

10 3

10 4

0.6
0.4

log
0.2
0
10 0

10

Proven bound
Loss
Exponential Fit

0

2

Proven bound
Loss
Exponential Fit

10 0

0

10

Training Loss

0.8

10

2

2

10

10

3

10

-50

10

-100

10

-150

10

-200

10

-10

10

-15

200
8

10

0

10-20

10

-5

10

Initial Iterations

1020

ISOLET (617-dimensional)

Train Error [0.0%]
Test Error [2.3%]
10 1

10

4

1

1

PENDIGIT
(d = 16, K = 10, N = 7494)

50

10 3

6

Test
Test Error
Error [%]
[%]

0.4

0
10 0

-40

10 2

70
8

Iteration [max = 2000]

Relative Error, Training Loss

Training Loss

0.6

0.2

10

Test
Test Error
Error [%]
[%]

Relative Error, Training Loss

0.8

-20

-15

Loss [-10.2]

0
0
10

OPTDIGIT (64-dimensional)
10

10

10

Train Error [0.0%]
Test Error [1.8%]

Iteration [max = 10000]

1

GLASS
(d = 9, K = 6, N = 53)

-5

-10

Training Loss

-300

10

10

10

-2

10 -4

-6
10

10 -8
0

200

400

600

800

1000

-250

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10

10000

0

10

-1

10

-2

0

50

100

150

200

-10

0

200

400

600

800

1000

1200

1400

1600

1800

2000

Iteration Number

Iteration Number

10 3

6
4
2

OPTDIGIT

0

Loss [-10.6]

ISOLET

(d = 64, K = 10, N = 3823)

Train Error [0.0%]
Test Error [3.5%]
10 1

Initial Iterations

101

10

-40

10 2

10 3

10

5

10

0

Proven bound
Loss
Exponential Fit

10 4

Iteration [max = 10000]

10

Training Loss

Figure 6. Training curves for classifiers trained on UCI datasets
with a range of dimensionalities. In all cases, the test error is
stable once it reaches its minimum.

10

-15
10

Initial Iterations

5

10 -20

10

10

2

10

0

Proven bound
Loss
Exponential Fit

-5

10 -10

10

10 -2

10

-4

10

-30
10

0

200

1000

400

2000

600

3000

800

10

0

10

1000

4000

2

5000

6000

7000

8000

9000

10000

Initial Iterations

-8

-5

0

10

10 -6

-25 100

10 -35

to a seemingly-constant rate of exponential decay. We have
not yet determined the characteristics (i.e. the theoretically
justified rates) of these observed trends, and relegate this
endeavor to future work.

(d = 617, K = 26, N = 6238)

Training Loss

10

0.8

Training Loss

0.6

PENDIGIT (16-dimensional)
1

Training Loss

-200

Test Error [%]

-100

10

Relative Error, Training Loss

Training Loss

10

Test Error [%]

Relative Error, Training Loss

GLASS (9-dimensional)
1
0.8

10 -10

0

-2

0

200

1000

400

2000

Iteration Number

600

800

3000

1000

4000

5000

6000

7000

8000

9000

10000

Iteration Number

Figure 7. Training losses for classifiers trained on UCI datasets.
The linear trend (visualized using a log-scaled y-axis) signifies
an exponential decrease in loss, albeit at a much faster rate than
established by our proven bound.

7. Comparison with Other Methods

Based on the same experimental setup as in (Shen et al.,
2014; Appel et al., 2016), competing methods are trained
to a maximum of 200 decision stumps. For each dataset,
five random splits are generated, with 50% of the samples
for training, 25% for validation (i.e. for setting hyperparameters where needed), and the remaining 25% for testing.
REBEL using localized similarities is the most accurate
method on five of the six datasets tested. In the Vowel
dataset, it achieves almost half of the error as the next best
method. Note that although our framework uses REBEL as
its boosting method, the localized similarities add an extra
edge, beating REBEL with decision trees in all runs.
Further, when limited to only using 1-point (i.e. isolating)
localized similarities, the performance is extremely poor,
validating the need for 2-point localized similarities as prescribed in Sec. 5.2. Overall, these results demonstrate the

[0
[0
[0
[0
[2
[0
[0
[0
[5

40

30

20

0
0
1
2
1
0
3
0
0

1] Ada 1vsAll
0] Ada.MH
0] Ada.ECC
0] Struct-Boost
1] CW-Boost
0] A0S0-Logit
2] RBL-Stump
0] RBL-Iso.Sim
0] RBL-Loc.Sim

PENDIGITS

4.6

3.3
69.7

5.6

MNIST

3.7
2.9
2.9
2.5*

LANDSAT

7.7

VOWEL

2.5
12.8
3.2
32.8
1.2*

11.0
13.4
15.8
12.5
9.3*
12.5
10.5
87.1
9.3*

GLASS

7.1
7.4
8.4
6.9

15.1
12.7
12.8
12.1
11.1
15.4
10.7
52.3
10.6*

0

21.1
18.8
20.6
17.5
22.4
20.6
17.4
62.3
9.5*

10
31.7
32.3
32.7
35.8
35.4
34.2
30.4
35.9
27.4*

Percent Error

In Sec. 5 we proved that our framework adheres to theoretical guarantees, and in Sec. 6 above, we showed
that it has promising empirical properties. In this section, we compete against several state-of-the-art boosting baselines. Specifically, we compared 1-vs-All AdaBoost and AdaBoost.MH (Schapire & Singer, 1999),
AdaBoost.ECC (Dietterich & Bakiri, 1995), Struct-Boost
(Shen et al., 2014), CW-Boost (Shen & Hao, 2011), AOSOLogitBoost (Sun et al., 2011), REBEL (Appel et al., 2016)
using shallow decision trees, REBEL using only 1-point
(isolating) similarities, and our full framework, REBEL using 2-point localized similarities.

SEGMENT

Figure 8. Test errors of various state-of-the-art and baseline
classification methods on MNIST and several UCI datasets.
REBEL using localized similarities (shown in yellow) is the bestperforming method on all but one of the datasets shown. When
constrained to use only 1-point (isolating) similarities (shown in
red), the resulting classifier is completely inadequate.

ability of our framework to produce easily interpretable
classifiers that are also empirically proficient.
7.1. Comparison with Neural Networks and SVMs
Complex neural networks are able to achieve remarkable
performance on large datasets, but they require an amount
of training data proportional to their complexity. In the
regime of small to medium amounts of data (within which
UCI and MNIST datasets belong, i.e. 10 < N < 106 training samples), such networks cannot be too complex. Accordingly, in Fig. 9, we compare our method against fullyconnected neural networks.

A Simple Multi-class Boosting Framework
100

C
V
V

Average Test Error [%]

G
G
30 G
Method

10

0.3
0.1

A
A
A

[2/10] NN
[2/10] SVM
[8/10] Ours

3
1

C
C

O
O
O

Dataset

G
S
V
P
L
A
O
I
M
C

I

L
L

I
I P

M
L

P

GLASS
SHUTTLE
VOWEL
PENDIGIT
LETTER
LANDSAT
OPTDIGIT
ISOLET
MNIST
CUB200

M
S

S
S

50
60 0
00
0

43

0
00
16

38
4423
3
55 5
629
74348
94

8
52

53

0.03

Number of Training Samples (N)

Figure 9. Comparison of our method versus Neural Networks and
Support Vector Machines on ten datasets of varying sizes and difficulties. Our method is the most accurate on almost all datasets.

Four neural networks were implemented, each having one
of the following architectures: [d−4d−K], [d−4K −K],
[d−2d−d−K], [d−4K −2K −K], where d is the number
of input dimensions and K is the number of output classes.
Only the one with the best test error is shown in the plot. A
multi-class SVM (Chang & Lin, 2011) was validated using
a 5 × 6 parameter sweep for C and γ. Our method was run
until the training loss fell below 1/N . Overall, REBEL using localized similarities achieves the best results on eight
of the ten datasets, decisively marking it as the method of
choice for this range of data.

8. Discussion
In Sec. 6, we observed that our classifiers tend to smoothen
the decision boundaries in the iterations beyond zero training error. In Fig. 10, we see that this is not the case with
the typically-used axis-aligned decision stumps. Why does
this happen with our framework?

Firstly, we note that the largest-margin boundary between
two points is the hyperplane that bisects them. Every twopoint localized similarity acts as such a bisector. Therefore, it is not surprising that with only a pool of localized
similarities, a classifier should have what it needs to place
good boundaries. Further, not all pairs need to be separated
(since many neighboring points belong to the same class);
hence, only a small subset of the ∼ N 2 possible learners
will ever need to be selected.
Secondly, we note that if some point (either an outlier or an
unfortunately-placed point) continues to increase in weight
until it can no-longer be ignored, it can simply be isolated
and individually dealt with using a one-point localized similarity, there is no need to combine it with other “innocentbystander” points. This phenomenon is observed in the mislabeled training experiments in Sec. 6.2.
Together, the two types of localized similarities complement each other. With the guarantee that every step reduces
the loss, each iteration focuses on either further smoothening out an existing boundary, or reducing the weight of a
single unfit point.

9. Conclusions
We have presented a novel framework for multi-class boosting that makes use of a simple family of weak learners
called localized similarities. Each of these learners has a
clearly understandable functionality; a test of similarity between a query point and some pre-defined samples.
We have proven that the framework adheres to theoretical
guarantees: the training loss is minimized at an exponential rate, and since the loss upper-bounds the training error
(which can only assume discrete values), our framework is
therefore able to achieve maximal accuracy on any dataset.
We further explored some of the empirical properties of
our framework, noting that the combination of localized
similarities and guaranteed loss reduction tend to lead to a
non-overfitting regime, in which the classifier focuses on
smoothing-out its decision boundaries. Finally, we compare our method against several state-of-the-art methods,
outperforming all of the methods in most of the datasets.
Altogether, we believe that we have achieved our goal of
presenting a simple multi-class boosting framework with
theoretical guarantees and empirical proficiency.

Acknowledgements
Figure 10. The contrasted difference between overtraining using
(a) classical decision stumps and (b) localized similarities. (a)
leads to massive overfitting of the training data, whereas (b) leads
to smoothening of the decision boundaries.

The authors would like to thank anonymous reviewers for
their feedback and Google Inc. and the Office of Naval
Research MURI N00014-10-1-0933 for funding this work.

A Simple Multi-class Boosting Framework

References
Allwein, E. L., Schapire, R. E., and Singer, Y. Reducing
multiclass to binary: a unifying approach for margin classifiers. JMLR, 2001.
Appel, R., Burgos-Artizzu, X. P., and Perona, P. Improved
multi-class cost-sensitive boosting via estimation of the
minimum-risk class. arXiv, (1607.03547), 2016.
Bache, K. and Lichman, M.
UCI machine
learning repository (uc irvine), 2013.
URL
http://archive.ics.uci.edu/ml.
Chang, C. and Lin, C. LIBSVM: A library for support vector machines. Transactions on Intelligent Systems and
Technology, 2011.
Dietterich, T. G. and Bakiri, G. Solving multiclass learning problems via error-correcting output codes. arXiv,
(9501101), 1995.
Freund, Y. Boosting a weak learning algorithm by majority.
Information and Computation, 1995.
Freund, Y. and Schapire, R. E. Experiments with a new
boosting algorithm. In Machine Learning International
Workshop, 1996.
LeCun, Y., Bengio, Y., and Hinton, G. E. Deep learning.
Nature Research, 2015.
Li, L. Multiclass boosting with repartitioning. In ICML,
2006.
Mukherjee, I. and Schapire, R. E. A theory of multiclass
boosting. In NIPS, 2010.
Saberian, M. and Vasconcelos, N. Multiclass boosting:
Theory and algorithms. In NIPS, 2011.
Schapire, R. E. The strength of weak learnability. Machine
Learning, 1990.
Schapire, R. E. and Singer, Y. Improved boosting algorithms using confidence-rated predictions. In Conference
on Computational Learning Theory, 1999.
Shen, C. and Hao, Z. A direct formulation for totallycorrective multi-class boosting. In CVPR, 2011.
Shen, G., Lin, G., and van den Hengel, A. Structboost:
Boosting methods for predicting structured output variables. PAMI, 2014.
Sun, P., Reid, M. D., and Zhou, J. Aoso-logitboost:
Adaptive one-vs-one logitboost for multi-class problem.
arXiv, (1110.3907), 2011.
Sun, Y., Todorovic, S., Li, J., and Wu, D. Unifying
the error-correcting and output-code adaboost within the
margin framework. In ICML, 2005.
Yu, F., Zhang, Y., Song, S., Seff, A., and Xiao, J. LSUN:
construction of a large-scale image dataset using deep
learning with humans in the loop. arXiv, (1506.03365),
2015.
Zhu, J., Zou, H., Rosset, S., and Hastie, T. Multi-class
adaboost. Statistics and its Interface, 2009.

