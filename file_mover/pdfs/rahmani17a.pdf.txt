Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery

Mostafa Rahmani 1 George Atia 1

Abstract
A remarkably simple, yet powerful, algorithm
termed Coherence Pursuit for robust Principal
Component Analysis (PCA) is presented. In the
proposed approach, an outlier is set apart from an
inlier by comparing their coherence with the rest
of the data points. As inliers lie in a low dimensional subspace, they are likely to have strong
mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit
low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with
a large number of data points. As Coherence
Pursuit only involves one simple matrix multiplication, it is significantly faster than the stateof-the-art robust PCA algorithms. We provide a
mathematical analysis of the proposed algorithm
under a random model for the distribution of the
inliers and outliers. It is shown that the proposed
method can recover the correct subspace even if
the data is predominantly outliers. To the best
of our knowledge, this is the first provable robust PCA algorithm that is simultaneously noniterative, can tolerate a large number of outliers
and is robust to linearly dependent outliers.

1. Introduction
Standard tools such as Principal Component Analysis
(PCA) have been instrumental in reducing dimensionality by finding linear projections of high-dimensional data
along the directions where the data is most spread out to
minimize information loss. These techniques are widely
applicable in a broad range of data analysis problems, including computer vision, image processing, machine learning and bioinformatics (Basri & Jacobs, 2003; Costeira &
Kanade, 1998; Hosseini et al., 2014).
Given a data matrix D ∈ Rm×n , PCA finds an r1
University of Central Florida, Orlando, Florida, USA. Correspondence to: Mostafa Rahmani <mostafa@knights.ucf.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

dimensional subspace by solving
minkD − ÛÛT DkF

subject to ÛT Û = I,

(1)

Û

where Û ∈ Rm×r is an orthonormal basis for the rdimensional subspace, I denotes the identity matrix and
k.kF the Frobenius norm. Despite its notable impact on
exploratory data analysis and multivariate analyses, PCA
is notoriously sensitive to outliers that prevail much of the
real world data since the solution to (1) can arbitrarily deviate from the true subspace in presence of a small number
of outlying data points that do not conform with the low dimensional model (Hauberg et al., 2016; Lerman & Zhang,
2014; Maronna, 2005; McCoy et al., 2011; Xu et al., 2010a;
Zhang, 2012) .
As a result, much research work was devoted to investigate
PCA algorithms that are robust to outliers. The corrupted
data can be expressed as
D=L+C,

(2)

where L is a low rank matrix whose columns span a
low-dimensional subspace, and the matrix C models the
data corruption, and is referred to as the outlier matrix.
There are two main models for the outlier matrix that
were considered in the literature, and these two models
are mostly incomparable in theory, practice and analysis
techniques. The first corruption model is the element-wise
model in which C is a sparse matrix with arbitrary support, whose entries can have arbitrarily large magnitudes
(Chandrasekaran et al., 2011; Candès et al., 2011; Netrapalli et al., 2014; Yi et al., 2016). In view of the arbitrary
support of C, any of the columns of L may be affected by
the non-zero elements of C. We do not consider this model
in this paper. The second model, which is the focus of our
paper, is a column-wise model wherein only a fraction of
the columns of C are non-zero, wherefore a portion of the
columns of L (the so-called inliers) remain unaffected by
C (Lerman & Maunu, 2014; Xu et al., 2010b; Chen et al.,
2011; Rahmani & Atia, 2017). Next, we formally describe
the data model adopted in this paper, which only focuses
on the column-wise outlier model.
Data Model 1. The given data matrix D satisfies the following.

Submission and Formatting Instructions for ICML 2017

2. Related Work

1. The matrix D can be expressed as
D = L + C = [A B] T ,
m×n1

(3)

m×n2

where A ∈ R
,B ∈ R
, and T is an arbitrary
permutation matrix.
2. The columns of A lie in an r-dimensional subspace U,
namely, the column space of L. The columns of B do not
lie entirely in U, i.e., the columns of A are the inliers and
the columns of B are the outliers.
The column-wise model for robust PCA has direct bearing
on a host of applications in signal processing and machine
learning, which spurred enormous progress in dealing with
subspace recovery in the presence of outliers. This work is
motivated by some of the limitations of existing techniques,
which we further detail in Section 2 on related work. The
vast majority of existing approaches to robust PCA have
high computational complexity, which makes them unsuitable in high dimensional settings. For instance, many of the
existing iterative techniques incur a long run time as they
require a large number of iterations, each with a Singular
Value Decomposition (SVD) operation. Also, most iterative solvers have no provable guarantees for exact subspace
recovery. Moreover, many of the existing robust PCA algorithms cannot tolerate a large number of outliers as they
rely on sparse outlier models, while others cannot handle
linearly dependent outliers. In this paper, we present a new
provable non-iterative robust PCA algorithm, dubbed Coherence Pursuit (CoP), which involves one simple matrix
multiplication, and thereby achieves remarkable speedups
over the state-of-the-art algorithms. CoP can tolerate a
large number of outliers – even if the ratio of inliers to outliers nn12 approaches zero – and is robust to linearly dependent outliers and to additive noise.
1.1. Notation and definitions
Bold-face upper-case letters are used to denote matrices
and bold-face lower-case letters are used to denote vectors.
Given a matrix A, kAk denotes its spectral norm and kAk∗
its nuclear norm. For a vector a, kakp denotes its `p -norm
and a(i) its ith element. Given two matrices A1 and A2
with an equal number of rows, the matrix
A3 = [A1 A2 ]
is the matrix formed by concatenating their columns. For a
matrix A, ai denotes its ith column, and A−i is equal to A
with the ith column removed. The function orth(·) returns
an orthonormal basis for the range of its matrix argument.
Definition 1. The mutual coherence of two non-zero vectors v1 ∈ Rm×1 and v2 ∈ Rm×1 is defined as
|v1T v2 |
.
kv1 kkv2 k

Some of the earliest approaches to robust PCA relied on
robust estimation of the data covariance matrix, such as Sestimators, the minimum covariance determinant, the minimum volume ellipsoid, and the Stahel-Donoho estimator
(Huber, 2011). This is a class of iterative approaches that
compute a full SVD or eigenvalue decomposition in each
iteration and generally have no explicit performance guarantees. The performance of these approaches greatly degrades for nn12 ≤ 0.5.
To enhance robustness to outliers, another approach is to
replace the Frobenius norm in (1) with other norms (Lerman et al., 2011). For example, (Ke & Kanade, 2005) uses
an `1 -norm relaxation commonly used for sparse vector estimation, yielding robustness to outliers (Candes & Tao,
2005; Candès et al., 2006; 2011). However, the approach
presented in (Ke & Kanade, 2005) has no provable guarantees and requires C to be column sparse, i.e., a very small
portion of the columns of C can be non-zero. The work in
(Ding et al., 2006) replaces the `1 -norm in (Ke & Kanade,
2005) with the `1,2 -norm. While the algorithm in (Ding
et al., 2006) can handle a large number of outliers, the complexity of each iteration is O(nm2 ) and its iterative solver
has no performance guarantees. Recently, the idea of using
a robust norm was revisited in (Lerman et al., 2012; Zhang
& Lerman, 2014). Therein, the non-convex constraint set is
relaxed to a larger convex set and exact subspace recovery
is guaranteed under certain conditions. The algorithm presented in (Lerman et al., 2012) obtains the column space
of L and (Zhang & Lerman, 2014) finds its complement.
However, the iterative solver of (Lerman et al., 2012) computes a full SVD of an m × m weighted covariance matrix in each iteration. Thus, the overall complexity of the
solver of (Lerman et al., 2012) is roughly O(m3 + nm2 )
per iteration, where the second term is the complexity of
computing the weighted covariance matrix. Similarly, the
solver of (Zhang & Lerman, 2014) has O(nm2 +m3 ) complexity per iteration. In (Tsakiris & Vidal, 2015), the complement of the column space of L is recovered via a series
of linear optimization problems, each obtaining one direction in the complement space. This method is sensitive to
linearly dependent outliers and requires the columns of L
not to exhibit a clustering structure, which in fact prevails
much of the real world data. Also, the approach presented
in (Tsakiris & Vidal, 2015) requires solving m−r linear optimization problems consecutively resulting in high computational complexity and long run time for high dimensional
data.
Robust PCA using convex rank minimization was first analyzed in (Chandrasekaran et al., 2011; Candès et al., 2011)
for the element-wise corruption model. In (Xu et al.,
2010b), the algorithm analyzed in (Chandrasekaran et al.,

Submission and Formatting Instructions for ICML 2017

2011; Candès et al., 2011) was extended to the columnwise corruption model where it was shown that the optimal
point of
min

kL̂k∗ + λkĈk1,2

L̂,Ĉ

(4)

subject to L̂ + Ĉ = D
yields the exact subspace and correctly identifies the outliers provided that C is sufficiently column-sparse. The
solver of (4) requires too many iterations, each computing
the SVD of an m × n dimensional matrix. Also, the algorithm can only tolerate a small number of outliers – the
ratio nn21 should be roughly less than 0.05.
A different approach to outlier detection was proposed
in (Soltanolkotabi & Candes, 2012; Elhamifar & Vidal,
2013), the idea being that outliers do not typically follow
low dimensional structures, thereupon few outliers cannot
form a linearly dependent set. While this approach can recover the correct subspace even if a remarkable portion of
the data is outliers, it cannot detect linearly dependent outliers and has O(n3 ) complexity per iteration (Elhamifar &
Vidal, 2013). In the outlier detection algorithm presented
in (Heckel & Bölcskei, 2013), a data point is identified as
an outlier if the maximum value of its mutual coherences
with the other data points falls below a predefined threshold. However, this approach is unable to detect outliers that
lie in close neighborhoods. For instance, any repeated outliers will be falsely detected as inliers since their mutual
coherence is 1.

dle a large number of outliers, albeit they fail to locate
outliers with high similarity or linearly dependent outliers.
Spherical PCA (SPCA) is a non-iterative robust PCA algorithm that is also scalable (Maronna et al., 2006). In
this algorithm, all the columns of D are first projected
onto the unit sphere Sm−1 , then the subspace is identified
as the span of the principal directions of the normalized
data. However, in the presence of outliers, the recovered
subspace is never equal to the true subspace and it significantly deviates from the underlying subspace when outliers
abound.
To the best of our knowledge, CoP is the first algorithm
that addresses these concerns all at once. In the proposed
method, we distinguish outliers from inliers by comparing
their degree of coherence with the rest of the data. The advantages of the proposed algorithm are summarized below.
• Coherence Pursuit (CoP) is a considerably simple
non-iterative algorithm which roughly involves one
matrix multiplication to compute the Gram matrix. It
also has provable performance guarantees (c.f. Section 4).
• CoP can tolerate a large number of outliers. It is
shown that exact subspace recovery is guaranteed with
high probability even if nn21 goes to zero provided that
n1 m
n2 r is sufficiently large.
• CoP is robust to linearly dependent outliers since it
measures the coherence of a data point with respect to
all the other data points.

2.1. Motivation and summary of contributions
This work is motivated by the limitations of prior work on
robust PCA as summarized below.
Complex iterations. Most of the state-of-the-art robust
PCA algorithms require a large number of iterations each
with high computational complexity. For instance, many of
these algorithms require the computation of the SVD of an
m×n, or m×m, or n×n matrix in each iteration (Hardt &
Moitra, 2012; Xu et al., 2010b; Lerman et al., 2012), which
leads to long run time.
Guarantees. While the optimal points of the optimization
problems underlying many of the existing robust subspace
recovery techniques yield the exact subspace, there are no
such guarantees for their corresponding iterative solvers.
Examples include the optimal points of the optimization
problems presented in (Xu et al., 2010b; Ding et al., 2006).
Robustness issues. Most existing algorithms are tailored
to one specific class of outlier models. For example, algorithms based on sparse outlier models utilize sparsity promoting norms, thus can only handle a small number of outliers. On the other hand, algorithms such as (Heckel &
Bölcskei, 2013; Soltanolkotabi & Candes, 2012) can han-

Algorithm 1 CoP: Proposed Robust PCA Algorithm
Initialization: Set p = 1 or p = 2.
1. Data Normalization: Define matrix X ∈ Rm×n as
xi = di /kdi k2 .
2. Mutual Coherence Measurement
2.1 Define G = XT X and set its diagonal elements to
zero.
2.2 Define vector p ∈ Rn×1 as p(i) = kgi kp , i =
1, . . . , n.
3. Subspace Identification: Construct matrix Y from the
columns of X corresponding to the largest elements of p
such that they span an r-dimensional subspace.
Output: The columns of Y are a basis for the column
space of L.

3. Proposed Method
In this section, we present the Coherence Pursuit algorithm
and provide some insight into its characteristics. The main
theoretical results are provided in Section 4. The table of

Submission and Formatting Instructions for ICML 2017

Figure 1. The values of vector p for different values of p and

n1
.
n2

Algorithm 1 presents the proposed method along with the
definitions of the used symbols.
Coherence: The inliers lie in a low dimensional subspace
U. In addition, for most practical purposes the inliers are
highly coherent with each other in the sense of having large
values of the mutual coherence in Definition 1. By contrast,
the outlying columns do not typically follow low dimensional structures and do not bear strong resemblance with
the rest of the data. As such, in CoP all mutual coherences
between the columns of D are first computed. Then, the
column space of A is obtained as the span of those columns
that have strong mutual coherence with the rest of the data.
For instance, assume that the distributions of the inliers and
the outliers follow the following assumption.
Assumption 1. The subspace U is a random r-dimensional
subspace in Rm . The columns of A are drawn uniformly
at random from the intersection of Sm−1 and U. The
columns of B are drawn uniformly at random from Sm−1 .
To simplify the exposition and notation, it is assumed that
T in (3) is equal to the identity matrix without any loss of
generality, i.e, D = [A B].
In addition, suppose that v1 and v2 are two inliers and u1
and u1 are two outliers. It can be shown that
E(v1T v2 )2 = 1/r
while

Figure 2. The elements of vector p with different values of parameter τ .

in which m = 400, r = 5, n1 = 50, and the distributions
of inliers and outliers follow Assumption 1. Fig. 1 shows
the vector p (c.f. Algorithm 1) for different values of p and
n2 . In all the figures, the maximum element is scaled to 1.
One can observe that even if n1 /n2 = 0.01, the proposed
technique can recover the exact subspace since there is a
clear gap between the values of p corresponding to outliers
and inliers, more so for p = 2.
3.2. Robustness to noise
In the presence of additive noise, we model the data as
D = [A B] T + E ,
where E represents the noise component.
The high coherence between the inliers (columns of A) and
the low coherence of the outliers (columns of B) with each
other and with the inliers result in the large gap between
the elements of p observed in Fig. 1 even when n1 /n2 <
0.01. This gap gives the algorithm tolerance to high levels
of noise. For example, assume r = 5, n1 = 50, n2 =
500 and the distribution of the inliers and outliers follow
Assumption 1. Define the parameter τ as
τ=

E(uT1 u2 )2 = 1/m , E(uT1 v1 )2 = 1/m .
Accordingly, if m  r, the inliers exhibit much stronger
mutual coherences.
3.1. Large number of outliers
Unlike most robust PCA algorithms which require n2 to
be much smaller than n1 , the proposed method tolerates a
large number of outliers. For instance, consider a setting

(5)

Ekek2
Ekak2

(6)

where a and e are arbitrary columns of A and E, respectively. Fig. 2 shows the entries of p for different values
of τ . As shown, the elements corresponding to inliers are
clearly separated from the ones corresponding to outliers
even at very low signal to noise ratio, e.g. τ = 0.5 and
τ = 1. The mathematical analysis of CoP with noisy data
confirms that the proposed method remains robust to high
levels of noise even with data that is predominantly outliers
(Rahmani & Atia, 2016).

Submission and Formatting Instructions for ICML 2017

1. In many applications, we may have an upper bound on
n2 /n. For instance, suppose we know that up to 40 percent
of the data could be outliers. In this case, we simply remove
40 percent of the columns corresponding to the smallest
values of p and obtain the subspace using the remaining
data points.
Figure 3. The values of vector p when the 301th to 305th columns
are repeated outliers.

3.3. Linearly dependent outliers
Many of the existing robust PCA algorithms cannot detect linearly dependent outliers (Hardt & Moitra, 2012;
Soltanolkotabi & Candes, 2012). For instance, in the
outlier detection algorithm presented in (Soltanolkotabi &
Candes, 2012) a data column is identified as an outlier if
it does not admit a sparse representation in the rest of the
data. As such, if some outlying columns are repeated, the
algorithm in (Soltanolkotabi & Candes, 2012) fails to detect them.
At a fundamental level, the proposed approach affords
a more comprehensive definition of an outlying column,
namely, a data column is identified as an outlier if it has
weak total coherency with the rest of the data. This global
view of a data point w.r.t. the rest of the data allows the algorithm to identify outliers that are linearly dependent with
few other data points.

2. The second technique is an adaptive sampling method
presented in the table of Algorithm 2. First, the data is
projected onto a random kr-dimensional subspace to reduce the computational complexity for some integer k > 1.
According to the analysis presented in (Rahmani & Atia,
2017; Li & Haupt, 2015), even k = 2 is sufficient to preserve the rank of A and the structure of the outliers B, i.e.,
the rank of ΦA is equal to r and the columns of ΦB do
not lie in the column space of ΦA, where Φ is the projection matrix. The parameter ν that thresholds the `2 -norms
of the columns of the projected data is chosen based on
the noise level (if the data is noise free, ν = 0). In Algorithm 2, the data is projected onto the span of the sampled
columns (step 2.3). Thus, a newly sampled column brings
innovation with respect to the previously sampled columns.
Therefore, redundant columns are not sampled.
Algorithm 2 Adaptive Column Sampling for the Subspace
Identification Step (step 3) of CoP
Initialization: Set k equal to an integer greater than 1, a
threshold ν greater than or equal to 0, and F an empty matrix.

For illustration, assume the given data follows Data model
1, r = 5, n1 = 50, n2 = 500, where the 301th to 305th
columns are repeated outliers. Fig. 3 shows the elements
of vector p with p = 1 and p = 2. All the elements corresponding to inliers are clearly greater than the elements
corresponding to outliers and the algorithm correctly identifies the subspace. Fig. 3 suggests that CoP with p = 1 is
better at handling repeated outliers since the entries of G
with large absolute magnitudes are more gracefully amplified by the `1 -norm.

1. Data Projection: Define Xφ ∈ Rkr×n as Xφ = ΦX,
where Φ ∈ Rkr×m projects the columns of X onto a random kr-dimensional subspace.

3.4. Subspace identification

End For

In the third step of Algorithm 1, we sample the columns
of X with the largest coherence values which span an rdimensional space. In this section, we present some tips
for efficient implementation of this step. One way is to
start sampling the columns with the highest coherence values and stop when the rank of the sampled columns is equal
to r. However, if the columns of L admit a clustering
structure and their distribution is highly non-uniform, this
method will sample many redundant columns, which can
in turn increase the run time. In this section, we propose
two low-complexity techniques to accelerate the subspace
identification step.

Output Construct Y using the columns of X that correspond to the columns that formed F.

2. Column Sampling
For i from 1 to r do
2.1 Set equal to zero the elements of p corresponding to
columns of Xφ with `2 -norms less than or equalto ν. 
2.2 Define j := arg max p(k), update F = orth [F xj ] ,
k

and set p(j) = 0.
2.3 Update Xφ = Xφ − FFT Xφ .

Remark 1. Suppose we run Algorithm 2 h times (each
time the sampled columns are removed from the data and
newly sampled columns are added to Y). If the given data
is noisy, the first r singular values of Y are the dominant
ones and the rest correspond to the noise component. If we
increase h, the span of the dominant singular vectors will
be closer to the column space of A. However, if h is chosen
unreasonably high, the sampler may also sample outliers.

Submission and Formatting Instructions for ICML 2017

3.5. Computational complexity
The main computational complexity is in the second step
of Algorithm 2 which is of order O(mn2 ). If we utilize
Algorithm 2 as the third step of Algorithm 1, the overall
complexity is of order O(mn2 + r3 n). However, since the
algorithm roughly involves only one matrix multiplication,
it is very fast and very simple for hardware implementation
(c.f. Section 5.2 on run time). Moreover, if we utilize the
randomized designs presented in (Rahmani & Atia, 2017;
Li & Haupt, 2015), the overall complexity can be reduced
to O(r4 ).

4. Theoretical Investigations
In this section, we first establish sufficient conditions to ensure that the expected value of the elements of the vector
p corresponding to the inliers are much greater than the
elements corresponding to the outliers, in which case the
algorithm is highly likely to yield exact subspace recovery.
Subsequently, we provide two theorems establishing performance guarantees for the proposed approach for p = 1
and p = 2.
The following lemmas establish sufficient conditions for
the expected value of the elements of p corresponding to
inliers to be at least twice as large as those corresponding
to outliers. Due to space limitations, the proofs of all lemmas and theorems are deferred to an extended version of
this paper (Rahmani & Atia, 2016).
Lemma 1. Suppose Assumption 1 holds, the ith column is
an inlier and the (n1 + j)th column is an outlier. If
! r
r
r
r
2
4r2
4
5 n2
2
n1
√
−
+
> √ +
, (7)
π
m
m
πr
r
4 m
then
E kgi k1 > 2 E kgn1 +j k1
recalling that gi is the ith column of matrix G.
Lemma 2. Suppose Assumption 1 holds, the ith column is
an inlier and the (n1 + j)th column is an outlier. If
n1
2r2
n2
1
(1 −
)>
+
r
m
m
r

(8)

then
E kgi k22 > 2 E kgn1 +j k22 .
Remark 2. The sufficient conditions provided in Lemma 1
and Lemma 2 reveal three important points.
I) The important performance factors are the ratios nr1 and
n2
n1
m . The intuition is that as r increases, the density of the
inliers in the subspace increases, and consequently their

mutual coherence also increases. Similarly, if nm2 increases,
the mutual coherence between the outliers increases. Thus,
the main requirement is that nr1 should be sufficiently larger
than nm2 .
II) In real applications, r  m and n1 > n2 , hence the
sufficient conditions are easily satisfied. This fact is observed in Fig. 1 which shows that Coherence Pursuit can
recover the correct subspace even if n1 /n2 = 0.01.
III) In high dimensional settings, r  m. Therefore, √mm
could be much greater than √rr . Accordingly, the conditions in Lemma 1 are stronger than the conditions of
Lemma 2, suggesting that with p = 2 Coherence Pursuit
can tolerate a larger number of outliers than with p = 1.
This is confirmed by comparing the plots in the last row of
Fig. 1.
The following theorems show that the same set of factors
are important to guarantee that the proposed algorithm recovers the exact subspace with high probability.
Theorem 3. If Assumption 1 is true and
s
!
r
√
2n1 log nδ1
√
2
r + 2 βκ r
n1
√
√
−
− 2 n1 −
π
r−1
r
m
(9)
s
2n2 log nδ2
√
n2
1
> √ + 2 n2 +
+√ ,
m−1
m
r
then Algorithm 1 with p = 1 recovers the exact subspace with probability at least 1 − 3δ, where β =
m
max(8 log n2 /δ, 8π) and κ = m−1
.
Theorem 4. If Assumption 1 is true and
√


1
1 r + 4ζκ + 4 ζrκ
(10)
−
− η1 > 2η2 + ,
n1
r
m
r
then Algorithm 1 with p = 2 recovers the correct subspace
with probability at least 1 − 4δ, where
!
r
4
n1
2rn1
2rn1
η1 = max
log
, 4 log
,
3
δ
r
δ
!
r
2mn2
n2
2mn2
4
log
, 4 log
,
η2 = max
3
δ
m
δ
m
ζ = max(8π, 8 log nδ2 ), and κ = m−1
.
Remark 3. The
dominant
factors
of
the
LHS and the RHS
q


2

2
of (10) are nr1 1 − rm and 4 nm2 log 2mn
δ , respectively.
As in Lemma 2, we see the factor nm2 , but under the square
root. Thus, the requirement of Theorem 4 is less stringent
than that of Lemma 2. This is because Theorem 4 guarantees that the elements corresponding to inliers are greater
than those corresponding to outliers with high probability,
but does not guarantee a large gap between the values as
in Lemma 2.

Submission and Formatting Instructions for ICML 2017

k

End For
Output: The matrices {D̂i }L
i=1 represent the clustered
i L
data and the matrices {Û }i=1 are the orthonormal bases
for the identified subspaces.

5. Numerical Simulations
In this section, the performance of the proposed method is
investigated with both synthetic and real data. We compare the performance of CoP with the state-of-the-art robust PCA algorithms including FMS (Lerman & Maunu,
2014), GMS (Zhang & Lerman, 2014), R1-PCA (Ding
et al., 2006), OP (Xu et al., 2010b), and SPCA (Maronna
et al., 2006).

10

8

n1 / r

Algorithm 3 Subspace Clustering Error Correction
Method
Input: The matrices {D̂i }L
i=1 represent the clustered data
(the output of a subspace clustering algorithm) and L is the
number of clusters.
Error Correction
For k from 1 to t do
1 Apply the robust PCA algorithm to the matrices {D̂i }L
i=1 .
Define the orthonormal matrices {Ûi }L
as
the
learned
i=1
bases for the inliers of {D̂i }L
i=1 , respectively.
2 Update the data clustering with respect to the obtained
i L
bases {Ûi }L
i=1 (the matrices {D̂ }i=1 are updated), i.e.,
a data point d is assigned to the ith cluster if i =
arg max kxT Ûk k2 .

6

4

2
5

10

15

20

25

30

35

n2 / m

Figure 4. The phase transition plot of CoP versus n1 /r and
n2 /m.

5.2. Running time
In this section, we compare the run time of CoP to the existing approaches. Table 1 presents the run time in seconds
for different data sizes. In all experiments, n1 = n/5 and
n2 = 4n/5. One can observe that CoP is remarkably faster
given its simplicity (single step algorithm).
Table 1. Running time of the algorithms

m=n
1000
2000
5000
10000

CoP
0.02
0.7
5.6
27

FMS
1
5.6
60
401

OP
45
133
811
3547

R1-PCA
1.45
10.3
83.3
598

5.1. Phase transition plot
Our analysis has shown that CoP yields exact subspace recovery with high probability if n1 /r is sufficiently greater
than n2 /m. In this experiment, we investigate the phase
transition of CoP in the n1 /r and n2 /m plane. Suppose
m = 100, r = 10, and the distributions of inliers/outliers
follow Assumption 1. Define U and Û as the exact and
recovered orthonormal bases for the span of inliers, respectively. A trial is considered successful if


kU − ÛÛT UkF /kUkF



≤ 10−5 .

In this simulation, we construct the matrix Y using 20
columns of X corresponding to the largest 20 elements of
the vector p. Fig. 4 shows the phase transition plot. White
indicates correct subspace recovery and black designates
incorrect recovery. As shown, if n2 /m increases, we need
higher values of n1 /r. However, one can observe that with
n1 /r > 4, the algorithm can yield exact recovery even if
n2 /m > 30.

5.3. Subspace recovery in presence of outliers
In this experiment, we assess the robustness of CoP to outliers in comparison to existing approaches. It is assumed
that m = 50, r = 10, n1 = 50 and the distribution of
inliers/outliers follows Assumption 1. Define U and Û as
before, and the recovery error as


Log-Recovery Error = log10 kU − ÛÛT UkF /kUkF .
In this simulation, we use 30 columns to form the matrix
Y. Fig. 5 shows the recovery error versus n2 /n1 for different values of n2 . In addition to its remarkable simplicity,
CoP gives the highest accuracy and yields exact subspace
recovery even if the data is overwhelmingly outliers.
5.4. Clustering error correction
In this section, we consider a very challenging robust subspace recovery problem with real data. We use the robust
PCA algorithm as a subroutine for error correction in a subspace clustering problem. In this experiment, the outliers

Submission and Formatting Instructions for ICML 2017
0.3

0

CoP
FMS
R1-PCA
GMS
SPCA
OP

-5

-10

Clustering Error

log-Recovery Error

0.25

CoP
FMS
R1-PCA
GMS
SPCA

0.2
0.15
0.1
0.05
0
0

-15
5

10
n2 / n 1

15

2

4

6

8

Iteration Number

20

Figure 6. The clustering error after each iteration of Algorithm 2.

Figure 5. The subspace recovery error versus n2 /n1 .

can be close to the inliers and can also be linearly dependent.
The subspace clustering problem is a general form of PCA
in which the data points lie in a union of an unknown number of unknown linear subspaces (Vidal, 2011; Rahmani &
Atia, 2015). A subspace clustering algorithm identifies the
subspaces and clusters the data points with respect to the
subspaces. The performance of the subspace clustering algorithms – especially the ones with scalable computational
complexity – degrades in presence of noise or when the
subspaces are closer to each other. Without loss of generality, suppose D = [D1 ... DL ] is the given data where the
L
columns of {Di }L
i=1 lie in the linear subspaces {Si }i=1 ,
respectively, and L is the number of subspaces. Define
{D̂i }L
i=1 as the output of some clustering algorithm (the
clustered data). Define the clustering error as the ratio of
misclassified points to the total number of data points. With
the clustering error, some of the columns of D̂i believed to
lie in Si may actually belong to some other subspace. Such
columns can be viewed as outliers in the matrix D̂i . Accordingly, the robust PCA algorithm can be utilized to correct the clustering error. We present Algorithm 3 as an error
correction algorithm which can be applied to the output of
any subspace clustering algorithm to reduce the clustering
error. In each iteration, Algorithm 3 applies the robust PCA
algorithm to the clustered data to obtain a set of bases for
the subspaces. Subsequently, the obtained clustering is updated based on the obtained bases.
In this experiment, we imagine a subspace clustering algorithm with 20 percent clustering error and apply Algorithm
3 to the output of the algorithm to correct the errors. We use
the Hopkins155 dataset which contains video sequences of
2 or 3 motions (Tron & Vidal, 2007). The data is generated
by extracting and tracking a set of feature points through

the frames. In motion segmentation, each motion corresponds to one subspace. Thus, the problem here is to cluster data lying in two or three subspaces (Vidal, 2011). We
use the traffic data sequences, which include 8 scenarios
with two motions and 8 scenarios with three motions.
When CoP is applied, 50 percent of the columns of X are
used to form the matrix Y. Fig. 6 shows the average clustering error (over all traffic data matrices) after each iteration of Algorithm 3 for different robust PCA algorithms.
CoP clearly outperforms the other approaches. As a matter
of fact, most of the robust PCA algorithms fail to obtain
the correct subspaces and end up increasing the clustering
error.
Acknowledgment
This work was supported by NSF CAREER Award CCF1552497 and NSF Grant CCF-1320547.

References
Basri, Ronen and Jacobs, David W. Lambertian reflectance
and linear subspaces. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(2):218–233, 2003.
Candes, Emmanuel J and Tao, Terence. Decoding by linear
programming. Information Theory, IEEE Transactions
on, 51(12):4203–4215, 2005.
Candès, Emmanuel J, Romberg, Justin, and Tao, Terence.
Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. Information Theory, IEEE Transactions on, 52(2):489–
509, 2006.
Candès, Emmanuel J, Li, Xiaodong, Ma, Yi, and Wright,
John. Robust principal component analysis? Journal of
the ACM (JACM), 58(3):11, 2011.
Chandrasekaran,

Venkat,

Sanghavi,

Sujay,

Parrilo,

Submission and Formatting Instructions for ICML 2017

Pablo A, and Willsky, Alan S. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on
Optimization, 21(2):572–596, 2011.
Chen, Yudong, Xu, Huan, Caramanis, Constantine, and
Sanghavi, Sujay. Robust matrix completion with corrupted columns. arXiv preprint arXiv:1102.2254, 2011.
Costeira, João Paulo and Kanade, Takeo. A multibody factorization method for independently moving objects. International Journal of Computer Vision, 29(3):159–179,
1998.
Ding, Chris, Zhou, Ding, He, Xiaofeng, and Zha,
Hongyuan. R1-PCA: rotational invariant l 1-norm principal component analysis for robust subspace factorization. In Proceedings of the 23rd international conference
on Machine learning, pp. 281–288. ACM, 2006.
Elhamifar, Ehsan and Vidal, Rene. Sparse subspace clustering: Algorithm, theory, and applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35
(11):2765–2781, 2013.
Hardt, Moritz and Moitra, Ankur. Algorithms and hardness for robust subspace recovery. arXiv preprint
arXiv:1211.1041, 2012.
Hauberg, Soren, Feragen, Aasa, Enficiaud, Raffi, and
Black, Michael. Scalable robust principal component
analysis using grassmann averages. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(11):
2298–2311, Nov 2016.
Heckel, Reinhard and Bölcskei, Helmut. Robust subspace clustering via thresholding.
arXiv preprint
arXiv:1307.4891, 2013.
Hosseini, Mohammad-Parsa, Nazem-Zadeh, Mohammad R, Mahmoudi, Fariborz, Ying, Hao, and SoltanianZadeh, Hamid. Support vector machine with nonlinearkernel optimization for lateralization of epileptogenic
hippocampus in mr images. In 2014 36th Annual
International Conference of the IEEE Engineering in
Medicine and Biology Society, pp. 1047–1050. IEEE,
2014.
Huber, Peter J. Robust statistics. Springer, 2011.
Ke, Qifa and Kanade, Takeo. Robust l 1 norm factorization
in the presence of outliers and missing data by alternative
convex programming. In Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference on, volume 1, pp. 739–746. IEEE, 2005.
Lerman, Gilad and Maunu, Tyler.
and non-convex subspace recovery.
arXiv:1406.6145, 2014.

Fast, robust
arXiv preprint

Lerman, Gilad and Zhang, Teng. {l p}-recovery of the
most significant subspace among multiple subspaces
with outliers. Constructive Approximation, 40(3):329–
385, 2014.
Lerman, Gilad, Zhang, Teng, et al. Robust recovery of
multiple subspaces by geometric lp minimization. The
Annals of Statistics, 39(5):2686–2715, 2011.
Lerman, Gilad, McCoy, Michael, Tropp, Joel A, and
Zhang, Teng. Robust computation of linear models, or
how to find a needle in a haystack. Technical report,
DTIC Document, 2012.
Li, Xingguo and Haupt, Jarvis. Identifying outliers in
large matrices via randomized adaptive compressive
sampling. Signal Processing, IEEE Transactions on, 63
(7):1792–1807, 2015.
Maronna, RARD, Martin, Douglas, and Yohai, Victor. Robust statistics. John Wiley & Sons, Chichester. ISBN,
2006.
Maronna, Ricardo. Principal components and orthogonal
regression based on robust scales. Technometrics, 47(3):
264–273, 2005.
McCoy, Michael, Tropp, Joel A, et al. Two proposals for
robust pca using semidefinite programming. Electronic
Journal of Statistics, 5:1123–1160, 2011.
Netrapalli, Praneeth, Niranjan, UN, Sanghavi, Sujay,
Anandkumar, Animashree, and Jain, Prateek. Nonconvex robust pca. In Advances in Neural Information
Processing Systems, pp. 1107–1115, 2014.
Rahmani, Mostafa and Atia, George. Innovation pursuit:
A new approach to subspace clustering. arXiv preprint
arXiv:1512.00907, 2015.
Rahmani, Mostafa and Atia, George. Coherence pursuit:
Fast, simple, and robust principal component analysis.
arXiv preprint arXiv:1609.04789, 2016.
Rahmani, Mostafa and Atia, George. Randomized robust
subspace recovery and outlier detection for high dimensional data matrices. IEEE Transactions on Signal Processing, 65(6):1580–1594, March 2017.
Soltanolkotabi, Mahdi and Candes, Emmanuel J. A geometric analysis of subspace clustering with outliers. The
Annals of Statistics, pp. 2195–2238, 2012.
Tron, Roberto and Vidal, René. A benchmark for the comparison of 3-d motion segmentation algorithms. In Computer Vision and Pattern Recognition, 2007. CVPR’07.
IEEE Conference on, pp. 1–8. IEEE, 2007.

Submission and Formatting Instructions for ICML 2017

Tsakiris, Manolis C and Vidal, René. Dual principal component pursuit. In Proceedings of the IEEE International
Conference on Computer Vision Workshops, pp. 10–18,
2015.
Vidal, Rene. Subspace clustering. IEEE Signal Processing
Magazine, 2(28):52–68, 2011.
Xu, Huan, Caramanis, Constantine, and Mannor, Shie.
Principal component analysis with contaminated
data: The high dimensional case. arXiv preprint
arXiv:1002.4658, 2010a.
Xu, Huan, Caramanis, Constantine, and Sanghavi, Sujay.
Robust pca via outlier pursuit. In Advances in Neural
Information Processing Systems, pp. 2496–2504, 2010b.
Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Caramanis, Constantine. Fast algorithms for robust pca via gradient descent. arXiv preprint arXiv:1605.07784, 2016.
Zhang, Teng. Robust subspace recovery by geodesically
convex optimization. arXiv preprint arXiv:1206.1386,
2012.
Zhang, Teng and Lerman, Gilad. A novel m-estimator for
robust pca. The Journal of Machine Learning Research,
15(1):749–808, 2014.

