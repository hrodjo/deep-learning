Analytical Guarantees on Numerical Precision of Deep Neural Networks

Charbel Sakr Yongjune Kim Naresh Shanbhag

Abstract
The acclaimed successes of neural networks often overshadow their tremendous complexity.
We focus on numerical precision - a key parameter defining the complexity of neural networks.
First, we present theoretical bounds on the accuracy in presence of limited precision. Interestingly, these bounds can be computed via the
back-propagation algorithm. Hence, by combining our theoretical analysis and the backpropagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to timeconsuming fixed-point simulations. We provide
numerical evidence showing how our approach
allows us to maintain high accuracy but with
lower complexity than state-of-the-art binary networks.

1. Introduction
Neural networks have achieved state-of-the-art accuracy on
many machine learning tasks. AlexNet (Krizhevsky et al.,
2012) had a deep impact a few years ago in the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) and
triggered intensive research efforts on deep neural networks. Recently, ResNet (He et al., 2016) has outperformed humans in recognition tasks.
These networks have very high computational complexity.
For instance, AlexNet has 60 million parameters and 650,000 neurons (Krizhevsky et al., 2012).
Its convolutional layers alone require 666 million
multiply-accumulates (MACs) per 227 × 227 image (13k
MACs/pixel) and 2.3 million weights (Chen et al., 2016).
Deepface’s network involves more than 120 million parameters (Taigman et al., 2014). ResNet is a 152-layer
The authors are with the University of Illinois at UrbanaChampaign, 1308 W Main St., Urabna, IL 61801 USA.
Correspondence to:
Charbel Sakr <sakr2@illinois.edu>,
Yongjune Kim <yongjune@illinois.edu>, Naresh Shanbhag
<shanbhag@illinois.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

deep residual network. This high complexity of deep
neural networks prevents its deployment on energy and
resource-constrained platforms such as mobile devices and
autonomous platforms.
1.1. Related Work
One of the most effective approaches for reducing resource
utilization is to implement fixed-point neural networks. As
mentioned in (Lin et al., 2016a), there are two approaches
for designing fixed-point neural networks: (1) directly train
a fixed-point neural network, and (2) quantize a pre-trained
floating-point neural network to obtain a fixed-point network.
As an example of fixed-point training, Gupta et al. (2015)
showed that 16-bit fixed-point representation incurs little
accuracy degradation by using stochastic rounding. A more
aggressive approach is to design binary networks such as
Kim & Smaragdis (2016) which used bitwise operations to
replace the arithmetic operations and Rastegari et al. (2016)
which explored optimal binarization schemes. BinaryConnect (Courbariaux et al., 2015) trained networks using
binary weights while BinaryNet (Hubara et al., 2016b)
trained networks with binary weights and activations.
Although these fixed-point training approaches make it
possible to design fixed-point neural networks achieving
excellent accuracy, training based on fixed-point arithmetic
is generally harder than floating-point training since the optimization is done in a discrete space.
Hence, in this paper, we focus on the second approach
that quantizes a pre-trained floating-pointing network to a
fixed-point network. This approach leverages the extensive work in training state-of-the-art floating-point neural
networks such as dropout (Srivastava et al., 2014), maxout
(Goodfellow et al., 2013), network-in-network (Lin et al.,
2013), and residual learning (He et al., 2016) to name a
few. In this approach, proper precision needs to be determined after training to reduce complexity while minimizing the accuracy loss. In (Hwang & Sung, 2014), exhaustive search is performed to determine a suitable precision
allocation. Recently, Lin et al. (2016a) offered an analytical
solution for non-uniform bit precision based on the signalto-quantization-noise ratio (SQNR). However, the use of
non-uniform quantization step sizes at each layer is diffi-

Analytical Guarantees on Numerical Precision of Deep Neural Networks

cult to implement as it requires multiple variable precision
arithmetic units.
In addition to fixed-point implementation, many approaches have been proposed to lower the complexity of
deep neural networks in terms of the number of arithmetic
operations. Han et al. (2015) employs a three-step training
method to identify important connections, prune the unimportant ones, and retrain on the pruned network. Zhang
et al. (2015) replaces the original convolutional layers by
smaller sequential layers to reduce computations. These
approaches are complementary to our technique of quantizing a pre-trained floating-point neural network into a fixedpoint one.
In this paper, we obtain analytical bounds on the accuracy of fixed-point networks that are obtained by quantizing
a conventionally trained floating-point network. Furthermore, by defining meaningful measures of a fixed-point
network’s hardware complexity viz. computational and
representational costs, we develop a principled approach to
precision assignment using these bounds in order to minimize these complexity measures.
1.2. Contributions
Our contributions are both theoretical and practical. We
summarize our main contributions:
• We derive theoretical bounds on the misclassification
rate in presence of limited precision and thus determine analytically how accuracy and precision tradeoff with each other.
• Employing the theoretical bounds and the backpropagation algorithm, we show that proper precision
assignments can be readily determined while maintaining accuracy close to floating-point networks.
• We analytically determine which of weights or activations need more precision, and we show that typically the precision requirements of weights are greater
than those of activations for fully-connected networks
and are similar to each other for networks with shared
weights such as convolutional neural networks.
• We introduce computational and representational
costs as meaningful metrics to evaluate the complexity
of neural networks under fixed precision assignment.
• We validate our findings on the MNIST and CIFAR10
datasets demonstrating the ease with which fixedpoint networks with complexity smaller than stateof-the-art binary networks can be derived from pretrained floating-point networks with minimal loss in
accuracy.
It is worth mentioning that our proposed method is general
and can be applied to every class of neural networks such as
multilayer perceptrons and convolutional neural networks.

2. Fixed-Point Neural Networks
2.1. Accuracy of Fixed-Point and Floating-Point
Networks
For a given floating-point neural network and its fixedpoint counterpart we define: 1) the floating-point error
probability pe,f l = Pr{Ŷf l 6= Y } where Ŷf l is the output
of the floating-point network and Y is the true label; 2) the
fixed-point error probability pe,f x = Pr{Ŷf x 6= Y } where
Ŷf x is the output of the fixed-point network; 3) the mismatch probability between fixed-point and floating-point
pm = Pr{Ŷf x 6= Ŷf l }. Observe that:
pe,f x ≤ pe,f l + pm

(1)

The right-hand-side represents the worst case of having no
overlap between misclassified samples and samples whose
predicted labels are in error due to quantization. We provide a formal proof of (1) in the supplementary section.
Note that pe,f x is a quantity of interest as it characterizes
the accuracy of the fixed-point system. We employ pm as
a proxy to pe,f x because it brings in the effects of quantization into the picture as opposed to pe,f l which solely
depends on the algorithm. This observation was made in
(Sakr et al., 2017) and allowed for an analytical characterization of linear classifiers as a function of precision.
2.2. Fixed-Point Quantization
The study of fixed-point systems and algorithms is well established in the context of signal processing and communication systems (Shanbhag, 2016). A popular example is the
least mean square (LMS) algorithm for which bounds on
precision requirements for input, weights, and updates have
been derived (Goel & Shanbhag, 1998). In such analyses,
it is standard practice (Caraiscos & Liu, 1984) to assume
all signed quantities lie in [−1, 1] and all unsigned quantities lie in [0, 2]. Of course, activations and weights can be
designed to satisfy this assumption during training. A Bbit fixed-point number af x would be related to its floatingpoint value a as follows:
af x = a + qa

(2)

where qa is the quantization noise which is modeled as
an
uniform random variable distributed over

 independent
∆
,
,
where
∆ = 2−(B−1) is the quantization step
−∆
2 2
(Caraiscos & Liu, 1984).
2.3. Complexity in Fixed-Point
We argue that the complexity of a fixed-point system has
two aspects: computational and representational costs. In
what follows, we consider activations and weights to be
quantized to BA and BW bits, respectively.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

The computational cost is a measure of the computational
resources utilized for generating a single decision, and is
defined as the number of 1 bit full adders (F As). A full
adder is a canonical building block of arithmetic units. We
assume arithmetic operations are executed using the commonly used ripple carry adder (Knauer, 1989) and BaughWooley multiplier (Baugh & Wooley, 1973) architectures
designed using F As. Consequently, the number of F As
used to compute a D-dimensional dot product of activations and weights is (Lin et al., 2016b):
DBA BW + (D − 1)(BA + BW + dlog2 (D)e − 1) (3)
Hence, an important aspect of the computational cost of a
dot product is that it is an increasing function of the product
of activation precision (BA ), weight precision (BW ), and
dimension (D).
We define the representational cost as the total number of
bits needed to represent all network parameters, i.e., both
activations and weights. This cost is a measure of the storage complexity and communications costs associated with
data movement. The total representational cost of a fixedpoint network is:

where qah and qwh are the quantization noise terms of the
activation ah and weight wh , respectively. Here, {qah }h∈A
are independent
 uniformly distributed random variables
on − ∆2A , ∆2A and {qwh }h∈W are independent uniformly


distributed random variables on − ∆2W , ∆2W , with ∆A =
2−(BA −1) and ∆W = 2−(BW −1) .
In quantization noise analysis, it is standard to ignore crossproducts of quantization noise terms as their contribution is
negligible. Therefore, using Taylor’s theorem, we express
the total quantization noise at the output of the fixed-point
network as:
qzi =

X

qah

h∈A

X
∂zi
∂zi
+
qwh
.
∂ah
∂wh

(7)

h∈W

Note that the derivatives in (7) are obtained as part of the
back-propagation algorithm. Thus, using our results, it
is possible to estimate the precision requirements of deep
neural networks during training itself. As will be shown
later, this requires one additional back-propagation iteration to be executed after the weights have converged.

3. Bounds on Mismatch Probability
|A| BA + |W| BW

(4)

bits, where A and W are the index sets of all activations
and weights in the network, respectively. Observe that the
representational cost is linear in activation and weight precisions as compared to the computational cost.
Equations (3) - (4) illustrate that, though computational and
representational costs are not independent, they are different. Together, they describe the implementation costs associated with a network. We shall employ both when evaluating the complexity of fixed-point networks.

3.1. Second Order Bound
We present our first result. It is an analytical upper bound
on the mismatch probability pm between a fixed-point neural network and its floating-point counterpart.
Theorem 1. Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point
counterpart is upper bounded as follows:

pm
2.4. Setup


 
P  ∂(Zi −ZŶf l ) 2
M
 ∂Ah
 
∆2 
 X h∈A

≤ AE

24  i=1
|Zi − ZŶf l |2 


i6=Ŷf l

Here, we establish notation. Let us consider neural networks deployed on a M -class classification task. For
a given input, the network would typically have M numerical outputs {zi }M
i=1 and the decision would be ŷ =
arg max zi . Each numerical output is a function of
i=1,...,M


 
P  ∂(Zi −ZŶf l ) 2


M
∂wh

 
∆2W 
 X h∈W

+
E

24  i=1
|Zi − ZŶf l |2 


(8)

i6=Ŷf l

weights and activations in the network:
zi = f ({ah }h∈A , {wh }h∈W )

(5)

for i = 1, . . . , M , where ah denotes the activation indexed
by h and wh denotes the weight indexed by h. When activations and weights are quantized to BA and BW bits, respectively, the output zi is corrupted by quantization noise
qzi so that:
zi + qzi = f ({ah + qah }h∈A , {wh + qwh }h∈W )

(6)

where expectations are taken over a random input and
{Ah }h∈A , {Zi }M
i=1 , and Ŷf l are thus random variables.
Proof. The detailed proof can be found in the supplementary section. Here, we provide the main idea and the intuition behind the proof.
The heart of the proof lies in evaluating
Pr zi + qzi > zj + qzj for any pair of outputs zi
and zj where zj > zi . Equivalently, we need to evaluate

Analytical Guarantees on Numerical Precision of Deep Neural Networks


Pr qzi − qzj > zj − zi . But from (7), we have:
qzi − qzj =

X
h∈A

qah

X
∂(zi − zj )
∂(zi − zj )
+
qwh
.
∂ah
∂wh
h∈W

(9)
In (9), we have a linear combination of quantization noise terms, qzi − qzj is a zero mean
random variable having a symmetric distribu
tion.
This means that Pr
q
−
q
>
z
−
z
=
z
z
j
i
i
j

1
2 Pr |qzi − qzj | > |zj − zi | , which allows us to use
Chebyshev’s inequality. Indeed, from (9), the variance of
qzi − qzj is given by:

2

2
∆2W X  ∂(zi − zj ) 
∆2A X  ∂(zi − zj ) 
 ∂ah  + 12
 ∂wh  ,
12
h∈A

h∈W

clearly highlights the gains in practicality of our analytical
approach over a trial-and-error based search.
Finally, (8) reveals a very interesting aspect of the trade-off
between activation precision BA and weight precision BW .
We rewrite (8) as:
pm ≤ ∆2A EA + ∆2W EW
where

 
P  ∂(Zi −ZŶf l ) 2
M
X
 ∂Ah
 


h∈A
EA = E 

2
 i=1 24|Zi − ZŶf l | 


i6=Ŷf l

and

As explained in the supplementary section, it is possible to
obtain to (8) from (10) using standard probabilistic arguments.
Before proceeding, we point out that the two expectations in (8) are taken over a random input but the weights
{wh }h∈W are frozen after training and are hence deterministic.
Several observations are to be made. First notice that the
mismatch probability pm increases with ∆2A and ∆2W . This
is to be expected as smaller precision leads to more mismatch. Theorem 1 says a little bit more: the mismatch
probability decreases exponentially with precision, because
∆A = 2−(BA −1) and ∆W = 2−(BW −1) .
Note that the quantities in the expectations in (8) can be
obtained as part of a standard back-propagation procedure.
Indeed, once the weights are frozen, it is enough to perform one forward pass on an estimation set (which should
have statistically significant cardinality), record the numerical outputs, perform one backward pass and probe all relevant derivatives. Thus, (8) can be readily computed.
Another practical aspect of Theorem 1 is that this operation needs to be done only once as these quantities do not
depend on precision. Once they are determined, for any
given precision assignment, we simply evaluate (8) and
combine it with (1) to obtain an estimate (upper bound) on
the accuracy of the fixed-point instance. This way the precision necessary to achieve a specific mismatch probability is obtained from a trained floating-point network. This


 
P  ∂(Zi −ZŶf l ) 2


M
X
∂wh

 


h∈W
= E
.
2
 i=1 24|Zi − ZŶf l | 


so that

Pr zi + qzi > zj + qzj




P
P
 ∂(zi −zj ) 2
 ∂(zi −zj ) 2
2
+
∆
∆2A h∈A  ∂a



W
h∈W
∂w
h
h
.
≤
2
24 |zi − zj |
(10)

(11)

EW

i6=Ŷf l

The first term in (11) characterizes the impact of quantizing
activations on the overall accuracy while the second characterizes that of weight quantization. It might be the case
that one of the two terms dominates the sum depending on
the values of EA and EW . This means that either the activations or the weights are assigned more precision than
necessary. An intuitive first step to efficiently get a smaller
upper bound is to make the two terms of comparable order.
That can be made by setting ∆2A EA = ∆2W EW which is
equivalent to
!
r
EA
BA − BW = round log2
(12)
EW
where round() denotes the rounding operation. This is an
effective way of taking care of one of the two degrees of
freedom introduced by (8).
A natural question to ask would be which of EA and EW
is typically larger. That is to say, to whom, activations or
weights, should one assign more precision. In deep neural networks, there are more weights than activations, a
trend particularly observed in deep networks with most layers fully connected. This trend, though not as pronounced,
is also observed in networks with shared weights, such as
convolutional neural networks. However, there exist a few
counterexamples such as the networks in (Hubara et al.,
2016b) and (Hubara et al., 2016a). It is thus reasonable to
expect EW ≥ EA , and consequently the precision requirements of weights will, in general, be more than those of
activations.
One way to interpret (11) is to consider minimizing the upper bound in (8) subject to BA +BW = c for some constant

Analytical Guarantees on Numerical Precision of Deep Neural Networks
∆A ∂(zi −zj )
2
∂ah

∆W ∂(zi −zj )
2
∂wh .

c. Indeed, it can be shown that (12) would be a necessary
condition of the corresponding solution. This is an application of the arithmetic-geometric mean inequality. Effectively, (11) is of particular interest when considering computational cost which increases as a function of the product
of both precisions (see Section 2.3).

where da,h =
yields:

3.2. Tighter Bound

We show that the right-hand-side is minimized over positive values of t when:

We present a tighter upper bound on pm based on the Chernoff bound.
Theorem 2. Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point
counterpart is upper bounded as follows:


pm

M
X
(i,Ŷf l )
(i,Ŷ ) (i,Ŷ ) 
≤ E
e−S
P1 f l P2 f l 



(13)

i=1
i6=Ŷf l

3(Zi − Zj )2

2 P
2 ,

(i,j)
(i,j)
D
+
D
wh
Ah
h∈A
h∈W

S (i,j) = P
(i,j)

∆A ∂(Zi − Zj )
,
2
∂Ah

∆W ∂(Zi − Zj )
(i,j)
Dw
,
=
h
2
∂wh


(i,j)
Y sinh T (i,j) DAh
(i,j)
P1
=
,
(i,j)
T (i,j) DAh
h∈A


(i,j)
sinh T (i,j) Dwh
Y
(i,j)
,
P2
=
(i,j)
T (i,j) Dwh
h∈W

and
T (i,j) =

S (i,j)
.
Zj − Zi

Proof. Again, we leave the technical details for the supplementary section. Here we also provide the main idea and
intuition.
As in Theorem 1, we shall focus on evaluating

Pr zi + qzi > zj + qzj = Pr qzi − qzj > zj − zi for
any pair of outputs zi and zj where zj > zi . The key
difference here is that we will use the Chernoff bound in
order to account for the complete quantization noise statistics. Indeed, letting v = zj − zi , we have:
h
i

Pr qzi − qzj > v ≤ e−tv E et(qzi −qzj )
for any t > 0. We show that:
i
h
Y sinh (tda,h ) Y sinh (tdw,h )
E et(qzi −qzj ) =
tda,h
tdw,h
h∈A


Pr qzi − qzj > v
Y sinh (tda,h ) Y sinh (tdw,h )
≤ e−tv
.
tda,h
tdw,h
h∈A

t= P

h∈A

h∈W

This

(14)

h∈W

(da,h

)2

3v
P
.
+ h∈W (dw,h )2

Substituting this value of t into (14) and using standard
probabilistic arguments, we obtain (13).
The first observation to be made is that Theorem 2 indicates
that, on average, pm is upper bounded by an exponentially
decaying function of the quantity S (i,Ŷf l ) for all i 6= Ŷf l up
(i,Ŷ )

where, for i 6= j,

DAh =

and dw,h =

(i,Ŷ )

to a correction factor P1 f l P2 f l . This correction factor is a product of terms typically centered around 1 (each
≈ 1 for small x). On the other
term is of the form sinh(x)
x
(i,Ŷf l )
hand, S
, by definition, is the ratio of the excess confidence the floating-point network has in the label Ŷf l over
the total quantization noise variance reflected at the output,
i.e., S (i,Ŷf l ) is the SQNR. Hence, Theorem 2 states that the
tolerance of a neural network to quantization is, on average, exponentially decaying with the SQNR at its output.
In terms of precision, Theorem 2 states that pm is bounded
by a double exponentially decaying function of precision
(that is an exponential function of an exponential function).
Note how this bound is tighter than that of Theorem 1.
This double exponential relationship between accuracy and
precision is not too surprising when one considers the problem of binary hypothesis testing under additive Gaussian
noise (Blahut, 2010) scenario. In this scenario, it is wellknown that the probability of error is an exponentially decaying function of the signal-to-noise ratio (SNR) in the
high-SNR regime. Theorem 2 points out a similar relationship between accuracy and precision but it does so using
rudimentary probability principles without relying on highSNR approximations.
While Theorem 2 is much tighter than Theorem 1 theoretically, it is not as convenient to use. In order to use Theorem
2, one has to perform a forward-backward pass and select
relevant quantities and apply (13) for each choice of BA
and BW . However, a lot of information, e.g. the derivatives, can be reused at each run, and so the runs may be
lumped into one forward-backward pass. In a sense, the
complexity of computing the bound in Theorem 2 lies between the evaluation of (11) and the complicated conventional trial-and-error based search.
We now illustrate the applications of these bounds.

Test error

Test error

Analytical Guarantees on Numerical Precision of Deep Neural Networks

C

A
B

(bits)

D

(a)

(bits)

(b)

Figure 1: Validity of bounds
q for MNIST when: (a) BW = BA and (b) BW = BA + 3 as dictated by (12) (EA = 41 and

EW = 3803 so that log2

EW
EA

≈ 3.2). FX Sim denotes fixed point simulations.

4. Simulation Results
We conduct numerical simulations to illustrate both the validity and usefulness of the analysis developed in the previous section. We show how it is possible to reduce precision
in an aggressive yet principled manner. We present results
on two popular datasets: MNIST and CIFAR-10. The metrics we address are threefold:
• Accuracy measured in terms of test error.
• Computational cost measured in #F As (see Section
2.3, (3) was used to compute #F As per MAC).
• Representational cost measured in bits (see Section
2.3, (4) was used).
We compare our results to similar works conducting similar experiments: 1) the work on fixed-point training with
stochastic quantization (SQ) (Gupta et al., 2015) and 2) BinaryNet (BN) (Hubara et al., 2016b).
4.1. DNN on MNIST
First, we conduct simulations on the MNIST dataset for
handwritten character recognition (LeCun et al., 1998).
The dataset consists of 60K training samples and 10K test
samples. Each sample consists of an image and a label. Images are of size 28 × 28 pixels representing a handwritten
digit between 0 and 9. Labels take the value of the corresponding digit.
In this first experiment, we chose an architecture of 784 −
512 − 512 − 512 − 10, i.e., 3 hidden layers, each of 512
units. We first trained the network in floating-point using
the back-propagation algorithm. We used a batch size of
200 and a learning rate of 0.1 with a decay rate of 0.978
per epoch. We restore the learning rate every 100 epochs,
the decay rate makes the learning rate vary between 0.1
and 0.01. We train the first 300 epochs using 15% dropout,
the second 300 epochs using 20% dropout, and the third

300 epochs using 25% dropout (900 epochs overall). It
appears from the original dropout work (Srivastava et al.,
2014) that the typical 50% dropout fraction works best for
very wide multi-layer perceptrons (MLPs) (4096 to 8912
hidden units). For this reason, we chose to experiment with
smaller dropout fractions.
The only pre-processing done is to scale the inputs between
−1 and 1. We used ReLU activations with the subtle addition of a right rectifier for values larger than 2 (as discussed in Section 2). The resulting activation is also called
a hard sigmoid. We also clipped the weights to lie in [−1, 1]
at each iteration. The resulting test error we obtained in
floating-point is 1.36%.
Figure 1 illustrates the validity of our analysis. Indeed,
both bounds (based on Theorems 1 & 2) successfully upper bound the test error obtained through fixed-point simulations. Figure 1 (b) demonstrates the utility of (12). Indeed, setting BW = BA allows us to reduce the precision
to about 6 or 7 bits before the accuracy start degrading. In
addition, under these conditions
q we found EA = 41 and
EW = 3803 so that log2 EEW
≈ 3.2. Thus, setting
A
BW = BA + 3 as dictated by (12) allows for more aggressive precision reduction. Activation precision BA can
now be reduced to about 3 or 4 bits before the accuracy degrades. To compute the bounds, we used an estimation set
of 1000 random samples from the dataset.
We compare our results with SQ which used a 784−1000−
1000−10 architecture on 16-bit fixed-point activations and
weights. A stochastic rounding scheme was used to compensate for quantization. We also compare our results with
BN with a 784 − 2048 − 2048 − 2048 − 10 architecture on
binary quantities. A stochastic rounding scheme was also
used during training.
Table 1 shows some comparisons with related works in
terms of accuracy, computational cost, and representational

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error (%)

Floating-point
(8, 8)
(6, 6)
(6, 9)
(4, 7)
SQ (16, 16) (Gupta et al., 2015)
BN (1, 1) (Hubara et al., 2016b)

1.36
1.41
1.54
1.35
1.43
1.4
1.4

Computational
Cost (106 F As)
N/A
82.9
53.1
72.7
44.7
533
117

Representational
Cost (106 bits)
N/A
7.5
5.63
8.43
6.54
28
10

Table 1: Results for MNIST: Comparison of accuracy, computational cost, and representational cost with state-of-the-art
related works. Chosen precision assignments are obtained from Figure 1.

A. Smallest (BA , BW ) such that BW = BA and
pm ≤ 1% as bounded by Theorem 1. In this case
(BA , BW ) = (8, 8).
B. Smallest (BA , BW ) such that BW = BA and
pm ≤ 1% as bounded by Theorem 2. In this case
(BA , BW ) = (6, 6).
C. Smallest (BA , BW ) such that BW = BA + 3 as dictated by (12) and pm ≤ 1% as bounded by Theorem
1. In this case (BA , BW ) = (6, 9).
D. Smallest (BA , BW ) such that BW = BA + 3 as dictated by (12) and pm ≤ 1% as bounded by Theorem
2. In this case (BA , BW ) = (4, 7).
As can be seen in Table 1, the accuracy is similar across
all design options including the results reported by SQ
and BN. Interestingly, for all four design options, our network has a smaller computational cost than BN. In addition, SQ’s computational cost is about 4.6× that of BN
(533M/117M). The greatest reduction in computational
cost is obtained for a precision assignment of (4, 7) corresponding to a 2.6× and 11.9× reduction compared to
BN (117M/44.7M) and SQ (533M/44.7M), respectively.
The corresponding test error rate is of 1.43%. Similar
trends are observed for representational costs. Again, our
four designs have smaller representational cost than even
BN. BN itself has 2.8× smaller representational cost than
SQ (28M/10M). Note that a precision assignment of (6, 6)
yields 1.8× and 5.0× smaller representational costs than
BN (10M/5.63M) and SQ (28M/5.63M), respectively. The
corresponding test error rate is 1.54%.
The fact that we are able to achieve lesser computational
and representational costs than BN while maintaining similar accuracy highlights two important points. First, the
width of a network severely impacts its complexity. We
made our network four times as narrow as BN’s and still
managed to use eight times as many bits per parameter
without exceeding BN’s complexity. Second, our results illustrate the strength of numbering systems, specifically, the

Test error

cost. For comparison, we selected four notable design options from Figures 1 (a,b):

A

B
(bits)

Figure 2: Validity of bounds for CIFAR when BW = BA
which is also dictated
q by (12) (EA = 21033 and EW =
31641 so that log2
point simulations.

EW
EA

≈ 0.29). FX Sim denotes fixed

strength of fixed-point representations. Our results indicate
that a correct and meaningful multi-bit representation of
parameters is better in both complexity and accuracy than
a 1-bit unstructured allocation.
4.2. CNN on CIFAR 10
We conduct a similar experiment on the CIFAR10 dataset
(Krizhevsky & Hinton, 2009). The dataset consists of
60K color images each representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and
trucks. 50K of these images constitute the training set, and
the 10K remaining are for testing. SQ’s architecture on
this dataset is a simple one: three convolutional layers, interleaved by max pooling layers. The output of the final
pooling layer is fed to a 10-way softmax output layer. The
reported accuracy using 16-bit fixed-point arithmetic is a
25.4% test error. BN’s architecture is a much wider and
deeper architecture based on VGG (Simonyan & Zisserman, 2014). The reported accuracy of the binary network
is an impressive 10.15% which is of benchmarking quality
even for full precision networks.
We adopt a similar architecture as SQ, but leverage re-

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Precision Assignment

Test error (%)

Floating-point
(12, 12)
(10, 10)
SQ (16, 16) (Gupta et al., 2015)
BN (1, 1) (Hubara et al., 2016b)

17.02
17.08
17.23
25.4
10.15

Computational
Cost (106 F As)
N/A
3626
2749
4203
3608

Representational
Cost (106 bits)
N/A
5.09
4.24
4.54
6.48

Table 2: Results for CIFAR10: Comparison of accuracy, computational cost, and representational cost with state-of-the-art
related works. Chosen precision assignments are obtained from Figure 2.
cent advances in convolutional neural networks (CNNs)
research. It has been shown that adding networks within
convolutional layers (in the ‘Network in Network’ sense)
as described in (Lin et al., 2013) significantly enhances
accuracy, while not incurring much complexity overhead.
Hence, we replace SQ’s architecture by a deep one which
we describe as 64C5 − 64C1 − 64C1 − M P 2 − 64C5 −
64C1−64C1−M P 2−64C5−64F C −64F C −64F C −
10, where C5 denotes 5 × 5 kernels, C1 denotes 1 × 1
kernels (they emulate the networks in networks), M P 2
denotes 2 × 2 max pooling, and F C denotes fully connected components. As is customary for this dataset, we
apply zero-phase component analysis (ZCA) whitening to
the data before training. Because this dataset is a challenging one, we first fine-tune the hyperparameters (learning
rate, weight decay rate, and momentum), then train for 300
epochs. The best accuracy we reach in floating point using
this 12-layer deep network is 17.02%.
Figure 2 shows the results of our fixed-point simulation and
analysis. Note that, while both bounds from Theorems 1
and 2 still successfully upper bound the test error, these are
not as tight as in our MNIST experiment. Furthermore, in
this case, (12) dictates keepingq
BW = BA as EA = 21033

≈ 0.29. The fact that
and EW = 31641 so that log2 EEW
A
EW ≥ EA is expected as there are typically more weights
than activations in a neural network. However, note that in
this case the contrast between EW and EA is not as sharp
as in our MNIST experiment. This is mainly due to the
higher weight to activation ratio in fully connected DNNs
than in CNNs.
We again select two design options:
A. Smallest (BA , BW ) such
pm ≤ 1% as bounded by
(BA , BW ) = (12, 12).
B. Smallest (BA , BW ) such
pm ≤ 1% as bounded by
(BA , BW ) = (10, 10).

that BW = BA and
Theorem 1. In this case
that BW = BA and
Theorem 2. In this case

Table 2 indicates that BN is the most accurate with 10.15%
test error. Interestingly, it has lesser computational cost but
more representational cost than SQ. This is due to the dependence of the computational cost on the product of BA

and BW . The least complex network is ours when setting
(BA , BW ) = (10, 10) and its test error is 17.23% which
is already a large improvement on SQ in spite of having
smaller computational and representational costs. This network is also less complex than that of BN.
The main take away here is that CNNs are quite different
from fully connected DNNs when it comes to precision requirements. Furthermore, from Table 2 we observe that
BN achieves the least test error. It seems that this better
accuracy is due to its greater representational power rather
than its computational power (BN’s representational cost
is much higher than the others as opposed to its computational cost).

5. Conclusion
In this paper we analyzed the quantization tolerance of neural networks. We used our analysis to efficiently reduce
weight and activation precisions while maintaining similar
fidelity as the floating-point initiation. Specifically, we obtained bounds on the mismatch probability between a fixedpoint network and its floating-point counterpart in terms
of precision. We showed that a neural network’s accuracy
degradation due to quantization decreases double exponentially as a function of precision. Our analysis provides a
straightforward method to obtain an upper bound on the
network’s error probability as a function of precision. We
used these results on real datasets to minimize the computational and representational costs of a fixed-point network
while maintaining accuracy.
Our work addresses the general problem of resource constrained machine learning. One take away is that it is imperative to understand the trade-offs between accuracy and
complexity. In our work, we used precision as a parameter to analytically characterize this trade-off. Nevertheless,
additional aspects of complexity in neural networks such as
their structure and their sparsity can also be accounted for.
In fact, more work can be done in that regard. Our work
may be viewed as a first step in developing a unified and
principled framework to understand complexity vs. accuracy trade-offs in deep neural networks and other machine
learning algorithms.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Acknowledgment
This work was supported in part by Systems on Nanoscale
Information fabriCs (SONIC), one of the six SRC STARnet
Centers, sponsored by MARCO and DARPA.

References
Baugh, Charles R and Wooley, Bruce A. A two’s complement parallel array multiplication algorithm. IEEE
Transactions on Computers, 100(12):1045–1047, 1973.
Blahut, Richard E. Fast algorithms for signal processing.
Cambridge University Press, 2010.
Caraiscos, Christos and Liu, Bede. A roundoff error analysis of the lms adaptive algorithm. IEEE Transactions on
Acoustics, Speech, and Signal Processing, 32(1):34–41,
1984.
Chen, Y. et al. Eyeriss: An energy-efficient reconfigurable
accelerator for deep convolutional neural networks. In
2016 IEEE International Solid-State Circuits Conference (ISSCC), pp. 262–263. IEEE, 2016.
Courbariaux, Matthieu et al. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in Neural Information Processing
Systems, pp. 3123–3131, 2015.
Goel, M. and Shanbhag, N. Finite-precision analysis of the
pipelined strength-reduced adaptive filter. Signal Processing, IEEE Transactions on, 46(6):1763–1769, 1998.
Goodfellow, Ian J et al. Maxout networks. ICML (3), 28:
1319–1327, 2013.

Hubara, Itay et al. Binarized neural networks. In Advances in Neural Information Processing Systems, pp.
4107–4115, 2016b.
Hwang, Kyuyeon and Sung, Wonyong. Fixed-point feedforward deep neural network design using weights+ 1, 0,
and- 1. In Signal Processing Systems (SiPS), 2014 IEEE
Workshop on, pp. 1–6. IEEE, 2014.
Kim, M. and Smaragdis, P. Bitwise Neural Networks.
arXiv preprint arXiv:1601.06071, 2016.
Knauer, Karl. Ripple-carry adder, June 13 1989. US Patent
4,839,849.
Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple
layers of features from tiny images. 2009.
Krizhevsky, Alex et al. Imagenet classification with deep
convolutional neural networks. In Advances in neural
information processing systems, pp. 1097–1105, 2012.
LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The MNIST database of handwritten digits,
1998.
Lin, Darryl et al. Fixed point quantization of deep convolutional networks. In Proceedings of The 33rd International Conference on Machine Learning, pp. 2849–2858,
2016a.
Lin, Min et al. Network in network.
arXiv:1312.4400, 2013.

arXiv preprint

Lin, Yingyan et al. Variation-tolerant architectures for convolutional neural networks in the near threshold voltage
regime. In Signal Processing Systems (SiPS), 2016 IEEE
International Workshop on, pp. 17–22. IEEE, 2016b.

Gupta, S. et al. Deep Learning with Limited Numerical
Precision. In Proceedings of The 32nd International
Conference on Machine Learning, pp. 1737–1746, 2015.

Rastegari, Mohammad et al. Xnor-net: Imagenet classification using binary convolutional neural networks. In
European Conference on Computer Vision, pp. 525–542.
Springer, 2016.

Han, Song et al. Learning both weights and connections
for efficient neural network. In Advances in Neural Information Processing Systems (NIPS), pp. 1135–1143,
2015.

Sakr, Charbel et al. Minimum precision requirements for
the SVM-SGD learning algorithm. In Acoustics, Speech
and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017.

He, Kaiming et al. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 770–778,
2016.

Shanbhag, Naresh R. Energy-efficient machine learning
in silicon: A communications-inspired approach. arXiv
preprint arXiv:1611.03109, 2016.

Hubara, Itay, Courbariaux, Matthieu, Soudry, Daniel, ElYaniv, Ran, and Bengio, Yoshua. Quantized neural networks: Training neural networks with low
precision weights and activations.
arXiv preprint
arXiv:1609.07061, 2016a.

Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
Srivastava, Nitish et al. Dropout: a simple way to prevent
neural networks from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Taigman, Yaniv et al. Deepface: Closing the gap to humanlevel performance in face verification. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1701–1708, 2014.
Zhang, Xiangyu et al. Accelerating very deep convolutional networks for classification and detection. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2015.

