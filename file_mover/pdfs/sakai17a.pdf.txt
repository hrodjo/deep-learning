Semi-Supervised Classification
Based on Classification from Positive and Unlabeled Data
Tomoya Sakai 1 2

Marthinus Christoffel du Plessis

Abstract
Most of the semi-supervised classification methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the cluster assumption. In contrast, recently developed methods of
classification from positive and unlabeled data
(PU classification) use unlabeled data for risk
evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we
extend PU classification to also incorporate negative data and propose a novel semi-supervised
classification approach. We establish generalization error bounds for our novel methods and
show that the bounds decrease with respect to
the number of unlabeled data without the distributional assumptions that are required in existing
semi-supervised classification methods. Through
experiments, we demonstrate the usefulness of
the proposed methods.

1. Introduction
Collecting a large amount of labeled data is a critical bottleneck in real-world machine learning applications due to the
laborious manual annotation. In contrast, unlabeled data
can often be collected automatically and abundantly, e.g.,
by a web crawler. This has led to the development of various semi-supervised classification algorithms over the past
decades.
To leverage unlabeled data in training, most of the existing semi-supervised classification methods rely on particular assumptions on the data distribution (Chapelle et al.,
2006). For example, the manifold assumption supposes that
samples are distributed on a low-dimensional manifold in
the data space (Belkin et al., 2006). In the existing framework, such a distributional assumption is encoded as a reg1

The University of Tokyo, Japan 2 RIKEN, Japan. Correspondence to: Tomoya Sakai <sakai@ms.k.u-tokyo.ac.jp>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Gang Niu 1

Masashi Sugiyama 2 1

ularizer for training a classifier and biases the classifier toward a better one under the assumption. However, if such a
distributional assumption contradicts the data distribution,
the bias behaves adversely, and the performance of the obtained classifier becomes worse than the one obtained with
supervised classification (Cozman et al., 2003; Sokolovska
et al., 2008; Li & Zhou, 2015; Krijthe & Loog, 2017).
Recently, classification from positive and unlabeled data
(PU classification) has been gathering growing attention
(Elkan & Noto, 2008; du Plessis et al., 2014; 2015; Jain
et al., 2016), which trains a classifier only from positive and
unlabeled data without negative data. In PU classification,
the unbiased risk estimators proposed in du Plessis et al.
(2014; 2015) utilize unlabeled data for risk evaluation, implying that label information is directly extracted from unlabeled data without restrictive distributional assumptions,
unlike existing semi-supervised classification methods that
utilize unlabeled data for regularization. Furthermore, theoretical analysis (Niu et al., 2016) showed that PU classification (or its counterpart, NU classification, classification
from negative and unlabeled data) is likely to outperform
classification from positive and negative data (PN classification, i.e., ordinary supervised classification) depending
on the number of positive, negative, and unlabeled samples. It is thus naturally expected that combining PN, PU,
and NU classification can be a promising approach to semisupervised classification without restrictive distributional
assumptions.
In this paper, we propose a novel semi-supervised classification approach by considering convex combinations of the
risk functions of PN, PU, and NU classification. Without
any distributional assumption, we theoretically show that
the confidence term of the generalization error bounds decreases at the optimal parametric rate with respect to the
number of positive, negative, and unlabeled samples, and
the variance of the proposed risk estimator is almost always
smaller than the plain PN risk function given an infinite
number of unlabeled samples. Through experiments, we
analyze the behavior of the proposed approach and demonstrate the usefulness of the proposed semi-supervised classification methods.

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

2. Background

2.3. PU Classification

In this section, we first introduce the notation commonly
used in this paper and review the formulations of PN, PU,
and NU classification.

In PU classification, we do not have labeled data for the
negative class, but we can use unlabeled data drawn from
marginal density p(x). The goal of PU classification is
to train a classifier using only positive and unlabeled data.
The basic approach to PU classification is to discriminate P
and U data (Elkan & Noto, 2008). However, naively classifying P and U data causes a bias.

2.1. Notation
Let random variables x ∈ Rd and y ∈ {+1, −1} be
equipped with probability density p(x, y), where d is a positive integer. Let us consider a binary classification problem
from x to y, given three sets of samples called the positive (P), negative (N), and unlabeled (U) data:
i.i.d.

nP
XP := {xP
i }i=1 ∼ pP (x) := p(x | y = +1),
i.i.d.

nN
XN := {xN
i }i=1 ∼ pN (x) := p(x | y = −1),

To address this problem, du Plessis et al. (2014; 2015) proposed a risk equivalent to the PN risk but where pN (x) is
not included. The key idea is to utilize unlabeled data to
evaluate the risk for negative samples in the PN risk. Replacing the second term in Eq. (1) with1
θN EN [ℓ(−g(x))] = EU [ℓ(−g(x))] − θP EP [ℓ(−g(x))],

i.i.d.

nU
XU := {xU
i }i=1 ∼ p(x) := θP pP (x) + θN pN (x),

we obtain the risk in PU classification (the PU risk) as

where
θP := p(y = +1),

e
RPU (g) := θP EP [ℓ(g(x))]
+ EU [ℓ(−g(x))]
C
= θP RP
(g) + RU,N (g),

θN := p(y = −1)

are the class-prior probabilities for the positive and negative
classes such that θP + θN = 1.
Let g : Rd → R be an arbitrary real-valued decision
function for binary classification, and classification is performed based on its sign. Let ℓ : R → R be a loss function such that ℓ(m) generally takes a small value for large
margin m = yg(x). Let RP (g), RN (g), RU,P (g), and
RU,N (g) be the risks of classifier g under loss ℓ:
RP (g) := EP [ℓ(g(x))],
RU,P (g) := EU [ℓ(g(x))],

RN (g) := EN [ℓ(−g(x))],
RU,N (g) := EU [ℓ(−g(x))],

where EP , EN , and EU denote the expectations over
pP (x), pN (x), and p(x), respectively. Since we do not
have any samples from p(x, y), the true risk R(g) =
Ep(x,y) [ℓ(yg(x))], which we want to minimize, should be
recovered without using p(x, y) as shown below.

C
e
e
(g) := EP [ℓ(g(x))]
and ℓ(m)
= ℓ(m) − ℓ(−m)
where RP
is a composite loss function.

Non-Convex Approach:

RPN (g) := θP EP [ℓ(g(x))] + θN EN [ℓ(−g(x))]
= θP RP (g) + θN RN (g),

(1)

which is equal to R(g), but p(x, y) is not included. If we
use the hinge loss function ℓH (m) := max(0, 1 − m), the
PN risk coincides with the risk of the support vector machine (Vapnik, 1995).

(3)

e
the composite loss function becomes ℓ(m)
= 2ℓ(m) − 1.
We thus obtain the non-convex PU risk as
RN-PU (g) := 2θP RP (g) + RU,N (g) − θP .

(4)

This formulation can be seen as cost-sensitive classification
of P and U data with weight 2θP (du Plessis et al., 2014).
The ramp loss used in the robust support vector machine
(Collobert et al., 2006),

2.2. PN Classification

The risk in PN classification (the PN risk) is defined as

If the loss function satisfies

ℓ(m) + ℓ(−m) = 1,

ℓR (m) :=

In standard supervised classification (PN classification), we
have both positive and negative data, i.e., fully labeled data.
The goal of PN classification is to train a classifier using
labeled data.

(2)

1
max(0, min(2, 1 − m)),
2

(5)

satisfies the condition (3). However, the use of the
ramp loss (and any other losses that satisfy the condition (3)) yields a non-convex optimization problem, which
may be solved locally by the concave-convex procedure
(CCCP) (Yuille & Rangarajan, 2002; Collobert et al., 2006;
du Plessis et al., 2014).
Convex Approach:
satisfies

If a convex surrogate loss function

ℓ(m) − ℓ(−m) = −m,
1

(6)

The equation comes from the definition of the marginal density p(x) = θP pP (x) + θN pN (x).

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

the composite loss function becomes a linear function
e
ℓ(m)
= −m (see Table 1 in du Plessis et al., 2015). We
thus obtain the convex PU risk as
L
RC-PU (g) := θP RP
(g) + RU,N (g),

L
where RP
(g) := EP [−g(x)] is the risk with the linear loss
ℓLin (m) := −m. This formulation yields the convex optimization problem that can be solved efficiently.

2.4. NU Classification
As a mirror of PU classification, we can consider NU classification. The risk in NU classification (the NU risk) is
given by
e
RNU (g) := θN EN [ℓ(−g(x))]
+ EU [ℓ(g(x))]
C
e
where RN
(g) := EN [ℓ(−g(x))]
is the risk function with
the composite loss. Similarly to PU classification, the nonconvex and convex NU risks are expressed as

L
RC-NU (g) := θN RN
(g) + RU,P (g),

γ
L
L
RC-PUNU
(g) = (1 − γ)θP RP
(g) + γθN RN
(g)
+ EU [(1 − γ)ℓ(g(x)) + γℓ(−g(x))].

Here, (1 − γ)ℓ(g(x)) + γℓ(−g(x)) can be regarded as a
loss function for unlabeled samples with weight γ.
When γ = 1/2, unlabeled samples incur the same loss for
the positive and negative classes. On the other hand, when
0 < γ < 1/2, a smaller loss is incurred for the negative
class than the positive class. Thus, unlabeled samples tend
to be classified into the negative class. The opposite is true
when 1/2 < γ < 1.
3.2. PNU Classification

C
= θN RN
(g) + RU,P (g),

RN-NU (g) := 2θN RN (g) + RU,P (g) − θN ,

On the other hand, γ = 1/2 is still effective when the conγ
dition (6) is satisfied. Its risk RC-PUNU
(g) can be expressed
as

(7)
(8)

L
(g) := EN [g(x)] is the risk with the linear loss.
where RN

3. Semi-Supervised Classification Based on
PN, PU, and NU Classification
In this section, we propose semi-supervised classification
methods based on PN, PU, and NU classification.
3.1. PUNU Classification
A naive idea to build a semi-supervised classifier is to combine the PU and NU risks. For γ ∈ [0, 1], let us consider a
linear combination of the PU and NU risks:
γ
RPUNU
(g) := (1 − γ)RPU (g) + γRNU (g).

Another possibility of using PU and NU classification in
semi-supervised classification is to combine the PN and
PU/NU risks. For γ ∈ [0, 1], let us consider linear combinations of the PN and PU/NU risks:
γ
RPNPU
(g) := (1 − γ)RPN (g) + γRPU (g),
γ
RPNNU
(g) := (1 − γ)RPN (g) + γRNU (g).

In practice, we combine PNPU and PNNU classification
and adaptively choose one of them with a new trade-off
parameter η ∈ [−1, 1] as
(
η
RPNPU
(g) (η ≥ 0),
η
RPNU (g) :=
−η
RPNNU (g) (η < 0).
We refer to the combined method as PNU classification.
Clearly, PNU classification with η = −1, 0, +1 corresponds to NU, PN, and PU classification. As η gets
large/small, the effect of the positive/negative classes is
more emphasized.
In the theoretical analyses in Section 4, we denote the
combinations of the PN risk with the non-convex PU/NU
γ
γ
risks by RN-PNPU
and RN-PNNU
, and that with the convex
γ
γ
PU/NU risks by RC-PNPU and RC-PNNU
.

We refer to this combined method as PUNU classification.
If we use a loss function satisfying the condition (3), the
γ
non-convex PUNU risk RN-PUNU
(g) can be expressed as
γ
RN-PUNU
(g) = 2(1 − γ)θP RP (g) + 2γθN RN (g)

+ EU [(1 − γ)ℓ(−g(x)) + γℓ(g(x))]
− (1 − γ)θP − γθN .

1/2
RN-PUNU (g)

Here,
agrees with RPN (g) due to the condition (3). Thus, when γ = 1/2, PUNU classification is reduced to ordinary PN classification.

3.3. Practical Implementation
We have so far only considered the true risks R (with
respect to the expectations over true data distributions).
When a classifier is trained from samples in practice, we
b where the expectations are reuse the empirical risks R
placed with corresponding sample averages.
More specifically, in the theoretical analysis in Section 4
and experiments in Section 5, we use a linear-in-parameter
Pb
⊤
model given by g(x) =
j=1 wj φj (x) = w φ(x),
where ⊤ denotes the transpose, b is the number of basis

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

functions, w = (w1 , . . . , wb )⊤ is a parameter vector, and
φ(x) = (φ1 (x), . . . , φb (x))⊤ is a basis function vector.
The parameter vector w is learned in order to minimize the
ℓ2 -regularized empirical risk:
b
min R(g)
+ λw⊤ w,
w

where λ ≥ 0 is the regularization parameter.

4. Theoretical Analyses
In this section, we theoretically analyze the behavior of
the empirical versions of the proposed semi-supervised
classification methods. We first derive generalization error bounds and then discuss variance reduction. Finally,
we discuss whether PUNU or PNU classification is more
promising. All proofs can be found in Appendix A.
4.1. Generalization Error Bounds
Let G be a function class of bounded hyperplanes:
G = {g(x) = hw, φ(x)i | kwk ≤ Cw , kφ(x)k ≤ Cφ },

Theorem 1 guarantees that when ℓR (m) is used, I(g) can
be bounded from above by two times the empirical risks,
bγ
bγ
bγ
i.e., 2R
N-PUNU (g), 2RN-PNPU (g), and 2RN-PNNU (g), plus
the corresponding confidence terms of order
√
√
√
Op (1/ nP + 1/ nN + 1/ nU ).

Since nP , nN , and nU can increase independently, this is already the optimal convergence rate without any additional
assumption (Vapnik, 1998; Mendelson, 2008).
Convex Methods: Analogously, we have ℓ0-1 (m) ≤
4ℓS (m) for the squared loss. However, it is too loose when
|m| ≫ 0. Fortunately, we do not have to use ℓS (m) if we
work on the generalization error rather than the estimation
error. To this end, we define the truncated (scaled) squared
loss ℓTS (m) as
(
ℓS (m)
ℓTS (m) =
ℓ0-1 (m)/4

0 < m ≤ 1,
otherwise,

where Cw and Cφ are certain positive constants. Since ℓ2 regularization is always included, we can naturally assume
that the empirical risk minimizer g belongs to a certain G.
Denote by ℓ0-1 (m) = (1 − sign(m))/2 the zero-one loss
and I(g) = Ep(x,y) [ℓ0-1 (yg(x))] the risk of g for binary
classification, i.e., the generalization error of g. In the following, we study upper bounds of I(g) holding uniformly
for all g ∈ G. We respectively focus on the (scaled) ramp
and squared losses for the non-convex and convex methods
due to limited space. Similar results can be obtained with a
little more effort if other eligible losses are used. For convenience, we define a function as
√
√
√
χ(cP , cN , cU ) = cP θP / nP + cN θN / nN + cU / nU .

so that ℓ0-1 (m) ≤ 4ℓTS (m) is much tighter. For ℓTS (m),
RC-PU (g) and RC-NU (g) need to be redefined as follows
(see du Plessis et al., 2015):

Non-Convex Methods: A key observation is that
ℓ0-1 (m) ≤ 2ℓR (m), and consequently I(g) ≤ 2R(g). Note
that by definition we have

Theorem 2 Let ℓTS (m) be the loss for defining the empirical risks (where RC-PU (g) and RC-NU (g) are redefined).
For any δ > 0, the following inequalities hold separately
with probability at least 1 − δ for all g ∈ G:

γ
γ
γ
RN-PUNU
(g) = RN-PNPU
(g) = RN-PNNU
(g) = R(g).

The theorem below can be proven using the Rademacher
analysis (see, for example, Mohri et al., 2012; Ledoux &
Talagrand, 1991).
Theorem 1 Let ℓR (m) be the loss for defining the empirical risks. For any δ > 0, the following inequalities hold
separately with probability at least 1 − δ for all g ∈ G:
bγ
I(g) ≤ 2R
N-PUNU (g) + Cw,φ,δ · χ(2 − 2γ, 2γ, |2γ − 1|),
bγ
I(g) ≤ 2R
(g) + Cw,φ,δ · χ(1 + γ, 1 − γ, γ),
N-PNPU

bγ
I(g) ≤ 2R
N-PNNU (g) + Cw,φ,δ · χ(1 − γ, 1 + γ, γ),
p
where Cw,φ,δ = 2Cw Cφ + 2 ln(3/δ).

′
RC-PU (g) := θP RP
(g) + RU,N (g),
′
RC-NU (g) := θN RN (g) + RU,P (g),
′
′
where RP
(g) and RN
(g) are simply RP (g) and RN (g)
w.r.t. the composite loss ℓeTS (m) = ℓTS (m) − ℓTS (−m).
The condition ℓeTS (m) 6= −m means the loss of convexity,
but the equivalence is not lost; indeed, we still have
γ
γ
γ
RC-PUNU
(g) = RC-PNPU
(g) = RC-PNNU
(g) = R(g).

′
bγ
I(g) ≤ 4R
C-PUNU (g) + Cw,φ,δ · χ(1 − γ, γ, 1),

′
bγ
I(g) ≤ 4R
C-PNPU (g) + Cw,φ,δ · χ(1, 1 − γ, γ),

′
bγ
I(g) ≤ 4R
C-PNNU (g) + Cw,φ,δ · χ(1 − γ, 1, γ),

′
where Cw,φ,δ
= 4Cw Cφ +

p
2 ln(4/δ).

Theorem 2 ensures that when ℓTS (m) is used (for evaluating the empirical risks rather than learning the empirical
risk minimizers), I(g) can be bounded from above by four
times the empirical risks plus confidence terms in the optimal parametric rate. As ℓTS (m) ≤ ℓS (m), Theorem 2 is
valid (but weaker) if all empirical risks are w.r.t. ℓS (m).

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

4.2. Variance Reduction
Our empirical risk estimators proposed in Section 3 are all
unbiased. The next question is whether their variance can
bPN (g), i.e., whether XU can help
be smaller than that of R
reduce the variance in estimating R(g). To answer this
question, pick any g of interest. For simplicity, we assume
that nU → ∞, to illustrate the maximum variance reduction that could be achieved. Due to limited space, we only
focus on the non-convex methods.
2
2
Similarly to RP (g) and RN (g), let σP
(g) and σN
(g) be the
corresponding variance:
2
σP
(g) := VarP [ℓ(g(x))],

2
σN
(g) := VarN [ℓ(−g(x))],

where VarP and VarN denote the variance over pP (x)
2 2
and pN (x). Moreover, denote by ψP = θP
σP (g)/nP
2 2
and ψN = θN σN (g)/nN for short, and let Var be the
P
N
N
variance over pP (xP
1 ) · · · pP (xnP ) · pN (x1 ) · · · pN (xnN ) ·
U
U
p(x1 ) · · · p(xnU ).
Theorem 3 Assume nU → ∞. For any fixed g, let
bγ
γN-PUNU = argmin Var[R
N-PUNU (g)] =
γ

Then, we have γN-PUNU
bγ
ther, Var[R
<
N-PUNU (g)]
γ ∈ (2γN-PUNU − 1/2, 1/2) if
γ ∈ (1/2, 2γN-PUNU − 1/2) if ψP

ψP
. (9)
ψP + ψN

∈
[0, 1].
FurbPN (g)] for all
Var[R
ψP < ψN , or for all
> ψ N .2

Theorem 3 guarantees that the variance is always reduced
bγ
by R
N-PUNU (g) if γ is close to γN-PUNU , which is optimal
for variance reduction. The interval of such good γ values has the length min{|ψP − ψN |/(ψP + ψN ), 1/2}. In
particular, if 3ψP ≤ ψN or ψP ≥ 3ψN , the length is 1/2.
Theorem 4 Assume nU → ∞. For any fixed g, let
ψN − ψP
, (10)
ψP + ψN
γ
ψP − ψN
bγ
γN-PNNU= argmin Var[R
. (11)
N-PNNU (g)] =
ψP + ψN
γ
bγ
γN-PNPU= argmin Var[R
N-PNPU (g)] =

Then, we have γN-PNPU ∈ [0, 1] if ψP ≤ ψN or γN-PNNU ∈
bγ
[0, 1] if ψP ≥ ψN . Additionally, Var[R
N-PNPU (g)] <
b
Var[RPN (g)] for all γ ∈ (0, 2γN-PNPU ) if ψP <
bγ
b
ψN , or Var[R
N-PNNU (g)] < Var[RPN (g)] for all γ ∈
(0, 2γN-PNNU ) if ψP > ψN .

bPN (g) is reTheorem 4 implies that the variance of R
γ
b
bγ
duced by either RN-PNPU (g) if ψP ≤ ψN or R
N-PNNU (g)
2

Being fixed means g is determined before seeing the data for
evaluating the empirical risk. For example, if g is trained by some
learning method, and the empirical risk is subsequently evaluated
on the validation/test data, g is regarded as fixed in the evaluation.

if ψP ≥ ψN , where γ should be close to γN-PNPU or
γN-PNNU . The range of such good γ values is of length
min{2|ψP − ψN |/(ψP + ψN ), 1}. In particular, if 3ψP ≤
bγ
ψN , R
N-PNPU (g) given any γ ∈ (0, 1) can reduce the varibγ
ance, and if ψP ≥ 3ψN , R
N-PNNU (g) given any γ ∈ (0, 1)
can reduce the variance.

As a corollary of Theorems 3 and 4, the minimum
bγ
bγ
variance achievable by R
N-PUNU (g), RN-PNPU (g), and
γ
b
R
N-PNNU (g) at their optimal γN-PUNU , γN-PNPU , and
γN-PNNU is exactly the same, namely, 4ψP ψN /(ψP + ψN ).
bγ
bγ
Nevertheless, R
N-PNPU (g) and RN-PNNU (g) have a much
bγ
wider range of nice γ values than R
N-PUNU (g).

If we further assume that σP (g) = σN (g), the condition in
Theorems 3 and 4 as to whether ψP ≤ ψN or ψP ≥ ψN will
be independent of g. Also, it will coincide with the condition in Theorem 7 in Niu et al. (2016) where the minimizers
bPN (g), R
bPU (g) and R
bNU (g) are compared.
of R
A final remark is that learning is uninvolved in Theorems 3
and 4, such that ℓ(m) can be any loss that satisfies ℓ(m) +
ℓ(−m) = 1, and g can be any fixed decision function. For
instance, we may adopt ℓ0-1 (m) and pick some g resulted
from some other learning methods. As a consequence, the
variance of IbPN (g) over the validation data can be reduced,
and then the cross-validation should be more stable, given
that nU is sufficiently large. Therefore, even without being
minimized, our proposed risk estimators are themselves of
practical importance.
4.3. PUNU vs. PNU Classification
We discuss here which approach, PUNU or PNU classification, is more promising according to state-of-the-art theoretical comparisons (Niu et al., 2016), which are based on
estimation error bounds.

bPN (g),
Let gbPN , gbPU , and gbNU be the minimizers of R
b
b
RPU (g), and RNU (g), respectively. Let αPU,PN :=
√
√
√
(θP / nP + 1/ nU )/(θN / nN ) and αNU,PN :=
√
√
√
(θN / nN + 1/ nU )/(θP / nP ). The finite-sample comparisons state that if αPU,PN > 1 (αNU,PN > 1), PN classification is more promising than PU (NU) classification,
i.e., R(b
gPN ) < R(b
gPU ) (R(b
gPN ) < R(b
gNU )); otherwise
PU (NU) classification is more promising than PN classification (cf. Section 3.2 in Niu et al., 2016).
Suppose that nU is not sufficiently large against nP and nN .
According to the finite-sample comparisons, PN classification is most promising, and either PU or NU classification
is the second best, i.e., R(b
gPN ) < R(b
gPU ) < R(b
gNU )
or R(b
gPN ) < R(b
gNU ) < R(b
gPU ). On the other hand,
if nU is sufficiently large (nU → ∞, which is faster
than nP , nN → ∞), we have the asymptotic compar∗
∗
isons: αPU,PN
= limnP ,nN ,nU →∞ αPU,PN , αNU,PN
=

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

In real-world applications, since we do not know whether
the number of unlabeled samples is sufficiently large or not,
a practical approach is to combine the best methods in both
the finite-sample and asymptotic cases. PNU classification
is the combination of the best methods in both cases, but
PUNU classification is not. In addition, PUNU classification includes the worst one in its combination in both
cases. From this viewpoint, PNU classification would be
more promising than PUNU classification, as demonstrated
in the experiments shown in the next section.

5. Experiments
In this section, we first numerically analyze the proposed
approach and then compare the proposed semi-supervised
classification methods against existing methods. All experiments were carried out using a PC equipped with two
2.60GHz Intel® Xeon® E5-2640 v3 CPUs.
5.1. Experimental Analyses
Here, we numerically analyze the behavior of our proposed
approach. Due to limited space, we show results on two out
of six data sets and move the rest to Appendix C.
Common Setup: AsP
a classifier, we use the Gaussian
n
kernel model: g(x) = i=1 wi exp(−kx − xi k2 /(2σ 2 )),
where n = nP + nN , {wi }ni=1 are the parameters,
{xi }ni=1 = XP ∪XN , and σ > 0 is the Gaussian bandwidth.
The bandwidth candidates are {1/8, 1/4, 1/2, 1, 3/2, 2} ×
median(kxi − xj kni,j=1 ). The classifier trained by minimizing the empirical PN risk is denoted by gbPN . The number of labeled samples for training is 20, where the classprior was 0.5. In all experiments, we used the squared loss
for training. We note that the class-prior of test data was
the same as that of unlabeled data.
Variance Reduction in Practice: Here, we numerically
investigate how many unlabeled samples are sufficient in
practice such that the variance of the empirical PNU risk
bη (g)] <
is smaller than that of the PN risk: Var[R
PNU
bPN (g)] given a fixed classifier g.
Var[R

As the fixed classifier, we used the classifier gbPN , where
the hyperparameters were determined by five-fold crossvalidation. To compute the variance of the empirical PN
bPN (b
bη (b
and PNU risks, Var[R
gPN )] and Var[R
PNU gPN )], we
V
repeatedly drew additional nV
P = 10 positive, nN = 10

1.4

1.4
θP = 0.3
θP = 0.5
θP = 0.7

1.2
1
0.8
0.6

Ratio of Variance

Ratio of Variance

∗
∗
limnP ,nN ,nU →∞ αNU,PN , and αPU,PN
·αNU,PN
= 1. From
∗
∗
the last equation, if αPU,PN < 1, then αNU,PN > 1, implying that PU (PN) classification is more promising than PN
(NU) classification, i.e., R(b
gPU ) < R(b
gPN ) < R(b
gNU ).
∗
∗
Similarly, when αPU,PN
> 1 and αNU,PN
< 1, R(b
gNU ) <
R(b
gPN ) < R(b
gPU ) (cf. Section 3.3 in Niu et al., 2016).

1.2
1
0.8
0.6
0.4

0

50

100

150

200

250

nvU

(a) Phoneme (d = 5)

300

0

50

100

150

200

250

300

nvU

(b) Magic (d = 10)

Figure 1. Average and standard error of the ratio between
the variance of empirical PNU risk and that of PN risk,
b
bη (b
gPN )], as a function of the number
Var[R
PNU gPN )]/ Var[RPN (b
of unlabeled samples over 100 trials. Although the variance reduction is proved for an infinite number of samples, it can be observed with a finite number of samples.

negative, and nV
U unlabeled samples from the rest of the
data set. The additional samples were also used for approximating σ
bP (b
gPN ) and σ
bN (b
gPN ) to compute η, i.e., γ in
Eqs.(10) and (11).

Figure 1 shows the ratio between the variance of
the empirical PNU risk and that of the PN risk,
bη (b
b
gPN )]. The number of unlaVar[R
PNU gPN )]/ Var[RPN (b
beled samples for validation nV
U increases from 10 to 300.
We see that with a rather small number of unlabeled samples, the ratio becomes less than 1. That is, the variance
of the empirical PNU risk becomes smaller than that of the
PN risk. This implies that although the variance reduction
is proved for an infinite number of unlabeled samples, it can
be observed under a finite number of samples in practice.
Compared to when θP = 0.3 and 0.7, the effect of variance
reduction is small when θP = 0.5. This is because if we
assume σP (g) ≈ σN (g), when nP ≈ nN and θP = 0.5,
we have γN-PNPU ≈ γN-PNNU ≈ 0 (because ψP ≈ ψN .
See Theorem 4). That is, the PNU risk is dominated by
bη (g)] ≈ Var[R
bPN (g)].
the PN risk, implying that Var[R
PNU
Note that the class-prior is not the only factor for variance reduction; for example, if θP = 0.5, nP ≫ nN , and
σP (g) ≈ σN (g), then γN-PNPU 6≈ 0 (because ψP ≪ ψN )
and the variance reduction will be large.
PNU Risk in Validation: As discussed in Section 4, the
empirical PNU risk will be a reliable validation score due
to its having smaller variance than the empirical PN risk.
We show here that the empirical PNU risk is a promising
alternative to a validation score.
To focus on the effect of validation scores only, we trained
two classifiers by using the same risk, e.g, the empirical
PN risk. We then tune the classifiers with the empirical
PN
PNU
PN and PNU risks denoted by gbPN
and gbPN
, respectively.
The number of validation samples was the same as in the
previous experiment.

1.04

θP = 0.3
θP = 0.5
θP = 0.7

1.02
1
0.98
0.96
0.94
0

50

100 150 200 250 300

Ratio of Misclassification Rate

Ratio of Misclassification Rate

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

Table 1. Average and standard error of the misclassification rates
of each method over 50 trials for benchmark data sets. Boldface
numbers denote the best and comparable methods in terms of average misclassifications rate according to a t-test at a significance
level of 5%. The bottom row gives the number of best/comparable
cases of each method.

1.04
1.02
1
0.98
0.96
0

50

100 150 200 250 300

nvU

nvU

(a) Phoneme (d = 5)

(b) Magic (d = 10)

Figure 2. Average and standard error of the ratio between the misPNU
PN
classification rates of gbPN
and gbPN
as a function of unlabeled
samples over 1000 trials. In many cases, the ratio becomes less
than 1, implying that the PNU risk is a promising alternative to
the standard PN risk in validation if unlabeled data are available.
PUNU

ER

LapSVM

SMIR

WellSVM

S4VM

102
101
100

a
w8

a

il2

a9

Co

na
Ba

on

na
em
e
Ma
gic
Im
ag
e
Su
sy
Ge
rm
an
Wa
vef
orm
ijc
nn
1
g5
0c
cov
typ
e
Sp
am
ba
se
Sp
lic
e
ph
i sh
i ng

10−1

Ph

Computation Time [sec.]

PNU

103

Figure 3. Average computation time over 50 trials for benchmark
data sets when nL = 50.

Figure 2 shows the ratio between the misclassification rate
PNU
PN
of gbPN
and that of gbPN
. The number of unlabeled samples for validation increases from 10 to 300. With a rather
small number of unlabeled samples, the ratio becomes less
PNU
PN
than 1, i.e., gbPN
achieves better performance than gbPN
.
PNU
In particular, when θP = 0.3 and 0.7, gbPN improved substantially; the large improvement tends to give the large
variance reduction (cf. Figure 1). This result shows that
the use of the empirical PNU risk for validation improved
the classification performance given a relatively large size
of unlabeled data.
5.2. Comparison with Existing Methods
Next, we numerically compare the proposed methods
against existing semi-supervised classification methods.
Common Setup: We compare our methods against five
conventional semi-supervised classification methods: entropy regularization (ER) (Grandvalet & Bengio, 2004),
the Laplacian support vector machine (LapSVM) (Belkin
et al., 2006; Melacci & Belkin, 2011), squared-loss mutual information regularization (SMIR) (Niu et al., 2013),
the weakly labeled support vector machine (WellSVM) (Li
et al., 2013), and the safe semi-supervised support vector
machine (S4VM) (Li & Zhou, 2015).
Among the proposed methods, PNU classification and

Data set

nL

Banana
d=2

10 30.1 (1.0) 32.1 (1.1) 35.8 (1.0) 36.9 (1.0) 37.7 (1.1) 41.8 (0.6) 45.3 (1.0)
50 19.0 (0.6) 26.4 (1.2) 20.6 (0.7) 21.3 (0.7) 21.1 (1.0) 42.6 (0.5) 38.7 (0.9)

Phoneme
d=5

10 32.5 (0.8) 33.5 (1.0) 33.4 (1.2) 36.5 (1.5) 36.4 (1.2) 28.4 (0.6) 33.7 (1.4)
50 28.1 (0.5) 32.8 (0.9) 27.8 (0.6) 27.0 (0.8) 28.6 (1.0) 26.8 (0.4) 25.1 (0.2)

Magic
d = 10

10 31.7 (0.8) 34.1 (0.9) 34.2 (1.1) 37.9 (1.3) 36.0 (1.2) 30.1 (0.8) 33.3 (0.9)
50 29.9 (0.8) 33.4 (0.9) 30.9 (0.5) 31.0 (0.9) 30.8 (0.9) 28.8 (0.8) 29.2 (0.4)

Image
d = 18

10 29.8 (0.9) 31.7 (0.8) 33.7 (1.1) 36.6 (1.2) 36.7 (1.2) 34.7 (1.1) 35.9 (1.0)
50 20.7 (0.8) 26.6 (1.1) 20.8 (0.8) 20.3 (1.0) 20.9 (0.9) 27.2 (1.0) 23.2 (0.7)

Susy
d = 18

10 44.6 (0.6) 45.0 (0.6) 47.7 (0.4) 48.2 (0.4) 45.1 (0.7) 48.0 (0.3) 46.8 (0.3)
50 38.9 (0.6) 41.5 (0.6) 37.9 (0.7) 43.1 (0.6) 43.9 (0.8) 43.8 (0.7) 42.1 (0.4)

German
d = 20

10 40.8 (0.9) 42.4 (0.7) 43.6 (0.9) 45.9 (0.7) 46.2 (0.8) 42.4 (0.8) 42.0 (0.7)
50 36.2 (0.8) 39.0 (0.8) 38.9 (0.6) 40.6 (0.6) 38.4 (1.1) 38.5 (1.0) 34.9 (0.5)

PNU

PUNU

ER

LapSVM

SMIR

WellSVM

S4VM

Waveform 10 17.4 (0.6) 18.0 (0.9) 18.5 (0.6) 24.9 (1.4) 18.0 (1.0) 16.7 (0.6) 20.8 (0.8)
d = 21 50 16.3 (0.6) 23.7 (1.2) 14.2 (0.4) 18.1 (0.8) 15.4 (0.6) 15.5 (0.5) 15.3 (0.3)
ijcnn1
d = 22

10 43.6 (0.6) 40.3 (1.0) 49.7 (0.1) 49.2 (0.3) 44.0 (1.0) 45.9 (0.7) 49.3 (0.8)
50 34.5 (0.8) 37.1 (0.9) 35.5 (0.8) 33.4 (1.1) 49.4 (0.3) 46.2 (0.8) 48.6 (0.4)

g50c
d = 50

10 11.4 (0.6) 12.5 (0.6) 23.3 (2.3) 39.8 (1.6) 21.9 (1.3) 6.6 (0.4) 27.0 (1.4)
50 12.5 (1.1) 10.1 (0.6) 8.7 (0.4) 22.5 (1.5) 10.6 (0.6) 7.4 (0.4) 12.1 (0.5)

covtype
d = 54

10 46.2 (0.4) 46.0 (0.4) 46.0 (0.5) 47.1 (0.5) 47.9 (0.5) 46.9 (0.6) 46.4 (0.4)
50 41.3 (0.5) 42.3 (0.5) 41.0 (0.4) 41.5 (0.5) 46.2 (0.8) 43.6 (0.6) 40.8 (0.4)

Spambase 10 27.2 (0.9) 28.1 (1.1) 31.8 (1.4) 39.7 (1.4) 30.9 (1.3) 23.8 (0.8) 36.1 (1.5)
d = 57 50 23.4 (1.0) 26.6 (1.0) 22.1 (0.7) 28.5 (1.3) 20.9 (0.5) 19.1 (0.4) 24.5 (0.9)
Splice
d = 60

10 38.3 (0.8) 39.3 (0.8) 43.9 (0.8) 47.9 (0.5) 41.6 (0.7) 42.0 (1.0) 42.4 (0.6)
50 30.6 (0.8) 34.7 (0.9) 30.9 (0.8) 38.8 (1.0) 30.6 (0.9) 40.9 (0.8) 35.9 (0.7)

phishing
d = 68

10 24.2 (1.2) 25.8 (1.0) 27.3 (1.6) 37.2 (1.6) 27.6 (1.6) 27.5 (1.4) 31.7 (1.3)
50 15.8 (0.6) 18.3 (0.8) 15.4 (0.5) 21.1 (1.3) 14.7 (0.8) 17.2 (0.7) 16.7 (0.8)

a9a
d = 83

10 31.4 (0.9) 31.3 (1.0) 34.3 (1.2) 41.0 (1.1) 37.3 (1.3) 33.1 (1.2) 34.3 (1.2)
50 27.9 (0.6) 29.9 (0.8) 28.6 (0.7) 33.3 (1.0) 26.9 (0.7) 28.9 (0.8) 26.2 (0.4)

Coil2
d = 241

10 38.7 (0.8) 40.1 (0.8) 42.8 (0.7) 43.9 (0.8) 43.2 (0.8) 39.1 (0.9) 44.0 (0.8)
50 23.2 (0.6) 30.5 (0.9) 23.6 (0.9) 22.8 (0.9) 25.1 (0.9) 22.6 (0.8) 25.4 (0.8)

w8a
d = 300

10 35.9 (0.9) 33.6 (1.0) 41.6 (1.0) 46.6 (0.8) 39.4 (0.9) 42.1 (0.8) 43.0 (0.8)
50 28.1 (0.7) 27.6 (0.6) 27.0 (0.9) 38.7 (0.8) 28.0 (0.9) 33.7 (0.8) 35.2 (1.0)

#Best/Comp.

23

13

11

4

9

13

PUNU classification with the squared loss were tested.3
Data Sets: We used sixteen benchmark data sets taken
from the UCI Machine Learning Repository (Lichman,
2013), the Semi-Supervised Learning book (Chapelle et al.,
2006), the LIBSVM (Chang & Lin, 2011), the ELENA
Project,4 and a paper by Chapelle & Zien (2005).5 Each
feature was scaled to [0, 1]. Similarly to the setting in Section 5.1, we used the Gaussian kernel model for all methods. The training data is {xi }ni=1 = XP ∪ XN ∪ XU , where
n = nP + nN + nU . We selected all hyper-parameters with
V
validation samples of size 20 (nV
P = nN = 10). For training, we drew nL labeled and nU = 300 unlabeled samples.
The class-prior of labeled data was set at 0.7 and that of unlabeled samples was set at θP = 0.5 that were assumed to
be known. In practice, the class-prior, θP , can be estimated
3
In preliminary experiments, we tested other loss functions
such as the ramp and logistic losses and concluded that the difference in loss functions did not provide noticeable difference.
4
https://www.elen.ucl.ac.be/neuralnets/Research/Projects/ELENA/elena.htm
5
http://olivier.chapelle.cc/lds/

7

Table 2. Average and standard error of misclassification rates over
30 trials for the Places 205 data set. Boldface numbers denote the
best and comparable methods in terms of the average misclassification rate according to a t-test at a significance level of 5%.
Data set

nU

θP

θbP

PNU

ER

LapSVM

SMIR

WellSVM

Arts

1000 0.50 0.49 (0.01) 27.4 (1.3) 26.6 (0.5) 26.1 (0.7) 40.1 (3.9) 27.5 (0.5)
5000 0.50 0.50 (0.01) 24.8 (0.6) 26.1 (0.5) 26.1 (0.4) 30.1 (1.6)
N/A
10000 0.50 0.52 (0.01) 25.6 (0.7) 25.4 (0.5) 25.5 (0.6)
N/A
N/A

Deserts

1000 0.73 0.67 (0.01) 13.0 (0.5) 15.3 (0.6) 16.7 (0.8) 17.2 (0.8) 18.2 (0.7)
5000 0.73 0.67 (0.01) 13.4 (0.4) 13.3 (0.5) 16.6 (0.6) 24.4 (0.6)
N/A
10000 0.73 0.68 (0.01) 13.3 (0.5) 13.7 (0.6) 16.8 (0.8)
N/A
N/A

Fields

1000 0.65 0.57 (0.01) 22.4 (1.0) 26.2 (1.0) 26.6 (1.3) 28.2 (1.1) 26.6 (0.8)
5000 0.65 0.57 (0.01) 20.6 (0.5) 22.6 (0.6) 24.7 (0.8) 29.6 (1.2)
N/A
10000 0.65 0.57 (0.01) 21.6 (0.6) 22.5 (0.6) 25.0 (0.9)
N/A
N/A

Stadiums

1000 0.50 0.50 (0.01) 11.4 (0.4) 11.5 (0.5) 12.5 (0.5) 17.4 (3.6) 11.7 (0.4)
5000 0.50 0.50 (0.01) 11.0 (0.5) 10.9 (0.3) 11.1 (0.3) 13.4 (0.7)
N/A
10000 0.50 0.51 (0.00) 10.7 (0.3) 10.9 (0.3) 11.2 (0.2)
N/A
N/A

1000 0.27 0.33 (0.01) 21.8 (0.5) 23.9 (0.6) 24.1 (0.5) 30.1 (2.3) 26.2 (0.8)
Platforms 5000 0.27 0.34 (0.01) 23.3 (0.8) 24.4 (0.7) 24.9 (0.7) 26.6 (0.3)
N/A
10000 0.27 0.34 (0.01) 21.4 (0.5) 24.3 (0.6) 24.8 (0.5)
N/A
N/A
Temples

1000 0.55 0.51 (0.01) 43.9 (0.7) 43.9 (0.6) 43.4 (0.6) 50.7 (1.6) 44.3 (0.5)
5000 0.55 0.54 (0.01) 43.4 (0.9) 43.0 (0.6) 43.1 (1.0) 43.6 (0.7)
N/A
10000 0.55 0.50 (0.01) 45.2 (0.8) 44.4 (0.8) 44.2 (0.7)
N/A
N/A

by methods proposed, e.g., by Blanchard et al. (2010), Ramaswamy et al. (2016), or Kawakubo et al. (2016).
Table 1 lists the average and standard error of the misclassification rates over 50 trials and the number of
best/comparable performances of each method in the bottom row. The superior performance of PNU classification
over PUNU classification agrees well with the discussion
in Section 4.3. With the g50c data set, which well satisfies the low-density separation principle, the WellSVM
achieved the best performance. However, in the Banana
data set, where the two classes are highly overlapped, the
performance of WellSVM was worse than the other methods. In contrast, PNU classification achieved consistently
better/comparable performance and its performance did
not degenerate considerably across data sets. These results show that the idea of using PU classification in semisupervised classification is promising.
Figure 3 plots the computation time, which shows that the
fastest computation was achieved using the proposed methods with the square loss.
Image Classification: Finally, we used the Places 205
data set (Zhou et al., 2014), which contains 2.5 million images in 205 scene classes. We used a 4096-dimensional feature vector extracted from each image by AlexNet under the
framework of Caffe,6 which is available on the project website7 . We chose two similar scenes to construct binary classification tasks (see the description of data sets in Appendix
B.3). We drew 100 labeled and nU unlabeled samples from
each task; the class-prior of labeled and unlabeled data
were respectively set at 0.5 and θP = mP /(mP + mN ),
where mP and mN respectively denote the number of total
samples in positive and negative scenes. We used a linear
6
7

http://caffe.berkeleyvision.org/
http://places.csail.mit.edu/

Computation Time [sec.]

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data
104

PNU

ER

LapSVM

103

102

Arts

Deserts

Fields

Stadiums Platforms Temples

Figure 4. Average computation time over 30 trials for the Places
205 data set when nU = 10000.

classifier g(x) = w⊤ x + w0 , where w is the weight vector
and w0 is the offset (in the SMIR, the linear kernel model
is used; see Niu et al. (2013) for details).
We selected hyper-parameters in PNU classification by apη̄
(g)
plying five-fold cross-validation with respect to RPNU
with the zero-one loss, where η̄ was set at Eq.(10) or
Eq.(11) with σP (g) = σN (g). The class-prior p(y =
+1) = θP was estimated using the method based on energy distance minimization (Kawakubo et al., 2016).
Table 2 lists the average and standard error of the misclassification rates over 30 trials, where methods taking more
than 2 hours were omitted and indicated as N/A. The results
show that PNU classification was most effective. The average computation times are shown in Figure 4, revealing
again that PNU classification was the fastest method.

6. Conclusions
In this paper, we proposed a novel semi-supervised classification approach based on classification from positive
and unlabeled data. Unlike most of the conventional methods, our approach does not require strong assumptions on
the data distribution such as the cluster assumption. We
theoretically analyzed the variance of risk estimators and
showed that unlabeled data help reduce the variance without the conventional distributional assumptions. We also
established generalization error bounds and showed that
the confidence term decreases with respect to the number of positive, negative, and unlabeled samples without
the conventional distributional assumptions in the optimal
parametric order. We experimentally analyzed the behavior
of the proposed methods and demonstrated that one of the
proposed methods, termed PNU classification, was most
effective in terms of both classification accuracy and computational efficiency. It was recently pointed out that PU
classification can behave undesirably for very flexible models and a modified PU risk has been proposed (Kiryo et al.,
2017). Our future work is to develop a semi-supervised
classification method based on the modified PU classification.

Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data

Acknowledgements
TS was supported by JSPS KAKENHI 15J09111. GN was
supported by the JST CREST program and Microsoft Research Asia. MCdP and MS were supported by the JST
CREST program.

References

Kiryo, R., Niu, G., du Plessis, M. C., and Sugiyama, M.
Positive-unlabeled learning with non-negative risk estimator. arXiv preprint arXiv:1703.00593, 2017.
Krijthe, J. H. and Loog, M. Robust semi-supervised least
squares classification by implicit constraints. Pattern
Recognition, 63:115–126, 2017.
Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes. Springer, 1991.

Belkin, M., Niyogi, P., and Sindhwani, V. Manifold regularization: A geometric framework for learning from
labeled and unlabeled examples. Journal of Machine
Learning Research, 7:2399–2434, 2006.

Li, Y.-F. and Zhou, Z.-H. Towards making unlabeled data
never hurt. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 37(1):175–188, 2015.

Blanchard, G., Lee, G., and Scott, C. Semi-supervised novelty detection. Journal of Machine Learning Research,
11:2973–3009, 2010.

Li, Y.-F., Tsang, I. W., Kwok, J. T., and Zhou, Z.-H. Convex and scalable weakly labeled SVMs. Journal of Machine Learning Research, 14(1):2151–2188, 2013.

Chang, C.-C. and Lin, C.-J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm.

Lichman, M. UCI machine learning repository, 2013. URL
http://archive.ics.uci.edu/ml.

Chapelle, O. and Zien, A. Semi-supervised classification
by low density separation. In AISTATS, pp. 57–64, 2005.

Mendelson, S. Lower bounds for the empirical minimization algorithm. IEEE Transactions on Information Theory, 54(8):3797–3803, 2008.

Chapelle, O., Schölkopf, B., and Zien, A. (eds.). SemiSupervised Learning. MIT Press, 2006.
Collobert, R., Sinz, F., Weston, J., and Bottou, L. Trading
convexity for scalability. In ICML, pp. 201–208, 2006.

Melacci, S. and Belkin, M. Laplacian support vector machines trained in the primal. Journal of Machine Learning Research, 12:1149–1184, 2011.

Mohri, M., Rostamizadeh, A., and Talwalkar, A. Foundations of Machine Learning. MIT Press, 2012.

Cozman, F. G., Cohen, I., and Cirelo, M. C. Semisupervised learning of mixture models. In ICML, pp.
99–106, 2003.

Niu, G., Jitkrittum, W., Dai, B., Hachiya, H., and
Sugiyama, M. Squared-loss mutual information regularization: A novel information-theoretic approach to semisupervised learning. In ICML, volume 28, pp. 10–18,
2013.

du Plessis, M. C., Niu, G., and Sugiyama, M. Analysis of
learning from positive and unlabeled data. In NIPS, pp.
703–711, 2014.

Niu, G., du Plessis, M. C., Sakai, T., Ma, Y., and Sugiyama,
M. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In NIPS, 2016.

du Plessis, M. C., Niu, G., and Sugiyama, M. Convex formulation for learning from positive and unlabeled data.
In ICML, volume 37, pp. 1386–1394, 2015.

Ramaswamy, H. G., Scott, C., and Tewari, A. Mixture proportion estimation via kernel embedding of distributions.
In ICML, 2016.

Elkan, C. and Noto, K. Learning classifiers from only positive and unlabeled data. In SIGKDD, pp. 213–220, 2008.

Sokolovska, N., Cappé, O., and Yvon, F. The asymptotics
of semi-supervised learning in discriminative probabilistic models. In ICML, pp. 984–991, 2008.

Grandvalet, Y. and Bengio, Y. Semi-supervised learning by
entropy minimization. In NIPS, pp. 529–536, 2004.
Jain, S., White, M., and Radivojac, P. Estimating the class
prior and posterior from noisy positives and unlabeled
data. In NIPS, 2016.
Kawakubo, H., du Plessis, M. C., and Sugiyama, M. Computationally efficient class-prior estimation under class
balance change using energy distance. IEICE Transactions on Information and Systems, E99-D(1):176–186,
2016.

Vapnik, V. N. Statistical Learning Theory. John Wiley &
Sons, 1998.
Vapnik, V.N. The Nature of Statistical Learning Theory.
Springer-Verlag, New York, NY, USA, 1995.
Yuille, A. L. and Rangarajan, A. The concave-convex procedure (CCCP). In NIPS, pp. 1033–1040, 2002.
Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., and Oliva,
A. Learning deep features for scene recognition using
places database. In NIPS, pp. 487–495, 2014.

