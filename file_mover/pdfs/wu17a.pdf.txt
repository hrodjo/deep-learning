A Unified View of Multi-Label Performance Measures
Xi-Zhu Wu 1 Zhi-Hua Zhou 1

Abstract
Multi-label classification deals with the problem
where each instance is associated with multiple
class labels. Because evaluation in multi-label
classification is more complicated than singlelabel setting, a number of performance measures
have been proposed. It is noticed that an algorithm usually performs differently on different
measures. Therefore, it is important to understand which algorithms perform well on which
measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification.
In particular, we define label-wise margin and
instance-wise margin, and prove that through
maximizing these margins, different corresponding performance measures are to be optimized.
Based on the defined margins, a max-margin approach called LIMO is designed and empirical
results validate our theoretical findings.

1. Introduction
Multi-label classification aims to build classification models for objects assigned with multiple labels simultaneously, which is a common learning paradigm in real-world
applications. In text categorization, a document may be
associated with a range of topics, such as science, entertainment, and news (Schapire & Singer, 2000); in image
classification, an image can have both field and mountain
tags (Boutell et al., 2004); in music information retrieval, a
piece of music can convey various messages such as classic, piano and passionate (Turnbull et al., 2008).
In traditional supervised classification, generalization performance of the learning system is usually evaluated by accuracy, or F-measure if misclassification costs are unequal.
In contrast to single-label classification, performance evaluation in multi-label classification is more complicated, as
1

National Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing 210023, China. Correspondence to:
Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

each instance can be associated with multiple labels simultaneously. For example, it is difficult to tell which mistake
of the following two cases is more serious: one instance
with three incorrect labels vs. three instances each with
one incorrect label. Therefore, a number of performance
measures focusing on different aspects have been proposed,
such as Hamming loss, ranking loss, one-error, average
precision, coverage (Schapire & Singer, 2000), micro-F1
and macro-F1 (Tsoumakas et al., 2011).
Multi-label learning algorithms usually perform differently
on different measures; however, there are only a few studies about multi-label performance measures. Dembczynski et al. (2010) showed that Hamming loss and subset
0/1 loss could not be optimized at the same time. Gao
& Zhou (2013) proposed to study the Bayes consistency
of surrogate losses for multi-label learning; they proved
that none of convex surrogate loss is consistent with ranking loss, and gave a consistent surrogate loss function for
Hamming loss in deterministic case. There are a number of
studies about F-measure, mostly focusing on single-label
tasks, including multi-label learning as application. For
example, Ye et al. (2012) gave justifications and connections about F-measure optimization using decision theoretic approaches (DTA) and empirical utility maximization
approaches (EUM). Later, Waegeman et al. (2014) studied
the F-measure optimality of inference algorithms from the
DTA perspective. Koyejo et al. (2015) devoted to study
of EUM optimal multi-label classifiers. These theoretical
studies offer much insight, though lacking a unified understanding of relation among a variety of multi-label performance measures. Moreover, some performance measures
which have been popularly used in evaluation (Zhang &
Wu, 2015) have not been theoretically studied.
In this paper, we try to disclose some shared properties among different measures and establish a unified understanding for multi-label performance evaluation. We
propose a margin view to revisit eleven commonly used
multi-label performance measures, including Hamming
loss, ranking loss, one-error, coverage, average precision,
macro-, micro- and instance-averaging F-measures and
AUCs. Specifically, we propose the concepts of labelwise margin and instance-wise margin, based on which the
corresponding effectiveness of multi-label classifiers is defined and then used as bridge to connect different perfor-

A Unified View of Multi-Label Performance Measures

mance measures. Our theoretical results show that by maximizing instance-wise margin, macro-AUC, macro-F1 and
Hamming loss are to be optimized, whereas by maximizing label-wise margin, the other eight performance measures except micro-AUC are to be optimized. Inspired
by the theoretical findings, we design the LIMO (Labelwise and Instance-wise Margins Optimization) approach to
maximize both the two margins. Experiments validate our
theoretical findings and demonstrate a flexible way to optimize different measures through one approach by different
parameter settings.
The rest of the paper is organized as follows. Section 2 introduces the notation and definitions of eleven multi-label
performance measures. Section 3 proposes the label-wise
and instance-wise margins, and presents our theoretical results. Section 4 presents the LIMO approach. Section 5
reports the results of experiments. Finally, Section 6 concludes and indicates several future issues.

2. Preliminaries
2.1. Notation
Assume that xi ∈ Rd×1 is a real value instance vector,
y i ∈ {0, 1}l×1 is a label vector for xi . m denotes the number of training samples. Therefore yij (i ∈ {1, . . . , m}, j ∈
{1, . . . , l}) means the jth label of the ith instance, and
yij = 1 or 0 means the jth label is relevant or irrelevant.
The instance matrix is X ∈ Rm×d and the label matrix
is Y ∈ {0, 1}m×l . H : Rd → {0, 1}l is the multi-label
classifier, which consists of l models, one for a label, so
H = {h1 , . . . , hl } and hj (xi ) denotes the prediction of
yij . Moreover, F : Rd → Rl is the multi-label predictor
and the predicted value can be regarded as the confidence of
relevance. Similarly, F can be decomposed as {f1 , . . . , fl }
where fj (xi ) denotes the predicted value of yij .
H can be induced from F via thresholding functions. For
example, hj (xi ) = [[fj (xi ) > t(xi )]] uses a thresholding
function based on the instance xi and outputs 1 if predicted
value is higher than the threshold. [[π]] returns 1 if predicate
π holds, and 0 otherwise.
For simplification, we use Y i· to denote the ith row vector
and Y ·j to denote the jth column vector of the label matrix.
Furthermore, Yi·+ (or Yi·− ) denotes the index set of relevant
(or irrelevant) labels of Y i· . Formally, Yi·+ = {j |yij = 1}
and Yi·− = {j |yij = 0}. In terms of jth column of label
matrix, Y·j+ = {i |yij = 1} denotes the index set of positive
instances of the jth label and Y·j− = {i |yij = 0} denotes
the set of negative instances similarly. We use | · | to denote
the cardinality of a set, thus, the number of relevant labels
of xi is |Yi·+ |.

2.2. Multi-label Performance Measures
Table 1 summarizes the eleven multi-label performance
measures commonly used in previous studies. The first five
measures (Hamming loss, ranking loss, one-error, coverage, average precision) are considered in Schapire & Singer
(2000) and a multitude of works, e.g., Huang et al. (2012)
and Zhang & Wu (2015). The next six measures are extensions of F-measure and AUC (the Area Under the ROC
Curve) in multi-label classification via different averaging
strategies. These F-measures are popluar both in algorithm
evaluation (Liu & Tsang, 2015) and theoretical analysis
(Koyejo et al., 2015). AUCs are used for algorithm evaluation such as in Lampert (2011), Pham et al. (2015) and
Zhang & Wu (2015).
Some of these measures are defined on classifier H,
and they care about the binary classification performance.
While some of these measures are defined on predictor F ,
and they usually measure the ranking performance of the
predictor. We have noticed that some performance measures on ranking are ill-defined when F is a constant function. For example, if F outputs 1 for all labels, then oneerror(F ) = 0, coverage(F ) = 0 and various AUCs will be
1, which are the optimal values respectively. In multi-label
learning community, there is often an underlying assumption that a total ranking can be induced from continuous
real-value predictions, which is common in practical cases.
In this paper, we still stick to the convention in previous
works and assume that no tie happens in continuous prediction to solve this definition flaw.

3. Theoretical Results
Here we define two new concepts: label-wise margin and
instance-wise margin.
Definition 1. Given a multi-label predictor F : Rd → Rl
and F = {f1 , . . . , fl }, a training set (X, Y ), the labelwise margin on instance xi is defined as:
γilabel = min{fu (xi ) − fv (xi ) | (u, v) ∈ Yi·+ × Yi·− }.
u,v

Yi·+ × Yi·− is the set of all the (relevant, irrelevant) label
index pairs of instance i.
Definition 2. Given a multi-label predictor F : Rd → Rl
and F = {f1 , . . . , fl }, a training set (X, Y ), the instancewise margin on label Y ·j is defined as:
γjinst = min{fj (xa ) − fj (xb ) | (a, b) ∈ Y·j+ × Y·j− }.
a,b

Y·j+ × Y·j− is the set of all the (positive, negative) instance
index pairs of label j.
Label-wise margin and instance-wise margin describe the
discriminative ability of F . The larger the label-wise mar-

A Unified View of Multi-Label Performance Measures

Measure

Table 1. Definitions of eleven multi-label performance measures
Formulation

Hamming loss

hloss(H) =

rloss(F ) =

ranking loss
i
Srank

one-error

coverage

1
ml

m
X

l
X

The fraction of misclassified labels

[[hij 6= yij ]]

i=1 j=1

m
i
1 X |Srank
|
+
m i=1 |Yi· ||Yi·− |

= {(u, v)|fu (xi ) ≤ fv (xi ), (u, v) ∈

+
Yi·

×

−
Yi· }

The fraction of instances whose most
confident label is irrelevant.

m
1 X
[[ max rankF (xi , j) − 1]]
m i=1 j∈Y +

The number of more labels on average should include to cover all relevant labels

i·

avgprec(F ) =

average precision

ij
m
X
|Sprecision
|
1 X 1
m i=1 |Yi·+ |
rank
(x
i , j)
F
+
j∈Y
i·

ij

+

Sprecision = {k ∈ Yi· |rankF (xi , k) ≤ rankF (xi , j)}

F-measure averaging on each label.

P
m
2 lj=1 yij hij
1 X
Pl
Pl
m i=1
j=1 yij +
j=1 hij

F-measure averaging on each instance.

P
P
2 lj=1 m
i=1 yij hij
Pm
Pl
Pm
j=1
i=1 yij +
j=1
i=1 hij

F-measure averaging on the prediction matrix.

macro-F1(H) =

instance-F1

instance-F1(H) =

micro-F1(H) = Pl

macro-AUC(F ) =

macro-AUC
j

+

l
j
|
1 X |Smacro
+
−
l j=1 |Y·j
||Y·j
|
−

Smacro = {(a, b) ∈ Y·j × Y·j |fj (xa ) ≥ fj (xb )}

instance-AUC(F ) =

instance-AUC
i

+

m
i
1 X |Sinstance
|
m i=1 |Yi·+ ||Yi·− |
−

Sinstance = {(u, v) ∈ Yi· × Yi· |fu (xi ) ≥ fv (xi )}

micro-AUC
Smicro

The average fraction of relevant labels ranked higher than one other relevant label.

P
l
2 m
1X
i=1 yij hij
Pm
Pm
l j=1
i=1 yij +
i=1 hij

macro-F1

micro-F1

The average fraction of reversely ordered label pairs of each instance.

m
1 X
+
[[arg max F (xi ) ∈
/ Yi· ]]
m i=1

one-error(F ) =

coverage(F ) =

Note

|Smicro |
micro-AUC(F ) = Pm
P
−
( i=1 |Yi·+ |) · ( m
i=1 |Yi· |)
+
−
= {(a, b, i, j)|(a, b) ∈ Y·i × Y·j , fi (xa ) ≥ fj (xb )}

gin, the easier to distinguish relevant and irrelevant labels
of an instance. Meanwhile, the larger the instance-wise
margin, the easier for F to distinguish positive and negative instances of a particular label. Therefore, we want
to maximize label-wise/instance-wise margin to get better
performance.
Although we prefer maximizing these two margins, with
respect to performance measures, the objective can be relaxed. We define three properties a predictor F can have:
label-wise effective, instance-wise effective and double effective.
Definition 3. If all the label-wise margins of F on a dataset
D = (X, Y ) are positive, this predictor F is label-wise
effective on D.

AUC averaging on each label. Smacro
is the set of correctly ordered instance
pairs on each label.

AUC averaging on each instance.
Sinstance is the set of correctly ordered
label pairs on each instance.

AUC averaging on prediction matrix.
Smicro is the set of correct quadruples.

Definition 4. If all the instance-wise margins of F on a
dataset D = (X, Y ) are positive, this predictor F is
instance-wise effective on D.
Definition 5. If all the label-wise margins and instancewise margins of F on a dataset D = (X, Y ) are positive,
this predictor F is double effective on D.
Roughly speaking, label-wise effective means F can exactly distinguish relevant and irrelevant labels of each instance and instance-wise effective means F can exactly distinguish positive and negative instances of every label. Not
surprisingly, double effective F has the strongest ability in
distinguishing.
In the next two subsections, we use the effectiveness to an-

A Unified View of Multi-Label Performance Measures

alyze different performance measures, and summarize the
analysis results in Section 3.3.

Therefore, the size of the correct ordered prediction value
pair on instance i is:
|{(u, v) ∈ Yi·+ × Yi·− |fu (xi ) ≥ fv (xi )}| = |Yi·+ ||Yi·− |.

3.1. Performance Measures on Ranking
Several multi-label performance measures can be empirically optimized according to the following theorems:
Theorem 1. If a multi-label predictor F is label-wise effective on D, then ranking loss, one-error, coverage, average
precision and instance-AUC are optimized on the dataset.

So instance-AUC(F ) = 1 and instance-AUC is optimized.

Similar to the proof of instance-AUC, we can prove the result of macro-AUC:
Theorem 2. If a multi-label predictor F is instance-wise
effective on D, then macro-AUC is optimized.

Proof. (a) Ranking loss: From the definition of labelwise effective, for every pair (u, v) ∈ Yi·+ × Yi·− , we have
i
fu (xi ) > fv (xi ). Therefore, the reversed set Srank
(in Table 1 ranking loss) is empty and the cardinality of the set is
zero, which implies the cardinality sum of all reversed sets
rloss(F ) = 0. Ranking loss is optimized.

Proof. Because of instance-wise effective, for a label vector Y ·j , we have:

(b) One-error: For a label-wise effective F , because labelwise margin is positive on an instance xi , we have:

Therefore, the size of the correct ordered prediction value
pair on label j is:
{(a, b) ∈ Y·j+ × Y·j− |fj (xa ) ≥ fj (xb )} = |Y·j+ ||Y·j− |.

max fu (xi ) > max fv (xi ), ∀u ∈ Yi·+ , ∀v ∈ Yi·− .
u

fj (xa ) > fj (xb ), ∀(a, b) ∈ Y·j+ × Y·j− .

v

So macro-AUC(F ) = 1 and macro-AUC is optimized.

Then
Thus, [[arg max F (xi ) ∈
/ Yi·+ ]] = 0 for every instance xi ,
and one-error(F ) = 0. One-error is optimized.

Micro-AUC sees the label matrix as a whole and cannot be
optimized by instance-wise effective F or label-wise effective F . However, the double effective F is much more powerful. We now prove the following result of micro-AUC.

(c) Coverage: When F is label-wise effective, the maximum rank of a relevant label is less than the minimum rank
of an irrelevant label, which means:

Theorem 3. If a multi-label predictor F is double effective
on D, then as the number of instances grows, micro-AUC
is optimized.

max rankF (xi , u) < min rankF (xi , v),

Proof. We first prove a result of random variables
Ai , B, C. If n random variables A1 , A2 , · · · , An are drawn
from uniform distribution U (0, 1), for a random constant a,
the event that at least one Ai is smaller than a is:

∀xi , arg max F (xi ) ∈ Yi·+ .

u∈Yi·+

v∈Yi·−

(1)

max rankF (xi , u) = |Yi·+ |.

u∈Yi·+

Pr[∃Ai , Ai ≤ a] = 1 − (1 − a)n .

Therefore, coverage can be calculated as:
coverage(F ) =

1 Xm
[|Yi·+ | − 1].
i=1
m

Another random variable B is uniformly distributed in
(0, min{Ai }), and the probability that a random variable
C ∼ U (0, 1) is bigger than B is:

Which is the optimal value of coverage.
(d) Average precision: Assume that j is a relevant label of
instance i, it follows from Equation (1) that:
rankF (xi , j) = |{k ∈ Yi·+ |rankF (xi , k) ≤ rankF (xi , j)}|
ij
Since rankF (xi , j) is exactly the definition of Sprecision
,
avgprec(F ) = 1, i.e, average precision is optimized.

(e) Instance-AUC: Because of label-wise effective, for an
instance xi , we have:
fu (xi ) > fv (xi ), ∀(u, v) ∈

Yi·+

×

Yi·− .

Pr[C > B] ≥ Pr[(C ≥ a) ∧ (∃Ai , Ai ≤ a)]
a
=(1 − )[1 − (1 − a)n ].
2

(2)

For any small a, we can choose a large enough n to make
Equation (2) close to 1.
Given a label matrix Y ∈ {0, 1}m×l and the corresponding prediction matrix F ∈ (0, 1)m×l , because predictor F
is double effective, the prediction matrix satisfies the following conditions:
Fij > Fiu if Yij = 1 ∧ Yiu = 0,
Fij > Fvj if Yij = 1 ∧ Yvj = 0.

A Unified View of Multi-Label Performance Measures

To force the value in F is in (0, 1), we further assume a
uniform distribution Fij ∼ U (0, 1) when Yij = 1.

for instance-wise effective F . It is reasonable to use either
t(xi ) or tj for double effective F .

If Yij = 0, then Fij should be less than Fiu if Yiu = 1
and Fvj if Yvj = 1. Suppose that the minimum value b is
defined as:
n
o
b = min min{Fvj |Yvj = 1}, min{Fiu |Yiu = 1} .

To formally analyze the performance measures on classification, we define the threshold error:

v

u

Then Fij is drawn from U (0, b). And we can choose a
small constant value a > b.
According to Equation (2), the probability that a random
pair (i, j, u, v) to be a correct micro pair is:
Pmicro = Pr[Fij > Fuv |Yij = 1, Yuv = 0]
a
≥ (1 − )[1 − (1 − a)n ],
2
k
(m + l − 2)
where n =
ml
In the practical case, the number of labels is proportional
to the number of instances: k ∝ m. We assume k = pm
where p is a constant smaller than l.
p
(m + l − 2) = ∞,
l
|Smicro |
lim Pm
= lim Pmicro = 1.
Pm
+
−
m→∞ |(
m→∞
|Y
i· |) · (
i=1
i=1 |Yi· |)|
lim n = lim

m→∞

m→∞

Therefore, micro-AUC is to be optimized as the number of
instances grows.
With the above analysis, we can conclude that a label-wise
effective F can optimize ranking loss, one-error, coverage, average precision, instance-AUC, micro-AUC and an
instance-wise effective F can optimize macro-AUC. For
micro-AUC, a double effective F can optimize it as the
number of instances increases.
3.2. Performance Measures on Classification
As mentioned in Section 2.2, there are some measures evaluating classifier H instead of predictor F . There are many
thresholding or binarization strategies (Fan & Lin, 2007;
Fürnkranz et al., 2008; Read et al., 2011). For simplicity,
we focus on two main strategies: thresholding on each instance and thresholding on each label.
A label-wise effective F can be equipped with a thresholding function based on each instance such as t(xi ) and
construct the H by hj (xi ) = [[fj (xi ) > t(xi )]]. However,
using t(xi ) on an instance-wise effective F is unreasonable since the predicted values on different labels may not
be comparable. In a word, we should use suitable threshold function on different effective F s, i.e., t(xi ) on each
instance for label-wise effective F , and tj on each label

Definition 6. Given a descending ordered real-value sequence x1 , x2 , . . . , xk with an optimal cut number c∗ ,
where c∗ ∈ N and 1 ≤ c∗ ≤ k. For a real value
threshold t ∈ (xk − 1, x1 + 1), the threshold error  =
| arg mini (xi ) − c∗ | where xi > t.
Intuitively, the threshold error  counts how many items
are incorrectly classified on a descending ordered sequence
where the correct answer is c∗ . Based on the threshold error, we propose the following theorems about performance
measures on classification.
Theorem 4. For a label-wise effective F , if the thresholding function makes at most i error on each instance i, the
micro-F1, instance-F1 and Hamming loss are bounded as
follows:
micro-F1(H) = instance-F1(H)
m
n 2(|Y + | −  )
2|Yi·+ | o
1 X
i
i·
min
,
,
≥
+
m i=1
2|Yi· | − i 2|Yi·+ | + i
1 Xm
hloss(H) ≤
i .
i=1
ml
The main idea of the above theorem is that, given the
threshold error and the number of relevant labels, we can
compute the gap between the worst possible and the perfect
contingency table. Hence the F-measure is based on the
contingency table, the lower bound can be deduced. The
detailed proof of Theorem 4 is in Appendix A.1.
Similar to Theorem 4, we can prove the results for labelwise effective F :
Theorem 5. For an instance-wise effective F , if the thresholding function makes at most j error on each label j, then
the macro-F1 and Hamming loss are bounded as follows:
macro-F1(H) ≥
hloss(H) ≤

l
n 2(|Y + | − j )
2|Y·j+ | o
1X
·j
min
,
,
l j=1
2|Y·j+ | − j 2|Y·j+ | + j

1 Xl
j .
j=1
ml

The detailed proof of Theorem 5 is in Appendix A.2.
With the above analysis, we can conclude that a labelwise effective F can optimize instance-F1 and micro-F1,
an instance-wise effective F can optimize macro-F1. Both
the two effective F s can optimize Hamming loss. For a
double effective F , because it enjoys both the properties,
it can optimize all the above mentioned performance measures if proper thresholds are used.

A Unified View of Multi-Label Performance Measures
Table 2. Summary of performance measures optimized by xeffective multi-label predictor (F ). ‘X’ means F in this cell is
proved to optimize this measure; ‘7’ means F in this cell does
not necessarily optimize the measure; ‘•’/‘◦’ means the calculation is with/without thresholding.

x-effective F
Threshold
label-wise inst-wise double
ranking loss
X
7
X
◦
X
7
X
◦
avg. precision
one-error
X
7
X
◦
coverage
X
7
X
◦
instance-AUC
X
7
X
◦
7
X
X
◦
macro-AUC
micro-AUC
7
7
X
◦
macro-F1
7
X
X
•
instance-F1
X
7
X
•
X
7
X
•
micro-F1
X
X
X
•
Hamming loss
Measure

3.3. Summary
Table 2 summarizes our theoretical results in Section 3.1
and 3.2. Each row shows the results of one multi-label performance measure. Note that double effective is a special
case of label-wise effective and instance-wise effective and
thus, if one performance measure is optimized by either
label-wise or instance-wise effective predictor, it will also
be optimized by double effective predictor.
In the light of the analysis, the performance on different
performance measures through optimizing margins can be
expected. For example, if one maximizes instance-wise
margin on each label, s/he will get good performance on
macro-AUC but may suffer higher loss on ranking loss,
coverage and some other measures where ‘7’ marked in the
inst-wise column. If one tries to maximize the label-wise
margin but pay no attention to instance-wise margin, s/he
may perform well on average precision but poor on macroF1 (e.g., Elisseeff & Weston (2002)). Maximzing both the
label-wise margin and instance-wise margins to get a double effective F is expected to be the best choice.

4. The LIMO Approach
The above analysis reveals that maximizing different margins will optimize different measures, and if possible, double effective F is prefered since it enjoys the benefits of
maximizing both the label-wise margin and the instancewise margin. Therefore, we propose the LIMO approach.
LIMO is a single approach which can optimize both the two
margins, and it can also be degenerated to optimize either
margin seperately via parameter setting.

4.1. Formulation
Suppose that F is a linear predictor, which means F (X) =
W T X where W = [w1 , w2 , · · · , wl ]. We propose the
following formulation:
arg min
W ,ξ

s.t.

l
X

||wi ||2 + λ1

i=1

m X
X

ξiuv + λ2

i=1 (u,v)

w>
u xi

−

w>
v xi

>1−

ξiuv ,

l X
X

j
ξab

j=1 (a,b)

ξiuv

≥ 0,

for i = 1, · · · , m and (u, v) ∈ Yi·+ × Yi·− ,
j
j
>
w>
j xa − w j xb > 1 − ξab , ξab ≥ 0,

for j = 1, · · · , l and (a, b) ∈ Y·j+ × Y·j− .
(3)
j
ξab

Here ξiuv and
are the slack variables, and λ1 , λ2 are the
trade-off parameters. When both λ1 and λ2 are positive,
both label-wise and instance-wise margins are considered.
If we set λ1 = 0 (or λ2 = 0), then only the instance-wise
(or label-wise) margin is considered. In this paper, if the
approach only considers instance-wise (or label-wise) margin , we call the approach as LIMO-inst (or LIMO-label).
And LIMO considers both the two margins.
Algorithm 1 LIMO
Input:
Data matrix X ∈ Rm×d , label matrix Y ∈ {0, 1}m×l ,
step size η, trade-off parameters λ1 , λ2 , and the maximium iteration number T .
Procedure:
√
1: Initialize W 0 with N (0, 1/ d) random values.
2: Compute the weight
vector cinst of each instance,
+
− Pm
inst
ci
= |Yi· ||Yi· |/ i=1 |Yi·+ ||Yi·− |.
3: Compute the weight vector clabel of each label,
Pl
clabel
= |Y·j+ ||Y·j− |/ j=1 |Y·j+ ||Y·j− |.
j
4: for t = 1, 2, · · · , T do
5:
Random sample an instance xti using weight cinst ,
6:
Random sample a positive label yiu and a negative
label yiv of instance xti .
t
> t
7:
if 1 − w>
u xi + w v xi > 0 then
t
t−1
8:
wu = wu − η(−λ1 xti + wt−1
u ).
t
t−1
9:
wtv = wt−1
−
η(λ
x
+
w
1 i
v
v ).
10:
end if
11:
Random sample index j of label using weight clabel .
12:
Random sample a positive instance xta and a negative instance xtb on label j.
t
> t
13:
if 1 − w>
j xa + w j xb > 0 then
14:
wtj = wjt−1 − η(λ2 (xtb − xta ) + wt−1
j ).
15:
end if
16: end for P
T
t
17: W = T1
t=1 W .
Output:
Multi-label linear model W .

A Unified View of Multi-Label Performance Measures
1

4.2. Algorithm
The objective Equation (3) is difficult to solve directly because of the large number of constraints and slack variables. For a training set with m instances and l labels, the
number of constraints will be O(m2 l + ml2 ), which may
exceed memory limit in real-world applicaitons.

A

0

BC
ABD

-1
-1

0

1

*
λ1 = λ2 = 100. Ten replications
of the experiment are
LIMO-inst-t
conducted
and
the
average
results
are
reported.LIMO-inst-t(x)
Because
0.837
micro F1
*
the range of performance measure coverage isLIMO-label-t
not [0, 1],
LIMO-label-t(x)
while
some
measures are
higher,
0.852better when
macro
F1 performance
*
* LIMO-t
and some are better when lower, we rescale all
the perforLIMO-t(x)
instance
F1
mance
values
into relative
values for0.804
clearer visualization.
*
The best one 1.0
is rescaled
0.8 0.6 to
0.41 and
0.2 the
0.0 worst one is rescaled to
value results, where the originally
0. Figure 2 showsrelative
the relative
worst performance value is given on the right.

Hamming loss

0.188

ranking loss

0.027

avg. precision

0.992

one-error

0.001

coverage

1.575

macro-AUC

0.828

instance-AUC

0.973

micro-AUC

absolute worst value

After the training procedure, we can use the linear model to
predict continuous confidence values on the training data,
then choose the best threshold value by optimizing a specific classification measure.

ABCD

absolute worst value

In order to deal with the computational problem, we solve
Equation (3) by stochastic gradient descent (SGD) with
fixed step size and the default averaging technique in
Shalev-Shwartz & Ben-David (2014, Chapter 14.3). The
key point of SGD is to find out a random vector, whose
expected value at each iteration equals the gradient direction. We randomly sample two kinds of triplets and use
them to compute the correct direction. At each iteration t,
we sample a triplet (xti , yiu , yiv ) where yiu is relevant and
yiv is irrelevant, and a triplet (j, xta , xtb ) where xta is a positive instance and xtb is a negative instance both on label j.
Then we use the two triplets to compute the random gradient vector for SGD. The detailed algorithm is presented
in Algorithm 1 and the proof that the random vector is an
unbiased estimation of the gradient direction is available in
Appendix A.3.

Figure 1. Input space consists of
four regions with different assignments of the label set {A, B, C, D}.
The center point is with coordinate
(0, 0).

LIMO-inst
LIMO-label
LIMO

0.854
1.0

0.8

0.6

0.4

0.2

0.0

relative value

Figure 2. Summary of the relative performance on ranking measures. The more to the left, the better the performance.

5. Experiments
We conduct experiments with LIMO on both synthetic and
benchmark data. Note that the main purpose of our work
is to study multi-label performance measures from the aspect of margin optimization, and thus, the goal of our experiments is to validate our theoretical findings rather than
claim that LIMO is superior, although its performance is
really highly competitive.
5.1. Synthetic Data

*

Hamming loss

*

micro F1
macro F1

0.837

*

0.852

*

instance F1
1.0

0.8

0.6

0.804
0.4

0.2

*

LIMO-inst-t
LIMO-inst-t(x)
LIMO-label-t
LIMO-label-t(x)
LIMO-t
LIMO-t(x)

0.0

relative value

Figure 3. Summary of the relative performance on classification
measures. The more to the left, the better the performance.
ranking loss

0.027

Figure
3 shows the relative performance
on classification.
0.992
avg. precision
We use
two
types
of
thresholding
discussed
inLIMO-inst
Section 3.2:
0.001
one-error
coverage

1.575

macro-AUC

0.828

instance-AUC

0.973

micro-AUC

0.854
1.0

0.8

0.6

0.4

relative value

0.2

0.0

absolute worst value

To demonstrate the relationship between margins and performance measures, we degenerate LIMO to only consider
either margin by setting the trade-off parameter λ1 or λ2
to zero. LIMO-inst sets λ1 = 0 and LIMO-label sets
λ2 = 0. The other parameter is set to 100 and LIMO sets

0.188

absolute worst value

We conduct experiments on synthetic data with 4 labels.
2000 data points are randomly generated from a (−1, +1)2
square, and the labels are assigned as in Figure 1. 50%
data are held out for testing. The synthetic data is designed
to simulate a typical real-world circumstance. The number
of co-occurrent labels varies, the regions of each label are
different and the data cannot be perfectly seperated by a
linear learner.

The results shown in Figure 2 support our theoretical findings in Table 2. For example, micro-AUC is considered to
be optimized by double effective F but not the other two,
therefore LIMO (the red circle) gets the best relative value.
For some measures proved to be optimized by label-wise
margin such as ranking loss, average precision, coverage
and instance-AUC, LIMO-label beats LIMO-inst. While
for macro-AUC, LIMO-inst wins. For one-error, all three
versions of LIMO do extremely well and get less than 0.001
absolute value, which may be the reason why the relative
values are unexpected.

LIMO-label
LIMO

A Unified View of Multi-Label Performance Measures

threshold function based on each instance or each label (denoted by -t(x) or -t in the legend). The thresholds are estimated on training data. This figure exactly shows our theoretical results: LIMO-label equipped with t(x) can optimize instance-F1 and micro-F1; LIMO-inst equipped with
t can optimize macro-F1. By considering both label-wise
margin and instance-wise margin, LIMO works well on all
four classificaiton measures.
5.2. Benchmark Data
We conduct experiments on eleven multi-label performance measures to further show that optimizing the labelwise or the instance-wise margin can lead to different results, as revealed in our theoretical analysis.
Five benchmark multi-label datasets1 are used in our experiments. We choose them because they denote different domains: (i) A music dataset CAL500, (ii) an email dataset
enron, (iii) a clinical text dataset medical, (iv) an image
dataset corel5k, (v) a tagging dataset bibtex. We randomly
split each dataset into two parts, i.e., 70% for training and
30% for testing. The experiments are repeated ten times,
and the averaged results are reported.
Because our algorithm optimizes a linear model, three linear methods called Binary Relevance (BR) (Zhang & Zhou,
2014), ML-kNN (Zhang & Zhou, 2007) and GFM (Waegeman et al., 2014) are provided for fair comparison. As
in experiments on synthetic data, we degenerate LIMO
(λ1 = λ2 = 1) to LIMO-inst (λ1 = 0, λ2 = 1) and LIMOlabel (λ1 = 1, λ2 = 0). The step size of SGD is set to 0.01.
For BR, L2-regularized SVM (Chang & Lin, 2011) with
C=1 is used as base learner. For ML-kNN and GFM, the
number of nearest neighbors is 10. Suitable thresholds discussed in Section 3.2 are used for classification measures.
We take the default parameter settings recommended by authors of the compared methods respectively. Because on
one hand, we believe the parameter settings recommended
by their authors are meaningful, on the other hand, it is hard
to say which parameter setting is better in terms of eleven
performance measures.
Because some measures are better when higher, and some
measures are better when lower, to demonstrate the results more clearly, we compute the average rank of each
approach over all datasets on a specific measure. For example, when we want to examine how LIMO performs on
ranking loss, we first compute the ranks on each dataset:
LIMO ranks 1st on CAL500, enron, bibtex and ranks 2nd
on medical, corel5k. Then the average rank of LIMO on
ranking loss is (1+1+1+2+2)/5=1.4. Figure 4 shows the average ranks. Due to the space limit, the detailed results
used to compute the ranks are provided in Appendix B.2.
1

http://mulan.sourceforge.net/datasets-mlc.html

ranking loss
avg. precision
one-error
coverage
Hamming loss
instance-F1
instance-AUC
macro-F1
macro-AUC
micro-F1
micro-AUC

*

*

*
*
*
*

1

2

*

3
4
5
average rank

*

*

BR
ML-kNN
GFM
LIMO-inst
LIMO-label
LIMO

*
*
6

Figure 4. Average rank on benchmark data. The smaller the rank
value, the better the performance.

The results in Figure 4 are consistent with our theoretical
findings. LIMO-inst (the square) performs well on marcoF1 and macro-AUC, while LIMO-label (the triangle) performs well on other performance measures. LIMO (the circle) almost ranks top on every performance measure.
The experiments on synthetic and benchmark data support
our theoretical analysis. Although different performance
measures focus on different aspects, they share the common property which is formalized in our work as labelwise margin and instance-wise margin. In practice, it is recommended to use higher weight (λ1 /λ2 ) on specific margin to optimize the required performance measure. LIMO
with nonlinear predictors may perform better, which needs
a novel optimization algorithm.

6. Conclusion
In this paper, we establish a unified view for a variety of
multi-label performance measures. Based on the proposed
concepts of label-wise/instance-wise margins, we prove
that some performance measures are to be optimized by
label-wise effective classifiers, whereas some by instancewise effective classifiers. Inspired by the theoretical findings, we design the LIMO approach which can be adjusted
to label-wise/instance-wise effective via different parameter settings.
Our work discloses that there are some shared properties
among different subsets of multi-label performance measures. This explains why some measures seem to be redundant in experiments, and suggests that in future empirical
studies, rather than randomly grasp a set of measures for
evaluation, it is more informative to evaluate using measures with different properties, such as some measures optimized by label-wise effective predictors and some optimized by instance-wise effective predictors. In the future,
it is encouraging to study the asymptotic properties of these
performance measures when the two margins are suboptimal. The margin view also sheds a light for the design of
novel multi-label algorithms.

A Unified View of Multi-Label Performance Measures

Acknowledgements
This research was supported by the NSFC (61333014), 973
Program (2014CB340501), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Authors want to thank reviewers for helpful comments, and thank Sheng-Jun Huang, Xiu-Shen Wei, Miao
Xu for reading a draft.

References
Boutell, Matthew R., Luo, Jiebo, Shen, Xipeng, and
Brown, Christopher M. Learning multi-label scene classification. Pattern Recognition, 37(9):1757–1771, 2004.
Chang, Chih-Chung and Lin, Chih-Jen. LIBSVM: A library for support vector machines. ACM Trans. Intelligent Systems and Technology, 2(3):27:1–27:27, 2011.
Dembczynski, Krzysztof, Waegeman, Willem, Cheng,
Weiwei, and Hüllermeier, Eyke. Regret analysis for performance metrics in multi-label classification: the case
of hamming and subset zero-one loss. In ECML/PKDD,
pp. 280–295. 2010.
Elisseeff, André and Weston, Jason. A kernel method
for multi-labelled classification. In NIPS, pp. 681–687,
2002.
Fan, Rong-En and Lin, Chih-Jen. A study on threshold
selection for multi-label classification. National Taiwan
University, Technical Report, pp. 1–23, 2007.
Fürnkranz, Johannes, Hüllermeier, Eyke, Mencı́a,
Eneldo Loza, and Brinker, Klaus. Multilabel classification via calibrated label ranking. Machine Learning, 73
(2):133–153, 2008.
Gao, Wei and Zhou, Zhi-Hua. On the consistency of multilabel learning. Artificial Intelligence, 199:22–44, 2013.
Huang, Sheng-Jun, Yu, Yang, and Zhou, Zhi-Hua. Multilabel hypothesis reuse. In ACM SIGKDD, pp. 525–533,
2012.
Koyejo, Oluwasanmi, Natarajan, Nagarajan, Ravikumar,
Pradeep, and Dhillon, Inderjit S. Consistent multilabel
classification. In NIPS, pp. 3321–3329, 2015.
Lampert, Christoph H. Maximum margin multi-label structured prediction. In NIPS, pp. 289–297, 2011.
Liu, Weiwei and Tsang, Ivor W. On the optimality of classifier chain for multi-label classification. In NIPS, pp.
712–720, 2015.
Pham, Anh T., Raich, Raviv, Fern, Xiaoli Z., and Arriaga,
Jesús Pérez. Multi-instance multi-label learning in the
presence of novel class instances. In ICML, pp. 2427–
2435, 2015.

Read, Jesse, Pfahringer, Bernhard, Holmes, Geoff, and
Frank, Eibe. Classifier chains for multi-label classification. Machine Learning, 85(3):333–359, 2011.
Schapire, Robert E and Singer, Yoram. Boostexter: A
boosting-based system for text categorization. Machine
Learning, 39(2):135–168, 2000.
Shalev-Shwartz, Shai and Ben-David, Shai. Understanding
Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.
Tsoumakas, Grigorios, Katakis, Ioannis, and Vlahavas,
Ioannis P. Random k-labelsets for multilabel classification. IEEE Trans. Knowledge and Data Engineering, 23
(7):1079–1089, 2011.
Turnbull, Douglas, Barrington, Luke, Torres, David A., and
Lanckriet, Gert R. G. Semantic annotation and retrieval
of music and sound effects. IEEE Trans. Audio, Speech
& Language Processing, 16(2):467–476, 2008.
Waegeman, Willem, Dembczynski, Krzysztof, Jachnik,
Arkadiusz, Cheng, Weiwei, and Hüllermeier, Eyke. On
the bayes-optimality of F-measure maximizers. Journal
of Machine Learning Research, 15(1):3333–3388, 2014.
Ye, Nan, Chai, Kian Ming Adam, Lee, Wee Sun, and
Chieu, Hai Leong. Optimizing F-measure: A tale of two
approaches. In ICML, 2012.
Zhang, Min-Ling and Wu, Lei. LIFT: Multi-label learning
with label-specific features. IEEE Trans. Pattern Analysis and Machine Intelligence, 37(1):107–120, 2015.
Zhang, Min-Ling and Zhou, Zhi-Hua. ML-KNN: A
lazy learning approach to multi-label learning. Pattern
Recognition, 40(7):2038–2048, 2007.
Zhang, Min-Ling and Zhou, Zhi-Hua. A review on multilabel learning algorithms. IEEE Trans. Knowledge and
Data Engineering, 26(8):1819–1837, 2014.

