A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank
Matrix Recovery
Lingxiao Wang * 1 Xiao Zhang * 1 Quanquan Gu 1

Abstract
We propose a generic framework based on a
new stochastic variance-reduced gradient descent
algorithm for accelerating nonconvex low-rank
matrix recovery. Starting from an appropriate
initial estimator, our proposed algorithm performs projected gradient descent based on a
novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon
the mild restricted strong convexity and smoothness conditions, we derive a projected notion of
the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear
convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to
both noiseless and noisy observations, where the
(near) optimal sample complexity and statistical
rate can be attained respectively. We further illustrate the superiority of our generic framework
through several specific examples, both theoretically and experimentally.

1. Introduction
Low-rank matrix recovery problem has been extensively
studied during the past decades, due to its wide range
of applications, such as collaborative filtering (Srebro
et al., 2004; Rennie & Srebro, 2005) and multi-label learning (Cabral et al., 2011; Xu et al., 2013). The objective of
low-rank matrix recovery is to estimate the unknown lowrank matrix X⇤ 2 Rd1 ⇥d2 from partial observations, such
as a set of linear measurements in matrix sensing or a subset
of its entries in matrix completion. Significant efforts have
been made to estimate low-rank matrices, among which
one of the most prevalent approaches is nuclear norm re*

Equal contribution 1 Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA. Correspondence to: Quanquan Gu <qg5w@virginia.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

laxation based optimization (Srebro et al., 2004; Candès &
Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015). While
such convex relaxation based methods enjoy a rigorous theoretical guarantee to recover the unknown low-rank matrix,
due to the nuclear norm regularization/minimization, these
algorithms involve a singular value decomposition at each
iteration, whose time complexity is O(d3 ) to recover a d⇥d
matrix. Hence, they are computationally very expensive.
In order to address the aforementioned computational issue, recent studies (Keshavan et al., 2009; 2010; Jain et al.,
2013a; Jain & Netrapalli, 2014; Hardt, 2014; Hardt &
Wootters, 2014; Hardt et al., 2014; Zhao et al., 2015; Chen
& Wainwright, 2015; Sun & Luo, 2015; Zheng & Lafferty,
2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015; Park
et al., 2016b; Wang et al., 2016) have been carried out to
perform factorization on the matrix space, which naturally
ensures the low-rankness of the produced estimator. Although this matrix factorization technique converts the previous optimization problem into a nonconvex one, which is
more difficult to analyze, it significantly improves the computational efficiency.
However, for large-scale matrix recovery, such nonconvex
optimization approaches are still computationally expensive, because they are based on gradient descent or alternating minimization, which involve the time-consuming
calculation of full gradient at each iteration. De Sa et al.
(2014) developed a stochastic gradient descent approach
for Gaussian ensembles, but the sample complexity (i.e.,
number of measurements or observations required for exact
recovery) of their algorithm is not optimal. Recently, Jin
et al. (2016) and Zhang et al. (2017b) proposed stochastic
gradient descent algorithms for noiseless matrix completion and matrix sensing, respectively. Although these algorithms achieve linear rate of convergence and improved
computational complexity over aforementioned deterministic optimization based approaches, they are limited to specific low-rank matrix recovery problems, and unable to be
extended to more general problems and settings.
In this paper, inspired by the idea of variance reduction
for stochastic gradient (Schmidt et al., 2013; Konečnỳ &
Richtárik, 2013; Johnson & Zhang, 2013; Defazio et al.,

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

2014a;b; Mairal, 2014; Xiao & Zhang, 2014; Konečnỳ
et al., 2014; Reddi et al., 2016; Allen-Zhu & Hazan, 2016;
Chen & Gu, 2016; Zhang & Gu, 2016), we propose a unified stochastic gradient descent framework with variance
reduction for low-rank matrix recovery, which integrates
both optimization-theoretic and statistical analyses. To the
best of our knowledge, this is the first unified accelerated
stochastic gradient descent framework for low-rank matrix
recovery with strong convergence guarantees. With a desired initial estimator given by a general initialization algorithm, we show that our algorithm achieves linear convergence rate and better computational complexity against the
state-of-the-art algorithms. The contributions of our work
are further highlighted as follows:
1. We develop a generic stochastic variance-reduced gradient descent algorithm for low-rank matrix recovery, which
can be applied to various low rank-matrix estimation problems, including matrix sensing, noisy matrix completion
and one-bit matrix completion. In particular, for noisy matrix sensing, it is guaranteed to linearly converge to the unknown low-rank matrix up to the minimax statistical precision (Negahban & Wainwright, 2011; Wang et al., 2016);
while for noiseless matrix sensing, our algorithm achieves
the optimal sample complexity (Recht et al., 2010; Tu et al.,
2015; Wang et al., 2016), and attains a linear rate of convergence. Besides, for noisy matrix completion, it achieves
the best-known sample complexity required by nonconvex
matrix factorization (Zheng & Lafferty, 2016).
2. At the core of our algorithm, we construct a novel
semi-stochastic gradient term, which is substantially different from the one if following the original stochastic
variance-reduced gradient using chain rule (Johnson &
Zhang, 2013). This uniquely constructed semi-stochastic
gradient has not appeared in the literature, and is essential
for deriving the minimax optimal statistical rate.

Park et al., 2016b; Wang et al., 2016). More specifically,
to achieve ✏ precision, the gradient complexity1 of our algorithm is O (N + 2 b) log(1/✏) . Here N denotes the
total number of observations, d denotes the dimensionality
of the unknown low-rank matrix X⇤ , b denotes the batch
size, and  denotes the condition number of X⇤ (see Section 2 for a detailed definition). In particular, if the condition number satisfies   N/b, our algorithm is computationally more efficient than the state-of-the-art generic
algorithm in Wang et al. (2016).
Notation. We use [d] and Id to denote {1, 2, . . . , d} and
d⇥d identity matrix respectively. We write A> A = Id2 , if
A 2 Rd1 ⇥d2 is orthonormal. For any matrix A 2 Rd1 ⇥d2 ,
we use Ai,⇤ and A⇤,j to denote the i-th row and j-th column of A, respectively. In addition, we use Aij to denote the (i, j)-th element of A. Denote the row space and
column space of A by row(A) and col(A) respectively.
Let d = max{d1 , d2 }, and ` (A) be the `-th largest singular value of A. For vector x 2 Rd , we use kxkq =
(⌃di=1 |xi |q )1/q to denote its `q vector norm for 0 < q < 1.
Denote the spectral and Frobenius norm of A by kAk2 and
kAkF respectively. We use kAk1,1 = maxi,j |Aij | to
denote the element-wise infinity norm of A, and we use
kAk2,1 to represent the largest `2 -norm of its rows. Given
two sequences {an } and {bn }, we write an = O(bn ) if
there exists a constant 0 < C1 < 1 such that an  C1 bn .
Note that other notations are defined throughout the paper.

2. Methodology
In this section, we present our generic stochastic gradient
descent algorithm with variance reduction as well as several illustrative examples.
2.1. Stochastic Variance-Reduced Gradient for
Low-Rank Matrix Recovery

3. Our unified framework is built upon the mild restricted
strong convexity and smoothness conditions (Negahban
et al., 2009; Negahban & Wainwright, 2011) regarding the
objective function. Based on the above mentioned conditions, we derive an innovative projected notion of the restricted Lipschitz continuous gradient property, which we
believe is of independent interest for other nonconvex problems to prove sharp statistical rates. We further establish
the linear convergence rate of our generic algorithm. Besides, for each specific examples, we verify that the conditions required in the generic setting are satisfied with
high probability, which demonstrates the applicability of
our framework.

First, we briefly introduce the general problem setup for
low-rank matrix recovery. Suppose X⇤ 2 Rd1 ⇥d2 is an unknown rank-r matrix. Let the singular value decomposition
⇤
⇤>
⇤
(SVD) of X⇤ be X⇤ = U ⌃⇤ V , where U 2 Rd1 ⇥r ,
⇤
d2 ⇥r
V 2 R
are orthonormal matrices, and ⌃⇤ 2 Rr⇥r
is a diagonal matrix. Let 1
···
0 be
2
r
the sorted nonzero singular values of X⇤ , and denote the
condition number of X⇤ by , i.e.,  = 1 / r . Besides, let
⇤
⇤
U⇤ = U (⌃⇤ )1/2 and V⇤ = V (⌃⇤ )1/2 . Recall that we
aim to recover X⇤ through a collection of N observations
or measurements. Let LN : Rd1 ⇥d2 ! R be the sample
loss function, which evaluates the fitness of any matrix X
associated with the total N observations. Then the low-rank

4. Our algorithm has a lower computational complexity compared with existing approaches (Jain et al., 2013a;
Zhao et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015;

1
Gradient complexity is defined as the number of gradients
calculated in total.

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

matrix recovery problem can be formulated as follows:
PN
minX2Rd1 ⇥d2 LN (X) := N1 i=1 `i (X),
subject to X 2 C, rank(X)  r,

(2.1)

where `i (X) measures the fitness of X associated with the
i-th observation. Here C ✓ Rd1 ⇥d2 is a feasible set, such
that X⇤ 2 C. In order to more efficiently estimate the unknown low-rank matrix, following Jain et al. (2013a); Tu
et al. (2015); Zheng & Lafferty (2016); Park et al. (2016a);
Wang et al. (2016), we decompose X as UV> and consider
the following nonconvex optimization problem via matrix
factorization:
PN
minU2C1 ,V2C2 LN (UV> ) := N1 i=1 `i (UV> ), (2.2)

where C1 ✓ Rd1 ⇥r , C2 ✓ Rd2 ⇥r are the rotation-invariant
sets induced by C. Recall X⇤ can be factorized as X⇤ =
U⇤ V⇤> , then we need to make sure that U⇤ 2 C1 and
V⇤ 2 C2 . Besides, it can be seen from (2.2) that the optimal
solution is not unique in terms of rotation. In order to deal
with such identifiability issue, following Tu et al. (2015);
Zheng & Lafferty (2016); Park et al. (2016b), we consider
the following regularized optimization problem:
minU2C1 ,V2C2 FN (U, V) := LN (UV> ) + R(U, V),
where the regularization term is defined as R(U, V) =
kU> U V> Vk2F /8. We further decompose the objective
function FN (U, V) into n components to apply stochastic
variance-reduced gradient descent:
Pn
FN (U, V) := n1 i=1 Fi (U, V),
(2.3)
where we assume N = nb, and b denotes batch size, i.e.,
the number of observations associated with each Fi . More
specifically, we have
Fi (U, V) = Li (UV> ) + R(U, V),
Pb
Li (UV> ) = 1b j=1 `ij (UV> ).

(2.4)

Therefore, based on (2.3) and (2.4), we are able to apply the
stochastic variance-reduced gradient, which is displayed as
Algorithm 1. As will be seen in later theoretical analysis,
the variance of the proposed stochastic gradient indeed decreases as the iteration number increases, which leads to a
faster convergence rate. Let PCi be the projection operator
onto the feasible set Ci in Algorithm 1, where i 2 {1, 2}.
Note that our proposed Algorithm 1 is different from the
standard stochastic variance-reduced gradient algorithm
(Johnson & Zhang, 2013) in several aspects. First, instead
of conducting gradient descent directly on X, our algorithm
performs alternating stochastic gradient descent on the factorized matrices U and V, which leads to a better computational complexity but a more challenging analysis. Second,

we construct a novel semi-stochastic gradient term for U
e
e
(resp. V) as rU Fit (U, V) rLit (X)V
+ rLN (X)V,
e
e +
which is different from rU Fit (U, V) rU Fit (U, V)
e V)
e if following the original stochastic variance
rU FN (U,
reduced gradient descent (Johnson & Zhang, 2013). This
uniquely devised semi-stochastic gradient is essential for
deriving the minimax optimal statistical rate. Last but not
least, we introduce a projection step to ensure that the estimator produced at each iteration belongs to a feasible set,
which is necessary for various low-rank matrix recovery
problems. We also note that Reddi et al. (2016); Allen-Zhu
& Hazan (2016) recently developed SVRG algorithms for
general nonconvex finite-sum optimization problem. However, their algorithms only guarantee a sublinear rate of
convergence to a stationary point, and cannot exploit the
special structure of low-rank matrix factorization. In stark
contrast, our algorithm is able to leverage the structure of
the problem and guaranteed to linearly converge to the unknown low-rank matrix instead of a stationary point.
Algorithm 1 Low-Rank Stochastic Variance-Reduced Gradient Descent (LRSVRG)
Input: loss function LN ; step size ⌘; number of iterations
e 0, V
e 0 ).
S, m; initial solution (U
for: s = 1, 2, . . . , S do
e =U
e s 1, V
e =V
e s 1, X
e =U
eV
e>
U
0
0
e
e
U = U, V = V
for: t = 0, 1, 2, . . . , m 1 do
Randomly pick it 2 {1, 2, . . . , n}
Ut+1 = PC1 Ut ⌘(rU Fit (Ut , Vt )
e t + rLN (X)V
e t)
rLit (X)V
Vt+1 = PC2 Vt ⌘(rV Fit (Ut , Vt )
e > Ut + rLN (X)
e > Ut )
rLit (X)
end for
e s, V
e s ) = (Ut , Vt ), random t 2 {0, . . . , m 1}
(U
end for
e S, V
e S ).
Output: (U
Algorithm 2 Initialization
Input: loss function LN ; step size ⌧ ; iteration number T .
initialize: X0 = 0
for: t = 1, 2, 3, . . . , T do
Xt = Pr Xt 1 ⌧ rLN (Xt 1 )
end for
0
0
[U , ⌃0 , V ] = SVDr (XT )
e 0 = U0 (⌃0 )1/2 , V
e 0 = V0 (⌃0 )1/2
U
e 0, V
e 0)
Output: (U

As will be seen in later analysis, Algorithm 1 requires a
good initial solution to guarantee the linear convergence
rate. To obtain such an initial solution, we employ the initialization algorithm in Algorithm 2, which is originally
proposed in Wang et al. (2016). For any rank-r matrix
X 2 Rd1 ⇥d2 , we use SVDr (X) to denote its singular

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

value decomposition. If SVDr (X) = [U, ⌃, V], we use
Pr (X) = U⌃V> to denote the best rank-r approximation
of X, or in other words, Pr denotes the projection operator
such that Pr (X) = argminrank(Y)r kX YkF .
2.2. Applications to Specific Models
In this subsection, we introduce two examples, which include matrix sensing and matrix completion, to illustrate
the applicability of our proposed algorithm (Algorithm 1).
The application of our algorithm to one-bit matrix completion can be found in Appendix A. To apply the proposed
method, we only need to specify the form of FN (U, V)
for each specific model, as defined in (2.3).
2.2.1. M ATRIX S ENSING
In matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), we intend to recover the unknown matrix
X⇤ 2 Rd1 ⇥d2 with rank-r from a set of noisy linear measurements such that y = A(X⇤ ) + ✏, where the linear
measurement operator A : Rd1 ⇥d2 ! RN is defined
as A(X) = (hA1 , Xi, hA2 , Xi, . . . , hAN , Xi)> , for any
X 2 Rd1 ⇥d2 . Here N denotes the number of observations,
and ✏ represents a sub-Gaussian noise vector with i.i.d. elements and parameter ⌫. In addition, for each sensing matrix Ai 2 Rd1 ⇥d2 , it has i.i.d. standard Gaussian entries.
Therefore, we formulate FN (U,
Pn V) for matrix sensing as
follows FN (U, V) = n 1 i=1 FSi (U, V), where for
each component function, we have FSi (U, V) = kySi
ASi (UV> )k22 /(2b) + R(U, V). Note that R(U, V) denotes the regularizer, which is defined in Section 2.1. In
addition, {Si }ni=1 denote the mutually disjoint subsets such
that [ni=1 Si = [N ], and ASi is defined as a linear measurement operator ASi : Rd1 ⇥d2 ! Rb , satisfying ASi (X) =
(hAi1 , Xi, hAi2 , Xi, . . . , hAib , Xi)> , with corresponding
observations ySi = (yi1 , yi2 , . . . , yib )> .
2.2.2. M ATRIX C OMPLETION
For matrix completion with noisy observations (Rohde
et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012), our primary goal is to recover the unknown
low-rank matrix X⇤ 2 Rd1 ⇥d2 from a set of randomly observed noisy elements. For example, one commonly-used
model is the uniform observation model, which is defined
as follows:
⇢ ⇤
Xjk + Zjk , with probability p,
Yjk :=
⇤,
otherwise,
where Z 2 Rd1 ⇥d2 is a noise matrix such that each
element Zjk follows i.i.d. Gaussian distribution with
variance ⌫ 2 /(d1 d2 ), and we call Y 2 Rd1 ⇥d2 the observation matrix. In particular, we observe each elements independently with probability p 2 (0, 1). We

denote ⌦ ✓ [d1 ] ⇥ [d2 ] by the index set of the observed entries, then F⌦ (U, V) P
for matrix completion is
n
formulated as F⌦ (U, V) = n 1 i=1 F⌦Si (U, V), where
each component function is defined as F⌦Si (U, V) =
P
>
Yjk )2 /(2b) + R(U, V). Note that
(j,k)2⌦Si (Uj⇤ Vk⇤
{⌦Si }ni=1 denote the mutually disjoint subsets such that
[ni=1 ⌦Si = ⌦. In addition, we have |⌦Si | = b for
i = 1, . . . , n such that |⌦| = nb.

3. Main Theory
In this section, we present our main theoretical results
for Algorithms 1 and 2. We first introduce several definitions for simplicity. Recall that the singular value de⇤
⇤>
composition of X⇤ is X⇤ = U ⌃⇤ V , then following Tu et al. (2015); Zheng & Lafferty (2016), we define
Y⇤ 2 R(d1 +d2 )⇥(d1 +d2 ) as the corresponding lifted positive semidefinite matrix of X⇤ 2 Rd1 ⇥d2 in higher dimension
 ⇤ ⇤>
U U
U⇤ V⇤>
Y⇤ =
= Z⇤ Z⇤> ,
⇤ ⇤>
V U
V⇤ V⇤>
⇤

⇤

where U⇤ = U (⌃⇤ )1/2 , V⇤ = V (⌃⇤ )1/2 , and Z⇤ is
defined as Z⇤ = [U⇤ ; V⇤ ] 2 R(d1 +d2 )⇥r . Besides, we
define the solution set in terms of the true parameter Z⇤ as
follows:
n
o
Z = Z 2 R(d1 +d2 )⇥r Z = Z⇤ R for some R 2 Qr ,
where Qr denotes the set of r ⇥ r orthonormal matrices.
According to this definition, for any Z 2 Z, we can obtain
X⇤ = Z U Z >
V , where ZU and ZV denote the top d1 ⇥ r and
bottom d2 ⇥ r matrices of Z 2 R(d1 +d2 )⇥r respectively.
Definition 3.1. Define the distance between Z and Z⇤ in
terms of the optimal rotation as d(Z, Z⇤ ) such that
d(Z, Z⇤ ) = min kZ
e
Z2Z

e F = min kZ
Zk
R2Qr

Z⇤ RkF .

p
Note that if d(Z, Z⇤ ) 
X ⇤ kF 
1 , we have kX
p
⇤
c 1 d(Z, Z ), where c is a constant (Yi et al., 2016).
Definition 3.2. Define the neighbourhood of Z⇤ with radius R as
n
o
B(R) = Z 2 R(d1 +d2 )⇥r d(Z, Z⇤ )  R .

Next, we lay out several conditions, which are essential for
proving our main theory. We impose restricted strong convexity (RSC) and smoothness (RSS) conditions (Negahban
et al., 2009; Loh & Wainwright, 2013) on the sample loss
function LN .
Condition 3.3 (Restricted Strong Convexity). Assume LN
is restricted strongly convex with parameter µ, such that for
all matrices X, Y 2 Rd1 ⇥d2 with rank at most 3r
µ
LN (Y) LN (X) + hrLN (X), Y Xi + kY Xk2F .
2

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Condition 3.4 (Restricted Strong Smoothness). Assume
LN is restricted strongly smooth with parameter L, such
that for all matrices X, Y 2 Rd1 ⇥d2 with rank at most 3r
LN (Y)  LN (X) + hrLN (X), Y

Xi +

L
kY
2

Xk2F .

Based on Conditions 3.3 and 3.4, we prove that the sample loss function LN satisfies a projected notion of the
restricted Lipschitz continuous gradient property as displayed in the following lemma.
Lemma 3.5. Suppose the sample loss function LN satisfies Conditions 3.3 and 3.4. For any rank-r matrices
X, Y 2 Rd1 ⇥d2 , let the singular value decomposition of
>
X be U1 ⌃1 V1 , then we have
LN (X)

LN (Y) + hrLN (Y), X Yi
1 e>
+
kU (rLN (X) rLN (Y))k2F
4L
1
e 2 ,
+
k(rLN (X) rLN (Y))Vk
F
4L

e 2 Rd1 ⇥r1 is an orthonormal matrix with r1  3r
where U
e and V
e 2 Rd2 ⇥r2 is an
which satisfies col(U1 ) ✓ col(U),
orthonormal matrix with r2  3r that satisfies col(V1 ) ✓
e and L is the RSS parameter.
col(V),
Lemma 3.5 is essential to analyze the nonconvex optimization for low-rank matrix recovery and derive a linear convergence rate. Since the RSC and RSS conditions can
only be verified over the subspace of low-rank matrices,
the standard Lipschitz continuous gradient property could
not be derived. That is why we need such a restricted version of Lipschitz continuous gradient property. To the best
of our knowledge, this new notion of Lipschitz continuous
gradient has never been proposed in the literature before.
We believe it can be of broader interests for other nonconvex optimization problems to prove tight bounds.
Moreover, we assume that the gradient of the sample loss
function rLN at X⇤ is upper bounded.
Condition 3.6. Recall the unknown rank-r matrix X⇤ 2
Rd1 ⇥d2 . Given a fixed sample size N and tolerance parameter 2 (0, 1), we let ✏(N, ) be the smallest scalar such
that with probability at least 1
, we have
krLN (X⇤ )k2  ✏(N, ),
where ✏(N, ) depends on sample size N and .
Finally, we assume that each component loss function Li in
(2.4) satisfies the restricted strong smoothness condition.
Condition 3.7 (Restricted Strong Smoothness for each
Component). Given a fixed batch size b, assume Li is restricted strongly smooth with parameter L0 , such that for

all matrices X, Y 2 Rd1 ⇥d2 with rank at most 3r
Li (Y)  Li (X) + hrLi (X), Y

Xi +

L0
kY
2

Xk2F .

In latter analysis for generic setting, we assume that Conditions 3.3-3.7 hold, while for each specific model, we will
verify these conditions respectively in the appendix.
3.1. Results for the Generic Setting
The following theorem shows that, in general, Algorithm 1
converges linearly to the unknown low-rank matrix X⇤ up
to a statistical precision.
Theorem 3.8 (LRSVRG). Suppose that Conditions 3.3,
3.4, 3.6, and 3.7 are satisfied. There exist constants
e0
e0 e0
c1 , c2 , c3 and c4 such that for any
p Z = [U ; V ] 2
p
B(c2 r ) with c2  min{1/4, 2µ0 /(5(3L + 1))}, if
the sample size N is large enough such that ✏2 (N, ) 
c22 (1 ⇢)µ0 r2 /(c3 r), where µ0 = min{µ, 1}, and the contraction parameter ⇢ is defined as follows:
✓
◆
10
1
02
⇢= 0
+ c4 ⌘ 1 L ,
µ
⌘m 1
then with the step size ⌘ = c1 / 1 and the number of itere S = [U
e S; V
e S]
ations m properly chosen, the estimator Z
outputed from Algorithm 1 satisfies
2
⇥
⇤
e S , Z ⇤ )  ⇢S d 2 ( Z
e 0 , Z⇤ ) + c3 r✏ (N, ) , (3.1)
E d2 ( Z
(1 ⇢)µ0 r

with probability at least 1

.

Remark 3.9. Theorem 3.8 implies that to achieve linear
rate of convergence, it is necessary to set the step size ⌘
to be small enough and the inner loop iterations m to be
large enough such that ⇢ < 1. Here we present a specific
example to demonstrate such ⇢ is attainable. As stated in
Theorem 3.8, if we set the step size ⌘ = c01 / 1 , where
c01 = µ0 / 15c4 L02 , then the contraction parameter ⇢ is
calculated as follows:
⇢=

10
mµ0 ⌘

1

2
+ .
3

Therefore, under the condition that m
c5 2 , we obtain
⇢  5/6 < 1, which leads to the linear convergence rate
of Algorithm 1. Besides, our algorithm also achieves the
linear convergence in terms of reconstruction error, since
e s X⇤ k2 can be upper bounded
the reconstruction error kX
F
e s , Z⇤ ), where C is a constant.
by C 1 · d2 (Z
Remark 3.10. The right hand side of (3.1) consists of two
parts, where the first one represents the optimization error and the second one denotes the statistical error. Note
that in the noiseless case, since ✏(N, ) = 0, the statistical error becomes zero. As stated in Remark 3.9, with

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

appropriate ⌘ and m, we are able to achieve the linear rate
of convergence. Therefore, in order to make sure the ope 0 , Z⇤ )  ✏, it suffices to
timization error satisfies ⇢S d2 (Z
perform S = O log(1/✏) outer loop iterations. Recall
that from Remark 3.9 we have m = O(2 ). Since for
each outer loop iteration, it is required to calculate m mixed
stochastic variance-reduced gradients and one full gradient,
the overall gradient complexity for our algorithm to achieve
✏ precision is
⇣
⇣ 1 ⌘⌘
.
O (N + 2 b) log
✏
However, the gradient complexity of the state-of-the-art
gradient descent based algorithm (Wang et al., 2016) to
achieve ✏ precision is O N  log(1/✏) . Therefore, provided that   n, our method is computationally more efficient than the state-of-the-art gradient descent approach.
The detailed comparison of the overall computational complexity among different methods for each specific model
can be found in next subsection.
e 0 2 B(c2 p r ) in Theorem
To satisfy the initial condition Z
3.8, according to Lemma 5.14 in Tu et al. (2015), it suffices
e 0 is close enough to the unknown rankto guarantee that X
⇤
e 0 X⇤ kF  c r , where c 
r matrix X such that kX
min{1/2, 2c2 }. The following theorem shows the output
of Algorithm 2 can satisfy this condition.
Theorem 3.11. (Wang et al., 2016) Suppose the sample
loss function LN satisfies Conditions 3.3, 3.4 and 3.6. Let
e0 = U
e 0V
e 0> , where (U
e 0, V
e 0 ) is the produced initial soX
lution in Algorithm 2. If L/µ 2 (1, 4/3), then with step
size ⌧ = 1/L, we have with probability at least 1
that
p
e 0 X⇤ kF  ⇢T kX⇤ kF + 2 3r✏(N, ) ,
kX
L(1 ⇢)
p
where ⇢ = 2 1 µ/L is the contraction parameter.

e0
Theorem 3.11 suggests that, in order to guarantee kX
⇤
X kF  c r , we need to perform at least T =
log(c0 r /kX⇤ kF )/ log(⇢) number of iterations to ensure
the optimization error is small enough, and it is also necessary to make sure the sample size
pN is large enough such
that ✏(N, )  c0 L(1 ⇢) r / 2 3r , which corresponds
to a sufficiently small statistical error.
3.2. Implications for Specific Models
In this subsection, we demonstrate the implications of our
generic theory to specific models. For each specific model,
we only need to verify Conditions 3.3-3.7. We denote d =
max{d1 , d2 } in the following discussions.
3.2.1. M ATRIX S ENSING
We provide the theoretical guarantee of our algorithm for
matrix sensing.

Corollary 3.12. Consider matrix sensing with standard
normal linear operator A and noise vector ✏, whose entries
follow i.i.d. sub-Gaussian distribution with parameter ⌫.
There exist constants {ci }8i=1 such that if the number of
observations satisfies N c1 rd and we choose the parameters ⌘ = c2 / 1 , where c2 = µ0 / c3  , m
c4 2 , then
p
0
e 2 B(c5 r ), with probfor any initial solution satisfies Z
ability at least 1 c6 exp
c7 d , the output of Algorithm
1 satisfies
⇥
⇤
e S , Z ⇤ )  ⇢S d 2 ( Z
e 0 , Z⇤ ) + c8 ⌫ 2 rd ,
E d2 ( Z
N

(3.2)

where the contraction parameter ⇢ < 1.

Remark 3.13. According to (3.2), in the
p noisy setting,
the output of our algorithm achieves O rd/N statistical error after O log(N/(rd)) number of outer loop iterations. This statistical error matches the minimax lower
bound for matrix sensing (Negahban & Wainwright, 2011).
In the noiseless case, to ensure the restricted strong convexity and smoothness conditions of our objective function, we require sample size N = O(rd), which attains
the optimal sample complexity for matrix sensing (Recht
et al., 2010; Tu et al., 2015; Wang et al., 2016). Most importantly, from Remark 3.10 we know that for the output
e S of our algorithm, the overall computational complexity
Z
of our algorithm to achieve ✏ precision for matrix sensing
is O (N d2 + 2 bd2 ) log(1/✏) . Nevertheless, the overall
computational complexity for the state-of-the-art gradient
descent algorithms in both noiseless (Tu et al., 2015) and
noisy (Wang et al., 2016) cases to obtain ✏ precision is
O N d2 log(1/✏) . Therefore, our algorithm is more efficient provided that   n, which is consistent with the
result obtained by (Zhang et al., 2017b). In their work,
they proposed an accelerated stochastic gradient descent
method for matrix sensing based on the restricted isometry property. However, since the restricted isometry property is more restrictive than the restricted strong convex and
smoothness conditions, their results cannot be applied to
more general low-rank matrix recovery problems.
3.2.2. M ATRIX C OMPLETION
We provide the theoretical guarantee of our algorithm for
matrix completion. In particular, we consider a partial observation model, which means only the elements over a
subset X ✓ [d1 ] ⇥ [d2 ] are observed. In addition, we assume a uniform sampling model for X , which is defined
as 8(j, k) 2 X , j ⇠ uniform([d1 ]), k ⇠ uniform([d2 ]).
To avoid overly sparse matrices (Gross, 2011; Negahban
& Wainwright, 2012), we impose the following incoherence condition (Candès & Recht, 2009). More specifically, suppose the singular value decomposition of X⇤
⇤
⇤>
is X⇤ = U ⌃⇤ V , wepassume the following condi⇤
⇤
tions hold kU k2,1 
r/d1 and kV k2,1 

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

p

r/d2 , where r denotes the rank of X⇤ , and
the incoherence parameter for X⇤ .

denotes

In order to make sure our produced estimator satisfies incoherence constraint, we need a projection step, which is
displayed in Algorithm 1. Therefore, we construct two feasible sets Ci = {A 2 Rdi ⇥r kAk2,1  ↵i }, where
p
↵i =
r 1 /di , and i 2 {1, 2}. Thus for any U 2 C1
and V 2 C2 , we have X = UV> 2 p
C = {A 2
Rd1 ⇥d2 kAk1,1  ↵}, where ↵ = r 1 / d1 d2 .

We have the following convergence result of our algorithm
for matrix completion.
Corollary 3.14. Consider noisy matrix completion under
uniform sampling model. Suppose X⇤ satisfies incoherence condition. There exist constants {ci }7i=1 such that if
we choose parameters ⌘ = c1 / 1 , where c1 = µ0 / c2  ,
m
c3 2 , and the number of observations satisfies
N
c4 r2 d log d, then for any initial solution satisfies
p
0
e
Z 2 B(c5 r ), then with probability at least 1 c6 /d,
the output of Algorithm 1 satisfies
⇤
e S , Z ⇤ )  ⇢S d 2 ( Z
e 0 , Z⇤ ) + c7 rd log d , (3.3)
E[d2 (Z
N
2
2 2
where
= max{⌫ , r
1 }, the contraction parameter
⇢ < 1.
Remark 3.15. Corollary 3.14 implies that after
O log(N/(r2 d log d)) number of outer loops, our
p
algorithm achieves O r d log d/N statistical error,
which is near
p optimal compared with the minimax lower
bound O( rd log d/N ) for matrix completion proved in
Negahban & Wainwright (2012); Koltchinskii et al. (2011).
And its sample complexity is O(r2 d log d), which matches
the best-known sample complexity of matrix completion
using nonconvex matrix factorization (Zheng & Lafferty,
2016). Recall that from Remark 3.10, the overall computational complexity of our algorithm to reach ✏ accuracy
for matrix completion is O (N + 2 b)r3 d log(1/✏) .
However, for the state-of-the-art gradient descent based
algorithms, the computational complexity for both noiseless (Zheng & Lafferty, 2016) and noisy (Wang et al.,
2016) cases to obtain ✏ accuracy is O N r3 d log(1/✏) .
Thus the computational complexity of our algorithm is
lower than the state-of-the-art gradient descent methods
if we have   n. In addition, for the online stochastic
gradient descent algorithm (Jin et al., 2016), the overall
computational complexity is O(r4 4 d log(1/✏)). Since
their results has a fourth power dependency on both r and
, our method can yield a significant improvement over
the online method when r,  is large.

4. Experiments
In this section, we present the experimental performance
of our proposed algorithm for different models based on

numerical simulations and real data experiments.
4.1. Numerical Simulations
We first investigate the effectiveness of our proposed algorithm compared with the state-of-the-art gradient descent
algorithm (Wang et al., 2016; Zheng & Lafferty, 2016).
Then, we evaluate the sample complexity required by both
methods to achieve exact recovery in the noiseless case.
Finally, we illustrate the statistical error of our method in
the noisy case. Note that both algorithms use the same initialization method (Algorithm 2) with optimal parameters
selected by cross validation. Furthermore, all results are
averaged over 30 trials. Note that due to the space limit,
we only lay out simulation results for matrix completion,
results for other models can be found in Appendix A.
For matrix completion, we consider the unknown low-rank
matrix X⇤ in the following settings: (i) d1 = 100, d2 =
80, r = 2; (ii) d1 = 120, d2 = 100, r = 3; (iii) d1 =
140, d2 = 120, r = 4. First, we generate the unknown lowrank matrix X⇤ as X⇤ = U⇤ V⇤> , where U⇤ 2 Rd1 ⇥r and
V⇤ 2 Rd2 ⇥r are randomly generated. Next, we use uniform observation model to obtain data matrix Y. Finally,
we consider two settings: (1) noisy case: the noise follows
i.i.d. normal distribution with variance 2 = 0.25 and (2)
noiseless case.
For the results of convergence rate, we show the mean
b X⇤ k2 /(d1 d2 ) in log scale versus numsquared error kX
F
ber of effective data passes. Figures 1(a) and 1(c) illustrate
the linear rate of convergence of our algorithm (LRSVRG)
in the setting (i). The results imply that after the same number of effective data passes, our algorithm is more efficient
than the state-of-the-art gradient descent algorithm in estimation error. For the results of sample complexity, we
illustrate the empirical probability of exact recovery under
b
rescaled sample size N/(rd log d). For the estimator X
given by different algorithms, it is considered to achieve exb X⇤ kF /kX⇤ kF is less
act recovery, if the relative error kX
3
than 10 . Figure 1(b) shows the empirical recovery probability of different methods in the setting (i). It implies a
phase transition around N = 3rd log d. Although our theoretical results requires O(r2 d log d) sample complexity,
the simulation results suggest that our method achieves the
optimal sample complexity N = O(rd log d). Note that
we leave out results in other settings to avoid redundancy
since we get similar patterns for these results. The results
of statistical error are displayed in Figure 1(d), which is
consistent with our main result in Corollary 3.14.
4.2. Real Data Experiments
We apply our proposed stochastic variance-reduced gradient algorithm for matrix completion to collaborative filtering in recommendation system, and compare it with sev-

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery
Table 1. Experimental results of collaborative filtering in terms of averaged RMSE and CPU time for different algorithms.
Performance

SVP

SOFTIMPUTE

ALTMIN

TNC

RIMP

NUCLEAR

SCAD

GD

LRSVRG

RMSE

4.7318

5.1211

4.8562

4.4803

4.3401

4.6910

4.1733

4.1832

4.1605

Time (s)

18.71

161.08

11.55

29.63

1.92

192.15

197.52

1.01

0.81

JESTER1
JESTER2
JESTER3

RMSE

4.7712

5.1523

4.8712

4.4511

4.3721

4.5597

4.2016

4.2177

4.1909

Time (s)

16.94

152.82

10.68

28.81

1.75

166.94

171.31

0.96

0.71

RMSE

8.7439

5.4532

9.5230

4.6712

4.9803

5.1231

4.6777

4.6867

4.6247

Time (s)

16.69

10.82

12.57

12.84

0.95

94.88

253.73

0.86

0.54

0

GD
LRSVRG

-2

1

0.15

0

GD
LRSVRG

0.8

-0.5

0.6

-4

-1

0.4

-6
-8

10

20

30

0

0.1

0.05

-1.5

0.2

0

d1=140,d 2=120,r=4
d1=120,d 2=100,r=3
d1=100,d 2=80,r=2

GD
LRSVRG
Frob.norm

Dataset

0

2

4

6

8

10

-2

0

10

20

30

0

1

2

3

4

5

6

7

N=rd log d

(a) Noiseless Case

(b) Sample Complexity

(c) Noisy Case

(d) Statistical Error

Figure 1. Numerical results for matrix completion. (a) and (c) Rate of convergence for matrix completion in the noiseless and noisy
b X⇤ k2F /(d1 d2 ) versus number of effective data passes, which demonstrates the
case, respectively: logarithm of mean squared error kX
effectiveness of our method; (b) Empirical probability of exact recovery versus N/(rd log d); (d) Statistical error for matrix completion:
b X⇤ k2F /(d1 d2 ) versus rescaled sample size N/(rd log d).
mean squared error in Frobenius norm kX

eral state-of-the-art matrix completion algorithms, including singular value projection (SVP) (Jain et al., 2010), trace
norm constraint (TNC) (Jaggi et al., 2010), alternating minimization (AltMin) (Jain et al., 2013b), spectral regularization (SoftImpute) (Mazumder et al., 2010), rank-one matrix pursuit (Wang et al., 2014), nuclear norm penalty (Negahban & Wainwright, 2011), nonconvex SCAD penalty
(Gui & Gu, 2015) and gradient descent (Zheng & Lafferty, 2016). In particular, we use three large recommendation datasets called Jester1, Jester2 and Jester3 (Goldberg et al., 2001), which contain anonymous ratings on
100 jokes from different users. The jester datasets consist
of {24983, 23500, 24938} rows and 100 columns respectively, with {106 , 106 , 6 ⇥ 105 } ratings correspondingly.
Besides, the rating scales take value from [ 10, 10]. Our
goal is to recover the whole rating matrix based on partial observations. Therefore, we randomly choose half of
the ratings as our observed data, and predict the other half
based on different matrix completion algorithms. We perform 10 different observed/unobserved entry splittings, and
record the averaged root mean square error (RMSE) as well
as CPU time for different algorithms. We summarize the
comparisons in Table 1, which suggests that our proposed
LRSVRG algorithm outperforms all the other baseline algorithms in terms of RMSE and CPU time, which aligns
well with our theory.

5. Conclusions and Future Work

We proposed a unified stochastic variance-reduced gradient descent framework for low-rank matrix recovery that
integrates both optimization-theoretic and statistical analyses. Based on the mild restricted strong convexity and
smoothness conditions, we derived a projected notion of
the restricted Lipschitz continuous gradient property, and
established the linear convergence rate of our proposed algorithm. With an appropriate initialization procedure, we
proved that our algorithm enjoys improved computational
complexity compared with existing approaches. There are
still many interesting problems along this line of research.
For example, we will study accelerating the low-rank plus
sparse matrix/tensor recovery (Gu et al., 2014; 2016; Yi
et al., 2016; Zhang et al., 2017a) through variance reduction technique in the future.

Acknowledgment
We would like to thank the anonymous reviewers for their
helpful comments. This research was sponsored in part
by the National Science Foundation IIS-1618948 and IIS1652539. The views and conclusions contained in this paper are those of the authors and should not be interpreted
as representing any funding agencies.

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

References
Agarwal, Alekh, Negahban, Sahand, and Wainwright, Martin J.
Fast global convergence rates of gradient methods for highdimensional statistical recovery. In Advances in Neural Information Processing Systems, pp. 37–45, 2010.
Allen-Zhu, Zeyuan and Hazan, Elad. Variance reduction for faster
non-convex optimization. arXiv preprint arXiv:1603.05643,
2016.
Bhaskar, Sonia A and Javanmard, Adel. 1-bit matrix completion under exact low-rank constraint. In Information Sciences
and Systems (CISS), 2015 49th Annual Conference on, pp. 1–6.
IEEE, 2015.
Bhojanapalli, Srinadh, Kyrillidis, Anastasios, and Sanghavi, Sujay. Dropping convexity for faster semi-definite optimization.
arXiv preprint, 2015.
Cabral, Ricardo Silveira, De la Torre, Fernando, Costeira,
João Paulo, and Bernardino, Alexandre. Matrix completion
for multi-label image classification. In NIPS, volume 201, pp.
2, 2011.
Cai, Tony and Zhou, Wen-Xin. A max-norm constrained minimization approach to 1-bit matrix completion. Journal of Machine Learning Research, 14(1):3619–3647, 2013.
Candès, Emmanuel J and Recht, Benjamin. Exact matrix completion via convex optimization. Foundations of Computational
mathematics, 9(6):717–772, 2009.
Candès, Emmanuel J and Tao, Terence. The power of convex
relaxation: Near-optimal matrix completion. Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.
Chen, Jinghui and Gu, Quanquan. Accelerated stochastic block
coordinate gradient descent for sparsity constrained nonconvex
optimization. In Conference on Uncertainty in Artificial Intelligence, 2016.
Chen, Yudong and Wainwright, Martin J. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.
Davenport, Mark A, Plan, Yaniv, van den Berg, Ewout, and Wootters, Mary. 1-bit matrix completion. Information and Inference,
3(3):189–223, 2014.
De Sa, Christopher, Olukotun, Kunle, and Ré, Christopher.
Global convergence of stochastic gradient descent for some
non-convex matrix problems. arXiv preprint arXiv:1411.1134,
2014.

Gross, David. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory,
57(3):1548–1566, 2011.
Gu, Quanquan, Gui, Huan, and Han, Jiawei. Robust tensor decomposition with gross corruption. In Advances in Neural Information Processing Systems, pp. 1422–1430, 2014.
Gu, Quanquan, Wang, Zhaoran Wang, and Liu, Han. Low-rank
and sparse structure pursuit via alternating minimization. In
Artificial Intelligence and Statistics, pp. 600–609, 2016.
Gui, Huan and Gu, Quanquan. Towards faster rates and oracle property for low-rank matrix estimation. arXiv preprint
arXiv:1505.04780, 2015.
Hardt, Moritz. Understanding alternating minimization for matrix
completion. In FOCS, pp. 651–660. IEEE, 2014.
Hardt, Moritz and Wootters, Mary. Fast matrix completion without the condition number. In COLT, pp. 638–678, 2014.
Hardt, Moritz, Meka, Raghu, Raghavendra, Prasad, and Weitz,
Benjamin. Computational limits for matrix completion. In
COLT, pp. 703–725, 2014.
Jaggi, Martin, Sulovsk, Marek, et al. A simple algorithm for
nuclear norm regularized problems. In Proceedings of the
27th International Conference on Machine Learning (ICML10), pp. 471–478, 2010.
Jain, Prateek and Netrapalli, Praneeth. Fast exact matrix completion with finite samples. arXiv preprint, 2014.
Jain, Prateek, Meka, Raghu, and Dhillon, Inderjit S. Guaranteed
rank minimization via singular value projection. In Advances
in Neural Information Processing Systems, pp. 937–945, 2010.
Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Sujay. Lowrank matrix completion using alternating minimization. In
STOC, pp. 665–674, 2013a.
Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Sujay. Lowrank matrix completion using alternating minimization. In Proceedings of the forty-fifth annual ACM symposium on Theory
of computing, pp. 665–674. ACM, 2013b.
Jin, Chi, Kakade, Sham M, and Netrapalli, Praneeth. Provable
efficient online matrix completion via non-convex stochastic
gradient descent. arXiv preprint arXiv:1605.08370, 2016.
Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances in
Neural Information Processing Systems, pp. 315–323, 2013.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. Saga:
A fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in Neural
Information Processing Systems, pp. 1646–1654, 2014a.

Keshavan, Raghunandan H, Oh, Sewoong, and Montanari, Andrea. Matrix completion from a few entries. In 2009 IEEE
International Symposium on Information Theory, pp. 324–328.
IEEE, 2009.

Defazio, Aaron J, Caetano, Tibério S, and Domke, Justin. Finito:
A faster, permutable incremental gradient method for big data
problems. In Proceedings of the International Conference on
Machine Learning, 2014b.

Keshavan, Raghunandan H, Montanari, Andrea, and Oh, Sewoong. Matrix completion from noisy entries. Journal of Machine Learning Research, 11(Jul):2057–2078, 2010.

Goldberg, Ken, Roeder, Theresa, Gupta, Dhruv, and Perkins,
Chris. Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval, 4(2):133–151, 2001.

Koltchinskii, Vladimir, Lounici, Karim, Tsybakov, Alexandre B,
et al. Nuclear-norm penalization and optimal rates for noisy
low-rank matrix completion. The Annals of Statistics, 39(5):
2302–2329, 2011.

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery
Konečnỳ, Jakub and Richtárik, Peter. Semi-stochastic gradient
descent methods. arXiv:1312.1666, 2013.
Konečnỳ, Jakub, Liu, Jie, Richtárik, Peter, and Takáč, Martin.
ms2gd: Mini-batch semi-stochastic gradient descent in the
proximal setting. arXiv:1410.4744, 2014.
Loh, Po-Ling and Wainwright, Martin J. Regularized mestimators with nonconvexity: Statistical and algorithmic theory for local optima. In Advances in Neural Information Processing Systems, pp. 476–484, 2013.
Mairal, Julien. Incremental majorization-minimization optimization with application to large-scale machine learning.
arXiv:1402.4419, 2014.
Mazumder, Rahul, Hastie, Trevor, and Tibshirani, Robert. Spectral regularization algorithms for learning large incomplete matrices. Journal of machine learning research, 11(Aug):2287–
2322, 2010.
Negahban, Sahand and Wainwright, Martin J. Estimation of
(near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, pp. 1069–1097, 2011.
Negahban, Sahand and Wainwright, Martin J. Restricted strong
convexity and weighted matrix completion: Optimal bounds
with noise. Journal of Machine Learning Research, 13(May):
1665–1697, 2012.
Negahban, Sahand, Yu, Bin, Wainwright, Martin J, and Ravikumar, Pradeep K. A unified framework for high-dimensional
analysis of m-estimators with decomposable regularizers. In
Advances in Neural Information Processing Systems, pp.
1348–1356, 2009.

Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
arXiv:1309.2388, 2013.
Srebro, Nathan, Rennie, Jason, and Jaakkola, Tommi S.
Maximum-margin matrix factorization. In Advances in neural
information processing systems, pp. 1329–1336, 2004.
Sun, Ruoyu and Luo, Zhi-Quan. Guaranteed matrix completion
via nonconvex factorization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pp. 270–
289. IEEE, 2015.
Tu, Stephen, Boczar, Ross, Soltanolkotabi, Mahdi, and Recht,
Benjamin. Low-rank solutions of linear matrix equations via
procrustes flow. arXiv preprint arXiv:1507.03566, 2015.
Vershynin, R. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Wang, Lingxiao, Zhang, Xiao, and Gu, Quanquan. A unified computational and statistical framework for nonconvex low-rank
matrix estimation. arXiv preprint arXiv:1610.05275, 2016.
Wang, Zheng, Lai, Ming-Jun, Lu, Zhaosong, Fan, Wei, Davulcu,
Hasan, and Ye, Jieping. Rank-one matrix pursuit for matrix
completion. In ICML, pp. 91–99, 2014.
Xiao, Lin and Zhang, Tong. A proximal stochastic gradient
method with progressive variance reduction. SIAM Journal on
Optimization, 24(4):2057–2075, 2014.
Xu, Miao, Jin, Rong, and Zhou, Zhi-Hua. Speedup matrix completion with side information: Application to multi-label learning. In Advances in Neural Information Processing Systems,
pp. 2301–2309, 2013.

Ni, Renkun and Gu, Quanquan. Optimal statistical and computational rates for one bit matrix completion. In Proceedings
of the 19th International Conference on Artificial Intelligence
and Statistics, pp. 426–434, 2016.

Yi, Xinyang, Park, Dohyung, Chen, Yudong, and Caramanis,
Constantine. Fast algorithms for robust pca via gradient descent. In Advances in Neural Information Processing Systems,
pp. 4152–4160, 2016.

Park, Dohyung, Kyrillidis, Anastasios, Bhojanapalli, Srinadh,
Caramanis, Constantine, and Sanghavi, Sujay. Provable burermonteiro factorization for a class of norm-constrained matrix
problems. stat, 1050:1, 2016a.

Zhang, Aston and Gu, Quanquan. Accelerated stochastic block
coordinate descent with optimal sampling. In Proceedings of
the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 2035–2044. ACM, 2016.

Park, Dohyung, Kyrillidis, Anastasios, Caramanis, Constantine, and Sanghavi, Sujay. Finding low-rank solutions to
matrix problems, efficiently and provably. arXiv preprint
arXiv:1606.03168, 2016b.
Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.
Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Póczós, Barnabás,
and Smola, Alex. Stochastic variance reduction for nonconvex
optimization. arXiv preprint arXiv:1603.06160, 2016.
Rennie, Jasson DM and Srebro, Nathan. Fast maximum margin
matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning,
pp. 713–719. ACM, 2005.
Rohde, Angelika, Tsybakov, Alexandre B, et al. Estimation of
high-dimensional low-rank matrices. The Annals of Statistics,
39(2):887–930, 2011.

Zhang, Xiao, Wang, Lingxiao, and Gu, Quanquan. A nonconvex free lunch for low-rank plus sparse matrix recovery. arXiv
preprint arXiv:1702.06525, 2017a.
Zhang, Xiao, Wang, Lingxiao, and Gu, Quanquan. Stochastic variance-reduced gradient descent for low-rank matrix recovery from linear measurements. arXiv preprint
arXiv:1701.00481, 2017b.
Zhao, Tuo, Wang, Zhaoran, and Liu, Han. A nonconvex optimization framework for low rank matrix estimation. In Advances in
Neural Information Processing Systems, pp. 559–567, 2015.
Zheng, Qinqing and Lafferty, John. A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements. In Advances in
Neural Information Processing Systems, pp. 109–117, 2015.
Zheng, Qinqing and Lafferty, John. Convergence analysis for
rectangular matrix completion using burer-monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051,
2016.

