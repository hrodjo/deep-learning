Variational Inference for Sparse and Undirected Models

John Ingraham 1 Debora Marks 1

Abstract
Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for
inference would be favorable in these contexts,
they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we
develop a framework for scalable Bayesian inference of discrete undirected models based on
two new methods. The first is Persistent VI,
an algorithm for variational inference of discrete
undirected models that avoids doubly intractable
MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under
sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations.
We find that, together, these methods for variational inference substantially improve learning of
sparse undirected graphical models in simulated
and real problems from physics and biology.

1. Introduction
Hierarchical priors that favor sparsity have been a central
development in modern statistics and machine learning,
and find widespread use for variable selection in biology,
engineering, and economics. Among the most widely used
and successful approaches for inference of sparse models
has been L1 regularization, which, after introduction in
the context of linear models with the LASSO (Tibshirani,
1996), has become the standard tool for both directed and
undirected models alike (Murphy, 2012).
Despite its success, however, L1 is a pragmatic compromise. As the closest convex approximation of the idealized
1

Harvard Medical School, Boston, Massachusetts. Correspondence to: John Ingraham <ingraham@fas.harvard.edu>, Debora
Marks <debbie@hms.harvard.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

L0 norm, L1 regularization cannot model the hypothesis
of sparsity as well as some Bayesian alternatives (Tipping,
2001). Two Bayesian approaches stand out as more accurate models of sparsity than L1 . The first, the spike and
slab (Mitchell & Beauchamp, 1988), introduces discrete latent variables that directly model the presence or absence
of each parameter. This discrete approach is the most direct and accurate representation of a sparsity hypothesis
(Mohamed et al., 2012), but the discrete latent space that
it imposes is often computationally intractable for models
where Bayesian inference is difficult.
The second approach to Bayesian sparsity uses the scale
mixtures of normals (Andrews & Mallows, 1974), a family of distributions that arise from integrating a zero meanGaussian over an unknown variance as


Z ‚àû
Œ∏2
1
‚àö
exp ‚àí 2 p(œÉ)dœÉ.
(1)
p(Œ∏) =
2œÉ
2œÄœÉ
0
Scale-mixtures of normals can approximate the discrete
spike and slab prior by mixing both large and small values of the variance œÉ 2 . The implicit prior of L1 regularization, the Laplacian, is a member of the scale mixture family
that results from an exponentially distributed variance œÉ 2 .
Thus, mixing densities p(œÉ 2 ) with subexponential tails and
more mass near the origin more accurately model sparsity
than L1 and are the basis for approaches often referred to
as ‚ÄúSparse Bayesian Learning‚Äù (Tipping, 2001). Both the
Student-t of Automatic Relevance Determination (ARD)
(MacKay et al., 1994) and the Horseshoe prior (Carvalho
et al., 2010) incorporate these properties.
Applying these favorable, Bayesian approaches to sparsity
has been particularly challenging for discrete, undirected
models like Boltzmann Machines. Undirected models possess a representational advantage of capturing ‚Äòcollective
phenomena‚Äô with no directions of causality, but their likelihoods require an intractable normalizing constant (Murray & Ghahramani, 2004). For a fully observed Boltzmann
Machine with x ‚àà {0, 1}D the distribution1 is
Ô£±
Ô£º
Ô£≤
Ô£Ω
X
1
exp
Jij xi xj ,
(2)
p(x|J) =
Ô£≥
Ô£æ
Z(J)
i<j

1

We exclude biases for simplicity.

Variational Inference for Sparse and Undirected Models

tional Inference (PVI) (Section 2).
‚Ä¢ We introduce a reparameterization approach for variational inference under sparsity-inducing scale-mixture
priors (e.g. the Laplacian, ARD, and the Horseshoe)
that significantly improves approximation quality by
capturing scale uncertainty (Section 3). When combined with Gaussian stochastic variational inference,
we call this Fadeout.

(i)

(ii)

(iii)

1

1
1

1

0

1

0

0

0

1

1

1

0

1

0

Figure 1. Bayesian inference for discrete undirected graphical
models with sparse priors is triply intractable, as the space of
possible models spans: (i) all possible sparsity patterns, each of
which possesses its own (ii) parameter space, for which every distinct set of parameters has its own (iii) intractable normalizing
constant.

where the partition function Z(J) depends on the couplings. Whenever a new set of couplings J are considered
during inference, the partition function Z(J) and corresponding density p(x|J) must be reevaluated. This requirement for an an intractable calculation embedded within
already-intractable nonconjugate inference has led some
to term Bayesian learning of undirected graphical models
D
‚Äúdoubly intractable‚Äù (Murray et al., 2006). When all 2( 2 )
patterns of discrete spike and slab sparsity are added on
top of this, we might call this problem ‚Äútriply intractable‚Äù
(Figure 1). Triple-intractability does not mean that this
problem is impossible, but it will typically require expensive approaches based on MCMC-within-MCMC (Chen &
Welling, 2012).
Here we present an alternative to MCMC-based approaches
for learning undirected models with sparse priors based
on stochastic variational inference (Hoffman et al., 2013).
We combine three ideas: (i) stochastic gradient variational
Bayes (Kingma & Welling, 2014; Rezende et al., 2014;
Titsias & LaÃÅzaro-Gredilla, 2014)2 , (ii) persistent Markov
chains (Younes, 1989), and (iii) a noncentered parameterization of scale-mixture priors, to inherit the benefits of hierarchical Bayesian sparsity in an efficient variational framework. We make the following contributions:
‚Ä¢ We extend stochastic variational inference to undirected models with intractable normalizing constants
by developing a learning algorithm based on persistent Markov chains, which we call Persistent Varia2
This is also a type of noncentered parameterization, but of the
variational distribution rather than the posterior.

‚Ä¢ We demonstrate how a Bayesian approach for learning sparse undirected graphical models with PVI and
Fadeout yields significantly improved inferences of
both synthetic and real applications in physics and biology (Section 4).

2. Persistent Variational Inference
Background: Learning in undirected models Undirected graphical models, also known as Markov Random
Fields, can be written in log-linear form as
( k
)
X
1
exp
Œ∏i fi (x) ,
(3)
p(x|Œ∏) =
Z(Œ∏)
i=1
where i indexes a set of kPfeaturesP
{fi (x)}ki=1 and the
partition function Z(Œ∏) = x exp { i Œ∏i fi (x)} normalizes the distribution (Koller & Friedman, 2009). Maximum
Likelihood inference selects parameters Œ∏ that maximize
the probability of data D = {x(1) , . . . , x(N ) } by ascending
the gradient of the (averaged) log likelihood
‚àÇ 1
log p(D|Œ∏) = ED [fi (x)] ‚àí Ep(x|Œ∏) [fi (x)] . (4)
‚àÇŒ∏i N
The first term in the gradient is a data-dependent average
of feature fi (x) over D, while the second term is a dataindependent average of feature fi (x) over the model distribution that often requires sampling (Murphy, 2012)3 .
Bayesian learning for undirected models is confounded by
the partition function Z(Œ∏). Given the
P data D, a prior p(Œ∏),
and the log potentials H[x|Œ∏] = ‚àí i Œ∏i fi (x) , the posterior distribution of the parameters is
Q
(i)
p(Œ∏) i e‚àíH[x |Œ∏] /Z(Œ∏)
p(Œ∏|D) = R
, (5)
Q
0
p(Œ∏ 0 ) i e‚àíH[x(i) |Œ∏ ] /Z(Œ∏ 0 )dŒ∏ 0
which contains an intractable partition function Z(Œ∏)
within the already-intractable evidence term. As a result,
most algorithms for Bayesian learning of undirected models require either doubly-intractable MCMC and/or approximations of the likelihood p(x|Œ∏).
3
Depending on the details of the MCMC and the community
these approaches are known as Boltzmann Learning, Stochastic Maximum Likelihood, or Persistent Contrastive Divergence
(Tieleman, 2008).

Variational Inference for Sparse and Undirected Models
Centered

Prior

5

4

3

3

3

2

2

2

1

1

1

0

0

0

-1

-1

-1

-10

0

10

20

-2
-20

Prior

5

-10

0

10

20

Likelihood

5

-2
-20

4

4

3

3

3

2

2

2

1

1

1

0

0

0

-1

-1

-1

-2

0

2

-2

-2

0

-10

2

-2

0

10

20

Posterior

5

4

-2

Posterior

5

4

-2
-20

Noncentered

Likelihood

5

4

-2

0

2

Figure 2. Variational inference for sparse priors with noncentered reparameterizations. Several sparsity-inducing priors such as the
Laplacian, Student-t, and Horseshoe (shown here) can be derived as scale-mixture priors in which each model parameter Œ∏ is drawn from
a zero-mean Gaussian with random variance œÉ 2 (top row). The dependency of Œ∏ on œÉ 2 gives rise to a strongly curved ‚Äúfunnel‚Äù distribution
(blue, top left and right) that is poorly modeled by a factorized variational distribution (not shown). A noncentered reparameterization
with Œ∏ÃÉ = Œ∏/œÉ trades independence of Œ∏ and œÉ 2 in the likelihood (blue, top center) for independence in the prior (blue, bottom left),
allowing a factorized variational distribution over noncentered parameters (black contours, bottom right) to implicitly capture the a priori
correlations between Œ∏ and œÉ 2 (black contours, top right). As a result, the variational distribution can more accurately model the bottom
of the ‚Äúfunnel‚Äù, which corresponds to sparse estimates.

A tractable estimator for ‚àáELBO of undirected models
Here we consider how to approximate the intractable posterior in (5) without approximating the partition function
Z(Œ∏) or the likelihood p(x|Œ∏) by using variational inference. Variational inference recasts inference with p(Œ∏|D)
as an optimization problem of finding a variational distribution q(Œ∏|œÜ) that is closest to p(Œ∏|D) as measured by KL
divergence (Jordan et al., 1999). This can be accomplished
by maximizing the Evidence Lower BOund

Rezende et al., 2014; Titsias & LaÃÅzaro-Gredilla, 2014) that
has been incredibly useful for directed
Qmodels. Consider a
variational approximation q(Œ∏|œÜ) = i q(Œ∏i |¬µi , si ) that is
a fully factorized (mean field) Gaussian with means ¬µ and
log standard deviations s. The ELBO expectations under
q(Œ∏|œÜ) can be rewritten as expectations wrt an independent
noise source  ‚àº N (0, I) where4 Œ∏() = ¬µ + exp {s}  .
Then the gradients are

L(œÜ) , Eq [log p(D, Œ∏) ‚àí log q(Œ∏|œÜ)] ‚â§ log p(D). (6)

‚àás L = E [‚àáŒ∏ log p(D, Œ∏())  (Œ∏() ‚àí ¬µ)] + 1. (9)

For scalability, we would like to optimize the ELBO with
methods that can leverage Monte Carlo estimators of the
gradient ‚àáœÜ L(œÜ). One possible strategy for this would be
would be to develop an estimator based on the score function (Ranganath et al., 2014) with a Monte-Carlo approximation of


p(D, Œ∏)
.
(7)
‚àáœÜ L = Eq ‚àáœÜ log q(Œ∏|œÜ) log
q(Œ∏|œÜ)
Naively substituting the likelihood (3) in the score function estimator (7) nests the intractable log partition function log Z(Œ∏) within the average over q(Œ∏|œÜ), making this
an untenable (and extremely high variance) approach to inference with undirected models.
We can avoid the need for a score-function estimator with
the ‚Äòreparameterization trick‚Äô (Kingma & Welling, 2014;

‚àá¬µ L = E [‚àáŒ∏ log p(D, Œ∏())] ,

(8)

Because these expectations require only the gradient of the
likelihood ‚àáŒ∏ log p(D|Œ∏), the gradient for the undirected
model (4) can be substituted to form a nested expectation
for ‚àáœÜ L(œÜ). This can then be used as a Monte Carlo gradient estimator by sampling  ‚àº N (0, I), x ‚àº p(x|Œ∏()).
Persistent gradient estimation In Stochastic Maximum
Likelihood estimation for undirected models, the intractable gradients of (4) are estimated by sampling p(x|Œ∏).
Although sampling-based approaches are slow, they can
be made considerably more efficient by running a set of
Markov chains in parallel with state that persists between
iterations (Younes, 1989). Persistent state maintains the
Markov chains near their equilibrium distributions, which
means that they can quickly re-equilibrate after perturbations to the parameters Œ∏ during learning.
4

The  operator is an element-wise product.

Variational Inference for Sparse and Undirected Models

We propose variational inference in undirected models
based on persistent gradient estimation of ‚àáŒ∏ log p(D|Œ∏)
and refer to this as Persistent Variational Inference (PVI)
(Algorithm in Appendix). Following the notation of PCDn (Tieleman, 2008), PVI-n refers to using n sweeps of
Gibbs sampling with persistent Markov chains between iterations. This approach is generally compatible with any
estimators of ‚àáELBO that are based on the gradient of the
log likelihood, several examples of which are explained in
(Kingma & Welling, 2014; Rezende et al., 2014; Titsias &
LaÃÅzaro-Gredilla, 2014).
Behavior of the solution for Gaussian q When the
variational approximation is a fully factorized Gaussian
q(Œ∏|¬µ, œÉ) and the prior is flat p(Œ∏) ‚àù 1, the solution to
¬µ? , œÉ ? = arg max¬µ,œÉ L(¬µ, œÉ) will satisfy
ED [fi (x)] = EpÃÉ [fi (x)] , œÉi? =

1
N EpÃÉ [i fi (x)]

(10)

where pÃÉ = p(x|Œ∏())p() is an extended system of the
original undirected model in which the parameters Œ∏i =
¬µi + i œÉi fluctuate according to the variational distribution. This bridges to the Maximum Likelihood solution as
N ‚Üí ‚àû and œÉi? ‚Üí 0, while accounting for uncertainty
in the parameters at finite sample sizes with the inverse of
‚Äòsensitivity‚Äô EpÃÉ [i fi (x)].

3. Fadeout
3.1. Noncentered Parameterizations of Hierarchical
Priors
Hierarchical models are powerful because they impose
a priori correlations between latent variables that reflect
problem-specific knowledge. For scale-mixture priors that
promote sparsity, these correlations come in the form of
scale uncertainty. Instead of assuming that the scale of a
parameter in a model is known a priori, we posit that it
is normally distributed with a randomly distributed variance p(œÉ 2 ). The joint prior p(Œ∏|œÉ 2 )p(œÉ 2 ) gives rise to a
strongly curved ‚Äòfunnel‚Äô shape (Figure 2) that illustrates a
simple but profound principle about hierarchical models:

Table 1. Common priors as scale-mixtures of normal distributions

Prior

Hyperprior
2

œÉ =

Gaussian (L2 )

1
2Œª

2

Laplacian (L1 )

œÉ ‚àº Exponential

Student-t (ARD)

œÉ 2 ‚àº Inv. Gamma

Horseshoe

œÉ ‚àº Half-Cauchy

p(log œÉ)
constant
2

2Œªe‚àíŒªœÉ œÉ 2
Œ≤
2Œ≤ Œ± ‚àí 2
e œÉ
Œì(Œ±)

œÉ ‚àí2Œ±

2s
œÉ
œÄ s2 +œÉ 2

Algorithm 1 Computing ‚àáELBO for Fadeout
Require: Global parameters {¬µœÑ , sœÑ }
Require: Local parameters {¬µŒ∏ÃÉ , ¬µlog œÉ , sŒ∏ÃÉ , slog œÉ }
Require: Hyperprior gradient ‚àálog œÉ,œÑ log p(log œÉ, œÑ )
Require: Likelihood gradient ‚àáŒ∏ p(x|Œ∏)
// Sample from variational distribution
z1 ‚àº N (0, I|œÑ | ), z2 ‚àº N (0, I|Œ∏ÃÉ| ), z3 ‚àº N (0, I|œÉ| )
œÑ ‚Üê ¬µœÑ + exp{sœÑ }  z1
Œ∏ÃÉ ‚Üê ¬µŒ∏ÃÉ + exp{sŒ∏ÃÉ }  z2
œÉ ‚Üê exp {¬µlog œÉ + exp {slog œÉ }  z3 }
Œ∏ ‚Üê Œ∏ÃÉ  œÉ
// Centered global parameters
‚àá¬µœÑ L ‚Üê ‚àáœÑ log p(log œÉ, œÑ )
‚àásœÑ L ‚Üê exp {sœÑ }  z1  ‚àá¬µœÑ L + 1
// Noncentered local parameters
‚àá¬µŒ∏ÃÉ L ‚Üê œÉ  ‚àáŒ∏ log p(x|Œ∏) ‚àí Œ∏ÃÉ
‚àá¬µlog œÉ L ‚Üê Œ∏ ‚àá
	 Œ∏ log p(x|Œ∏) + ‚àálog œÉ log p(log œÉ, œÑ )
‚àásŒ∏ÃÉ L ‚Üê exp sŒ∏ÃÉ  z2  ‚àá¬µŒ∏ÃÉ L + 1
‚àáslog œÉ L ‚Üê exp {slog œÉ }  z3  ‚àá¬µlog œÉ L + 1

as the hyperparameter log œÉ decreases and the prior accepts
a smaller range of values for Œ∏, normalization increases the
probability density at the origin, favoring sparsity. This
normalization-induced sharpening has been called called a
Bayesian Occam‚Äôs Razor (MacKay, 2003).
While normalization-induced sharpening gives rise to sparsity, these extreme correlations are a disaster for meanfield variational inference. Even if a tremendous amount of
probability mass is concentrated at the base of the funnel,
an uncorrelated mean-field approximation will yield estimates near the top. The result is a potentially non-sparse
estimate from a very-sparse prior.
The strong coupling of hierarchical funnels also plagues
exact methods based on MCMC with slow mixing, but
the statistics community has found that these geometry
pathologies can be effectively managed by transformations.
Many models can be rewritten in a noncentered form where
the parameters and hyperparmeters are a priori independen (Papaspiliopoulos et al., 2007; Betancourt & Girolami,
2013). For the scale-mixtures of normals, this change of
variables is


Œ∏
, log œÉ
(11)
{Œ∏, log œÉ} ‚Üí
œÉ
Then Œ∏ÃÉ , œÉŒ∏ ‚àº N (0, 1) while preserving Œ∏ÃÉœÉ ‚àº N (0, œÉ 2 ).
In noncentered form, the joint prior is independent and well
approximated by a mean-field Gaussian, while the likelihood will be variably correlated depending on the strength
of the data (Figure 2). In this sense, centered parameterizations (CP) and noncentered parameterizations (NCP)
are usually framed as favorable in strong and weak data

Variational Inference for Sparse and Undirected Models
a priori correlated

a priori independent

Ferromagnet, 4x4x4 cube

sJ

sJ

500

œÉ24
œÉ23

œÉ15

œÉ35
œÉ34

œÉ14

œÉ45

œÉ13
œÉ12

JÃÉ 15

œÉ25
œÉ24

œÉ23

JÃÉ 14

œÉ35
œÉ34

œÉ45

x1

x2

JÃÉ 13
JÃÉ 23

x4

x5

Coupling
z-scores

JÃÉ 25
JÃÉ 24

JÃÉ 12

JÃÉ 35
JÃÉ 34

JÃÉ 45

J 15
J 14

Couplings

J 13
J 12

Mean-field
PL, decimation
PL, L1 (10xCV)

1000

J 25
J 24

J 23

MPF, L1(10xCV)

J 35
J 34

J 45

Data

x1

x2

x3

x4

x5

Biases

h1

h2

h3

h4

h5

œÉ1

œÉ2

œÉ3

œÉ4

œÉ5

sh

œÉ1

œÉ2

œÉ3

œÉ4

x3

œÉ5

hÃÉ 1

hÃÉ 2

hÃÉ 3

hÃÉ 4

PCD-3, L1

2000

Data

hÃÉ 5

PVI-3, Half-Cauchy

10 -2

Bias
z-scores

RMS error, couplings J

10 -1

sh

Spin glass, ER topology (N=100, p=0.02)
Global scale

500

Local scales

Figure 3. An undirected model with a scale mixture prior (factor graph on left) can be given a priori independence of the latent variables by a noncentered parameterization (factor graph on
right). This is advantageous for mean-field variational inference
that imposes a posteriori independence.

Sample size

œÉ13
œÉ12

œÉ25

Sample size

œÉ15
œÉ14

1000

2000
10 -2

10 -1

RMS error, couplings J

10 0

regimes, respectively.5
We propose the use of non-centered parameterizations of
scale-mixture priors for mean-field Gaussian variational inference. For convenience, we like to call this Fadeout (see
next section). Fadeout can be easily implemented by either
(i) using the chain rule to derive the gradient of the Evidence Lower BOund (ELBO) (Algorithm 1) or, for differentiable models, (ii) rewriting models in noncentered form
and using automatic differentiation tools such as Stan (Kucukelbir et al., 2017) or autograd6 for ADVI. The only
two requirements of the user are the gradient of the likelihood function and a choice of a global hyperprior, several
options for which are presented in Table 1.
Estimators for the centered posterior. Fadeout optimizes a mean-field Gaussian variational distribution over
the noncentered parameters q(Œ∏ÃÉ, log œÉ). As an estimator
for the centered parameters, we use the mean-field property to compute the centered posterior mean as Eq [Œ∏] =
Eq [Œ∏ÃÉ]  Eq [œÉ], giving 7


1
Œ∏ÃÇ = ¬µŒ∏ÃÉ  exp ¬µlog œÉ + e2slog œÉ
2
5

(12)

Although ‚Äúweak data‚Äù may seem unrepresentative of typical
problems in machine learning, it is important to remember that a
sufficiently large and expressive model can make most data weak.
6
github.com/HIPS/autograd
7
The term 12 e2slog œÉ is optional in the sense that including it
corresponds to averaging over the hyperparameters, whereas discarding it corresponds to optimizing the hyperparameters (Empirical Bayes). We included it for all experiments.

Figure 4. Inverse Ising. Combining Persistent VI with a noncentered Horseshoe prior (Half-Cauchy hyperprior) attains lower error on simulated Ising systems than standard methods for point
estimation including: Pseudolikelihood (PL) with L1 or decimation regularization (Schmidt, 2010; Aurell & Ekeberg, 2012;
Decelle & Ricci-Tersenghi, 2014), Minimum Probability Flow
(MPF) (Sohl-Dickstein et al., 2011), and Persistent Contrastive
Divergence (PCD) (Tieleman, 2008). For the spin glass, error
bars are two logarithmic standard deviations across 5 simulated
systems.

3.2. Connection to Dropout
Dropout regularizes neural networks by perturbing hidden
units in a directed network with multiplicative Bernoulli or
Gaussian noise (Srivastava et al., 2014). Although it was
originally framed as a heuristic, Dropout has been subsequently interpreted as variational inference under at least
two different schemes (Gal & Ghahramani, 2016; Kingma
et al., 2015). Here, we interpret Fadeout the reverse way,
where we introduced it as variational inference and now notice that it looks similar to lognormal Dropout.8 If we take
the uncertainty in Œ∏ÃÉ as low and clamp the other variational
parameters, the gradient estimator for Fadeout is:
z ‚àº N (0, I|Œ∏| )
œÉ ‚Üê exp {¬µlog œÉ + exp {slog œÉ }  z}
Œ∏ ‚Üê Œ∏ÃÉ exp {¬µlog œÉ + exp {slog œÉ }  z}
‚àá¬µŒ∏ÃÉ L ‚Üê œÉ  ‚àáŒ∏ log p(x|Œ∏) ‚àí Œ∏ÃÉ
8
Rather than attempting to explain Dropout, the intent is to
lend intuition about noncentered scale-mixture VI.

Variational Inference for Sparse and Undirected Models
Truth

PL, L2 (5xCV)

PL, Group L1 (5xCV)

PL, Group L1

PVI-10, Half-Cauchy

Accuracy

1

Fraction correct

0.9
0.8
0.7

0

25

PL, L2 (5xCV),

J

= 10

PL, Group L 1,

G

= 30.0

40

60

PL, Group L 1 (5xCV),

0.6

G

= 10

PVI-10, Half-Cauchy

0.5

25

0

20

80

100

Top N interactions

120

Figure 5. Synthetic protein. For reconstructing interactions in a synthetic 20-letter spin-glass, a hierarchical Bayesian approach based
on Persistent VI and a noncentered group Horseshoe prior (Half-Cauchy hyperprior) identifies true interactions with more accuracy and
less shrinkage than Group L1 . Each i, j pair is the norm of a 20 √ó 20 factor coupling the amino acid at position i to the amino acid at
position j.

This is the gradient estimator for a lognormal version of
Dropout with an L2 weight penalty of 12 . At each sample
from the variational distribution, Fadeout introduces scale
noise rather than the Bernoulli noise of Dropout. The connection to Dropout would seem to follow naturally from
the common interpretation of scale mixtures as continuous
relaxations of spike and slab priors (Engelhardt & Adams,
2014) and the idea that Dropout can be related to variational
spike and slab inference (Louizos, 2015).

4. Experiments
4.1. Physics: Inferring Spin Models
Ising model The Ising model is a prototypical undirected
model for binary systems that includes both pairwise interactions and (potentially) sitewise biases. It can be seen as
the fully observed case of the Boltzmann machine, and is
typically parameterized with signed spins x ‚àà {‚àí1, 1}D
and a likelihood given by
p(x|h, J) =

)
(
X
X
1
exp
hi xi +
Jij xi xj . (13)
Z(h, J)
i
i<j

Originally proposed as a minimal model of how long range
order arises in magnets, it continues to find application in
physics and biology as a model for phase transitions and
quenched disorder in spin glasses (Nishimori, 2001) and
collective firing patterns in neural spike trains (Schneidman
et al., 2006; Shlens et al., 2006).
Hierarchical sparsity prior One appealing feature of
the Ising model is that it allows a sparse set of underlying couplings J to give rise to long-range, distributed correlations across a system. Since many physical systems
are thought to be dominated by a small number of relevant interactions, L1 regularization has been a favored approach for inferring Ising models. Here, we examine how
a more accurate model of sparsity based on the Horseshoe
prior (Figure 3) can improve inferences in these systems.

Each coupling Jij and bias parameter hi is given its own
scale parameter which are in turn tied under a global HalfCauchy prior for the scales (Figure 3, Appendix).
Simulated datasets We generated synthetic couplings
for two kinds of Ising systems: (i) a slightly sub-critical
cubic ferromagnet (Jij > 0 for neighboring spins) and (ii)
a Sherrington-Kirkpatrick spin glass diluted on an ErdoÃàsRenyi random graph with average degree 2. We sampled
synthetic data for each system with the Swendsen-Wang
algorithm (Appendix) (Swendsen & Wang, 1987).
Results On both the ferromagnet and the spin glass, we
found that Persistent VI with a noncentered Horseshoe
prior (Fadeout) gave estimates with systematically lower
reconstruction error of the couplings J (Figure 4) versus a
variety of standard methods in the field (Appendix).
4.2. Biology: Reconstructing 3D Contacts in Proteins
from Sequence Variation
Potts model The Potts model generalizes the Ising model
to non-binary categorical data. The factor graph is the same
(Figure 3), except each spin xi can adopt q different categories with x ‚àà {1, . . . , q}D and each Jij is a q √ó q matrix
as
p(x|h, J) =

(
)
X
X
1
exp
hi (xi ) +
Jij (xi , xj ) .
Z(h, J)
i
i<j
(14)

The Potts model has recently generated considerable excitement in biology, where it has been used to infer 3D contacts in biological molecules solely from patterns of correlated mutations in the sequences that encode them (Marks
et al., 2011; Morcos et al., 2011). These contacts are have
been sufficient to predict the 3D structures of proteins, protein complexes, and RNAs (Marks et al., 2012).
Group sparsity Each pairwise factor Jij in a Potts model
contains q √ó q parameters capturing all possible joint configurations of xi and xj . One natural way to enforce spar-

Variational Inference for Sparse and Undirected Models
SH3 domain

PL, L2

PL, Group L1

PVI-10, Exponential

PVI-10, Half-Cauchy

Comparison with structure
1
0.95

Distance in 3D

Inferred coupling strength

0

9.7

Fraction < 10 √Ö

0.9
0.85
0.8
0.75
PL, L2

0.7

J

= 9.6

PL, Group L 1

G

= 30.0

PVI-10, Exponential
PVI-10, Half-Cauchy

0.65
0.6
0

25

0

√Ö

21

0

9.7

50

100

150

200

Top N interactions

Figure 6. Unsupervised protein contact prediction. When inferring a pairwise undirected model for protein sequences in the SH3 domain
family, hierarchical Bayesian approaches based on Persistent VI and noncentered scale mixture priors (Half-Cauchy for Group Horseshoe
and Exponential for a Multivariate Laplace) identify local interactions that are close in 3D structure without tuning parameters. When
group L1 -regularized maximum Pseudolikelihood estimation is tuned to give the same largest effect size as the Multivariate Laplace, the
hierarchical approaches based on Persistent VI are more predictive of 3D proximity (right).

sity in a Potts model is at the level of each q √óq group. This
can be accomplished by introducing a single scale parameter œÉij for all q √ó q z-scores JÃÉij . We adopt this with the
same Half-Cauchy hyperprior as the Ising problem, giving
the same factor graph (Figure 3) now corresponding to a
Group Horseshoe prior (HernaÃÅndez-Lobato et al., 2013).
In the real protein experiment, we also consider an exponential hyperprior, which corresponds to a Multivariate
Laplace distribution (Eltoft et al., 2006) over the groups.
Synthetic protein data We first investigated the performance of Persistent VI with group sparsity on a synthetic protein experiment. We constructed a synthetic Potts
spin glass with a topology inspired by biological macromolecules. We generated synthetic parameters based on
contacts in a simulated polymer and sampled 2000 sequences with 2 √ó 106 steps of Gibbs sampling (Appendix).
Results for a synthetic protein We inferred couplings
with 400 of the sampled sequences using PVI with group
sparsity and two standard methods of the field: L2 and
Group L1 regularized maximum pseudolikelihood (Appendix). PVI with a noncentered Horseshoe yielded more
accurate (Figure 5, right), less shrunk (Figure 5, left) estimates of interactions that were more predictive of the 1600
remaining test sequences (Table 2). The ability to generalize well to new sequences will likely be important to the
related problem of predicting mutation effects with unsupervised models of sequence variation (Hopf et al., 2017;
Figliuzzi et al., 2015).
Results for natural sequence variation We applied the
hierarchical Bayesian model from the protein simulation
to model across-species amino acid covariation in the SH3

domain family (Figure 6). Transitioning from simulated to
real protein data is particularly challenging for Bayesian
methods because available sequence data are highly nonindependent due to a shared evolutionary history. We developed a new method for estimating the effective sample size (Appendix) which, when combined standard sequence reweighting techniques, yielded a reweighted effective sample size of 1,012 from 10,209 sequences.
The hierarchical Bayesian approach gave highly localized,
sparse estimates of interactions compared to the two predominant methods in the field, L2 and group L1 regularized
pseudolikelihood (Figure 6). When compared to solved 3D
structures for SH3 (Appendix), we found that the inferred
interactions were considerably more accurate at predicting
amino acids close in structure. Importantly, the hierarchical
Bayesian approach accomplished this inference of strong,
accurate interactions without a need to prespecify hyperparameters such as Œª for L2 or L1 regularization. This is
particularly important for natural biological sequences because the non-independence of samples limits the utility of
cross validation for setting hyperparameters.

5. Related work
5.1. Variational Inference
One strategy for improving variational inference is to introduce correlations in variational distribution by geometric
transformations. This can be made particularly powerful
Table 2. Average log-pseudolikelihood for test sequences.
Method
‚àí log PL(x|h, J) Runtime (s)
PL, L2 (5xCV)
67.3
375
PL, Group L1 (5xCV)
59.6
303
PVI-3, Half-Cauchy
54.2
585

Variational Inference for Sparse and Undirected Models

by using backpropagation to learn compositions of transformations that capture the geometry of complex posteriors
(Rezende & Mohamed, 2015; Tran et al., 2016). Noncentered parameterizations of models may be complementary
to these approaches by enabling more efficient representations of correlations between parameters and hyperparameters.
Most related to this work, (Louizos et al., 2017; Ghosh
& Doshi-Velez, 2017) show how variational inference
with noncentered scale-mixture priors can be useful for
Bayesian learning of neural networks, and how group sparsity can act as a form of automatic compression and model
selection.
5.2. Maximum Entropy
Much of the work on inference of undirected graphical
models has gone under the name of the Maximum Entropy
method in physics and neuroscience, which can be equivalently formulated as maximum likelihood in an exponential
family (MacKay, 2003). From this maximum likelihood
interpretation, L1 regularized-maximum entropy modeling (MaxEnt) corresponds to the disfavored ‚Äúintegrate-out‚Äù
approach to inference in hierarchical models9 (MacKay,
1996) that will introduce significant biases to inferred parameters (Macke et al., 2011). One solution to this bias was
foreshadowed by methods for estimating entropy and Mutual Information, which used hierarchical priors to integrate
over a large range of possible model complexities (Nemenman et al., 2002; Archer et al., 2013). These hierarchical
approaches are favorable because in traditional MAP estimation any top level parameters that are fixed before inference (e.g. a global pseudocount Œ±) introduce strong constraints on allowed model complexity. The improvements
from PVI and Fadeout may be seen as extending this hierarchical approach to full systems of discrete variables.

6. Conclusion
We introduced a framework for scalable Bayesian sparsity
for undirected graphical models composed of two methods.
The first is an extension of stochastic variational inference
to work with undirected graphical models that uses persistent gradient estimation to bypass estimating partition
functions. The second is a variational approach designed
to match the geometry of hierarchical, sparsity-promoting
priors. We found that, when combined, these two methods give substantially improved inferences of undirected
graphical models on both simulated and real systems from
physics and computational biology.
9
To see this, note that L1 -regularized MAP estimation is
equivalent to integrating out a zero-mean Gaussian prior with unknown, exponentially-distributed variance

Acknowledgements
We thank David Duvenaud, Finale Doshi-Velez, Miriam
Huntley, Chris Sander, and members of the Marks lab for
helpful comments and discussions. JBI was supported by
a NSF Graduate Research Fellowship DGE1144152 and
DSM by NIH grant 1R01-GM106303. Portions of this
work were conducted on the Orchestra HPC Cluster at Harvard Medical School.

References
Andrews, David F and Mallows, Colin L. Scale mixtures
of normal distributions. Journal of the Royal Statistical
Society. Series B (Methodological), pp. 99‚Äì102, 1974.
Archer, Evan, Park, Il Memming, and Pillow, Jonathan W.
Bayesian and quasi-bayesian estimators for mutual information from discrete data. Entropy, 15(5):1738‚Äì
1755, 2013.
Aurell, Erik and Ekeberg, Magnus. Inverse ising inference using all the data. Physical review letters, 108(9):
090201, 2012.
Betancourt, MJ and Girolami, Mark.
Hamiltonian
monte carlo for hierarchical models. arXiv preprint
arXiv:1312.0906, 2013.
Carvalho, Carlos M, Polson, Nicholas G, and Scott,
James G. The horseshoe estimator for sparse signals.
Biometrika, pp. asq017, 2010.
Chen, Yutian and Welling, Max. Bayesian structure learning for markov random fields with a spike and slab prior.
In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, pp. 174‚Äì184. AUAI
Press, 2012.
Decelle, AureÃÅlien and Ricci-Tersenghi, Federico. Pseudolikelihood decimation algorithm improving the inference of the interaction network in a general class of ising
models. Physical review letters, 112(7):070603, 2014.
Eltoft, Torbj√∏rn, Kim, Taesu, and Lee, Te-Won. On the
multivariate laplace distribution. IEEE Signal Processing Letters, 13(5):300‚Äì303, 2006.
Engelhardt, Barbara E and Adams, Ryan P. Bayesian
structured sparsity from gaussian fields. arXiv preprint
arXiv:1407.2235, 2014.
Figliuzzi, Matteo, Jacquier, HerveÃÅ, Schug, Alexander,
Tenaillon, Oliver, and Weigt, Martin. Coevolutionary
landscape inference and the context-dependence of mutations in beta-lactamase tem-1. Molecular biology and
evolution, pp. msv211, 2015.
Gal, Yarin and Ghahramani, Zoubin. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1050‚Äì1059, 2016.
Ghosh, Soumya and Doshi-Velez, Finale. Model selection
in bayesian neural networks via horseshoe priors. arXiv

Variational Inference for Sparse and Undirected Models

preprint arXiv:1705.10388, 2017.
HernaÃÅndez-Lobato,
Daniel,
HernaÃÅndez-Lobato,
JoseÃÅ Miguel, and Dupont, Pierre. Generalized spikeand-slab priors for bayesian group feature selection
using expectation propagation. Journal of Machine
Learning Research, 14(1):1891‚Äì1945, 2013.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303‚Äì1347,
2013.
Hopf, Thomas A, Ingraham, John B, Poelwijk, Frank J,
SchaÃàrfe, Charlotta PI, Springer, Michael, Sander, Chris,
and Marks, Debora S. Mutation effects predicted from
sequence co-variation. Nature biotechnology, 35(2):
128‚Äì135, 2017.
Jordan, Michael I, Ghahramani, Zoubin, Jaakkola,
Tommi S, and Saul, Lawrence K. An introduction
to variational methods for graphical models. Machine
learning, 37(2):183‚Äì233, 1999.
Kingma, Diederik P and Welling, Max. Auto-encoding
variational bayes. In Proceedings of the International
Conference on Learning Representations (ICLR), 2014.
Kingma, DP, Salimans, T, and Welling, M. Variational
dropout and the local reparameterization trick. Advances
in Neural Information Processing Systems, 28:2575‚Äì
2583, 2015.
Koller, Daphne and Friedman, Nir. Probabilistic graphical
models: principles and techniques. MIT press, 2009.
Kucukelbir, Alp, Tran, Dustin, Ranganath, Rajesh, Gelman, Andrew, and Blei, David M. Automatic differentiation variational inference. Journal of Machine Learning
Research, 18(14):1‚Äì45, 2017.
Louizos, Christos. Smart regularization of deep architectures. Master‚Äôs thesis, University of Amsterdam, 2015.
Louizos, Christos, Ullrich, Karen, and Welling, Max.
Bayesian compression for deep learning. arXiv preprint
arXiv:1705.08665, 2017.
MacKay, David JC. Hyperparameters: Optimize, or integrate out? In Maximum entropy and bayesian methods,
pp. 43‚Äì59. Springer, 1996.
MacKay, David JC. Information theory, inference and
learning algorithms. Cambridge university press, 2003.
MacKay, David JC et al. Bayesian nonlinear modeling for
the prediction competition. ASHRAE transactions, 100
(2):1053‚Äì1062, 1994.
Macke, Jakob H, Murray, Iain, and Latham, Peter E. How
biased are maximum entropy models? In Advances in
Neural Information Processing Systems, pp. 2034‚Äì2042,
2011.
Marks, Debora S, Colwell, Lucy J, Sheridan, Robert, Hopf,
Thomas A, Pagnani, Andrea, Zecchina, Riccardo, and
Sander, Chris. Protein 3d structure computed from evolutionary sequence variation. PloS one, 6(12):e28766,
2011.

Marks, Debora S, Hopf, Thomas A, and Sander, Chris. Protein structure prediction from sequence variation. Nature
biotechnology, 30(11):1072‚Äì1080, 2012.
Mitchell, Toby J and Beauchamp, John J. Bayesian variable
selection in linear regression. Journal of the American
Statistical Association, 83(404):1023‚Äì1032, 1988.
Mohamed, Shakir, Ghahramani, Zoubin, and Heller,
Katherine A. Bayesian and l1 approaches for sparse unsupervised learning. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp.
751‚Äì758, 2012.
Morcos, Faruck, Pagnani, Andrea, Lunt, Bryan, Bertolino,
Arianna, Marks, Debora S, Sander, Chris, Zecchina, Riccardo, Onuchic, JoseÃÅ N, Hwa, Terence, and Weigt, Martin. Direct-coupling analysis of residue coevolution captures native contacts across many protein families. Proceedings of the National Academy of Sciences, 108(49):
E1293‚ÄìE1301, 2011.
Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.
Murray, Iain and Ghahramani, Zoubin. Bayesian learning
in undirected graphical models: approximate mcmc algorithms. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pp. 392‚Äì399. AUAI
Press, 2004.
Murray, Iain, Ghahramani, Zoubin, and MacKay,
David JC. Mcmc for doubly-intractable distributions. In
Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pp. 359‚Äì366. AUAI
Press, 2006.
Nemenman, Ilya, Shafee, Fariel, and Bialek, William. Entropy and inference, revisited. Advances in neural information processing systems, 1:471‚Äì478, 2002.
Nishimori, Hidetoshi. Statistical physics of spin glasses
and information processing: an introduction. Number
111. Clarendon Press, 2001.
Papaspiliopoulos, Omiros, Roberts, Gareth O, and SkoÃàld,
Martin. A general framework for the parametrization
of hierarchical models. Statistical Science, pp. 59‚Äì73,
2007.
Ranganath, Rajesh, Gerrish, Sean, and Blei, David. Black
box variational inference. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pp. 814‚Äì822, 2014.
Rezende, Danilo and Mohamed, Shakir. Variational inference with normalizing flows. In Proceedings of The
32nd International Conference on Machine Learning,
pp. 1530‚Äì1538, 2015.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of
The 31st International Conference on Machine Learning, pp. 1278‚Äì1286, 2014.
Schmidt, Mark. Graphical model structure learning

Variational Inference for Sparse and Undirected Models

with l1-regularization. PhD thesis, UNIVERSITY OF
BRITISH COLUMBIA (Vancouver, 2010.
Schneidman, Elad, Berry, Michael J, Segev, Ronen, and
Bialek, William. Weak pairwise correlations imply
strongly correlated network states in a neural population.
Nature, 440(7087):1007‚Äì1012, 2006.
Shlens, Jonathon, Field, Greg D, Gauthier, Jeffrey L,
Grivich, Matthew I, Petrusca, Dumitru, Sher, Alexander,
Litke, Alan M, and Chichilnisky, EJ. The structure of
multi-neuron firing patterns in primate retina. The Journal of neuroscience, 26(32):8254‚Äì8266, 2006.
Sohl-Dickstein, Jascha, Battaglino, Peter B, and DeWeese,
Michael R. New method for parameter estimation in
probabilistic models: minimum probability flow. Physical review letters, 107(22):220601, 2011.
Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929‚Äì1958, 2014.
Swendsen, Robert H and Wang, Jian-Sheng. Nonuniversal
critical dynamics in monte carlo simulations. Physical
review letters, 58(2):86, 1987.
Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267‚Äì288, 1996.
Tieleman, Tijmen. Training restricted boltzmann machines
using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pp. 1064‚Äì1071. ACM, 2008.
Tipping, Michael E. Sparse bayesian learning and the relevance vector machine. The journal of machine learning
research, 1:211‚Äì244, 2001.
Titsias, Michalis and LaÃÅzaro-Gredilla, Miguel. Doubly
stochastic variational bayes for non-conjugate inference.
In Proceedings of the 31st International Conference on
Machine Learning (ICML-14), pp. 1971‚Äì1979, 2014.
Tran, Dustin, Ranganath, Rajesh, and Blei, David M. The
variational gaussian process. In Proceedings of the International Conference on Learning Representations, 2016.
Younes, Laurent. Parametric inference for imperfectly observed gibbsian fields. Probability theory and related
fields, 82(4):625‚Äì645, 1989.

