Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to
Non-smooth Concave Maximization
Bo Liu 1 Xiao-Tong Yuan 2 Lezi Wang 1 Qingshan Liu 2 Dimitris N. Metaxas 1

Abstract
Iterative Hard Thresholding (IHT) is a class of
projected gradient descent methods for optimizing sparsity-constrained minimization models,
with the best known efficiency and scalability
in practice. As far as we know, the existing
IHT-style methods are designed for sparse minimization in primal form. It remains open to
explore duality theory and algorithms in such a
non-convex and NP-hard problem setting. In this
paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization
with `2 -regularized loss function and proposing
an IHT-style algorithm for dual maximization.
Our sparse duality theory provides a set of sufficient and necessary conditions under which the
original NP-hard/non-convex problem can be equivalently solved in a dual formulation. The
proposed dual IHT algorithm is a super-gradient
method for maximizing the non-smooth dual objective. An interesting finding is that the sparse
recovery performance of dual IHT is invariant to
the Restricted Isometry Property (RIP), which is
required by virtually all the existing primal IHT
algorithms without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed
for large-scale stochastic optimization. Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal
IHT-style algorithms in model estimation accuracy and computational efficiency.

1. Introduction
Sparse learning has emerged as an effective approach
to alleviate model overfitting when feature dimension
1
Department of CS, Rutgers University, Piscataway, NJ,
08854, USA. 2 B-DAT Lab, Nanjing University of Information
Science & Technology, Nanjing, Jiangsu, 210044, China. Correspondence to: Bo Liu <lb507@cs.rutgers.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

outnumbers training sample. Given a set of training
d
samples{(xi , yi )}N
i=1 in which xi ∈ R is the feature representation and yi ∈ R the corresponding label, the following sparsity-constrained `2 -norm regularized loss minimization problem is often considered in high-dimensional
analysis:
min P (w) :=

kwk0 ≤k

N
λ
1 X
l(w> xi , yi ) + kwk2 .
N i=1
2

(1)

Here l(·; ·) is a convex loss function, w ∈ Rd is the
model parameter vector and λ controls the regularization
strength. For example, the squared loss l(a, b) = (b − a)2
is used in linear regression and the hinge loss l(a, b) =
max{0, 1 − ab} in support vector machines. Due to the presence of cardinality constraint kwk0 ≤ k, the problem (1)
is simultaneously non-convex and NP-hard in general, and
thus is challenging for optimization. A popular way to address this challenge is to use proper convex relaxation, e.g.,
`1 norm (Tibshirani, 1996) and k-support norm (Argyriou
et al., 2012), as an alternative of the cardinality constraint. However, the convex relaxation based techniques tend to
introduce bias for parameter estimation.
In this paper, we are interested in algorithms that directly minimize the non-convex formulation in (1). Early efforts mainly lie in compressed sensing for signal recovery,
which is a special case of (1) with squared loss. Among
others, a family of the so called Iterative Hard Thresholding (IHT) methods (Blumensath & Davies, 2009; Foucart,
2011) have gained significant interests and they have been
witnessed to offer the fastest and most scalable solutions in
many cases. More recently, IHT-style methods have been
generalized to handle generic convex loss functions (Beck
& Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well
as structured sparsity constraints (Jain et al., 2016). The
common theme of these methods is to iterate between gradient descent and hard thresholding to maintain sparsity of
solution while minimizing the objective value.
Although IHT-style methods have been extensively studied,
the state-of-the-art is only designed for the primal formulation (1). It remains an open problem to investigate the
feasibility of solving the original NP-hard/non-convex formulation in a dual space that might potentially further im-

Dual Iterative Hard Thresholding

prove computational efficiency. To fill this gap, inspired by
the recent success of dual methods in regularized learning
problems, we systematically build a sparse duality theory
and propose an IHT-style algorithm along with its stochastic variant for dual optimization.
Overview of our contribution. The core contribution of
this work is two-fold in theory and algorithm. As the theoretical contribution, we have established a novel sparse Lagrangian duality theory for the NP-hard/non-convex problem (1) which to the best of our knowledge has not been
reported elsewhere in literature. We provide in this part a
set of sufficient and necessary conditions under which one
can safely solve the original non-convex problem through
maximizing its concave dual objective function. As the algorithmic contribution, we propose the dual IHT (DIHT)
algorithm as a super-gradient method to maximize the nonsmooth dual objective. In high level description, DIHT iterates between dual gradient ascent and primal hard thresholding pursuit until convergence. A stochastic variant of
DIHT is proposed to handle large-scale learning problems. For both algorithms, we provide non-asymptotic convergence analysis on parameter estimation error, sparsity
recovery, and primal-dual gap as well. In sharp contrast
to the existing analysis for primal IHT-style algorithms,
our analysis is not relying on Restricted Isometry Property
(RIP) conditions and thus is less restrictive in real-life highdimensional statistical settings. Numerical results on synthetic datasets and machine learning benchmark datasets demonstrate that dual IHT significantly outperforms the
state-of-the-art primal IHT algorithms in accuracy and efficiency. The theoretical and algorithmic contributions of
this paper are highlighted in below:
• Sparse Lagrangian duality theory: we established a
sparse saddle point theorem (Theorem 1), a sparse
mini-max theorem (Theorem 2) and a sparse strong
duality theorem (Theorem 3).
• Dual optimization: we proposed an IHT-style algorithm along with its stochastic extension for nonsmooth dual maximization. These algorithms have
been shown to converge at the rate of 1 ln 1 in dual
parameter estimation error and 12 ln 12 in primal-dual
gap (see Theorem 4 and Theorem 5). These guarantees are invariant to RIP conditions which are required
by virtually all the primal IHT-style methods without
using relaxed sparsity levels.

of nonzero entries of x. We conventionally define kxk∞ =
maxi |[x]i | and define xmin = mini∈supp(x) |[x]i |. For a
matrix A, σmax (A) (σmin (A)) denotes its largest (smallest)
singular value.
Organization. The rest of this paper is organized as follows: In §2 we briefly review some relevant work. In
§3 we develop a Lagrangian duality theory for sparsityconstrained minimization problems. The dual IHT-style algorithms along with convergence analysis are presented in
§4. The numerical evaluation results are reported in §5.1.
Finally, the concluding remarks are made in §6. All the
technical proofs are deferred to the appendix sections.

2. Related Work
For generic convex objective beyond quadratic loss, the rate
of convergence and parameter estimation error of IHT-style
methods were established under proper RIP (or restricted
strong condition number) bound conditions (Blumensath,
2013; Yuan et al., 2014; 2016). In (Jain et al., 2014), several
relaxed variants of IHT-style algorithms were presented for
which the estimation consistency can be established without requiring the RIP conditions. In (Bahmani et al., 2013),
a gradient support pursuit algorithm is proposed and analyzed. In large-scale settings where a full gradient evaluation on all data becomes a bottleneck, stochastic and variance reduction techniques have been adopted to improve
the computational efficiency of IHT (Nguyen et al., 2014;
Li et al., 2016; Chen & Gu, 2016).
Dual optimization algorithms have been widely used in various learning tasks including SVMs (Hsieh et al., 2008) and
multi-task learning (Lapin et al., 2014). In recent years,
stochastic dual optimization methods have gained significant attention in large-scale machine learning (ShalevShwartz & Zhang, 2013a;b). To further improve computational efficiency, some primal-dual methods are developed
to alternately minimize the primal objective and maximize
the dual objective. The successful examples of primal-dual
methods include learning total variation regularized model (Chambolle & Pock, 2011) and generalized Dantzig selector (Lee et al., 2016). More recently, a number of stochastic variants (Zhang & Xiao, 2015; Yu et al., 2015)
and parallel variants (Zhu & Storkey, 2016) were developed to make the primal-dual algorithms more scalable and
efficient.

3. A Sparse Lagrangian Duality Theory
Notation. Before continuing, we define some notations to
be used. Let x ∈ Rd be a vector and F be an index set. We
use HF (x) to denote the truncation operator that restricts x
to the set F . Hk (x) is a truncation operator which preserves
the top k (in magnitude) entries of x and sets the remaining
to be zero. The notation supp(x) represents the index set

In this section, we establish weak and strong duality theory that guarantees the original non-convex and NP-hard
problem in (1) can be equivalently solved in a dual space.
The results in this part build the theoretical foundation of
developing dual IHT methods.

Dual Iterative Hard Thresholding

From now on we abbreviate li (w> xi ) = l(w> xi , yi ). The
convexity of l(w> xi , yi ) implies that li (u) is also convex. Let li∗ (αi ) = max{αi u − li (u)} be the convex
u

conjugate of li (u) and F ⊆ R be the feasible set of
αi . According to the well-known expression of li (u) =
maxαi ∈F {αi u − li∗ (αi )}, the problem (1) can be reformulated into the following mini-max formulation:
min

kwk0 ≤k

N
λ
1 X
max {αi w> xi − li∗ (αi )} + kwk2 . (2)
N i=1 αi ∈F
2

The following Lagrangian form will be useful in analysis:

HF̄ (P 0 (w̄)) = 0, w̄min ≥

1 0
kP (w̄)k∞ .
λ

The following sparse mini-max theorem guarantees that the
min and max in (2) can be safely switched if and only if
there exists a sparse saddle point for L(w, α).
Theorem 2 (Sparse Mini-Max Theorem). The mini-max
relationship
max

N
 λ
1 X
L(w, α) =
αi w> xi − li∗ (αi ) + kwk2 ,
N i=1
2

min L(w, α) = min

α∈F N kwk0 ≤k

max L(w, α)

kwk0 ≤k α∈F N

(4)

holds if and only if there exists a sparse saddle point (w̄, ᾱ)
for L.

where α = [α1 , ..., αN ] ∈ F N is the vector of dual variables. We now introduce the following concept of sparse
saddle point which is a restriction of the conventional saddle point to the setting of sparse optimization.
d

Definition 1 (Sparse Saddle Point). A pair (w̄, ᾱ) ∈ R ×
F N is said to be a k-sparse saddle point for L if kw̄k0 ≤ k
and the following holds for all kwk0 ≤ k, α ∈ F N :
L(w̄, α) ≤ L(w̄, ᾱ) ≤ L(w, ᾱ).

PN
Remark 2. Let us consider P 0 (w̄) = N1 i=1 ᾱi xi +
λw̄ ∈ ∂P (w̄). Denote F̄ = supp(w̄). It is easy to verify that the condition (c) in Theorem 1 is equivalent to

(3)

Different from the conventional definition of saddle point,
the k-sparse saddle point only requires the inequality (3)
holds for any arbitrary k-sparse vector w. The following result is a basic sparse saddle point theorem for L.
Throughout the paper, we will use f 0 (·) to denote a subgradient (or super-gradient) of a convex (or concave) function f (·), and use ∂f (·) to denote its sub-differential (or
super-differential).
Theorem 1 (Sparse Saddle Point Theorem). Let w̄ ∈ Rd
be a k-sparse primal vector and ᾱ ∈ F N be a dual vector.
Then the pair (w̄, ᾱ) is a sparse saddle point for L if and
only if the following conditions hold:
(a) w̄ solves the primal problem in (1);
(b) ᾱ ∈ [∂l1 (w̄> x1 ), ..., ∂lN (w̄> xN )];


PN
1
(c) w̄ = Hk − λN
i=1 ᾱi xi .

Proof. A proof of this result is given in Appendix A.2.
The sparse mini-max result in Theorem 2 provides sufficient and necessary conditions under which one can safely
exchange a min-max for a max-min, in the presence of sparsity constraint. The following corollary is a direct consequence of applying Theorem 1 to Theorem 2.
Corollary 1. The mini-max relationship
max

min L(w, α) = min

α∈F N kwk0 ≤k

holds if and only if there exist a k-sparse primal vector
w̄ ∈ Rd and a dual vector ᾱ ∈ F N such that the conditions (a)∼(c) in Theorem 1 are satisfied.
The mini-max result in Theorem 2 can be used as a basis for
establishing sparse duality theory. Indeed, we have already
shown the following:
min

max L(w, α) = min P (w).

kwk0 ≤k α∈F N

Remark 1. Theorem 1 shows that the conditions (a)∼(c)
are sufficient and necessary to guarantee the existence of a
sparse saddle point for the Lagrangian form L. This result
is different from from the traditional saddle point theorem
which requires the use of the Slater Constraint Qualification to guarantee the existence of saddle point.

kwk0 ≤k

This is called the primal minimization problem and it is
the min-max side of the sparse mini-max theorem. The
other side, the max-min problem, will be called as the
dual maximization problem with dual objective function
D(α) := minkwk0 ≤k L(w, α), i.e.,
max D(α) = max

α∈F N

Proof. A proof of this result is given in Appendix A.1.

max L(w, α)

kwk0 ≤k α∈F N

min L(w, α).

α∈F N kwk0 ≤k

(5)

The following Lemma 1 shows that the dual objective function D(α) is concave and explicitly gives the expression of
its super-differential.
Lemma 1. The dual objective function D(α) is given by
D(α) =

N
1 X ∗
λ
−l (αi ) − kw(α)k2 ,
N i=1 i
2

Dual Iterative Hard Thresholding





PN
where w(α) = Hk − N1λ i=1 αi xi . Moreover, D(α)
is concave and its super-differential is given by

Algorithm 1 Dual Iterative Hard Thresholding (DIHT)
Input : Training set {xi , yi }N
i=1 . Regularization strength
parameter λ. Cardinality constraint k. Step-size η.
(0)
(0)
1
(0)
∗
∂D(α) = [w(α)> x1 −∂l1∗ (α1 ), ..., w(α)> xN −∂lN
(αN )]. Initialization w = 0, α1 = ... = αN = 0.
N
for t = 1, 2, ..., T do
(S1) Dual projected super-gradient ascent: ∀ i ∈
∗
Particularly, if w(α) is unique at α and {li }i=1,...,N are
{1, 2, ..., N },
differentiable, then ∂D(α) is unique and it is the super

gradient of D(α).
(t)
(t−1)
(t−1)
αi = PF αi
+ η (t−1) gi
,
(6)
Proof. A proof of this result is given in Appendix A.3.
Based on Theorem 1 and Theorem 2, we are able to further
establish a sparse strong duality theorem which gives the
sufficient and necessary conditions under which the optimal values of the primal and dual problems coincide.
Theorem 3 (Sparse Strong Duality Theorem). Let w̄ ∈ Rd
is a k-sparse primal vector and ᾱ ∈ F N be a dual vector. Then ᾱ solves the dual problem in (5), i.e., D(ᾱ) ≥
D(α), ∀α ∈ F N , and P (w̄) = D(ᾱ) if and only if the
pair (w̄, ᾱ) satisfies the conditions (a)∼(c) in Theorem 1.

(t−1)

0

(t−1)

(t−1)
where gi
= N1 (x>
− li∗ (αi
)) is the
i w
super-gradient and PF (·) is the Euclidian projection
operator with respect to feasible set F.
(S2) Primal hard thresholding:
!
N
1 X (t)
(t)
α xi .
(7)
w = Hk −
λN i=1 i

end
Output: w(T ) .

Proof. A proof of this result is given in Appendix A.4.
4.1. Algorithm
We define the sparse primal-dual gap P D (w, α) :=
P (w) − D(α). The main message conveyed by Theorem 3 is that the sparse primal-dual gap reaches zero at the
primal-dual pair (w̄, ᾱ) if and only if the conditions (a)∼(c)
in Theorem 1 hold.
The sparse duality theory developed in this section suggests a natural way for finding the global minimum of the
sparsity-constrained minimization problem in (1) via maximizing its dual problem in (5). Once the dual maximizer
ᾱ is estimated, the primal sparse minimizer w̄ can then be
recovered from it according to the prima-dual connection
PN
1
w̄ = Hk − λN
i=1 ᾱi xi as given in the condition (c).
Since the dual objective function D(α) is shown to be concave, its global maximum can be estimated using any convex/concave optimization method. In the next section, we
present a simple projected super-gradient method to solve
the dual maximization problem.

4. Dual Iterative Hard Thresholding
Generally, D(α) is a non-smooth function since: 1) the
conjugate function li∗ of an arbitrary convex loss li is generally non-smooth and 2) the term kw(α)k2 is non-smooth
with respect to α due to the truncation operation involved
in computing w(α). Therefore, smooth optimization methods are not directly applicable here and we resort to subgradient-type methods to solve the non-smooth dual maximization problem in (5).

The Dual Iterative Hard Thresholding (DIHT) algorithm, as outlined in Algorithm 1, is essentially a projected super-gradient method for maximizing D(α). The
procedure generates a sequence of prima-dual pairs
(w(0) , α(0) ), (w(1) , α(1) ), . . . from an initial pair w(0) = 0
and α(0) = 0. At the t-th iteration, the dual update step S1
conducts the projected super-gradient ascent in (6) to update α(t) from α(t−1) and w(t−1) . Then in the primal update step S2, the primal variable w(t) is constructed from
α(t) using a k-sparse truncation operation in (7).
When a batch estimation of super-gradient D0 (α) becomes
expensive in large-scale applications, it is natural to consider the stochastic implementation of DIHT, namely SDIHT,
as outlined in Algorithm 2. Different from the batch computation in Algorithm 1, the dual update step S1 in Algorithm 2 randomly selects a block of samples (from a given
block partition of samples) and update their corresponding
dual variables according to (8). Then in the primal update
step S2.1, we incrementally update an intermediate accuPN
(t)
1
mulation vector w̃(t) which records − λN
i=1 αi xi as a
weighted sum of samples. In S2.2, the primal vector w(t) is
updated by applying k-sparse truncation on w̃(t) . The SDIHT is essentially a block-coordinate super-gradient method
for the dual problem. Particularly, in the extreme case
m = 1, SDIHT reduces to the batch DIHT. At the opposite extreme end with m = N , i.e., each block contains
one sample, SDIHT becomes a stochastic coordinate-wise
super-gradient method.

Dual Iterative Hard Thresholding

Algorithm 2 Stochastic Dual Iterative Hard Thresholding
(SDIHT)
Input : Training set {xi , yi }N
i=1 . Regularization strength
parameter λ. Cardinality constraint k. Step-size
η. A block disjoint partition {B1 , ..., Bm } of the
sample index set [1, ..., N ].
(0)
(0)
Initialization w(0) = w̃(0) = 0, α1 = ... = αN = 0.
for t = 1, 2, ..., T do
(S1) Dual projected super-gradient ascent: Uniformly
(t)
randomly select an index block Bi ∈ {B1 , ..., Bm }.
(t)
(t)
For all j ∈ Bi update αj as
(t)
αj
(t)

= PF



(t−1)
αj

(t−1)

+

(t−1)
η (t−1) gj



.

(8)

(t)

Set αj = αj
, ∀j ∈
/ Bi .
(S2) Primal hard thresholding:
– (S2.1) Intermediate update:
w̃(t) = w̃(t−1) −

1 X
(t)
(t−1)
(αj − αj
)xj . (9)
λN
(t)
j∈Bi

– (S2.2) Hard thresholding: w(t) = Hk (w̃(t) ).
end
Output: w(T ) .

The dual update (8) in SDIHT is much more efficient than
DIHT as the former only needs to access a small subset of
samples at a time. If the hard thresholding operation in primal update becomes a bottleneck, e.g., in high-dimensional
settings, we suggest to use SDIHT with relatively smaller
number of blocks so that the hard thresholding operation in
S2.2 can be less frequently called.

Particularly, σmax (X, d) = σmax (X) and σmin (X, d) =
σmin (X). We say a univariate differentiable function f (x)
is γ-smooth if ∀x, y, f (y) ≤ f (x) + hf 0 (x), y − xi + γ2 |x −
y|2 . The following is our main theorem on the dual parameter estimation error, support recovery and primal-dual gap
of DIHT.
Theorem 4. Assume that li is 1/µ-smooth.
Set
λN 2
.
Define
constants
c
=
η (t) = (λN µ+σmin
1
(X,k))(t+1)

2
3
2
N (r+λρ)
(X,k)
2
1 + σmax
.
(λN µ+σmin (X,k))2 and c2 = (r + λρ)
µλN
(a) Parameter estimation error:
The sequence
{α(t) }t≥1 generated by Algorithm 1 satisfies the
following estimation error inequality:


1 ln t
(t)
2
kα − ᾱk ≤ c1
+
,
t
t
(b) Support recovery and primal-dual gap: Assume additionally that ¯ := w̄min − λ1 kP 0 (w̄)k∞ > 0. Then,
supp(w(t) ) = supp(w̄) when


2
2
(X)
(X) 12c1 σmax
12c1 σmax
ln
t ≥ t0 =
.
λ2 N 2 ¯2
λ2 N 2 ¯2
Moreover, for any  > 0, the primal-dual gap sat(t)
isfies
≤  when
t ≥ max{t0 , t1 } where t1 =
 3c1 c2P D 3c

1 c2
ln
.
2
2
2
2
λ N
λ N
Proof. A proof of this result is given in Appendix A.5.
Remark 3. The theorem allows µ = 0 when σmin (X, k) >
0. If µ > 0, then σmin (X, k) is allowed to be zero and thus
N
the step-size can be set as η (t) = µ(t+1)
.
(t)

We now analyze the non-asymptotic convergence behavior of DIHT and SDIHT. In the following analysis, we
will denote ᾱ = arg maxα∈F N D(α) and use the abbre(t)
viation P D := P D (w(t) , α(t) ). Let r = maxa∈F |a|
be the bound of the dual feasible set F and ρ =
0
maxi,a∈F |li∗ (a)|. For example, such quantities exist
when li and li∗ are Lipschitz continuous (Shalev-Shwartz
& Zhang, 2013b). We also assume without loss of generality that kxi k ≤ 1. Let X = [x1 , ..., xN ] ∈ Rd×N be the
data matrix. Given an index set F , we denote XF as the
restriction of X with rows restricted to F . The following
quantities will be used in our analysis:

Consider primal sub-optimality P := P (w(t) ) − P (w̄).
(t)
(t)
Since P ≤ P D always holds, the convergence rates
in Theorem 4 are applicable to the primal sub-optimality
as well. An interesting observation is that these conver(t)
gence results on P are not relying on the Restricted Isometry Property (RIP) (or restricted strong condition number)
which is required in most existing analysis of IHT-style algorithms (Blumensath & Davies, 2009; Yuan et al., 2014).
In (Jain et al., 2014), several relaxed variants of IHT-style
algorithms are presented for which the estimation consistency can be established without requiring the RIP conditions. In contrast to the RIP-free sparse recovery analysis
in (Jain et al., 2014), our Theorem 4 does not require the
sparsity level k to be relaxed.
For SDIHT, we can establish similar non-asymptotic convergence results as summarized in the following theorem.

4.2. Convergence analysis

2
σmax
(X, s) = sup



	
u> XF> XF u | |F | ≤ s, kuk = 1 ,

2
σmin
(X, s)



	
u> XF> XF u | |F | ≤ s, kuk = 1 .

u∈Rn ,F

=

infn

u∈R ,F

Theorem 5. Assume that li is 1/µ-smooth. Set η (t) =
λmN 2
(λN µ+σmin (X,k))(t+1) .

Dual Iterative Hard Thresholding
100

0.6

0.5

0.4

0.3

Moreover, with probability at least 1 − δ, the primal(t)
dual gap satisfies
 when
 12mc1cP2 D ≤
 t ≥ max{4t2 , t3 }
1 c2
where t3 = λ2 δ2 N 2 ln λ12mc
2 δ 2 N 2 .
Proof. A proof of this result is given in Appendix A.6.
Remark 4. Theorem 5 shows that, up to scaling factors,
the expected or high probability iteration complexity of SDIHT is almost identical to that of DIHT. The scaling factor m appeared in t2 and t3 reflects a trade-off between
the decreased per-iteration cost and the increased iteration
complexity.

5. Experiments

0.1
50

60

IHT(d=500)
HTP(d=500)
DIHT(d=500)
IHT(d=300)
HTP(d=300)
DIHT(d=300)

40

20

0.2

(b) Support recovery and primal-dual gap: Assume additionally that ¯ := w̄min − λ1 kP 0 (w̄)k∞ > 0. Then,
for any δ ∈ (0, 1), with probability at least 1 − δ, it
holds that supp(w(t) ) = supp(w̄) when


2
2
(X)
12mc1 σmax
(X) 12mc1 σmax
ln
.
t ≥ t2 =
λ2 δ 2 N 2 ¯2
λ2 δ 2 N 2 ¯2

80

P SSR(%)

IHT(d=500)
HTP(d=500)
DIHT(d=500)
IHT(d=300)
HTP(d=300)
DIHT(d=300)

7
wk
7
kw ! wk=k

(a) Parameter estimation error:
The sequence
{α(t) }t≥1 generated by Algorithm 2 satisfies the
following expected estimation error inequality:


1 ln t
(t)
2
E[kα − ᾱk ] ≤ mc1
+
,
t
t

75

100

125

150

N

0
50

75

100

125

150

N

(a) Model estimation error

(b) Percentage of support recovery success

Figure 1. Model parameter estimation performance comparison
between DIHT and baseline algorithms on the two synthetic
dataset settings. The varying number of training sample is denoted by N .

produced independently. The task is to solve the following
`2 -regularized sparse linear regression problem:
min

kwk≤k

N
1 X
λ
lsq (yi , w> xi ) + kwk2 ,
N i=1
2

where lsq (yi , w> xi ) = (yi − w> xi )2 . The responses
>
{yi }N
i=1 are produced by yi = w̄ xi + εi , where εi ∼
N (0, 1). The convex conjugate of lsq (yi , w> xi ) is known
α2

A synthetic model is generated with sparse model parameter w̄ = [1, 1, · · · , 1, 0, 0, · · · , 0]. Each xi ∈ Rd of the
| {z } | {z }

∗
(αi ) = 4i + yi αi (Shalev-Shwartz & Zhang, 2013b).
as lsq
We consider solving the problem under the sparsity level
k = k̄. Two measurements are calculated for evaluation.
The first is parameter estimation error kw − w̄k/kw̄k. Apart from it we calculate the percentage of successful support recovery (P SSR) as the second performance metric.
A successful support recovery is obtained if supp(w̄) =
supp(w). The evaluation is conducted on the generated
batch data copies to calculate the percentage of successful
support recovery. We use 50 data copies as validation set
to select the parameter λ from {10−6 , ..., 102 } and the percentage of successful support recovery is evaluated on the
other 100 data copies.

N training data examples {xi }N
i=1 is designed to have two components. The first component is the top-k̄ feature
dimensions drawn from multivariate Gaussian distribution
N (µ1 , Σ). Each entry in µ1 ∈ Rk̄ independently follows
standardnormal distribution. The entries of covariance
1
i=j
Σij =
. The second component consist0.25 i 6= j
s the left d − k̄ feature dimensions. It follows N (µ2 , I)
where each entry in µ2 ∈ Rd−k̄ is drawn from standard
normal distribution. We simulate two data parameter settings: (1) d = 500, k̄ = 100; (2) d = 300, k̄ = 100.
In each data parameter setting 150 random data copies are

Iterative hard thresholding (IHT) (Blumensath & Davies,
2009) and hard thresholding pursuit (HTP) (Foucart, 2011)
are used as the baseline primal algorithms. The parameter
estimation error and percentage of successful support recovery curves under varying training size are illustrated in
Figure 1. We can observe from this group of curves that DIHT consistently achieves lower parameter estimation error
and higher rate of successful support recovery than IHT and
HTP. It is noteworthy that most significant performance gap
between DIHT and the baselines occurs when the training
size N is comparable to or slightly smaller than the sparsity
level k̄.

This section dedicates in demonstrating the accuracy and
efficiency of the proposed algorithms. We first show the
model estimation performance of DIHT when applied to
sparse ridge regression models on synthetic datasets. Then
we evaluate the efficiency of DIHT/SDIHT on sparse `2 regularized Huber loss and Hinge loss minimization tasks
using real-world datasets.
5.1. Model parameter estimation accuracy evaluation

k̄

d−k̄

Dual Iterative Hard Thresholding
10 2

10 2

10 3

10 3

10 2

10 1

10 1

Running time

0

Running time

10

Running time

Running time

10 2
10 1

10 0

10 0

10 0
IHT
10 -1
200

HTP
400

SVR-GHT
600

DIHT

SDIHT

800

IHT
10 -1
200

1000

(a) RCV1, λ = 0.002

10 1

HTP

SVR-GHT

400

DIHT

600

SDIHT

800

IHT
10 -1
20000

1000

(b) RCV1, λ = 0.0002

HTP
40000

SVR-GHT
60000

DIHT

IHT

SDIHT

80000

10 -1
20000

100000

(c) News20, λ = 0.002

HTP
40000

SVR-GHT

DIHT

60000

SDIHT

80000

100000

(d) News20, λ = 0.0002

Figure 2. Huber loss: Running time (in second) comparison between the considered algorithms.
10 0

10 2

10 -1

10 0
10 -1

10 -1

10 -2

-2

10 -3

10 1

Primal-dual gap

Primal-dual gap

Primal-dual gap

Primal-dual gap

10 0

0

10 -1

10

=0.002
=0.0002

=0.002
=0.0002

10 2

10 1

10

10 1

10 3

=0.002
=0.0002

=0.002
=0.0002

10 -2

20

40

60

80

10 -2

2

(a) DIHT on RCV1

4

6

8

Epoch

Epoch

(b) SDIHT on RCV1

10

10 -3

20

40

60

Epoch

(c) DIHT on News20

80

10 -3

2

4

6

8

10

Epoch

(d) SDIHT on News20

Figure 3. Huber loss: The primal-dual gap evolving curves of DIHT and SDIHT. k = 600 for RCV1 and k = 60000 for News20.

5.2. Model training efficiency evaluation
5.2.1. H UBER LOSS MODEL LEARNING

model training (N  d). For news20 dataset, all of the
19, 996 samples are used as training data (d  N ).

We evaluate the algorithm efficiency of DIHT and SDIHT
by comparing their running time against three primal baseline algorithms: IHT, HTP and gradient hard thresholding
with stochastic variance reduction (SVR-GHT) (Li et al.,
2016). We first run IHT by setting its convergence criterion
N
(t)
λ
1 X
2
(w(t−1) )|
min
lHuber (yi x>
w)
+
kwk
,
(10)
to be |P (w P)−P
≤ 10−4 or maximum number of
i
(t) )
(w
2
kwk0 ≤k N
i=1
iteration is reached. After that we test the time cost spend
by other algorithms to make the primal loss reach P (w(t) ).
where
The parameter update step-size of all the considered algo
rithms is tuned by grid search. The parameter γ is set to be
yi x>
w
≥
1
 0
i
γ
>
>
0.25. For the two stochastic algorithms SDIHT and SVR1
−
y
x
w
−
y
x
w
<
1
−
γ
lHuber (yi x>
w)
=
.
i
i
i
i
2
 1 (1 − iy x> w)
2
GHT we randomly partition the training data into |B| = 10
otherwise
i
i
2γ
mini-batches.
It is known that (Shalev-Shwartz & Zhang, 2013b)
Figure 2 shows the running time curves on both datasets

under varying sparsity level k and regularization strength
γ 2
yi αi + 2 αi if yi αi ∈ [−1, 0]
∗
lHuber
(αi ) =
.
λ = 0.002, 0.0002. It is obvious that under all tested
+∞
otherwise
(k, λ) configurations on both datasets, DIHT and SDIHT
need
much less time than the primal baseline algorithms,
Two binary benchmark datasets from LibSVM data reposiIHT,
HTP
and SVR-GHT to reach the same primal sub1
tory , RCV1 (d = 47, 236) and News20 (d = 1, 355, 191),
optimality.
Figure 3 shows the primal-dual gap converare used for algorithm efficiency evaluation and comparigence
curves
with respect to the number of epochs. This
son. We select 0.5 million samples from RCV1 dataset for
group of results support the theoretical prediction in Theo1
https://www.csie.ntu.edu.tw/˜cjlin/
rem 4 and 5 that P D converges non-asymptotically.
We now evaluate the considered algorithms on the following `2 -regularized sparse Huber loss minimization problem:

libsvmtools/datasets/binary.html

Dual Iterative Hard Thresholding
10 2

10 3

10 2

10 2
IHT

HTP

SVR-GHT

DIHT

SDIHT

IHT

HTP

SVR-GHT

DIHT

SDIHT

10

1

10

Running time

10 0

Running time

Running time

Running time

10 2
10 1

1

10 1

10 0

10 0

IHT
10 -1
200

HTP

SVR-GHT

400

600

DIHT

SDIHT

800

IHT
10 -1
200

1000

(a) RCV1, λ = 0.002

HTP

SVR-GHT

400

DIHT

600

SDIHT

800

10 0
20000

1000

(b) RCV1, λ = 0.0002

40000

60000

80000

10 -1
20000

100000

(c) News20, λ = 0.002

40000

60000

80000

100000

(d) News20, λ = 0.0002

Figure 4. Hinge loss: Running time (in second) comparison between the considered algorithms.
10 0

10 2

10 -1

10 1

Primal-dual gap

10 -1

Primal-dual gap

Primal-dual gap

Primal-dual gap

10 1

0

10 0
10 -1

10 0

10 -1

10 -2

10 -3

=0.002
=0.0002

=0.002
=0.0002

10 2

10 1

10

10 2

10 3

=0.002
=0.0002

=0.002
=0.0002

10 -2

20

40

60

80

10 -2

2

(a) DIHT on RCV1

4

6

8

Epoch

Epoch

(b) SDIHT on RCV1

10

10 -3

20

40

60

Epoch

(c) DIHT on News20

80

10 -2

2

4

6

8

10

Epoch

(d) SDIHT on News20

Figure 5. Hinge loss: The primal-dual gap evolving curves of DIHT and SDIHT. k = 600 for RCV1 and k = 60000 for News20.

5.2.2. H INGE LOSS MODEL LEARNING
We further test the performance of our algorithms when
applied to the following `2 -regularized sparse hinge loss
minimization problem:
min

kwk0 ≤k

N
1 X
λ
2
lHinge (yi x>
i w) + kwk ,
N i=1
2

>
where lHinge (yi x>
i w) = max(0, 1 − yi w xi ). It is standard to know (Hsieh et al., 2008)

yi αi if yi αi ∈ [−1, 0]
∗
lHinge
(αi ) =
.
+∞ otherwise

We follow the same experiment protocol as in § 5.2.1
to compare the considered algorithms on the benchmark
datasets. The time cost comparison is illustrated in Figure 4 and the prima-dual gap sub-optimality is illustrated
in Figure 5. This group of results indicate that DIHT and
SDIHT still exhibit remarkable efficiency advantage over
the considered primal IHT algorithms even when the loss
function is non-smooth.

6. Conclusion
In this paper, we systematically investigate duality theory
and algorithms for solving the sparsity-constrained minimization problem which is NP-hard and non-convex in its

primal form. As a theoretical contribution, we develop a
sparse Lagrangian duality theory which guarantees strong
duality in sparse settings, under mild sufficient and necessary conditions. This theory opens the gate to solve the
original NP-hard/non-convex problem equivalently in a dual space. We then propose DIHT as a first-order method to
maximize the non-smooth dual concave formulation. The
algorithm is characterized by dual super-gradient ascent and primal hard thresholding. To further improve iteration efficiency in large-scale settings, we propose SDIHT
as a block stochastic variant of DIHT. For both algorithms we have proved sub-linear primal-dual gap convergence
rate when the primal loss is smooth, without assuming RIPstyle conditions. Based on our theoretical findings and numerical results, we conclude that DIHT and SDIHT are theoretically sound and computationally attractive alternatives
to the conventional primal IHT algorithms, especially when
the sample size is smaller than feature dimensionality.

Acknowledgements
Xiao-Tong Yuan is supported in part by Natural Science Foundation of China (NSFC) under Grant 61402232,
Grant 61522308, and in part by Natural Science Foundation of Jiangsu Province of China (NSFJPC) under Grant
BK20141003. Qingshan Liu is supported in part by NSFC
under Grant 61532009.

Dual Iterative Hard Thresholding

References
Argyriou, Andreas, Foygel, Rina, and Srebro, Nathan. Sparse prediction with the k-support norm. In Advances
in Neural Information Processing Systems, 2012.
Bahmani, Sohail, Raj, Bhiksha, and Boufounos, Petros T.
Greedy sparsity-constrained optimization. Journal of
Machine Learning Research, 14(Mar):807–841, 2013.
Beck, Amir and Eldar, Yonina C. Sparsity constrained
nonlinear optimization: Optimality conditions and algorithms. SIAM Journal on Optimization, 23(3):1480–
1509, 2013.

Lee, Sangkyun, Brzyski, Damian, and Bogdan, Malgorzata. Fast saddle-point algorithm for generalized dantzig
selector and fdr control with the ordered `1 -norm. In
International Conference on Artificial Intelligence and
Statistics, 2016.
Li, Xingguo, Zhao, Tuo, Arora, Raman, Liu, Han, and
Haupt, Jarvis. Stochastic variance reduced optimization
for nonconvex sparse learning. In International Conference on Machine Learning, 2016.
Nguyen, Nam, Needell, Deanna, and Woolf, Tina. Linear convergence of stochastic iterative greedy algorithms
with sparse constraints. arXiv preprint arXiv:1407.0088,
2014.

Blumensath, Thomas. Compressed sensing with nonlinear
observations and related nonlinear optimization problems. IEEE Transactions on Information Theory, 59(6):
3466–3474, 2013.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated minibatch stochastic dual coordinate ascent. In Advances in
Neural Information Processing Systems, 2013a.

Blumensath, Thomas and Davies, Mike E. Iterative hard
thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265–274, 2009.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized loss. The
Journal of Machine Learning Research, 14(1):567–599,
2013b.

Chambolle, Antonin and Pock, Thomas. A first-order
primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging
and Vision, 40(1):120–145, 2011.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), pp. 267–288, 1996.

Chen, Jinghui and Gu, Quanquan. Accelerated stochastic block coordinate gradient descent for sparsity constrained nonconvex optimization. In Conference on Uncertainty in Artificial Intelligence, 2016.
Foucart, Simon. Hard thresholding pursuit: an algorithm
for compressive sensing. SIAM Journal on Numerical
Analysis, 49(6):2543–2563, 2011.
Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, Sellamanickam. A dual
coordinate descent method for large-scale linear svm. In
International conference on Machine learning, 2008.
Jain, Prateek, Tewari, Ambuj, and Kar, Purushottam. On iterative hard thresholding methods for high-dimensional
m-estimation. In Advances in Neural Information Processing Systems, 2014.
Jain, Prateek, Rao, Nikhil, and Dhillon, Inderjit. Structured
sparse regression via greedy hard-thresholding. arXiv
preprint arXiv:1602.06042, 2016.
Lapin, Maksim, Schiele, Bernt, and Hein, Matthias. Scalable multitask representation learning for scene classification. In IEEE Conference on Computer Vision and
Pattern Recognition, 2014.

Yu, Adams Wei, Lin, Qihang, and Yang, Tianbao. Doubly stochastic primal-dual coordinate method for empirical risk minimization and bilinear saddle-point problem.
arXiv preprint arXiv:1508.03390, 2015.
Yuan, Xiao-Tong., Li, Ping, and Zhang, Tong. Gradient hard thresholding pursuit for sparsity-constrained optimization. In International Conference on Machine
Learning, 2014.
Yuan, Xiao-Tong, Li, Ping, and Zhang, Tong. Exact recovery of hard thresholding pursuit. In Advances in Neural
Information Processing Systems, 2016.
Zhang, Yuchen and Xiao, Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In International Conference on Machine Learning,
2015.
Zhu, Zhanxing and Storkey, Amos J. Stochastic parallel block coordinate descent for large-scale saddle point
problems. In AAAI Conference on Artificial Intelligence,
2016.

