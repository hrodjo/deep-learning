Estimating the unseen from multiple populations

Aditi Raghunathan 1 Gregory Valiant 1 James Zou 1 2

Abstract
Given samples from a distribution, how many
new elements should we expect to find if we continue sampling this distribution? This is an important and actively studied problem, with many
applications ranging from unseen species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to
the multiple population setting, where population
j has an unknown distribution Dj from which we
observe nj samples. We derive an optimal estimator for the total number of elements we expect
to find among new samples across the populations. Surprisingly, we prove that our estimator’s
accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem
of estimating multi-population frequency distributions. We validate our methods and theory
through extensive experiments. Finally, on a real
dataset of human genomes across multiple ancestries, we demonstrate how our approach for
unseen estimation can enable cohort designs that
can discover interesting mutations with greater
efficiency.

1. Introduction
Given samples from a distribution, many settings in machine learning and statistics involves estimating properties
of the unseen portion of the distribution, i.e. elements in
the support of the distribution that are not observed in the
samples collected so far. One important example of estimating the unseen is the problem of predicting the number
of distinct new elements in additional samples collected.
This question is famously illustrated by the case of Corbet’s
butterflies. Alexander Corbet was a British naturalist who
1
Stanford University, Stanford, CA 2 Chan Zuckerberg Biohub,
San Francisco, CA. Correspondence to: Aditi Raghunathan <aditir@stanford.edu>, Gregory Valiant <valiant@stanford.edu>,
James Zou <jamesz@stanford.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

spent two years in Malaya trapping butterflies. He found
118 rare species of butterflies for which he found only one
specimen, another 74 species with two specimens, 44 with
three specimens, etc. Corbet was naturally interested in
the butterflies that are heretofore unseen. In particular, he
wanted to estimate how many distinct new species of butterflies he can expect to discover if he were to conduct a
new expedition to Malaya—such an estimate could help
determine whether a new experiment is warranted. GoodToulmin, extending earlier work of Ronald Fisher, came
up with the remarkable estimate that the number of new
species Corbet can expect to find is simply the alternating sum 118 - 74 + 44 - ... The Good-Toulmin estimator
sparked the investigation into how to estimate the discovery rate of new elements and this remains an active area of
research. Estimating the discovery rate has many important
applications beyond the original species collection setting.
In genomics, for example, an important question is: given
the genetic variation already identified in the genomes of
individuals from some population (say, East Asia), how
many additional mutations do we expect to find by sequencing the genomes of additional individuals from East
Asia. An accurate answer to this question can improve the
cohort design of new population sequencing experiments.
Predicting the number of new elements is a particular instance of estimating the unseen. In other applications, one
may want to estimate different statistics that also depend on
the currently unobserved elements. For example, one may
want to predict how many new elements will be observed
at least twice (for reproducibility) or at most three times (if
the focus is on rare elements). More generally, one may
want to estimate the histogram of the underlying distribution, which summarizes the frequency distribution of all the
elements (see Sec. 2 for precise definition) and from which
these other statistics can be derived.
The unseen estimation literature has focused on the setting
where there is a single distribution which generate current
samples as well as any future samples. In practice, we
often have multiple distinct distributions and we observe
varying number of samples from each distribution. In the
genomics example above, in addition to sequencing data
from East Asians, we also have genome sequences of individuals from Europe, Africa, etc. The relevant question
is: given we currently have the genomes of ni individuals

Estimating the unseen from multiple populations

from population i, i ∈ {1, ..., m}, and we have identified
all the genetic variants in this group, how many total new
mutations do we expect to find if we sequence additional
bi individuals from population i. Moreover, given a finite
budget Nnew of new genomes that we can sequence, how
should we allocate this budget across the different populations to maximize the expected number of new mutations
oberved? Similarly, suppose Corbet had also collected butterflies in Brunei and Indonesia, in addition to Malaya.
Then he might want to know how many totally new species
he can expect to find if he was to spend, say, another six
month in Malaya and one year in Brunei. He might also be
interested in estimating the joint frequency distribution of
butterflies across all three regions.
Our contributions. In this paper, we address the general problem of estimating the unseen when we have samples from multiple populations, each corresponding to a
potentially distinct distribution. Despite being very natural, this multi-population problem has not been systematically studied to the best of our knowledge. We derive a
multi-population generalization of the Good-Toulmin estimator for the expected number of new elements. Surprisingly, we prove that the accuracy of our extrapolation estimator is independent of the number of populations. Moreover, it achieves the optimal super-linear extrapolation rate.
Next, we develop an efficient optimization method to estimate the more general multi-population joint frequency
distribution. This complements our extrapolation estimator, and outperforms the generalized Good-Toulmin estimator in most settings. This more general approach also
enables predictions for other statistics of interest. We systematically validate these two algorithms on synthetic data
as well as real datasets from population genetics and from
English books. Moreover, we illustrate that by estimating
the joint frequency distribution, we can significantly improve the discovery power under a budget constraint.

2. Related works
The problem of estimating the properties of the unobserved
portion of a distribution, given n samples, and the related
problem of estimating the number of new domain elements
that are likely to be observed if an additional cn samples
are collected, dates back to works of I.J. Good and A. Turing (Good, 1953), and R.A. Fisher (Fisher et al., 1943).
This was quickly followed by (Good & Toulmin, 1956),
which introduced the Good-Toulmin estimator. While the
Good-Toulmin estimator is always unbiased, the variance
increases rapidly for c ≥ 1. Subsequent works, including (Efron & Thisted, 1976) have suggested “smoothing”
approaches that tradeoff the bias and variance for this type
of approach. The recent work of Orlitsky et al. (2016) describes a clever variant that achieves good performance for

c = O(log n). This ability to accurately estimate the number of domain elements seen in a second sample of size
up to O(n log n), where n denotes the size of the original
sample, was concurrently shown via a different approach
in (Valiant & Valiant, 2016). This logarithmic factor extrapolation matches the lower bounds of (Valiant & Valiant,
2011), to constant factors. The linear estimators that we
propose in Section 4 for the multiple population setting,
and their analysis, are extensions of the smoothed GoodToulmin estimators of (Orlitsky et al., 2016).
A different approach to this problem was proposed
by Efron & Thisted (1976), who considered a linearprogramming approach to estimating this property by implicitly finding a label-less representation of the underlying distribution that was consistent with the observed frequency counts, then returning the support size of this distribution. This approach was adapted and rigorously analyzed in (Valiant & Valiant, 2011; 2013), who showed
that it provably yields an accurate representation of the frequency distribution of the underlying distribution, which
can subsequently be leveraged to yield estimates of distributional properties, including entropy, distance metrics
between distributions, and approximations for the number
of new elements that would be observed in larger samples.
Recent works (Valiant & Valiant, 2016; Zou et al., 2016)
also established that this approach can accurately estimate
the number of new elements that will be observed in samples of size up to O(n log n). Our optimization-based algorithm, described in Section 5, generalizes this approach.

3. Definitions and examples
Let Ω denote the domain, and D1 , ..., Dm denote m probability distributions over Ω. Di represents the frequency
of elements in population i. Note that it is not restrictive
to assume that the populations share the same domain Ω
since different Di ’s may have distinct supports. We model
the multi-population unseen estimation as a two stage process. In the first period, we observe nj independent samples from the j-th population, {Xij }j=1,...,m
i=1,...,nj . This is the
seen data. In period two, which is in the future, we will
sample additional tj nj samples from the j-th population,
{Yij }j=1,...,m
i=1,...,tj nj . The period two samples are unseen and
we would like to estimate some statistic U ({Yij }, {Xij }).
We can think of tj ≥ 0 as the extrapolation factors. If tj
is large, then we will obtain many more samples from population j in the second period compared to what we have,
and the problem of estimating U could be more challenging. We can take tj as given for the purpose of estimating
U . We later discuss how we to leverage our estimator of
U to optimize the tj ’s in order to maximize the number of
new discoveries. Note that in general, the nj ’s and tj ’s can
differ arbitrarily across the populations.

Estimating the unseen from multiple populations

A particularly important statistic is U = the total number
of new elements in {Yij } that are not observed in the period one samples {Xij }. A good estimator for this U quantifies the expected information gain of the second period.
In the one population setting, this statistic is the focus of
Good-Toulmin and a large number of papers. Other useful
choices of U could be the number of distinct new elements
that are observed at least twice in {Yij }, which could be
relevant if we want some reproducibility.

0
·
1

0
1

1
1
2

2
0
2

The (1, 1) entry is 2 because A, B are observed once in
each set of samples; the (1, 0) entry is 1 because exactly
one element, C, is observed once in the samples from Population 1 and zero times in the samples from Population 2.
By convention, we omit the (0, 0) element.

Beyond estimating these single parameters, we could also
hope to use the samples {Xij } to estimate the histogram of
D1 , ..., Dm . The multi-population histogram, defined below, captures all of the information about the populations,
other than the labels of the domain.
Definition 3.1 (Multi-population histogram). Given a collection of m distributions D1 , . . . , Dm over a common domain Ω, the corresponding multi-population histogram H
is a mapping from [0, 1]m \ 0m 7→ N ∪ {0}. For each
α = (α1 , α2 , . . . αm ) ∈ [0, 1]m \ 0m , H(α) = |{y ∈ Ω |
Dj (y) = αj , 1 ≤ j ≤ m}|, where Di (y) is the probability
mass of domain element y. in the ith distribution Di .

4. A linear estimator

Any symmetric multi-population statistic—one that is invariant to permuting the labels of the domain—is a function of only the histogram. Such statistics include distance
metrics between the distributions/populations, measures of
the entropy of the populations, and the number of new elements that one is likely to observe in a second batch of
samples. The multi-population histogram is also of intrinsic interest; in population genetics, H is exactly the joint
frequency distribution of mutations, and reveals information about demographic history (e.g. historical variations
in population size) and selective pressures. One benefit of
focusing on the histogram is that, while it does not contain
as much information as the actual labeled distributions, it
can often be accurately recovered even when given too few
samples to learn the (labeled) distributions to any significant accuracy (Valiant & Valiant, 2011).

Proposition 4.1. For any number of populations m, and
any extrapolation factors tj ≥ 0, j = 1, ...m, Û is an unbiased estimator of U .

Both for directly predicting U and estimating H, we rely
on a label-less representation of the samples, termed the
fingerprint of {Xij }. The fingerprint of the samples is the
analog of the histogram of the distributions, and captures
all the information of {Xij } that is relevant for estimating
symmetric statistics.
Definition 3.2 (Multi-population fingerprint). Given the
samples {Xij }, its fingerprint is an m-dimensional tensor
Φ whose i1 ...im -th entry, φi1 ...im , is the number of distinct
elements observed exactly ij times in the samples from population j. Here each ij can range from 0 to nj .
Example 3.1. Suppose we have five samples from Population 1, (A, B, C, E, F ), and seven from Population 2,
(A, B, D, E, E, F, F ). The corresponding 2-dimensional
fingerprint of this data is given by the following matrix:

Unbiased estimator. Given the empirical fingerprints Φ
and the extrapolation factors tj , j = 1, ..., m, we define the
following estimator


m
X
Y
 (−tj )ij  φi1 ...im .
(1)
Û = −
i1 ,...,im :

P

ij >0

j=1

Û is a weighted alternating sum of the empirical fingerprints where the weights are determined by the extrapolation factors tj .

Proof of the proposition appears in Appendix 7.
Û is linear in the fingerprint entries. Its computational
cost is
Plinear in the total number of period one samples,
n = j nj , since there can be at most n non-zero fingerprint entries. To build more intuition for Û , we illustrate its
application in two simple settings.
Example 4.2. Consider the setting where all m distribution are identical, i.e. all the samples are drawn from the
same discrete distribution D. Let tj = 1, ∀j for simplicity.
After rearranging terms, Û can be written as


X
X
(−1)k+1 
φi1 ...im  .
Û =
k=1

(i1 ,...,im ):

P

ij =k

Because the populations are identical, the sum in the
parenthesis is just the number of elements that are observed
k times from all the samples so far. Hence the general estimator Û reduces to the one dimensional Good-Toulmin
estimator when all m populations are identical.
Example 4.3. Suppose the supports of the distributions Di
are disjoint. Then the only possible non-zero fingerprint
entries are φi1 ...im where exactly one of the ij is great than
0 and all the other ij ’s are zero. For simplicity, assume
Pk P
tj = 1 for all j. Then Û = j=1 i (−1)k+1 φki , where
φki is the marginal fingerprint entry of the number of elements that are observed i times in population k. Hence

Estimating the unseen from multiple populations

when the populations are disjoint, the expected number of
new elements is the sum of the expected number of new elements in each population. When the populations have overlapping support, we have the nontrivial interaction terms
due to the cross-population fingerprint entries.
General weighted linear estimator. While Û is unbiased,
its variance could be large if some of the extrapolation factors tj ’s are greater than 1. This is because the powers of tj
appear in Eqn. 1. To address this issue, we introduce a general class of multi-population weighted linear estimators.


m
X
Y
 (−tj )ij  φi1 ,...,im W (i1 , . . . , im ).
Û W = −
ij :

P

ij >0

j=1

We focus on a particular weighting scheme, which is an
extension of that introduced
in (Orlitsky
et al., 2016):


P
W (i1 , i2 , . . . im ) = P L ≥ j∈A ij where L ∼ Poi(r)
and A = {j : tj > 1} are the populations that we would
like to extrapolate beyond the original sample size. If
tj ≤ 1 ∀j, then W = 1 and Û W is just the unbiased estimator Û . The Poisson rate r is a tuning parameter that determines the bias/variance tradeoff of Û W . As r increases, all
the weights approaches 1 and Û W approaches the unbiased
estimator Û . As r decreases, the fingerprint entries φi1 ...im
with some large ij ’s—which are also the terms with high
variance—are weighted by a factor that is close to 0. This
reduces the total variance of Û W at the cost of introducing
bias. We will see how to set r as a function of the nj ’s and
tj ’s in order to minimize the overall estimation error. In the
rest of the paper, unless otherwise specified, we will use
Û W to denote the multi-population linear estimator with
Poisson weights.
Performance guarantee of the weighted
estimator.
We

2 
W
Û
−U
use relative mean squared error, E P nj tj
, to quantify the performance
of Û W . This is a natural error metric,
P
because
nj tj is the number of samples in period two
and we care about how the error in the predicted number of
new elements scales with the number of samples. Without
loss of generality, we can relabel the populations so that
t1 = maxj tj . We are especially interested in the setting
when t1 ≥ 1 (i.e. large extrapolation).
Proposition 4.4. Suppose
t1 = maxj tj ≥ 1 and the PoisP
son rate is r =


log(

Û W − U
P
nj tj

E

j

nj (tj +1))
,
2t1

!2 
≤



then

n1 t1 +

P

n1 t1

W
Û
P −U
nj tj

2 

≤  with

t1 = O(log n1 / log(1/)). This means that Û W has low
relative error even when the largest extrapolation factor t1
is logarithmic in its initial sample size n1 .
Remark 4.6 (no dependence on m). Note that the relative
error in Eqn. 2 does not depend on the number of populations m. This is somewhat surprising since the number of terms in Û W potentially grows exponentially with
m and the variance of each fingerprint entry φi1 ...im also
increases as the number of population increases. This population agnostic property of Û W guarantees its accuracy
even when m is arbitrarily large.
Remark 4.7 (lower bound). Here we have focused on a
specific form of the estimator Û W where the weights W
of the fingerprint entries correspond to the tail probability of Poisson distributions. A natural question is whether
there exists a different form of the weights or a different estimator altogether that can consistently be more accurate
than our current Û W . The answer is essentially no due to
the following lower bound for one population extrapolation (Orlitsky et al., 2016; Valiant & Valiant, 2011): There
exists universal constants c, c0 such that for all estimators
Û , if the
factor t > c, then ∃ distribution such
extrapolation
2 
0
Û −U
> n−c /t . Here n is the number of samthat E
nt
∼

ples drawn from this distribution in period one. This lower
bound implies that in order to guarantee that the relative
error is less than  in general, the extrapolation factor can
be at most O(log n/ log(1/)), matching Prop. 4.4.
Outline of the proof of Prop. 4.4 (detailed analysis is in
the Appendix). To analyze the relative error, we separately
quantify the bias and variance of Û W in terms of nj , tj , r.
Lemma 4.8 (Bias). Let r denote the rate of the Poisson
weights, then




X


nj (tj + 1) e−r
E[Û W − U ] ≤ 
j∈A

Lemma 4.9 (Variance). Without loss of generality, let t1 =
maxj tj and suppose t1 ≥ 1 then
X
X
Var(Û W − U ) ≤
nj e2r(t1 −1) +
nj tj .
j

To obtain the optimal r given in the statement of Prop. 4.4,
we set r to balance the squared bias and variance.
j

nj



−1/t1

n1

.

(2)

Remark 4.5 (log extrapolation factor). Suppose the ratio Pn1nj is bounded, then Prop. 4.4 guarantees that for
j

any  > 0, we can achieve E



5. Estimating the multi-population frequency
distribution
While we have a linear estimator for the number of unseen elements in a new sample, it is challenging to con-

Estimating the unseen from multiple populations

struct good estimators of other statistics (e.g. number of
new elements observed ≥ 2) directly from the fingerprints.
As discussed in Sec. 3, we can also take the less direct
approach of first trying to estimating the true underlying
multi-population histogram. Given an accurate reconstruction of this underlying histogram, we can then estimate any
symmetric statistic of the future samples. We discuss some
of the uses of such a representation in Section 5.
Recovering the frequency distribution The core of our
algorithm to recover the multi-population histogram is a
natural extension of the single population algorithm presented in Valiant & Valiant (2011; 2013).
Estimating the multi-population histogram: Core
Approach.
Input: Multi-population fingerprint Φ of samples,
Output: Two estimates, Ĥcounts and Ĥll of histogram corresponding to the distributions underlying
fingerprint Φ.
• Compute Ĥcounts and Ĥll minimizing the following expressions:
Ĥcounts = arg min

X

Ĥll = arg max

X

H

H

Where [Φ̂(H)]i =

i

√

1
|Φi − Φ̂(H)i |.
1 + Φi

log poi(Φi , Φ̂(H)i ),

i

X
α

H(α)

m
Y

bino(αj , nj , ij ).

j=1

The intuition behind these two optimization problems is the
following. The histogram corresponding to a set of distributions is an unlabeled representation of the underlying
distributions, hence it makes intuitive sense to try to recover the histogram that maximizes the likelihood of the
unlabeled representation of the samples, namely the fingerprint Φ. Recent work (Acharya et al., 2016) provided
rigorous support for this intuition. In general, however,
this likelihood might be difficulty to compute. Nevertheless, an efficiently computable proxy for this likelihood can
be obtained by treating the distribution of the fingerprint,
corresponding to a histogram H, as a product distribution,
with Φi1 ,...,im distributed according to the Poisson distribution with appropriate expectation EH [Φi1 ,...,im ]. The
recent central limit theorem for “Poisson Multinomials”
from (Valiant & Valiant, 2011) provides at least some corroboration for the reasonableness of having a proxy for the
log-likelihood that decomposes linearly across the differ1
ent elements of Φ. The motivation for the √1+Φ
scaling
i
on the first proxy likelihood function is that this expression
penalizes discrepancies between the observed and expected

fingerprint entries according to a rough approximation of
the standard deviation of that fingerprint entry, as the variance of a Poisson random variable is equal to its expectation, and the observed fingerprint entry is an approximation
for the expected fingerprint entry given the true underlying
histogram.
The work (Valiant & Valiant, 2013) focused on recovering
Ĥcounts , as this optimization problem can be formulated as
a linear program, whose variables correspond to a fine discretization of the potential support of the histogram. Unfortunately, in the present multi-distribution setting, the number of variables required by this linear programming approach would scale exponentially with the number of distributions in question. Even for fingerprints derived from
modest-sized samples from two distributions, the resulting
linear program becomes impractical.
Instead of pursuing the linear programming based approach, we instead propose a black-box optimization
approach to finding a histogram that optimizes either
of the two proxy likelihood functions. In this optimization approach, the dimensionality of the optimization problem is specified by the user, and corresponds to the number of (i1 , . . . , im ) tuples for which
the returned histogram Ĥ is nonzero. Denoting this
quantity by s, the resulting optimization problem can
be regarded as the problem of specifying s vectors
(h1 , α1,1 , . . . , α1, m ), . . . , (hs , αs,1 , . . . , αs,m ). These s
vectors are then interpreted as a histogram H with
H(αj ) = hj for all j ∈ {1, . . . , s}, and H(α) = 0 for
all other vectors α.
The one additional modification that leads to a substantial
improvement in runtime is to only evaluate the proxy likelihood expressions for fingerprint entries Φi1 ,...,im ≥ 2. The
intuition for this is two-fold. First, the number of vectors
(i1 , . . . , im ) for which Φi1 ,...,im = 0 will scale exponentially with m, as opposed to scaling as some parameter
of the sample sizes; this is clearly undesirable. Second,
given that we wish to avoid evaluating the contribution to
the proxy likelihood from fingerprint entries that are zero,
we must now be careful in dealing with fingerprint entries
that are equal to 1. Suppose we have 1 element with true
probability ni and suppose we observe that fingerprint entry
Φi = 1, and the other fingerprints near i are 0. Since we are
maximizing the likelihood that Φi = 1 (without taking into
√
account the nearby 0 entries), we would assign roughly i
elements to probability ni which is undesirable. Removing
the ones largely resolves this issue. Note that the Φj = 2
entries do not cause as much of an issue, as such collisions
are unlikely to occur in regions of the fingerprint in which
there is not a significant number of domain elements.
In this one-distribution example, a constraint on the total
probability mass being 1 would resolve this issue, though

Estimating the unseen from multiple populations

analogs of this issue in the multiple distribution setting cannot be resolved in this way. Hence, we adopt the crude, but
effective approach of viewing all the empirical fingerprint
entries that are equal to 1 as being reflective of an element
in the underlying set of distributions whose probability is
close to the empirical probability of the corresponding element. We summarize the complete algorithm below:
Estimating the multi-population histogram: Full
Algorithm.
Input: Multi-population fingerprint Φ derived from
samples from m distributions of respective sizes
n1 , . . . , n m .
Output: Two estimates, Ĥcounts and Ĥll of histogram
corresponding to the distributions underlying fingerprint Φ.
• Remove fingerprint entries that are 1, and add to empirical portion of histogram:
1. Initialize m-distribution histogram Ĥemp to be
identically zero.
2. For each vector i = (i1 , . . . , im ) such that
) = 1.
Φi = 1, set Ĥemp ( ni11 , . . . , nim
m
• Compute Ĥcounts and Ĥll minimizing the following expressions:
X

Ĥcounts = arg min
H

H

i:Φ(i)≥2

X

Ĥll = arg max

√

1
|Φi − Φ̂(H)i |.
1 + Φi

log poi(Φi , Φ̂(H)i .

i:Φ(i)≥2

Where Φ̂(H)i =

X

H(α)

α

m
Y

bino(αj , nj , ij ).

j=1

Subject to the constraint that, together with Ĥemp ,
the total mass in all the distributions is 1. Namely
for all i ∈ {1, . . . , m},
X
X
αi Ĥll (α) +
αi Ĥ∗ (α) = 1.
α

α

• Return the concatenation of the empirical portion
of the histogram and the portion returned by the
optimization: Ĥcount := Ĥcount + Ĥemp , and
Ĥll := Ĥll + Ĥemp

Leveraging Ĥ for approximating the value of additional
data. An accurate representation of the histogram corresponding to the multi-population distribution underlying a
given set of observations can be leveraged to estimate a
number of useful properties. These properties include estimating the number of new domain elements that would

likely be seen given additional samples from the populations. Specifically, given a histogram Ĥ, corresponding to
m populations, we can estimate the expected number of
distinct elements that will be observed in samples from the
m populations of respective sizes n1 , . . . , nm via the simple formula:
!
m
X
Y
E[num observed] =
Ĥ(α) 1 −
(1 − αi )ni . (3)
α

i=1

An accurate approximation to the histogram can also be
leveraged to answer many other questions about the populations that can not be readily addressed via the linear estimators of Section 4. These include tasks such as estimating
the amount of data that must be collected to capture, say,
99% of the mass of the distributions in question.

6. Experiments
Evaluating the weighted linear estimator for large m.
We empirically evaluated the performance of the weighted
linear estimator Û W . The experiments were conducted
for three types of distributions—Uniform, Dirichlet and
Geometric—that are commonly used to evaluate extrapolation algorithms. Each experiment contains m = 100 populations. We have a total of 3000 distinct elements. In
the Uniform setting, each population has support on 100
elements that are randomly sampled from the 3000. For
Dirichlet, each population also has support on 100 random
elements (from the 3000), and the weights on these 100 elements are sampled from a Dirichlet prior. For the Geometric experiments, each population corresponds to a random
ordering of the 3000 elements and the k-th element is assigned probability ∝ (1 − p)k p. In period one, ten samples
are observed in each of the 100 populations. In period two,
95 randomly chosen populations have extrapolation factor
t ∈ [0, 1] and five populations have extrapolation factor
10t. This simulates the setting where we can obtain substantially more samples from a subset of the populations.
Figure 1(a, b, c) shows the results of the experiments for
Uniform, Dirichlet(1) and Geometric with p = 0.05 respectively. The results for other parameter settings are
qualitatively similar. The black curves indicate the true
number of distinct new elements we expect to observe in
period two by sampling from the true underlying distributions. The red curves are the predictions of the weighted
linear estimator (shaded regions indicate one standard deviation across 100 experiments). In all three settings, Û W
provides accurate estimate with low variance when the
maximum extrapolation factor is relatively small (≤ 3).
For Uniform and Geometric distributions, the accuracy is
high up to 10 fold extrapolation. For Zipf, the bias is low
but variance becomes large for the maximum extrapolation
factor around 10. The downward bias in the predictions

Estimating the unseen from multiple populations

Figure 1. Performance of the weighted linear estimator of Prop. 4.4 for (a) Uniform, (b) Dirichlet and (c) Geometric distributions. Each
experiment contains 100 populations. The x-axis corresponds to the maximum extrapolation factor among the 100 populations (t1 in
Prop. 4.4). The black curve indicates the true number of distinct new elements that we expect to observe in the new samples, and the red
curve shows the predicted number of new elements. The red shaded region corresponds to one standard deviation over 100 independent
experiments. (d) The 3-population earthmover distance (EMD) between the recovered histograms and the true histogram corresponding
to the populations from which the samples were drawn. The blue line corresponds to the histogram of the empirical distribution of the
samples, and the red and green lines correspond to the histograms returned by our multi-population histogram estimation algorithm,
using the count-objective and likelihood objectives, respectively. Plots depict the mean and standard deviation over 5 independent runs.
The true underlying distribution is supported on 4 · 105 domain elements. (e) Estimating the number of new domain elements that will be
observed given additional samples in the same 3-population setting. Estimates are made for when the new samples are evenly distributed
among the populations (equal) and when the majority come from one population (skewed). Error bars depict one standard deviation
about the mean, calculated based on 10 independent trials.

Figure 2. (a) Estimating the total number of unique words combining three different books using hist-opt-counts. Predictions are based
on fingerprints of samples of words—sampled without replacement either randomly (blue) or from a contiguous block of text (green)
from each book. Error bars depict one standard deviation over 10 independent runs. (b) Estimating the number of new mutations
that would be observed given additional samples from four different populations using hist-opt-counts. We consider different ratios of
sampling within these populations and observe the change in number of new mutations that would be observed.

is due to the weighting scheme. The relative error of the
2
 W
, is 0.09, 0.08 and 0.08 for
weighted estimator, ÛP n−U
j tj
the Uniform, Dirichlet and Geometric distributions when

the maximum extrapolation factor is 10. This confirms
the theoretical results of Prop. 4.4 on the accuracy of the
weighted linear estimator.

Estimating the unseen from multiple populations

Evaluating the histogram estimators. We first validated
the performance of Ĥcount and Ĥll on a three population
setting with synthetic data. The true population consists of
three uniform distributions over 200k elements, whose supports have 100k elements in common, and 100k elements
unique to each distribution. In Figure 1(d), the x-axis corresponds to the number of samples we observe from each
population, and the y-axis indicates the earthmover distance (EMD) between Ĥcount , Ĥll and the true histogram.
As a baseline, we also compute the EMD between the empirical histogram of the observed samples and the true histogram. Ĥcount and Ĥll performed roughly equally well
and both are substantially better than the empirical estimator especially when the number of observed samples is
small. Figure 1(e) illustrates the extrapolation accuracy of
our histogram estimators. We estimated Ĥcount and Ĥll using 16K from each population, and then used Eqn. 3 to estimate the number of unseen elements in additional samples.
We tested two different settings: 1) when the additional
samples are equally drawn from the three populations, and
2) a skewed mixture where 5/6 of the new samples are from
population 1 and 1/12 each are drawn from population 2
and 3. Ĥcount and Ĥll gave extremely accurate predictions.
In comparison, the weighted linear estimator Û W was accurate for the initial extrapolations but has downward bias
when the extrapolation increases, consistent with Fig. 1(ac).
Additionally, we evaluate the performance of Ĥcount on a
real dataset, in which we sampled words from three books–
Hamlet (32K total words), Treasure Island (40K) and The
Sun Also Rises (72K). We used the true word frequencies
(over the entire text) as the true histogram. We sampled a
small number of words (equal in all books) either randomly
or from a contiguous block of text and used Ĥcounts to predict the total number of distinct words in total in all three
books. In Figure 2(a), the red line is the true value, and blue
and green lines are predictions based on Ĥcount derived
from samples of either random words, or words occurring
in a random contiguous block of text, respectively. We obtain accurate estimates using a fraction of words (10K from
each book). The estimates based on independent samples
of words is more accurate than that based on contiguous
blocks of text—likely due to correlation in words that occur near each other.
Optimizing discovery rate. Given the estimated histogram Ĥcount or Ĥll , we can optimize the allocation of
new samples across the populations to maximize the number of unseen
P elements we can expect to discover given a
bound on j tj nj . To illustrate, we obtained genome sequencing data of 45K individuals from the Exome Aggregation Consortium (Lek et al., 2016). The individuals come
from four ancestries: Europeans, Africans, East Asians and
Latinos. We used all the observed mutations from the 45K

samples to construct a four population frequency distribution. For the experiment, we treat this as the ground truth
and sampled 105 mutations from each population to obtain
“seen” data. Suppose we have budget to sample 3 × 106
variants (10 fold extrapolation from current sample size),
how should we allocate these new samples across the four
populations in order to maximize the number of new variants discovered? We use Ĥcount to predict the extrapolation curves for three scenarios: 1. all the samples are allocated to Europeans (current genomic studies are heavily
enriched of Europeans); 2. the samples are evenly allocated across the four populations; 3) we explicitly optimize
the factors tj using Ĥcount . The dotted curves in Fig. 2(b)
correspond to the predictions, and the solid curves are the
actual numbers using the true distribution, showing good
agreement. Optimization using Ĥcount led to 10 % increase in the number of new variants discovered. This is a
simplistic example (there are many other factors in the design of real cohorts) but it illustrate the potential power in
having multi-population histogram estimates. In Appendix
Fig. 3, we also show that Ĥcount gives accurate predictions
for a different statistic—the number of new variants we expect to find at least twice in the new samples.

7. Discussion
We introduce and formalize the problem of multipopulation unseen estimation. We provide a weighted linear estimator for the number of new elements and a general
optimization algorithm to estimate the multi-population
histogram. These two approaches have complementary
strength. The weighted linear estimator Û W specifically
estimates the number of unseen elements. It’s accuracy
is independent of the number of populations, m, and it is
worst-case optimal. This can be a good method especially
when m is large and the extrapolation factor is small compared to log of the number of observed samples. When the
extrapolation is larger, however, Û W is consistently downward biased due to its variance-reducing weights. For relatively small number of populations (m = 2, 3, 4) and larger
extrapolation factors, the unseen predictions of our histogram estimators, Ĥcount and Ĥll are significantly more
accurate than Û W . While both likely have comparable
worst-case performance, the linear estimator nearly always
incurs this worst-case loss and is largely incapable of extrapolating beyond this worst-case logarithmic factor. In
contrast, the histogram-based estimators seem to perform
well for much larger extrapolation factors on all of the distributions that we considered. Ĥcount and Ĥll are computationally more expensive than Û W , but are still tractable
for many applications—each run of our experiments took
less than 20 minutes on a single laptop.

Estimating the unseen from multiple populations

Acknowledgments
Gregory Valiant’s contributions were supported by NSF
CAREER CCF-1351108 and a Sloan Research Fellowship.
James Zou is a Chan Zuckerberg Biohub investigator and
is supported by NSF CISE-1657155.

References
Acharya, Jayadev, Das, Hirakendu, Orlitsky, Alon, and
Suresh, Ananda Theertha. A unified maximum likelihood approach for optimal distribution property estimation. arXiv preprint arXiv:1611.02960, 2016.
Efron, Bradley and Thisted, Ronald. Estimating the number of unsen species: How many words did shakespeare
know? Biometrika, pp. 435–447, 1976.
Fisher, Ronald A, Corbet, A Steven, and Williams, Carrington B. The relation between the number of species
and the number of individuals in a random sample of an
animal population. The Journal of Animal Ecology, pp.
42–58, 1943.
Good, IJ and Toulmin, GH. The number of new species,
and the increase in population coverage, when a sample
is increased. Biometrika, 43(1-2):45–63, 1956.
Good, Irving J. The population frequencies of species and
the estimation of population parameters. Biometrika, pp.
237–264, 1953.
Lek, Monkol, Karczewski, Konrad J, Minikel, Eric V,
Samocha, Kaitlin E, Banks, Eric, Fennell, Timothy,
ODonnell-Luria, Anne H, Ware, James S, Hill, Andrew J, Cummings, Beryl B, et al. Analysis of proteincoding genetic variation in 60,706 humans. Nature, 536
(7616):285–291, 2016.
Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.
Optimal prediction of the number of unseen species.
Proceedings of the National Academy of Sciences, pp.
201607774, 2016.
Valiant, Gregory and Valiant, Paul. Estimating the unseen:
an n/log (n)-sample estimator for entropy and support
size, shown optimal via new clts. In Proceedings of the
forty-third annual ACM symposium on Theory of computing, pp. 685–694. ACM, 2011.
Valiant, Gregory and Valiant, Paul. Instance optimal learning of discrete distributions. In Proceedings of the 48th
Annual ACM SIGACT Symposium on Theory of Computing, pp. 142–155. ACM, 2016.
Valiant, Paul and Valiant, Gregory. Estimating the unseen:
improved estimators for entropy and other properties. In
Advances in Neural Information Processing Systems, pp.
2157–2165, 2013.

Zou, James, Valiant, Gregory, Valiant, Paul, Karczewski,
Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol,
Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G.
Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects. Nature Communications, 7, 2016.

