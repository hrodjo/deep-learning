StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent

Tyler B. Johnson 1 Carlos Guestrin 1

Abstract
Coordinate descent (CD) is a scalable and simple
algorithm for solving many optimization problems in machine learning. Despite this fact,
CD can also be very computationally wasteful. Due to sparsity in sparse regression problems, for example, often the majority of CD
updates result in no progress toward the solution. To address this inefficiency, we propose a
modified CD algorithm named ‚ÄúStingyCD.‚Äù By
skipping over many updates that are guaranteed
to not decrease the objective value, StingyCD
significantly reduces convergence times. Since
StingyCD only skips updates with this guarantee, however, StingyCD does not fully exploit the problem‚Äôs sparsity.
For this reason, we also propose StingyCD+, an algorithm
that achieves further speed-ups by skipping updates more aggressively. Since StingyCD and
StingyCD+ rely on simple modifications to CD,
it is also straightforward to use these algorithms with other approaches to scaling optimization. In empirical comparisons, StingyCD and
StingyCD+ improve convergence times considerably for `1 -regularized optimization problems.

1. Introduction
Known to be simple and fast, coordinate descent is a highly
popular algorithm for training machine learning models.
For `1 -regularized loss minimization problems, such as the
Lasso (Tibshirani, 1996), CD iteratively updates just one
weight variable at a time. As it turns out, these small yet
inexpensive updates efficiently lead to the desired solution.
Another attractive property of CD is its lack of parameters
that require tuning, such as a learning rate.
Due to its appeal, CD has been researched extensively in
1

University of Washington, Seattle, WA. Correspondence
to: Tyler Johnson <tbjohns@washington.edu>, Carlos Guestrin
<guestrin@cs.washington.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

recent years. This includes theoretical (Nesterov, 2012;
Shalev-Shwartz & Tewari, 2011) and more applied (Fan
et al., 2008; Friedman et al., 2010) contributions. Many
works also consider scaling CD using parallelism (Bradley
et al., 2011; RichtaÃÅrik & TakaÃÅcÃå, 2016). For surveys of research on CD, see (Wright, 2015) or (Shi et al., 2016).
Despite its popularity, CD has a significant drawback: in
many applications, the majority of coordinate updates yield
no progress toward convergence. In sparse regression, most
entries of the optimal weight vector equal zero. When CD
updates these weights during optimization, the weights often equal zero both before and after they are updated. This
is immensely wasteful! Computing these ‚Äúzero updates‚Äù
requires time yet leaves the iterate unchanged.
In this work, we propose StingyCD, an improved CD algorithm for sparse optimization and linear SVM problems. With minimal added overhead, StingyCD identifies
many coordinate updates that are guaranteed to result in no
change to the current iterate. By skipping over these zero
updates, StingyCD obtains much faster convergence times.
StingyCD is related to safe screening tests (El Ghaoui et al.,
2012), which for Lasso problems, guarantee some weights
equal zero at the solution. The algorithm can subsequently
ignore screened weights for the remainder of optimization.
Unfortunately, for screening to be effective, a good approximate solution must already be available. For this reason,
screening often has little impact until convergence is near
(Johnson & Guestrin, 2016).
By identifying zero updates rather than zero-valued
weights at the solution, StingyCD drastically improves
convergence times compared to safe screening. At the same
time, we find that skipping only updates that are guaranteed to be zero is limiting. For this reason, we also propose
StingyCD+, an algorithm that estimates a probability that
each update is zero. By also skipping updates that are likely
zero, StingyCD+ achieves even greater speed-ups.
StingyCD and StingyCD+ require only simple changes to
CD. Thus, we can combine these algorithms with other
improvements to CD. In this work, we apply StingyCD+
to proximal Newton and working set algorithms. In both
cases, incorporating StingyCD+ leads to efficiency improvements, demonstrating that ‚Äústingy updates‚Äù are a ver-

StingyCD

Iteration t requires O(NNZ (Ai )) time, where NNZ (Ai )
is the number of nonzero entries in column
 Ai . Bottleneck

operations are computing the dot product Ai , r(t‚àí1) and
updating r(t) . We note implementations typically compute
2
kAi k once and then cache this value for later updates.

Algorithm 1 Coordinate descent for solving (P)
(0)

m

(0)

initialize x ‚Üê 0 ; r ‚Üê b
for t = 1, 2, . . . T do
i ‚Üê get next
 coordinate()

(t‚àí1)
i‚àíŒª
(t‚àí1) hAi ,r
Œ¥ ‚Üê max ‚àíxi
,
kA k2
i

(t)

2.2. Wasteful updates in coordinate descent

(t‚àí1)

x ‚Üêx
+ Œ¥ei
r(t) ‚Üê r(t‚àí1) ‚àí Œ¥Ai
return x(T )

Because of the nonnegativity constraint and regularization
(t‚àí1)
penalty in (P), often xi
= 0 in Algorithm 1. In this
(t‚àí1)
case, if r
lies outside of the ‚Äúactive update‚Äù region

satile and effective tool for scaling CD algorithms.

Ai = {r : hAi , ri ‚àí Œª > 0} ,

2. StingyCD for nonnegative Lasso
We introduce StingyCD for solving the problem
minimize
m
x‚ààR

s.t.

f (x) :=

1
2

2

kAx ‚àí bk + Œª h1, xi

x‚â•0

.

(P)

(P) is known as the ‚Äúnonnegative Lasso.‚Äù Importantly, applications of StingyCD are not limited to (P). In ¬ß4, we explain how to apply StingyCD to general Lasso and sparse
logistic regression objectives as well as SVM problems.
In (P), A is an n √ó m design matrix, while b ‚àà Rn is a labels vector. Solving (P) results in a set of learned weights,
which define a linear predictive model. The right term
in the objective‚Äîcommonly written as Œª kxk1 for Lasso
problems without the nonnegativity constraint‚Äîis a regularization term that encourages the weights to have small
value. The parameter Œª > 0 controls the impact of the
regularization term. Due to the nonnegativity constraint, a
solution to (P) is sparse for sufficiently large Œª. That is, the
majority of entries in a solution to (P) have value zero.
Advantages of sparsity include reduced resources needed
at test time, more interpretable models, and statistical efficiency (Wainwright, 2009). In this paper, we propose an
algorithm that exploits sparsity for efficient optimization.
2.1. Coordinate descent
Coordinate descent (CD) is a popular algorithm for solving
(P). Algorithm 1 defines a CD algorithm for this problem.
During iteration t, a coordinate i ‚àà [m] is selected, usually
at random or in round-robin fashion. The ith entry in x(t)
(t)
(t‚àí1)
is updated via xi ‚Üê xi
+ Œ¥, while remaining weights
do not change. The value of Œ¥ is chosen to maximally decrease the objective subject to the nonnegativity constraint.
Defining the residuals vector r(t‚àí1) = b ‚àí Ax(t‚àí1) , we
can write Œ¥ as
n
D
E
o
(t‚àí1)
Œ¥ = max ‚àíxi
, kA1 k2
Ai , r(t‚àí1) ‚àí Œª
. (1)
i




meaning Ai , r(t‚àí1) ‚àí Œª ‚â§ 0, then (1) implies that Œ¥ = 0.
In this scenario, weight i equals zero at the beginning and
end of iteration t. When solutions to (P) are sufficiently
sparse, these ‚Äúzero updates‚Äù account for the majority of iterations in naive CD algorithms. Computing these updates
is very wasteful! Each zero update requires O(NNZ (Ai ))
time yet results in no progress toward convergence.
2.3. Stingy updates
Our proposed algorithm, StingyCD, improves convergence
times for CD by ‚Äúskipping over‚Äù many zero updates. Put
differently, StingyCD computes some zero updates in O(1)
time rather than O(NNZ (Ai )) time by guaranteeing Œ¥ = 0
without computing this quantity via (1).
We saw in ¬ß2.2 that sufficient conditions for Œ¥ = 0 are
(t‚àí1)
(i) xi
= 0 and (ii) r(t‚àí1) ‚àà
/ Ai . Since directly testing
the second condition requires O(NNZ (Ai )) time, simply
checking these conditions does not lead to a useful method
for quickly guaranteeing Œ¥ = 0.
The insight that enables StingyCD is that we can relax the
condition r(t‚àí1) ‚àà
/ Ai to form a condition that is testable
in constant time. This relaxation depends on a region S (t)
for which r(t‚àí1) ‚àà S (t) . In particular, S (t) is a ball:
n
o
2
S (t) = r : kr ‚àí rrk < q (t‚àí1) ,

2
where q (t‚àí1) = r(t‚àí1) ‚àí rr .
Above, rr is a ‚Äúreference residuals‚Äù vector‚Äîa copy of the
residuals from a previous iteration. Formally, rr = r(t‚àík)
for some k ‚â• 1 (to be defined more precisely later). Note
that r(t‚àí1) lies on the boundary of S (t) .
(t‚àí1)

At any iteration t such that xi
= 0, StingyCD considers
whether S (t) ‚à©Ai = ‚àÖ before computing Œ¥. If this condition
is true, it is guaranteed that Œ¥ = 0, and StingyCD continues
to iteration t+1 without computing Œ¥ directly. We illustrate
this concept in Figure 1. Defining gi = ‚àí hAi , rri + Œª, we

StingyCD

Algorithm 2 StingyCD for solving (P)
initialize x(0) ‚Üê 0m ; r(0) ‚Üê b; rr ‚Üê r(0) ;
q (0) ‚Üê 0; œÑ ‚Üê compute thresholds(x(0) )
for t = 1, 2, . . . T do
# Update reference residuals on occasion:
if should update reference() then
rr ‚Üê r(t‚àí1)
œÑ ‚Üê compute thresholds(x(t‚àí1) )
q (t‚àí1) ‚Üê 0
i ‚Üê get next coordinate()
Figure 1. Geometry of StingyCD. At iteration t of CD, if
(t‚àí1)
xi
= 0 and r(t‚àí1) ‚àà
/ Ai , then Œ¥ = 0. In this case, computing
Œ¥ is wasteful because the ‚Äúupdate‚Äù makes no change to x(t‚àí1) .
StingyCD skips over many zero updates by establishing a region
S (t) for which r(t‚àí1) ‚àà S (t) . If S (t) ‚à© Ai = ‚àÖ, it is guaranteed
that Œ¥ = 0, and StingyCD continues to iteration t + 1 without
computing Œ¥ directly. In the illustration, StingyCD successfully
guarantees Œ¥ = 0, since q (t‚àí1) ‚â§ œÑi . In contrast, StingyCD
‚àö
would compute Œ¥ directly if q (t‚àí1) > œÑi . We note œÑi is welldefined in the illustration; since rr ‚àà
/ Ai , we have œÑi ‚â• 0.

can simplify the condition S (t) ‚à© Ai = ‚àÖ as follows:
S (t) ‚à© Ai = ‚àÖ

‚áî

max hAi , ri ‚àí Œª ‚â§ 0

r‚ààS (t)

‚áî
‚áî

‚àígi + kAi k

p

q (t‚àí1) ‚â§ 0
g2

q (t‚àí1) ‚â§ sign (gi ) kAi k2 := œÑi .
i

Thus, if q (t‚àí1) ‚â§ œÑi , then r(t‚àí1) ‚àà
/ Ai (implying Œ¥ = 0
(t‚àí1)
if also xi
= 0). We note that œÑi can take positive or
negative value, depending on if rr ‚àà Ai . If rr ‚àà
/ Ai , then
gi ‚â• 0, which implies œÑi ‚â• 0. However, if rr ‚àà Ai , then
œÑi < 0, and since q (t‚àí1) is nonnegative, it cannot be true
that q (t‚àí1) ‚â§ œÑi ‚ÄîStingyCD does not skip over coordinate
i in this case. Thus, the magnitude of œÑi is not significant to
StingyCD when œÑi < 0, though this magnitude has greater
importance for StingyCD+ in ¬ß3.
Importantly, the condition q (t‚àí1) ‚â§ œÑi can be tested with
minimal overhead by (i) updating rr only occasionally,
(ii) precomputing hAi , rri and œÑi for all i whenever rr is
updated, and (iii) caching the value of q (t‚àí1) , which is updated appropriately after each nonzero coordinate update.
2.4. StingyCD definition and guarantees
StingyCD is defined in Algorithm 2. StingyCD builds
upon Algorithm 1 with three simple changes. First, during some iterations, StingyCD updates a reference residuals vector, rr ‚Üê r(t‚àí1) . When rr is updated, StingyCD
also computes a thresholds vector, œÑ . This requires evaluating hAi , rri for all columns in A. While updating rr
is relatively costly, more frequent updates to rr result in
greater computational savings due to skipped updates.

(t‚àí1)

if q (t‚àí1) ‚â§ œÑi and xi
= 0 then
# Skip update:
x(t) ‚Üê x(t‚àí1) ; r(t) ‚Üê r(t‚àí1) ; q (t) ‚Üê q (t‚àí1)
continue
# Perform 
coordinate update:

(t‚àí1)
i‚àíŒª
(t‚àí1) hAi ,r
Œ¥ ‚Üê max ‚àíxi
,
kA k2
i

x(t) ‚Üê x(t‚àí1) + Œ¥ei
r(t) ‚Üê r(t‚àí1) ‚àí Œ¥A
i

2
q (t) ‚Üê q (t‚àí1) ‚àí 2Œ¥ Ai , r(t‚àí1) ‚àí rr + Œ¥ 2 kAi k
return x(T )
function compute thresholds(x)
initialize œÑ ‚Üê 0m
for i ‚àà [m] do
gi ‚Üê hAi , Ax ‚àí bi + Œª
g2
œÑi ‚Üê sign (gi ) kAi k2
i

return œÑ
The second change to CD is that StingyCD tracks the

2
quantity q (t) = r(t) ‚àí rr . After each update to rr,
StingyCD sets q (t) to 0. After each nonzero residuals update, r(t) ‚Üê r(t‚àí1) ‚àí Œ¥Ai , StingyCD makes a corresponding update to q (t) . Importantly,
the quantities
required for

2 

this update to q (t) ‚ÄîkAi k , Ai , r(t‚àí1) , hAi , rri, and Œ¥‚Äî
have all been computed earlier by the algorithm. Thus, by
caching these values, updating q (t) requires negligible time.
The final modification to CD is StingyCD‚Äôs use of stingy
updates. Before computing Œ¥ during each iteration t,
(t‚àí1)
StingyCD checks whether q (t‚àí1) ‚â§ œÑi and xi
= 0.
If both are true, StingyCD continues to the next iteration
without computing Œ¥. The threshold œÑi is computed after
each update to rr. If rr ‚àà
/ Ai , the value of œÑi equals the
squared distance between rr and Ai . If rr ‚àà Ai , this quantity is the negative squared distance between rr and AC
i .
StingyCD‚Äôs choice of œÑ ensures that each skipped update
is ‚Äúsafe.‚Äù We formalize this concept with our first theorem:
Theorem 2.1 (Safeness of StingyCD). In Algorithm 2, every skipped update would, if computed, result in Œ¥ = 0.

StingyCD
(t‚àí1)

That is, if q (t‚àí1) ‚â§ œÑi and xi
(
max

(t‚àí1)
‚àíxi
,




= 0, then

)

Ai , b ‚àí Ax(t‚àí1) ‚àí Œª
2

kAi k

= 0.

We prove Theorem 2.1 in Appendix A.
Theorem 2.1 is useful because it guarantees that although
StingyCD skips many updates, CD and StingyCD have
identical weight vectors for all iterations (assuming each
algorithm updates coordinates in the same order). Our next
theorem formalizes the notion that these skipped updates
come nearly ‚Äúfor free.‚Äù We prove this result in Appendix B.
Theorem 2.2 (Per iteration time complexity of StingyCD).
Algorithm 2 can be implemented so that iteration t requires
‚Ä¢ Less time than an identical iteration of Algorithm 1 if
(t‚àí1)
q (t‚àí1) ‚â§ œÑi and xi
= 0 (the update is skipped)
and rr is not updated. Specifically, StingyCD requires
O(1) time, while CD requires O(NNZ (Ai )) time.
‚Ä¢ The same amount of time (up to an O(1) term) as a
CD iteration if the update is not skipped and rr is not
updated. In particular, both algorithms require the
same number of O(NNZ (Ai )) operations.
‚Ä¢ More time than a CD iteration if rr is updated. In this
case, StingyCD requires O(NNZ (A)) time.
Note StingyCD requires no more computation than CD for
nearly all iterations (and often much less). However, the
cost of updating rr is not negligible. To ensure updates to
rr do not overly impact convergence times, we schedule
reference updates so that StingyCD invests less than 20%
of its time in updating rr. Specifically, StingyCD first updates rr after the second epoch and records the time that
this update requires. Later on, rr is updated each time an
additional 5x of this amount of time has passed.

3. Skipping extra updates with StingyCD+
As we will see in ¬ß6, StingyCD can significantly reduce
convergence times. However, StingyCD is also limited by
the requirement that only updates guaranteed to be zero are
skipped. In cases where q (t‚àí1) is only slightly greater than
œÑi , intuition suggests that these updates will likely be zero
too. Perhaps StingyCD should skip these updates as well.
In this section, we propose StingyCD+, an algorithm that
also skips many updates that are not guaranteed to be zero.
To do so, StingyCD+ adds two components to StingyCD:
(i) a computationally inexpensive model of the probability
that each update is zero, and (ii) a decision rule that applies
this model to determine whether or not to skip each update.

Figure 2. Probability of a useful update. StingyCD skips update
(t‚àí1)
t iff q (t‚àí1) ‚â§ œÑi and xi
= 0, which guarantee Œ¥ = 0. To skip
more updates, StingyCD+ applies the intuition that if q (t‚àí1) is
only slightly larger than œÑi , it is unlikely that r(t‚àí1) ‚àà Ai , making it unlikely that Œ¥ 6= 0. To do so, StingyCD+ models the probability that Œ¥ 6= 0 during iteration t, denoted P (U (t) ). Assum(t‚àí1)
ing xi
= 0 in the illustrated scenario, StingyCD+ computes
(t)
P (U ) by dividing the length of the black arc (bd(S (t) ) ‚à© Ai )
by the circumference of S (t) .

3.1. Modeling the probability of nonzero updates
(t‚àí1)

During iteration t of StingyCD, suppose xi
= 0 but
œÑi < q (t‚àí1) . StingyCD does not skip update t. For now,
we also assume œÑi > ‚àíq (t‚àí1) (otherwise we can guarantee r(t‚àí1) ‚àà Ai , which implies Œ¥ 6= 0). Let U (t) be
a variable that is true if Œ¥ 6= 0‚Äîupdate t is useful‚Äîand
false otherwise. From the algorithm‚Äôs perspective, it is uncertain whether U (t) is true or false when iteration t begins. Whether or not U (t) is true depends on whether
r(t‚àí1) ‚àà Ai , which requires O(NNZ (Ai )) time to test.
This computation will be wasteful if U (t) is false.
StingyCD+ models the probability that U (t) is true using
information available to the algorithm. Specifically, r(t‚àí1)
lies on the p
boundary of S (t) , which is a ball with center rr
and radius q (t‚àí1) . This leads to a simple assumption:
Assumption 3.1 (Distribution of r(t‚àí1) ). To model the
probability P (U (t) ), StingyCD+ assumes r(t‚àí1) is uniformly distributed on the boundary of S (t) .
By making this assumption, P (U (t) ) is tractable. In particular, we have the following equation for P (U (t) ):
(t‚àí1)

Theorem 3.2 (Equation for P (U (t) )). Assume xi
=0
and œÑi ‚àà (‚àíq (t‚àí1) , q (t‚àí1) ). Then Assumption 3.1 implies
(
1
n‚àí1 1
2 I(1‚àíœÑi /q (t‚àí1) ) ( 2 , 2 ) if œÑi ‚â• 0,
(t)
P (U ) =
1
1 ‚àí 12 I(1+œÑi /q(t‚àí1) ) ( n‚àí1
2 , 2 ) otherwise,
where Ix (a, b) is the regularized incomplete beta function.
Included in Appendix C, Theorem 3.2‚Äôs proof calculates
the probability that r(t‚àí1) ‚àà Ai by dividing the area of

StingyCD

Ai ‚à© bd(S (t) ) by that of bd(S (t) ) (illustrated in Figure 2).
This fraction is a function of the incomplete beta function
since Ai ‚à© bd(S (t) ) is a hyperspherical cap (Li, 2011).
Using Theorem 3.2, StingyCD+ can approximately evaluate P (U (t) ) efficiently using a lookup table. Specifically,
for 128 values of x ‚àà (0, 1), our implementation defines
1
an approximate lookup table for Ix ( n‚àí1
2 , 2 ) prior to iteration 1. Before update t, StingyCD+ computes œÑi /q (t‚àí1)
and then finds an appropriate estimate of P (U (t) ) using the
table. We elaborate on this procedure in Appendix D.
So far, P (U (t) ) models the probability that Œ¥ 6= 0 when
(t‚àí1)
œÑi ‚àà (‚àíq (t‚àí1) , q (t‚àí1) ) and xi
= 0. We can also de(t‚àí1)
(t‚àí1)
(t)
fine P (U ) for other xi
and œÑi . When xi
6= 0,
(t‚àí1)
(t)
(t‚àí1)
we define P (U ) = 1. If œÑi ‚â• q
and xi
= 0
(the scenario in which StingyCD skips update t), we let
(t‚àí1)
P (U (t) ) = 0. If œÑi ‚â§ ‚àíq (t‚àí1) and xi
= 0, we de(t)
fine P (U ) = 1 (in this final case, we can show that
S (t) ‚äÜ Ai , which guarantees r(t‚àí1) ‚àà Ai and Œ¥ 6= 0).
3.2. Decision rule for skipping updates
Given P (U (t) ), consider the decision of whether to skip
update t. Let tlast
denote the most recent iteration during
i
which StingyCD+ updated (did not skip) coordinate i. If
this has not yet occurred, define tlast
= 0. We define the
i
(t)
‚Äúdelay‚Äù Di as the number of updates that StingyCD+ did
not skip between iterations tlast
and t ‚àí 1 inclusive.
i
Our intuition for StingyCD+ is that during iteration t, if
(t)
Di is large and U (t) is true, then StingyCD+ should not
(t)
skip update t. However, if Di is small and U (t) is true,
the algorithm may want to skip the update in favor of updating a coordinate with larger delay. Finally, if U (t) is false,
(t)
StingyCD+ should skip the update, regardless of Di .
Based on this intuition, StingyCD+ skips update t if the
(t)
‚Äúexpected relevant delay,‚Äù defined as E[Di U (t) ], is small.
(t)
That is, given a threshold Œæ , StingyCD+ skips update t if
(t)

P (U (t) )Di < Œæ (t) .

(2)

Inserting (2) in place of StingyCD‚Äôs condition for skipping
updates is the only change from StingyCDto StingyCD+.
In practice, we define Œæ (t) = NNZ x(t‚àí1) . Defining Œæ (t)
in this way leads to the following convergence guarantee:
Theorem 3.3 (StingyCD+ converges to a solution
of (P)).

In StingyCD+, assume Œæ (t) ‚â§ NNZ x(t‚àí1) for all t > 0.
Also, for each i ‚àà [m], assume the largest number of consecutive iterations during which get next coordinate()
does not return i is bounded as t ‚Üí ‚àû. Then
lim f (x(t) ) = f (x? ) .

t‚Üí‚àû

Proven in Appendix E, Theorem 3.3 ensures StingyCD+
convergences to a solution when Œæ (t) ‚â§ NNZ x(t‚àí1)
for all t. As long as Œæ (t) is smaller than this limit, at
least one coordinate‚Äîspecifically a coordinate for which
(t‚àí1)
xi
6= 0‚Äîwill satisfy (2) during a future iteration. By
defining Œæ (t) as this limit in practice, StingyCD+ achieves
fast convergence times by skipping many updates.

4. Extending StingyCD to other objectives
For simplicity, we introduced StingyCD for nonnegative
Lasso problems. In this section, we briefly describe how
to apply StingyCD to some other objectives.
4.1. General (not nonnegative) Lasso problems
It is simple to extend StingyCD to general Lasso problems:
minimize
fL (x) :=
m
x‚ààR

1
2

2

kAx ‚àí bk + Œª kxk1

(PL)

(PL) can be transformed into an instance of (P) by introducing two features (a positive and negative copy) for each
column of A. That is, we can solve (PL) with design matrix
A by solving (P) with design matrix [A, ‚àíA]. Importantly,
we perform this feature duplication implicitly in practice.
Two modifications to Algorithm 2 are needed to solve (PL).
First, we adapt each update Œ¥ to the new objective:
Œ¥ ‚Üê argmin fL (x(t‚àí1) + Œ¥ei ) .
Œ¥

Second, we consider a positive and negative copy of Ai in
the condition for skipping update t. Specifically, we define
2

i ,rri)
œÑi+ ‚Üê sign (Œª ‚àí hAi , rri) (Œª‚àíhA
, and
kA k2
i

2

i ,rri)
œÑi‚àí ‚Üê sign (Œª + hAi , rri) (Œª+hA
.
kA k2
i

(t‚àí1)

StingyCD skips update t if and only if xi
= 0 and
q (t‚àí1) ‚â§ min{œÑi+ , œÑi‚àí }. Modifying StingyCD+ to solve
(PL) is similar. P (U (t) ) becomes the sum of two probabilities corresponding to features +Ai and ‚àíAi . Specifi(t)
(t)
(t)
cally, P (U (t) ) = P (U+ ) + P (U‚àí ). We define P (U+ )
(t)
and P (U‚àí ) the same way as we define P (U (t) ) in ¬ß3.1
except we use œÑi+ and œÑi‚àí in place of œÑi .
4.2. General `1 -regularized smooth loss minimization
We can also use StingyCD to solve problems of the form
Pn
minimize
(PL1)
i=1 œÜi (hai , xi) + Œª kxk1 ,
m
x‚ààR

where each œÜi is smooth. To solve this problem, we redefine r(t‚àí1) as a vector of derivatives:






r(t‚àí1) = [‚àíœÜ01 ( a1 , x(t‚àí1) ), . . . , ‚àíœÜ0n ( an , x(t‚àí1) )]T .

StingyCD

When updating coordinate i, it remains true that Œ¥ = 0 if
(t‚àí1)
xi
= 0 and r(t‚àí1) ‚àà
/ Ai ‚Äîthe same geometry from
Figure 1 applies. Unfortunately, updating q (t‚àí1) no longer
requires negligible computation. This is because in general,
r(t) 6= r(t‚àí1) ‚àíŒ¥Ai . Thus, the update to q (t) in Algorithm 2

2
no longer applies. In other words, q (t) = r(t) ‚àí rr
cannot be computed from q (t‚àí1) using negligible time.
Nevertheless, we can use StingyCD to efficiently solve
(PL1) by incorporating StingyCD into a proximal
Newton
P
algorithm. At each outer-iteration, the loss i œÜi (hai , xi)
is approximated by a second-order Taylor expansion. This
results in a subproblem of the form (PL), which we solve
using StingyCD. CD-based proximal Newton methods are
known to be very fast for solving (PL1), especially in the
case of sparse logistic regression (Yuan et al., 2012).
4.3. Linear support vector machines
We can also apply StingyCD to train SVMs:
minimize
n
x‚ààR

s.t.

1
2

2

kMxk ‚àí h1, xi

x ‚àà [0, C]n

.

(PSVM)

This is the dual problem for training linear support vector
machines. For this problem, we can apply concepts from
(t‚àí1)
¬ß2.3 to guarantee Œ¥ = 0 for many updates when xi
=0
(t‚àí1)
or xi
= C. To do so, the changes to StingyCD are
straightforward. Due to limited space, we provide details
in Appendix F.

5. Related work
StingyCD is related to safe screening tests as well as alternative strategies for prioritizing coordinate updates in CD.
5.1. Safe screening
Introduced in (El Ghaoui et al., 2012) for `1 -regularized
objectives, safe screening tests use sufficient conditions for
which entries of the solution equal zero. Coordinates satisfying these conditions are discarded to simplify the problem. Follow-up works considered other problems, including sparse group Lasso (Wang & Ye, 2014), SVM training (Wang et al., 2014), and low-rank problems (Zhou &
Zhao, 2015). Recent works proposed more flexible tests
that avoid major issues of prior tests (Bonnefoy et al., 2014;
2015; Fercoq et al., 2015; Ndiaye et al., 2015; 2016), such
as the fact that initial tests apply only prior to optimization.
The current state-of-the-art screening test was proposed in
(Johnson & Guestrin, 2016). For the problem (P), this test
relies on geometry similar to Figure 1. Specifically, the test
defines a ball, S Screen , that is proven to contain the residual
vector of a solution to (P). If S Screen ‚à© cl(Ai ) = ‚àÖ, it is
guaranteed that the ith weight in (P)‚Äôs solution has value 0.

The radius of S Screen is typically much larger than that of
S (t) in StingyCD, however. Unlike S (t) , S Screen must contain the optimal residual vector. Unless a good approximate
solution is available already, S Screen is overly large, often
resulting in few screened features (Johnson & Guestrin,
2016). By ensuring only that S (t) contains the current
residual vector and identifying zero-valued updates rather
than zero-valued entries in a solution, StingyCD improves
convergence times drastically more compared to screening.
5.2. Other approaches to prioritizing CD updates
Similar to StingyCD, recent work by (Fujiwara et al., 2016)
also uses a reference vector concept for prioritizing updates
in CD. Unlike StingyCD, this work focuses on identifying
nonzero-valued coordinates, resulting in an active set algorithm. The reference vector is also a primal weight vector
as opposed to a residual vector.
Similarly, shrinking heuristics (Fan et al., 2008; Yuan et al.,
2012) and working set algorithms (Johnson & Guestrin,
2015; 2016) have been shown to be effective for prioritizing computation in CD algorithms. These algorithms
solve a sequence of smaller subproblems which consider
only prioritized subsets of coordinates. In these algorithms,
StingyCD could be used to solve each subproblem to further prioritize computation. In ¬ß6, we show that using
StingyCD+ instead of CD for solving subproblems in the
working set algorithm from (Johnson & Guestrin, 2015)
can lead to further convergence time improvements.
Finally, recent work has also considered adaptive sampling
approaches for CD (Csiba et al., 2015). While also an interesting direction, this work does not apply to (P) due to a
strong convexity requirement. Currently this approach also
requires an additional pass over the data before each epoch
as well as additional overhead for non-uniform sampling.

6. Empirical comparisons
This section demonstrates the impact of StingyCD and
StingyCD+ in practice. We first compare these algorithms
to CD and safe screening for Lasso problems. Later, we
show that StingyCD+ also leads to speed-ups when combined with working set and proximal Newton algorithms.
6.1. Lasso problem comparisons
We implemented CD, CD with safe screening, StingyCD,
and StingyCD+ to solve (PL). Coordinates are updated in
round-robin fashion. We normalize columns of A and include an unregularized intercept term. We also remove
features that have nonzero values in fewer than ten examples. For CD with safe screening, we apply the test from
(Johnson & Guestrin, 2016), which is state-of-the-art to our
knowledge. Following (Fercoq et al., 2015), screening is

0

1

2

3

4

1.00

1.00

0.95

0.95

Support set recall

10‚àí1
10‚àí2
10‚àí3
10‚àí4
10‚àí5
10‚àí6
10‚àí7
10‚àí8
10‚àí9

Support set precision

Relative suboptimality

StingyCD

0.90
0.85
0.80
0.75
0.0

5

0.5

0

10

20

30

2.0

0.80
0.75
0.70
0.0

2.5

0.5

40

50

1.00

1.00

0.95

0.95

0.90
0.85
0.80
0.75

0

5

Time (s)

10

15

StingyCD

1.5

2.0

2.5

20

25

20

25

0.90
0.85
0.80
0.75
0.70

0

Time (s)
StingyCD+

1.0

Time (min)

Support set recall

10‚àí1
10‚àí2
10‚àí3
10‚àí4
10‚àí5
10‚àí6
10‚àí7
10‚àí8
10‚àí9

1.5

0.85

Time (min)
Support set precision

Relative suboptimality

Time (min)

1.0

0.90

5

10

15

Time (s)

CD + Safe Screening

CD

Figure 3. Lasso results. (above) finance. (below) allstate.

performed after every ten epochs. Performing screening
requires a full pass over the data, which is non-negligible.

Precision and recall are arguably more important than suboptimality since (PL) is typically used for feature selection.

We compare the algorithms using a financial document
dataset (finance)1 and an insurance claim prediction task
(allstate)2 . finance contains 1.6 √ó 104 examples, 5.5 √ó 105
features, and 8.8 √ó 107 nonzero values. The result included
in this section uses regularization Œª = 0.05Œªmax , where
Œªmax is the smallest Œª value that results in an all-zero solution. The solution contains 1746 nonzero entries.

Results of these experiments are included in Figure 3. We
see that StingyCD and StingyCD+ both greatly improve
convergence times. For the reasons discussed in ¬ß5, safe
screening provides little improvement compared to CD in
these cases‚Äîeven with the relative suboptimality is plotted
until 10‚àí9 . StingyCD provides a ‚Äúsafeness‚Äù similar to safe
screening yet with drastically greater impact.

The allstate data contains 2.5 √ó 105 examples, 1.5 √ó 104
features, and 1.2 √ó 108 nonzero values. For this problem,
we set Œª = 0.05Œªmax , resulting in 1404 selected features.
We include results for additional Œª values in Appendix G.
StingyCD seems to have slightly greater impact when Œª is
larger, but the results generally do not change much with Œª.
We evaluate the algorithms using three metrics. The first
metric is relative suboptimality, defined as
(t)

Relative suboptimality =

This section demonstrates that StingyCD+ can be useful
when combined with other algorithms. We consider the
problem of sparse logistic regression, an instance of (PL1)
in which each œÜi term is a logistic loss function. For each
training example (ai , bi ) ‚àà Rm √ó [‚àí1, 1], we have
œÜi (hai , xi) = log(1 + exp(‚àíbi hai , xi)) .

?

f (x ) ‚àí f (x )
,
f (x? )

where f (x(t) ) is the objective value at iteration t, and x?
is the problem‚Äôs solution. The other metrics are support set
(t)
precision and recall. Let F (t) = {i : xi 6= 0}, and let
F ? be the analogous set for x? . We define
 (t)

 (t)

F ‚à© F ? 
F ‚à© F ? 


.
Precision =
F (t)  , Recall =
|F ? |
1

6.2. Combining StingyCD+ with working sets

https://www.csie.ntu.edu.tw/‚àºcjlin/libsvmtools/
datasets/regression.html#E2006-log1p
2
https://www.kaggle.com/c/allstate-claims-severity

In this section, we use StingyCD+ as a subproblem solver
for a proximal Newton algorithm and a working set algorithm. Specifically, we implement StingyCD+ within
the ‚ÄúBlitz‚Äù working set algorithm proposed in (Johnson &
Guestrin, 2015). At each iteration of Blitz, a subproblem
is formed by selecting a set of priority features. The objective is then approximately minimized by updating weights
only for features in this working set. Importantly, each subproblem in Blitz is solved approximately with a proximal
Newton algorithm (overviewed in ¬ß4.2), and each proximal
Newton subproblem is solved approximately with CD.
For these comparisons, we have replaced the aforemen-

10‚àí3
10

‚àí4

10‚àí5
0

1

2

3

4

5

6

7

1.00

1.00

0.95

0.95

0.90
0.85
0.80
0.75
0.70

8

Support set recall

10‚àí2

Support set precision

Relative suboptimality

StingyCD

0

1

2

3

‚àí3

10‚àí4
10‚àí5
0

5

10

15

20

6

7

0.80
0.75
0.70

8

0

1

2

3

25

30

1.00

1.00

0.95

0.95

0.90
0.85
0.80
0.75
0.70

0

5

10

Time (s)
StingyCD+ ProxNewton with Working Sets

15

20

4

5

6

7

8

Time (min)

Support set recall

10‚àí2
10

5

0.85

Time (min)
Support set precision

Relative suboptimality

Time (min)

4

0.90

25

30

0.90
0.85
0.80
0.75
0.70

0

5

10

Time (s)
CD ProxNewton with Working Sets

15

20

25

30

Time (s)
StingyCD+ ProxNewton

CD ProxNewton

Figure 4. Combining StingyCD+ with other algorithms for sparse logistic regression. (above) kdda (below) lending club

tioned CD implementation with a StingyCD+ implementation. We demonstrate the effects of this change when
working sets are and are not used. In the case that working
sets are omitted, we refer to the algorithm as ‚ÄúStingyCD+
ProxNewton‚Äù or ‚ÄúCD ProxNewton,‚Äù depending on whether
StingyCD+ is incorporated. We note that Blitz and the
proximal Newton solver have not otherwise been modified,
although it is likely possible to achieve improved convergence times by accounting for the use of StingyCD+. For
example, Blitz could likely be improved by including more
features in each working set, since StingyCD+ provides an
additional layer of update prioritization.

the working set approach depended significantly on Œª, at
least in the case of lending club. For this dataset, when Œª
is relatively large (and thus the solution is very sparse), we
observed little or no improvement due to StingyCD+. However, for smaller values of Œª, StingyCD+ produced more
significant gains. Moreover, StingyCD+ was the best performing algorithm in some cases (though in other cases,
Blitz was much faster). This observation suggests that
there likely exists a better approach to using working sets
with StingyCD+‚Äîan ideal algorithm would obtain excellent performance across all relevant Œª values.

The datasets used for this comparison are an educational
performance dataset (kdda)3 and a loan default prediction
task (lending club)4 . After removing features with fewer
than ten nonzeros, kdda‚Äôs design matrix contains 8.4 √ó 106
examples, 2.2 √ó 106 features, and 2.8 √ó 108 nonzero values. We solve this problem with Œª = 0.005Œªmax , which results in 692 nonzero weights at the problem‚Äôs solution. The
lending club data contains 1.1 √ó 105 examples, 3.1 √ó 104
features, and 1.0 √ó 108 nonzero values. We solve this problem with Œª = 0.02Œªmax , resulting in 878 selected features.
We include plots for additional Œª values in Appendix H.

7. Discussion

Results of this experiment are shown in Figure 4. We see
that replacing CD with StingyCD+ in both Blitz and ProxNewton can result in immediate efficiency improvements.
We remark that the amount that StingyCD+ improved upon
3

https://www.csie.ntu.edu.tw/‚àºcjlin/libsvmtools/
datasets/binary.html#kdd2010(algebra)
4
https://www.kaggle.com/wendykan/
lending-club-loan-data

We proposed StingyCD, a coordinate descent algorithm
that avoids large amounts of wasteful computation in applications such as sparse regression. StingyCD borrows
geometric ideas from safe screening to guarantee many updates will result in no progress toward convergence. Compared to safe screening, StingyCD achieves considerably
greater convergence time speed-ups. We also introduced
StingyCD+, which applies a probabilistic assumption to
StingyCD in order to further prioritize coordinate updates.
In general, we find the idea of ‚Äústingy updates‚Äù to be deserving of significantly more exploration. Currently this
idea is limited to CD algorithms and, for the most part,
objectives with quadratic losses. However, it seems likely
that similar ideas would apply in many other contexts. For
example, it could be useful to use stingy updates in distributed optimization algorithms in order to significantly
reduce communication requirements.

StingyCD

Acknowledgments
We thank our anonymous reviewers for their thoughtful feedback.
This work is supported in part by
PECASE N00014-13-1-0023, NSF IIS-1258741, and the
TerraSwarm Research Center 00008169.

Li, S. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of Mathematics and Statistic, 4(1):66‚Äì70, 2011.

References

Ndiaye, E., Fercoq, O., Gramfort, A., and Salmon, J. GAP
safe screening rules for sparse multi-task and multi-class
models. In Advances in Neural Information Processing
Systems 28, 2015.

Bonnefoy, A., Emiya, V., Ralaivola, L., and Gribonval, R.
A dynamic screening principle for the lasso. In 22nd
European Signal Processing Conference, 2014.

Ndiaye, E., Fercoq, O., Gramfort, A., and Salmon, J. Gap
safe screening rules for sparse-group lasso. In Advances
in Neural Information Processing Systems 29, 2016.

Bonnefoy, A., Emiya, V., Ralaivola, L., and Gribonval, R.
Dynamic screening: Accelerating first-order algorithms
for the lasso and group-lasso. IEEE Transactions on Signal Processing, 63(19):5121‚Äì5132, 2015.

Nesterov, Y. Efficiency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341‚Äì362, 2012.

Bradley, J. K., Kyrola, A., Bickson, D., and Guestrin,
C. Parallel coordinate descent for L1 -regularized loss
minimization. In International Conference on Machine
Learning, 2011.
Csiba, D., Qu, Z., and RichtaÃÅrik, P. Stochastic dual coordinate ascent with adaptive probabilities. In International
Conference on Machine Learning, 2015.
El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature
elimination for the Lasso and sparse supervised learning problems. Pacific Journal of Optimization, 8(4):667‚Äì
698, 2012.

RichtaÃÅrik, P. and TakaÃÅcÃå, M. Parallel coordinate descent
methods for big data optimization. Mathematical Programming, 156(1):433‚Äì484, 2016.
Shalev-Shwartz, S. and Tewari, A. Stochastic methods for
`1 -regularized loss minimization. Journal of Machine
Learning Research, 12(June):1865‚Äì1892, 2011.
Shi, H.-J. M., Tu, S., Xu, Y., and Yin, W. A primer
on coordinate descent algorithms. Technical Report
arXiv:1610.00040, 2016.
Tibshirani, R. Regression shrinkage and selection via the
Lasso. Journal of the Royal Statistical Society, Series B,
58(1):267‚Äì288, 1996.

Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and
Lin, C.-J. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:
1871‚Äì1874, 2008.

Wainwright, M. J. Sharp thresholds for high-dimensional
and noisy sparsity recovery using `1 -constrained
quadratic programming (Lasso). IEEE Transactions on
Information Theory, 55(5):2183‚Äì2202, 2009.

Fercoq, O., Gramfort, A., and Salmon, J. Mind the duality
gap: safer rules for the lasso. In International Conference on Machine Learning, 2015.

Wang, J. and Ye, J. Two-layer feature reduction for sparsegroup lasso via decomposition of convex sets. In Advances in Neural Information Processing Systems 27,
2014.

Friedman, J., Hastie, T., and Tibshirani, R. Regularization
paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1‚Äì22, 2010.
Fujiwara, Y., Ida, Y., Shiokawa, H., and Iwamura, S. Fast
lasso algorithm via selective coordinate descent. In Proceedings of the Thirtieth AAAI Conference on Artificial
Intelligence, 2016.
Johnson, T. B. and Guestrin, C. Blitz: a principled metaalgorithm for scaling sparse optimization. In International Conference on Machine Learning, 2015.
Johnson, T. B. and Guestrin, C. Unified methods for exploiting piecewise linear structure in convex optimization. In Advances in Neural Information Processing Systems 29, 2016.

Wang, J., Wonka, P., and Ye, J. Scaling SVM and least
absolute deviations via exact data reduction. In International Conference on Machine Learning, 2014.
Wright, S. J. Coordinate descent algorithms. Mathematical
Programming, 151(1):3‚Äì34, 2015.
Yuan, G. X., Ho, C. H., and Lin, C. J. An improved GLMNET for L1-regularized logistic regression. Journal of
Machine Learning Research, 13:1999‚Äì2030, 2012.
Zhou, Q. and Zhao, Q. Safe subspace screening for nuclear norm regularized least squares problems. In International Conference on Machine Learning, 2015.

