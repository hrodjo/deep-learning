Probabilistic Submodular Maximization in Sub-Linear Time
Serban Stan 1 Morteza Zadimoghaddam 2 Andreas Krause 3 Amin Karbasi 1

Abstract
In this paper, we consider optimizing submodular functions that are drawn from some unknown
distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of
items may depend on a user-specific submodular utility function. In modern applications, the
ground set of items is often so large that even
the widely used (lazy) greedy algorithm is not
efficient enough. As a remedy, we introduce the
problem of sublinear time probabilistic submodular maximization: Given training examples of
functions (e.g., via user feature vectors), we seek
to reduce the ground set so that optimizing new
functions drawn from the same distribution will
provide almost as much value when restricted to
the reduced ground set as when using the full
set. We cast this problem as a two-stage submodular maximization and develop a novel efficient algorithm for this problem which offers a
1
1
2 (1 ‚àí e2 ) approximation ratio for general monotone submodular functions and general matroid
constraints. We demonstrate the effectiveness of
our approach on several real-world applications
where running the maximization problem on the
reduced ground set leads to two orders of magnitude speed-up while incurring almost no loss.

utility functions for valuating sets of items, and a prototypical problem is to find sets of say k items with nearmaximal value (Krause & Golovin, 2012; Mirzasoleiman
et al., 2016b). Even though efficient greedy algorithms exist for submodular maximization, those become infeasible
when serving many users and optimizing over large item
collections. To this end, given training examples of (users
and their utility) functions drawn from some unknown distribution, we seek to reduce the ground set to a small (ideally sublinear) size. The hope is that optimizing new functions drawn from the same distribution will incur little loss
when optimized on the reduced set compared to optimizing
over the full set.

1. Introduction

Optimizing the empirical objective is an instance of twostage submodular maximization, a problem recently considered by Balkanski et al. (2016) who provide a 0.316
approximation guarantee for the case of coverage functions (more about it in the related work). One of our
key technical contributions is a computationally efficient
novel local-search based algorithm, called ReplacementGreedy, for two-stage submodular maximization, that provides a constant-factor 0.432 approximation guarantee for
the general case of monotone submodular functions. Our
approach also generalizes to arbitrary matroid constraints,
and empirically compares favorably to prior work. We further analyze conditions under which our approach provably enables approximate submodular optimization based
on substantially reduced ground sets, resulting the first viable approach towards sublinear-time submodular maximization.

Motivated by applications in data summarization (Lin &
Bilmes, 2011; Wei et al., 2013; Mirzasoleiman et al.,
2016d) and recommender systems (El-Arini et al., 2009;
Yue & Guestrin, 2011; Mirzasoleiman et al., 2016a), we
tackle the challenge of efficiently solving many statistically
related submodular maximization problems. In these applications, submodularity arises in the form of user-specific

Lastly, we demonstrate the effectiveness of our approach
on recommender systems for which we compare the utility
value and running time of maximizing a submodular function over the full data set and its reduced version returned
by our algorithm. We consistently observe that the loss we
incur is negligible (around 1%) while the speed up is enormous (about two orders of magnitude).

1
Yale University, New Haven, Connecticut, USA 2 Google
Research, New York, NY 10011, USA 3 ETH Zurich,
Zurich, Switzerland.
Correspondence to: Amin Karbasi
<amin.karbasi@yale.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Related work Submodular maximization has found
many applications in machine learning, ranging from feature/variable selection (Krause & Guestrin, 2005) to dictionary learning (Das & Kempe, 2011) to data summarization (Wei et al., 2014b; Lin & Bilmes, 2011; Tschiatschek
et al., 2014) to recommender systems (Yue & Guestrin,

Probabilistic Submodular Maximization in Sub-Linear Time

2011; El-Arini et al., 2009). A seminal result of Nemhauser
et al. (1978) proves that a simple greedy algorithm provides an optimal constant factor (1 ‚àí 1/e) approximation guarantee. Given the size of modern data sets, much
work has focused on solving submodular maximization at
scale. This work ranges from accelerating the greedy algorithm itself (Minoux, 1978; Mirzasoleiman et al., 2015;
2016a; Badanidiyuru & Jan, 2014; Buchbinder et al., 2014)
to distributed (Kumar et al., 2013; Mirzasoleiman et al.,
2013) and streaming (Krause & Gomes, 2010; Badanidiyuru et al., 2014) approaches, as well as algorithms based
on filtering the ground set in multiple stages (Wei et al.,
2014a; Feldman et al., 2017). All of these approaches aim
to solve a single fixed problem instance, and have computational cost at least linear in the ground set size. In contrast,
we seek to solve multiple related problems with sublinear
effort.
Solving multiple submodular problems arises in online
submodular optimization (Streeter & Golovin, 2008; Hazan
& Kale, 2009; Jegelka & Bilmes, 2011). In this setting the
goal is to design algorithms that perform well in hindsight
(minimize some form of regret). The computational complexity of these algorithms is typically (super)-linear in the
ground set size. Studying online variants of our problem is
an interesting direction for future work.
Closest to our work is the approach of Balkanski et al.
(2016) that we build on and compare within this paper.
For general submodular objectives, they propose two algorithms: one based on continuous optimization (with prohibitive computational complexity in terms of the size of
the ground set n), which provides constant factor approximation guarantees only for large values of k (i.e., cardinality constraint), as well as one that has exponential complexity in terms of k. To circumvent the large computational complexity of the above algorithms, they also proposed a heuristic local search method that offers a 21 (1 ‚àí 1e )
approximation for the special case of coverage functions.
The query complexity of this algorithm is O(km`n2 log n)
where ` is the size of the summary and m is the number
of considered submodular functions in the two-stage optimization. One of our key contributions is a novel and
computationally efficient algorithm for the two-stage problem. More specifically, our method ReplacementGreedy
provides a 21 (1 ‚àí e‚àí2 ) approximation guarantee for general monotone submodular functions subject to a general
matroid constraint with only O(rm`n) query complexity
(here r denotes the rank of the matroid). As argued by
Balkanski et al. (2016), two-stage submodular maximization can be seen as a discrete analogue of representation
learning problems like dictionary learning. It is important
to note that the two-stage submodular maximization problem is fractionally subadditive (XOS) (Feige, 2009). Although it is tempting to use the XOS property of our two-

stage function especially given the positive results for social welfare XOS maximization, there are several obstacles
preventing us from doing so. First, evaluating the two stage
function is NP-hard, so we cannot have access to oracle
value queries. Second, as shown by Singer (2010); Shahar Dobzinski & Schapira (2005); Ashwinkumar Badanidiyuru (2012), any n1/2‚àí approximation of a XOS requires exponentially many oracle value queries. Therefore
the XOS property itself is not sufficient to get any positive
algorithmic result for our problem. Nevertheless, we can
still provide constant factor approximation with computationally efficient algorithms.
Repeatedly optimizing related classes of submodular functions is a key subroutine in many applications, such
as structured prediction (Lin & Bilmes, 2012) or linear submodular bandits (Yue & Guestrin, 2011). In
both of these problems, one needs to repeatedly maximize weighted combinations of submodular functions, for
changing weights. Our work can be viewed as providing an
approach towards accelerating this central subroutine.

2. Problem Setup
In this paper, we consider the problem of frequently optimizing monotone submodular functions f : 2‚Ñ¶ ‚Üí R+
that are drawn from some unknown probability distribution
D. Hereby, ‚Ñ¶ denotes the ground set of size n over which
the submodular functions are defined. W.l.o.g. we assume
that the maximum value of any function f drawn from D
does not exceed 1. This setting arises in many applications,
such as recommender systems, where the random function
f = fu refers to the (predicted) valuation over sets of items
for a particular user u, which may vary depending on their
features (c.f., Yue & Guestrin (2011)). These applications
typically dictate some constraints, i.e., one seeks to solve
T ‚àó = arg max f (T ) s.t. T ‚àà I,
where I ‚äÜ 2‚Ñ¶ is a collection of feasible sets. In this
paper we primarily consider cardinality constraints, i.e.,
I = {T ‚äÜ ‚Ñ¶ : |T | ‚â§ k}. Our results will hold also in
the more general setting where I is the collection of independent sets in some matroid (Calinescu et al., 2011).
Throughout the paper r denotes the rank of the matroid. In
the special case of cardinality constraint, the rank is r = k.
While NP-hard, good approximation algorithms are known
for submodular maximization. For example, the classical
greedy algorithm of Nemhauser et al. (1978) or its accelerated variants (Minoux, 1978; Mirzasoleiman et al., 2015;
2016a; Badanidiyuru & Jan, 2014; Buchbinder et al., 2014)
provide an optimal constant factor (1‚àí1/e) approximation
for maximization under cardinality constraints. In modern
applications, however, the system may face a large number of users, and large collections of items ‚Ñ¶. Hence the

Probabilistic Submodular Maximization in Sub-Linear Time

naive strategy of even greedily optimizing fu for each user
separately may be too costly.
To remedy this situation, in this paper we consider the following approach: Given training data (i.e., a sample collection of functions f1 , . . . , fm ), we invest computation once
to obtain a reduced ground set S of size `  n = |‚Ñ¶|.
The hope is that optimizing new functions arising at test
time will provide almost as much value when restricting
the choice to items in S, than when considering arbitrary
items in ‚Ñ¶, while being substantially more computationally
efficient.
More formally, the expected performance when using the
candidate reduced ground set S is
G(S) = Ef ‚àºD max f (T ),
T ‚ààI(S)

(1)

where we use I(S) ‚â° {T ‚àà I and T ‚äÜ S} to refer to
the collection of feasible sets restricted to those containing
only elements from S. The optimum achievable performance would be G(‚Ñ¶). Our goal will be to pick a set S
of small size ` to maximize G(S), or equivalently make
|G(‚Ñ¶) ‚àí G(S)| small.
Special cases. Some observations are in order. If D is deterministic, i.e., puts all mass on a single function f , then
we simply recover classical constrained submodular maximization, since G(S) = f (S) for sets up to size k. If D
is known to be the uniform distribution over m functions,
G(S) becomes
m

Gm (S) =

1 X
max fi (T ).
m i=1 T ‚ààI(S)

(2)

Gm (S) is not generally submodular, but Balkanski et al.
(2016) have developed approximation algorithms for maximizing Gm under the constraint that |S| ‚â§ `,1 i.e., for the
problem:
m

1 X
max fi (T ).
S‚äÇ‚Ñ¶,|S|‚â§` m i=1 T ‚ààI(S)

S m,` = arg max

(3)

(4)

S‚äÇ‚Ñ¶,|S|‚â§`

3. Probabilistic Submodular Maximization
Since the distribution D is unknown, we cannot find S`‚àó
or its corresponding optimum value, G(S`‚àó ). Instead, we
1

error

= |G(‚Ñ¶) ‚àí G(S m,` )|
‚â§

|G(‚Ñ¶) ‚àí G(S`‚àó )| + |G(S`‚àó ) ‚àí G(S m,` )|.(5)
compression error

approximation error

Note that once we have error ‚â§  for some small  > 0 then
maximizing over S m,` is almost as good as maximizing
over ‚Ñ¶ (but possibly much faster). To that end we need
to bound both compression error and approximation error.
In fact we can prove the following result for the required
number of samples to ensure small approximation error.
Theorem 1. For any , Œ¥ > 0, and any set S of size at most
` we can ensure that
Ô£´
Ô£∂
Pr Ô£≠ max |G(S) ‚àí Gm (S)| > Ô£∏ < Œ¥
S‚äÇ‚Ñ¶
|S|‚â§`

as long as m = O((` log(n) + log(1/Œ¥))/2 ).
In contrast, the compression error cannot be made arbitrarily small in general as the following example shows.
Bad Example. Suppose ‚Ñ¶ = [n], m = n and D is the
uniform distribution over the functions f1 , . . . , fn , where
fi (S) is 1 if i ‚àà S, 0 otherwise. Each fi is in fact modular.
Let k = 1. It is easy to see that G(S) = |S|/n. Hence to
achieve compression error less than , |S| must be greater
than (1 ‚àí )n. In particular for  < 1/n, S must be equal
to the full set.
Sufficient conditions for sublinear `. The above example shows that one needs additional structural assumptions.
One simple special case arises when the union of the optimal sets for all functions in the support of D is of size
` < n, i.e.,
 ‚àó

{T : T ‚àó = arg max f (T ) for some f ‚àà supp(D)} = `.
T ‚ààI

The general case. In this paper, we consider the problem
of maximizing G(S) for general distributions D. I.e., we
seek to solve:
S`‚àó = arg max G(S).

can sample functions from D and construct the empirical
average, similar to Problem (2), and try to optimize it by
finding S m,` . Hence, the total generalization error we incur
in this process is bounded by

Balkanski et al. (2016) only consider cardinality constraints.

I.e., if D is any distribution over at most m functions,
clearly ` ‚â• km suffices.
This assumption might be too strong in practice, however.
Instead, we will consider another set of assumptions that allow bounding the compression error in the case of cardinality constraints. We assume ‚Ñ¶ is endowed with a metric d.
This metric is extended to sets of equal size so that for any
sets T, T 0 of equal size, d(T, T 0 ) is the weight of a minimal
matching of elements in T to elements in T 0 , where the
weight of (v, v 0 ) for v ‚àà T and v 0 ‚àà T 0 is d(v, v 0 ). We will
assume that any function f ‚àº D is L-Lipschitz continuous
w.r.t. d, i.e., |f (T )‚àíf (T 0 )| ‚â§ Ld(T, T 0 ) for some constant

Probabilistic Submodular Maximization in Sub-Linear Time

L for all sets T and T 0 of size k. Many natural submodular
functions arising in data summarization tasks satisfy this
condition, such as exemplar-based clustering and certain
log-determinants, see, e.g., (Mirzasoleiman et al., 2016c).
Theorem 2. Suppose each function f ‚àº D is L-Lipschitz
continuous in (‚Ñ¶, d). Then, for any  > 0, the compression
error is bounded by  as long as ` ‚â• kn/2kL , where nŒ¥ is
the Œ¥-covering number of (‚Ñ¶, d).
The proofs of Theorems 1 and 2 are given in the appendix.

4. Algorithm
From the discussions in the previous section, we concluded
that under appropriate statistical conditions, we can ensure
that the error in Eq. 5 can be made small if we have enough
samples dictated by Theorem 1. However, our conclusion
heavily relied on the fact that we can find the set S m,` in
Problem 3. As we noted earlier, the objective function in
Problem 3 is not submodular in general (Balkanski et al.,
2016), thus the classical greedy algorithm may not provide
any approximation guarantees.
Our proposed algorithm ReplacementGreedy works in `
rounds where in each round it tries to augment the solution in a particular greedy fashion. It starts out with an
empty set S = ‚àÖ, and checks (in each round) whether a
new element can be added without violating the matroid
constraint (i.e., stay an independent set) or otherwise it can
be replaced with an element in the current solution while
increasing the value of the objective function. To describe
how these decisions are made we need a few definitions.
Let
‚àÜi (x, A) = fi ({x} ‚à™ A) ‚àí fi (A)
denote the marginal gain of adding x to the set A if
we consider function fi . Similarly, we can define the
gain of removing an element y and replacing it with x
as ‚àái (x, y, A) = fi ({x} ‚à™ A \ {y}) ‚àí fi (A). Since
fi is monotone we know that ‚àÜi (x, A) ‚â• 0. However,
‚àái (x, y, A) may or may not be positive. Let us consider
I(x, A) = {y ‚àà A : A ‚à™ {x} \ {y} ‚àà I}. This is the set
of all elements in A such that if we replace them with x we
will not violate the matroid constraint. Then, we define the
replacement gain of x w.r.t. a set A as follows:
(
‚àái (x, A) =

‚àÜi (x, A)
if A ‚à™ {x} ‚àà I,
max{0, maxy‚ààI(x,A) ‚àái (x, y, A)}
o.w.

In words, ‚àái (x, A) denotes how much we can increase the
value of fi (A) by either inserting x into A or replacing x
with one element of A while keeping A an independent
set. Finally, let Repi (x, A) be the element that should be
replaced by x to maximize the gain and stay independent.

Formally,
(
Repi (x, A) =

‚àÖ
if A ‚à™ {x} ‚àà I,
arg maxy‚ààI(x,A) ‚àái (x, y, A)
o.w.

With the above definitions it is easy to explain how ReplacementGreedy works. At all times, it maintains a solution S and a collection of feasible solutions Ti ‚äÜ S for
each function fi (all initialized to the empty set in the beginning). In each iteration, it picks the top element x‚àó from
the P
ground set ‚Ñ¶, based on its total contribution to fi ‚Äôs,
m
i.e, i=1 ‚àái (x, Ti ), and updates S. Then ReplacementGreedy checks whether any of Ti ‚Äôs can be augmented.
This is done either by simply adding x‚àó (without violating the matroid constraint) or replacing it with an element
from Ti . The condition ‚àái (x‚àó , Ti ) > 0 ensures that such
replacement is executed only if the gain is positive.
Why does ReplacementGreedy work? Note that
solving maxT ‚ààI(S) fi (T ) is an N P-hard problem. However, ReplacementGreedy finds and maintains independent sets Ti ‚äÜ S throughout the course of the algorithm. In
fact, the
collection {S, T1 , . . . , Tm } lower bounds Gm (S)
Pm
1
iteration increases the
by m
i ). Moreover, each
i=1 fi (TP
Pm
m
aggregate value i=1 fi (Ti ) by i=1 ‚àái (x‚àó , Ti ). What
we show in the following theorem is that after ` iterations,
the accumulation of those gains reaches a constant factor
approximation to the optimum value.
Theorem 3. In only O(`mnr) function evaluations ReplacementGreedy returns a set S of size at most ` along
with independent sets Ti ‚àà I(S) such that


1
1
1 ‚àí 2 Gm (S m,` ).
Gm (S) ‚â•
2
e
A few comments are in order. Balkanski et al. (2016) proposed an algorithm with (1 ‚àí 1/e)/2-approximation guarantee for the case where fi ‚Äôs are coverage functions and the
constraint is a uniform matroid. This is achieved by solving (potentially large) linear programs while maintaining
O(k`mn2 log n) function evaluations. In contrast, our result holds for any collection of monotone submodular functions and any matroid constraint. Our approximation guarantee (1 ‚àí e‚àí2 )/2 is better than (1 ‚àí 1/e)/2. Finally, our
algorithm is arguably faster in terms of both running time
(as it simply runs a modified greedy method) and query
complexity (as it is linear in all the parameters).

5. Experiments
In this section, we describe our experimental setting. We
offer details on the datasets we used and the baselines
we ran ReplacementGreedy against. We first show that
ReplacementGreedy is highly efficient (i.e., high utility,

Probabilistic Submodular Maximization in Sub-Linear Time

Algorithm 1 ReplacementGreedy
S ‚Üê ‚àÖ, Ti ‚Üê ‚àÖ (‚àÄ1 ‚â§ i ‚â§ m)
for 1 ‚â§ j ‚â§ ` do
Pm
x‚àó ‚Üê arg maxx‚àà‚Ñ¶ i=1 ‚àái (x, Ti )
S ‚Üê S ‚à™ {x‚àó }
for all 1 ‚â§ i ‚â§ m do
if ‚àái (x‚àó , Ti ) > 0 then
Ti ‚Üê Ti ‚à™ {x‚àó }\Repi (x‚àó , Ti )
end if
end for
end for
Return sets S and T1 , T2 , ¬∑ ¬∑ ¬∑ , Tm

low running time) when solving the two-stage submodular maximization problem on two concrete summarization
applications: article summarization and image summarization. We then demonstrate sublinear submodular maximization by showing that ReplacementGreedy can efficiently reduce the dataset while incurring minimum loss.
We test the performance of ReplacementGreedy on a
movie dataset where movies should be recommended to
users with user-specific utility functions.
5.1. Baselines
LocalSearch. This is the main algorithm described in
Balkanski et al. (2016)2 . In our experiments, we use  =
0.2. We initialize S by incrementally picking ` elements
such that at each step we maximize the sum of marginal
gains for the m functions fi .
GreedySum. It greedily selects ` elements
Pmwhile maximizing the submodular function FÃÇ (S) = i=1 fi (S). To
find k elements for each fi it runs another greedy algorithm.
GreedyMerge. It ideally serves as an upper bound for the
objective value, by greedily selecting k elements for each
function fi and returning their union. GreedyMerge can
easily violate the cardinality constraint ` as its solution can
have as many as mk elements.
5.2. Metrics
Objective value. We compare algorithms‚Äô solutions S according to the scores Gm (S) for the two-stage (empirical)
problem, and G(S) for the sublinear maximization problem.
Loss. For the sublinear maximization problem, we measure
the relative loss in performance when using the summary,
compared to the full ground set, i.e., report 1‚àíG(S)/G(‚Ñ¶).
2
We thank the authors for providing us with their implementation.

Running time. We also compare the algorithms based on
their wall-clock running time. The experiments were ran
in a Python 2.7 environment on a OSX 10.12.13 machine.
The processor was a 2.5 GHz Intel Core i7 with 16 GB
1600 MHz DDR3 memory.
5.3. Two-Stage Submodular Maximization
Article summarization on Wikipedia. The aim is to select a small, highly relevant subset of wikipedia articles
from a larger corpus. For this task, we reproduce the experiment from Balkanski et al. (2016) on Wikipedia articles
for Machine Learning. The dataset contains n = 407 articles divided into m = 22 categories, where each category
represents a subtopic from Machine Learning. The relevance of a set S with respect to a category i is measured
by the submodular function fi that counts the number of
pages that belong to category i with a link to at least one
page in S. These submodular functions are L-Lipschitz
with L = 1 by considering the distance between two articles as the fraction of all pages that have a link to exactly
one of these two articles. Fig. 1a and 1b show the objective
values for a fix k = 5 (and varying `) and a fixed ` = 20
(and varying k). We find that ReplacementGreedy and
LocalSearch perform the same while GreedySum falls
off somewhat for larger values of `. However, if we look at
the running times (log scale) in Fig. 1e and 1f we observe
that ReplacementGreedy is considerably faster than LocalSearch and close to GreedySum.
Image summarization on VOC2012. For this application
we use a subset of the VOC2012 dataset (Everingham et al.,
2014) where we consider n = 150 with m = 20 categories.
Each category indicates a certain visual queue appearing in
the image such as chair, bird, hand, etc. We wish to obtain
a subset of these images that are relevant to all the categories. To that end we use Exemplar Based Clustering (c.f.,
Mirzasoleiman et al. (2013)). Let ‚Ñ¶i be the portion of the
ground set associated to category i. For any set S we also
let Si = ‚Ñ¶i ‚à© S denote its subset that is part of category
i. We define fP
i (S) = Li ({e0 }) ‚àí Li (S ‚à™ {e0 }) where
Li (S) = |‚Ñ¶1i | x‚àà‚Ñ¶i miny‚ààSi d(x, y). Here, d measures
the distance between images (e.g., `2 norm), and e0 is an
auxiliary element. With respect to distance d, our submodular functions are L-Lipschitz with L = 1. Also, images
are represented by feature vectors obtained from categories.
For example, if there were two categories a and b, and
an image had features [a, a, b], its feature vector would be
(2, 1). Again if we look at Fig. 1c (for fixed k = 5 and
varying `) and Fig. 1d (for ` = 20 and varying k) we see
that ReplacementGreedy and LocalSearch achieve the
same objective value. However, Fig. 1g and 1h show that
LocalSearch is significantly slower than ReplacementGreedy.

Probabilistic Submodular Maximization in Sub-Linear Time

(a) Wikipedia (k = 5)

(b) Wikipedia (` = 20)

(c) VOC (k = 5)

(d) VOC (` = 20)

(e) Wikipedia (k = 5)

(f) Wikipedia (` = 20)

(g) VOC (k = 5)

(h) VOC (` = 20)

Figure 1. Objective values and runtimes for our experiments. The two columns on the left correspond to the Wikipedia dataset and the
two columns on the right correspond to the VOC2012 dataset. Note that the runtimes are all in log-scale. We let k = 5 and vary ` or fix
` = 20 and vary k.

Figure 2. Images chosen by our algorithm on the VOC2012 dataset, for the case when ` = 7 and k = 5

5.4. Sublinear Summarization
In this part, we experimentally show how ReplacementGreedy can reduce the size of the dataset to `  n without
incurring too much loss in the process. To do so, we consider a movie recommendation application.
Movie recommendation with missing ratings. The
dataset consists of user ratings on a scale of 1 to 5, along
with some movie information. There are 20 genres (Animation, Comedy, Thriller, etc) and each movie can have
one or more of these genres assigned to it. The goal is to
find a small set S of movies with the property that each user
will be able to find k enjoyable movies from S.
In our setting we consider the top 2000 highest rated
movies (that were rated by at least 20 users) alongside the
top 200 users ordered by number of movies they rated from
this set. We assign a user-specific utility function fi to each
user i as follows. Let Ai be the subset of A which user i
rated. Let g be the number of movie genres. Furthermore,
we let R(i, A, j) represent the highest rating given by user i
to a movie from the set A with genre j. We also define wi,j
to be the proportion of movies in genre j that user i rated
out of all the movie-genre ratings she provided. So wi,j will
be higher for the genres that the user provided more feed-

back on, indicating that she might like these genres better.
Then, the valuation function by user i is as follows:
fi (A) =

g
X

wi,j R(i, A, j).

j=1

In words, the way a user evaluates a set A is by picking the
highest rated movie from each genre (contained in A) and
then take a weighted average. If we define the distance between two movies to be the maximum difference of ratings
they received from the same user, the submodular functions
fi will be L-Lipschitz with L = 1.
For the experiment, we split the users in half. We use the
first 100 of them as a training set for the algorithms to built
up their reduced ground set S of size `. We then compare submodular maximization with cardinality constraint
k = 3 on the reduced sets S (returned by the baselines)
and that of the whole ground set ‚Ñ¶. To this end, we sample 20 users from the test group and compute their average
values. Given the size of this experiment, we were unable
to run LocalSearch alongside ReplacementGreedy and
GreedySum. In its place, we introduce the random baseline that simply returns a set of size ` at random. Fig 3a
shows what fraction of the utility is preserved if we reduce
the ground set by ReplacementGreedy, GreedySum,

Probabilistic Submodular Maximization in Sub-Linear Time

(a) Movielens

(b) ` = 10

(c) ` = 30

(d) ` = 60

(e) Movielens Completed

(f) ` = 10

(g) ` = 30

(h) ` = 60

Figure 3. Experimental results in the sublinear regime. The first row corresponds to the sublinear experiment where the data set has
missing entries, and the second row refers to the experiment where we used a completed user-ratings matrix. The first column portrays
the ratio between the mean objective obtained on the summary set versus the ground set for 20 random users from the test set. Columns
2 through 4 showcase the loss versus runtime for ` = 10, ` = 30 and ` = 60 respectively. In these experiments k = 3.

and RandomSelection. Clearly, RandomSelection performs poorly. ReplacementGreedy has the highest utility, starting from 80% and practically closing the gap by reducing the ground set to only 60 movies. GreedySum also
closely follows ReplacementGreedy. Fig. 3b, 3c, 3d
show the loss versus the running time for ` = 10, ` = 30,
and ` = 60. Of course, if we use use the whole ground set
‚Ñ¶, the loss will be zero. This is shown by GreedyMerge.
However, this comes at the cost of maximizing the utility
of each user on 2000 movies. Instead, by using ReplacementGreedy, we see that the loss is negligible (even for
` = 30) while the running time suddenly improves by two
orders of magnitude.
Complete matrix movie recommendation. The previous experiment suffers from the potential issue that values are estimated conservatively: i.e., a user derives value
only if we happen to select movies that she actually rated
in the data. To explore this potential bias, we repeat the
same experiment, this time by first completing the matrix
of (movie, rating) using standard techniques (CandeÃÅs &
Recht, 2008). We again consider 200 users and divide them
into training and test sets. The results are shown in Fig. 3e,
3f, 3g, 3h. We observe exactly the same trends. Basically,
a dataset with size 60 is approximately as good as a data set
of size 2000 if the reduced ground set is carefully selected
by ReplacementGreedy.

6. Analysis
In this section, we prove that Algorithm ReplacementGreedy returns a valid solution S of at most ` elements
along with independent sets Ti for all different categories

Pm
1
such that the aggregate value m
i=1 fi (Ti ) is at least
1
1
(1
‚àí
)
>
0.43
fraction
of
the
optimum
solution‚Äôs ob2
2
e
jective value, namely Gm (S m,` ). We note that the objective value P
of ReplacementGreedy‚Äôs solution, Gm (S), is
m
1
at least m
i=1 fi (Ti ), and therefore Algorithm ReplacementGreedy is a 12 (1 ‚àí e12 )-approximation algorithm for
our two stage submodular maximization problem.
Proof of Theorem 3. Since Algorithm ReplacementGreedy runs in ` iterations, and adds an element to S
in each iteration, the final output size, |S|, will not be
more than `. Each set Ti is initialized with ‚àÖ at the
beginning which is an independent set. Also whenever
ReplacementGreedy adds an element x‚àó to Ti , it removes Repi (x‚àó , Ti ). By definition, either Repi (x‚àó , Ti ) is
equal to some element y ‚àà Ti such that set {x‚àó } ‚à™ Ti \ {y}
is an independent set or Repi (x‚àó , Ti ) is the empty set and
{x} ‚à™ Ti is an independent set. In either case, set Ti after
the update will remain an independent set. Therefore the
output of ReplacementGreedy consists of independent
sets Ti ‚àà I(S) for every category i. It remains to lower
bound the aggregate values of these m sets.
We lower bound the aggregate increments of values incurred by adding each element we add to SP
in terms of
m
1
the gap between the current objective value m
i=1 fi (Ti )
m,`
and the optimum objective value Gm (S ). This way, we
can show that for each of the ` elements added to S, the current objective value is incremented enough that in total we
reach at least 21 (1 ‚àí e12 ) fraction of the optimum value. By
definition of ‚àá and the update operations of Algorithm ReplacementGreedy,
element x to S, increases
Pm addition of P
m
the summation i=1 fi (Ti ) by i=1 ‚àái (x, Ti ). We also

Probabilistic Submodular Maximization in Sub-Linear Time

note that the selected element x‚àó maximizes this aggregate
increment. We prove a lower bound benchmark according
to the potential increments of values by elements in S m,` if
we add them instead of x‚àó . In particular, we know that:
m
X

‚àái (x‚àó , Ti ) ‚â•

i=1

1
|S m,` |

m
X X

‚àái (x, Ti )

x‚ààS m,` i=1

This equation holds because the rightmost side is the average increments of values of optimum elements if we add
them instead of x‚àó , and we know that x‚àó is the maximizer of the total value increments. Let Sim,` be the independent subset of S m,` with maximum fi value, i.e.
Sim,` = arg maxA‚ààI(S m,` ) fi (A). Since the ‚àá values are
all non-negative, we can narrow down the rightmost side of
above equation, and imply that:
m
X

‚àó

‚àái (x , Ti ) ‚â•

i=1

1
|S m,` |

m
X
X

‚àái (x, Ti )

(6)

It is well-known (see Lemma 5 P
of (Bateni et al., 2010))
that submodularity of fi implies x‚ààS m,` \Ti ‚àÜi (x, Ti ) ‚â•
i
P
fi (Sim,` )‚àífi (Ti ). We also have x‚ààS m,` \Ti ‚àÜi (œÄ(x), Ti \
i
P
{œÄ(x)}) ‚â§ y‚ààTi ‚àÜi (y, Ti \ {y}) because range of mapping œÄ is a subset of Ti , and no two elements are mapped
to
Pthe same y ‚àà Ti . By submodularity of fi , the latter term
y‚ààTi ‚àÜi (y, Ti \ {y}) is upper bounded by fi (Ti ). We
concludeP
that in each step the total value is increased by at
m
m,`
1
) ‚àí 2f (Ti ) (we assume |S m,` | = `
least `m
i=1 fi (Si
since objective function Gm is monotone increasing). In
other words, in each iteration 1 ‚â§ t ‚â§ `, the increment
2
Xt ‚àí Xt‚àí1 is at least 1` Gm (S m,`
P)m‚àí ` Xt‚àí1 where Xt is
1
defined to be the total value m i=1 fi (Ti ) at the end of
iteration t. Solving this recurrence equation inductively
yields Xt ‚â• 21 (1 ‚àí (1 ‚àí 1` )2t )Gm (S m,` ). We note that
X0 = 0 denotes the total value before the algorithm starts.
The induction step is proved as follows:

i=1 x‚ààS m,`
i

We can apply exchange properties of matroids to lower
bound the total ‚àá values (the rightmost side) for each category. By Corollary 39.12a of (Schrijver, 2003), we know
that there exists a mapping œÄ that maps every element of
Sim,` \ Ti to either the empty set or an element of Ti \ Sim,`
such that:
‚Ä¢ for every x ‚àà Sim,` \ Ti , the set {x} ‚à™ Ti \ {œÄ(x)} is
an independent set, and
Sim,`

‚Ä¢ for every x1 , x2 ‚àà
\ Ti , we either have œÄ(x1 ) 6=
œÄ(x2 ) or both of œÄ(x1 ) and œÄ(x2 ) are equal to the
empty set.

Gm (S m,` ) 2Xt
‚àí
`
`
2
Gm (S m,` )
+ (1 ‚àí )Xt
Xt+1 ‚â•
`
`
m,`
Gm (S )
2 1
1
+ (1 ‚àí ) (1 ‚àí (1 ‚àí )2t )Gm (S m,` )
`
` 2
`
1
1 2t+2
m,`
(1 ‚àí (1 ‚àí )
)Gm (S )
2
`
Xt+1 ‚àí Xt ‚â•

=‚áí
‚â•
‚â•

At the end of Algorithm ReplacementGreedy, the total
value X` is at least 21 (1 ‚àí (1 ‚àí 1` )2` )Gm (S m,` ) ‚â• 12 (1 ‚àí
1
m,`
) which completes the proof.
e2 )Gm (S

7. Conclusions
We note that Corollary 39.12a of (Schrijver, 2003) is stated
for equal size sets (e.g. |Ti | = |Sim,` |) which can be easily adapted to non-equal size sets and achieve the above
mapping by applying the exchange property of matroids iteratively, and making these two sets equal size. Given the
mapping œÄ, the next step is to lower bound each ‚àái (x, Ti )
by how much we can increase the value of Ti by replacing œÄ(x) with x in set Ti . Since {x} ‚à™ Ti \ {œÄ(x)} is an
independent set, we have
‚àái (x, Ti ) ‚â•
=

fi ({x} ‚à™ Ti \ {œÄ(x)}) ‚àí fi (Ti )
‚àÜi (x, Ti ) ‚àí ‚àÜi (œÄ(x), Ti ‚à™ {x} \ {œÄ(x)})

‚â• ‚àÜi (x, Ti ) ‚àí ‚àÜi (œÄ(x), Ti \ {œÄ(x)})
where the equality holds by definition of ‚àÜi values, and the
last inequality holds by submodularity of fi . Combining
this lower bound on ‚àá with Equation 6 implies that in each
step, adding element x‚àó to set S increases the total value of
the current solution by at least:
1
|S m,` |

m
X

X

i=1 x‚ààS m,` \Ti
i

‚àÜi (x, Ti ) ‚àí ‚àÜi (œÄ(x), Ti \ {œÄ(x)})

In this paper, we have studied the novel problem of sublinear time probabilistic submodular maximization: By investing computational effort once to reduce the ground set
based on training instances, faster optimization is achieved
on test instances. Our key technical contribution is ReplacementGreedy, a novel algorithm for two-stage submodular maximization (the empirical variant of our problem). Compared to prior approaches, ReplacementGreedy provides constant factor approximation guarantees
while applying to general submodular objectives, handling
arbitrary matroid constraints and scaling linearly in all relevant parameters.
Acknowledgments. This work was supported by DARPA
Young Faculty Award (D16AP00046), Simons-Berkeley
fellowship, and ERC StG 307036. This work was done in
part while Amin Karbasi and Andreas Krause were visiting
Simons Institute for the Theory of Computing.

Probabilistic Submodular Maximization in Sub-Linear Time

References
Ashwinkumar Badanidiyuru, Shahar Dobzinski, Sigal Oren. Optimization with demand oracles. EC, 2012.
Badanidiyuru, Ashwinkumar and Jan, VondraÃÅk. Fast algorithms for maximizing submodular functions. In SODA,
2014.
Badanidiyuru, Ashwinkumar, Mirzasoleiman, Baharan,
Karbasi, Amin, and Krause, Andreas. Streaming submodular maximization: Massive data summarization on
the fly. In ACM KDD, 2014.
Balkanski, Erik, Krause, Andreas, Mirzasoleiman, Baharan, and Singer, Yaron. Learning sparse combinatorial
representations via two-stage submodular maximization.
In Proc. International Conference on Machine Learning
(ICML), June 2016.
Bateni, MohammadHossein, Hajiaghayi, MohammadTaghi, and Zadimoghaddam, Morteza. Submodular
secretary problem and extensions. In Approximation,
Randomization, and Combinatorial Optimization. Algorithms and Techniques, pp. 39‚Äì52. Springer, 2010.
Buchbinder, Niv, Feldman, Moran, Naor, Joesph (Seffi),
and Schwartz, Roy. Submodular maximization with cardinality constrains. In SODA, 2014.
Calinescu, Gruia, Chekuri, Chandra, PaÃÅl, Martin, and
VondraÃÅk, Jan. Maximizing a monotone submodular
function subject to a matroid constraint. SIAM Journal
on Computing, 40(6):1740‚Äì1766, 2011.
CandeÃÅs, E. and Recht, B. Exact matrix completion via
convex optimization. In Foundations of Computational
Mathematics, 2008.
Das, Abhimanyu and Kempe, David.
Submodular
meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection.
arXiv:1102.3975, 2011.
El-Arini, Khalid, Veda, Gaurav, Shahaf, Dafna, and
Guestrin, Carlos. Turning down the noise in the blogosphere. In KDD, 2009.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J.,
and Zisserman, A. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results, 2014.

Goemans, Michel. Chernoff bounds, and some applications, 2015.
Hazan, Elad and Kale, Satyen. Online submodular minimization. In NIPS, 2009.
Jegelka, Stefanie and Bilmes, Jeff A. Online submodular
minimization for combinatorial structures. In International Conference on Machine Learning (ICML), Bellevue, Washington, 2011.
Krause, A. and Guestrin, C. Near-optimal nonmyopic value
of information in graphical models. In UAI, 2005.
Krause, Andreas and Golovin, Daniel.
Submodular
function maximization.
In Tractability: Practical
Approaches to Hard Problems. Cambridge University
Press, 2012.
Krause, Andreas and Gomes, Ryan G. Budgeted nonparametric learning from data streams. In ICML, 2010.
Kumar, Ravi, Moseley, Benjamin, Vassilvitskii, Sergei, and
Vattani, Andrea. Fast greedy algorithms in mapreduce
and streaming. In SPAA, 2013.
Lin, Hui and Bilmes, Jeff. A class of submodular functions
for document summarization. In ACL, 2011.
Lin, Hui and Bilmes, Jeff. Learning mixtures of submodular shells with application to document summarization.
In Uncertainty in Artificial Intelligence (UAI), Catalina
Island, USA, July 2012. AUAI.
Minoux, Michel. Accelerated greedy algorithms for maximizing submodular set functions. In Proc. of the 8th
IFIP Conference on Optimization Techniques. Springer,
1978.
Mirzasoleiman, Baharan, Karbasi, Amin, Sarkar, Rik, and
Krause, Andreas. Distributed submodular maximization:
Identifying representative elements in massive data. In
NIPS, 2013.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.
Lazier than lazy greedy. In AAAI, 2015.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
and Karbasi, Amin. Fast constrained submodular maximization: Personalized data summarization. In ICML,
2016a.

Feige, Uriel. On maximizing welfare when utility functions
are subadditive. SIAM Journal on Computing, 2009.

Mirzasoleiman, Baharan, Karbasi, Amin, Sarkar, Rik, and
Krause, Andreas. Distributed submodular maximization.
Journal of Machine Learning Research, 17:1‚Äì44, 2016b.

Feldman, Moran, Harshaw, Christopher, and Karbasi,
Amin. Greed is good: Near-optimal submodular maximization via greedy optimization. In COLT, 2017.

Mirzasoleiman, Baharan, Karbasi, Amin, Sarkar, Rik, and
Krause, Andreas. Distributed submodular maximization.
Journal of Machine Learning Research (JMLR), 2016c.

Probabilistic Submodular Maximization in Sub-Linear Time

Mirzasoleiman, Baharan, Zadimoghaddam, Morteza, and
Karbasi, Amin. Fast distributed submodular cover:
Public-private data summarization. In NIPS, 2016d.
Nemhauser, George L., Wolsey, Laurence A., and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functions - I. Mathematical Programming, 1978.
Schrijver, Lex. Combinatorial optimization-polyhedra and
efficiency. Algorithms and Combinatorics, 24:1‚Äì1881,
2003.
Shahar Dobzinski, Noam Nisan and Schapira, Michael.
Approximation algorithms for combinatorial auctions
with complement-free bidders. STOC, 2005.
Singer, Yaron. Budget feasible mechanisms. FOCS, 2010.
Streeter, Matthew and Golovin, Daniel. An online algorithm for maximizing submodular functions. In NIPS,
2008.
Tschiatschek, Sebastian, Iyer, Rishabh, Wei, Haochen, and
Bilmes, Jeff. Learning Mixtures of Submodular Functions for Image Collection Summarization. In Neural
Information Processing Systems (NIPS), 2014.
Wei, Kai, Liu, Yuzong, Kirchhoff, Katrin, and Bilmes, Jeff.
Using document summarization techniques for speech
data subset selection. In Proceedings of Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, 2013.
Wei, Kai, Iyer, Rishabh, and Bilmes, Jeff. Fast multi-stage
submodular maximization. ICML, 2014a.
Wei, Kai, Liu, Yuzong, Kirchhoff, Katrin, Bartels, Chris,
and Bilmes, Jeff. Submodular subset selection for largescale speech training data. In ICASSP, 2014b.
Yue, Yisong and Guestrin, Carlos. Linear submodular bandits and their application to diversified retrieval. NIPS,
2011.

