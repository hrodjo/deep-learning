Scaling Up Sparse Support Vector Machines
by Simultaneous Feature and Sample Reduction

Weizhong Zhang * 1 2 Bin Hong * 1 3 Wei Liu 2 Jieping Ye 3 Deng Cai 1 Xiaofei He 1 Jie Wang 3

Abstract
Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable
features and identify the support vectors. It has
achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely
high-dimensional features, solving sparse SVMs remains challenging. By noting that sparse
SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which
is based on accurate estimations of the primal and
dual optima of sparse SVMs, to simultaneously
identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can
remove the identified inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacrificing accuracy. To
the best of our knowledge, the proposed method
is the first static feature and sample reduction
method for sparse SVM. Experiments on both
synthetic and real datasets (e.g., the kddb dataset
with about 20 million samples and 30 million
features) demonstrate that our approach significantly outperforms state-of-the-art methods and
the speedup gained by our approach can be orders of magnitude.

1. Introduction
Sparse support vector machine (SVM) (Bi et al., 2003;
Wang et al., 2006) is a powerful technique that can simultaneously perform classification by margin maximiza*

Equal contribution 1 State Key Lab of CAD&CG, Zhejiang
University, China 2 Tencent AI Lab, Shenzhen, China 3 University
of Michigan, USA. Correspondence to: Jie Wang <jiewangustc@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

tion and variable selection by `1 -norm penalty. The last
few years have witnessed many successful applications of sparse SVMs, such as text mining (Joachims, 1998;
Yoshikawa et al., 2014), bioinformatics (Narasimhan & Agarwal, 2013) and image processing (Mohr & Obermayer, 2004; Kotsia & Pitas, 2007). Many algorithms (Hastie
et al., 2004; Fan et al., 2008; Catanzaro et al., 2008; Hsieh
et al., 2008; Shalev-Shwartz et al., 2011) have been proposed to efficiently solve sparse SVM problems. However, the applications of sparse SVMs to large-scale learning
problems, which involve a huge number of samples and extremely high-dimensional features, remain challenging.
An emerging technique, called screening (El Ghaoui et al.,
2012), has been shown to be promising in accelerating
large-scale sparse learning. The essential idea of screening
is to quickly identify the zero coefficients in the sparse solutions without solving any optimization problems such that
the corresponding features or samples—that are called inactive features or samples—can be removed from the training phase. Then, we only need to perform optimization on
the reduced datasets instead of the full datasets, leading to
substantial savings in the computational cost and memory
usage. Here, we need to emphasize that screening differs
greatly from feature selection methods, although they look
similar at the first glance. To be precise, screening is devoted to accelerating the training of many sparse models
including Lasso, Sparse SVM, etc., while feature selection
is the goal of these models. In the past few years, many
screening methods are proposed for a large set of sparse
learning techniques, such as Lasso (Tibshirani et al., 2012;
Xiang & Ramadge, 2012; Wang et al., 2013), group Lasso (Ndiaye et al., 2016), `1 -regularized logistic regression
(Wang et al., 2014), and SVM (Ogawa et al., 2013). Empirical studies indicate that screening methods can lead to
orders of magnitude of speedup in computation time.
However, most existing screening methods study either feature screening or sample screening individually (Shibagaki
et al., 2016) and their applications have very different scenarios. Specifically, to achieve better performance (say,
in terms of speedup), we favor feature screening methods when the number of features p is much larger than the
number of samples n, while sample screening methods are

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

preferable when n  p. Note that there is another class
of sparse learning techniques, like sparse SVMs, which induce sparsities in both feature and sample spaces. All these
screening methods are helpless in accelerating the training
of these models with large n and p. We also cannot address
this problem by simply combining the existing feature and
sample screening methods. The reason is that they could
mistakenly discard relevant data as they are specifically designed for different sparse models. Recently, Shibagaki et
al. (Shibagaki et al., 2016) consider this problem and propose a method to simultaneously identify the inactive features and samples in a dynamic manner (Bonnefoy et al.,
2014); that is, during the optimization process, they trigger
their testing rule when there is a sufficient decrease in the duality gap. Thus, the method in (Shibagaki et al., 2016) can
discard more inactive features and samples as the optimization proceeds and one has small-scale problems to solve in
the late stage of the optimization. Nevertheless, the overall
speedup can be limited as the problems’ size can be large
in the early stage of the optimization. To be specific, the
method in (Shibagaki et al., 2016) depends heavily on the
duality gap during the optimization process. The duality
gap in the early stage can always be large, which makes the
dual and primal estimations inaccurate and finally results in
ineffective screening rules. Hence, it is essentially solving
a large problem in the early stage.
In this paper, to address the limitations in the dynamic
screening method, we propose a novel screening method
that can Simultaneously identify Inactive Features and
Samples (SIFS) for sparse SVMs in a static manner, that
is, we only need to perform SIFS once before (instead of
during) optimization. Thus, we only need to run the optimization algorithm on small-scale problems. The major
technical challenge in developing SIFS is that we need to
accurately estimate the primal and dual optima. The more
accurate the estimations are, the more effective SIFS is in
detecting inactive features and samples. Thus, our major
technical contribution is a novel framework, which is based
on the strong convexity of the primal and dual problems of
sparse SVMs [see problems (P∗ ) and (D∗ ) in Section 2] for
deriving accurate estimations of the primal and dual optima
(see Section 3). Another appealing feature of SIFS is the
so-called synergy effect (Shibagaki et al., 2016). Specifically, the proposed SIFS consists of two parts, i.e., Inactive
Feature Screening (IFS) and Inactive Samples Screening
(ISS). We show that discarding inactive features (samples)
identified by IFS (ISS) leads to a more accurate estimation
of the primal (dual) optimum, which in turn dramatically
enhances the capability of ISS (IFS) in detecting inactive
samples (features). Thus, SIFS applies IFS and ISS in an
alternating manner until no more inactive features and samples can be identified, leading to much better performance
in scaling up large-scale problems than the application of

ISS or IFS individually. Moreover, SIFS (see Section 4) is
safe in the sense that the detected features and samples are
guaranteed to be absent from the sparse representations. To
the best of our knowledge, SIFS is the first static screening
rule for sparse SVM that is able to simultaneously detect
inactive features and samples. Experiments (see Section 5)
on both synthetic and real datasets demonstrate that SIFS significantly outperforms the state-of-the-art (Shibagaki
et al., 2016) in improving the efficiency of sparse SVMs and the speedup can be orders of magnitude. Detailed
proofs of theoretical results in the main text are in the supplementary supplements.
Notations: Let k · k1 , k · k, and k · k∞ be the `1 , `2 , and
`∞ norms, respectively. We denote the inner product of
vectors x and y by hx, yi, and the i-th component of x by
[x]i . Let [p] = {1, 2..., p} for a positive integer p. Given
a subset J := {j1 , ..., jk } of [p], let |J | be the cardinality of J . For a vector x, let [x]J = ([x]j1 , ..., [x]jk )T .
For a matrix X, let [X]J = (xj1 , ..., xjk ) and J [X] =
((xj1 )T , ..., (xjk )T )T , where xi and xj are the ith row and
j th column of X, respectively. For a scalar t, we denote
max{0, t} by [t]+ .

2. Basics and Motivations
In this section, we briefly review some basics of sparse
SVMs and then motivate SIFS via the KKT conditions.
Specifically, we focus on the `1 -regularized SVM with a smoothed hinged loss that has strong theoretical guarantees
(Shalev-Shwartz & Zhang, 2016), which takes the form of
n

minp P (w; α, β) =

w∈R

α
1X
`(1 − hx̄i , wi) + kwk2
n i=1
2
+ β||w||1 ,

(P∗ )

where w is the parameter vector to be estimated,
{xi , yi }ni=1 is the training set, xi ∈ Rp , yi ∈ {−1, +1},
x̄i = yi xi , α and β are positive parameters, and the loss
function `(·) : R → R is

`(t) =



0,

if t < 0,
if 0 ≤ t ≤ γ,

γ
t − 2 , if t > γ,
t2
,
2γ


where γ ∈ (0, 1). We present the Lagrangian dual problem
of problem (P∗ ) and the KKT conditions in the following
theorem, which plays a fundamentally important role in developing our screening rule.
Theorem 1. Let X̄ = (x̄1 , x̄2 , ..., x̄n ) and Sβ (·) be
the soft-thresholding operator (Hastie et al., 2015), i.e.,
[Sβ (u)]i = sign([u]i )(|[u]i | − β)+ . Then, for problem
(P∗ ), the followings hold:

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(i) : The dual problem of (P∗ ) is
min n D(θ; α, β) =

θ∈[0,1]

 
2

1 
Sβ 1 X̄θ  + γ kθk2


2α
n
2n
1
− h1, θi,
(D∗ )
n

where 1 ∈ Rn is a vector with all components equal to 1.
(ii) : Denote the optima of (P∗ ) and (D∗ ) by w∗ (α, β) and
θ∗ (α, β), respectively. Then,


1
1
X̄θ∗ (α, β) ,
(KKT-1)
w∗ (α, β) = Sβ
α
n


if 1 − hx̄i , w∗ (α, β)i < 0;
0,
[θ∗ (α, β)]i = 1,
if 1 − hx̄i , w∗ (α, β)i > γ;

1
∗
otherwise .
γ (1 − hx̄i , w (α, β)i),
(KKT-2)
According to KKT-1 and KKT-2, we define 4 index sets:


1
∗
F = j ∈ [p] : |[X̄θ (α, β)]j | ≤ β ,
n
R = {i ∈ [n] : 1 − hw∗ (α, β), x̄i i < 0},
E = {i ∈ [n] : 1 − hw∗ (α, β), x̄i i ∈ [0, γ]},

Lemma 1 indicates that, if we can identify index sets F̂ and
D̂ and the cardinalities of F̂ c and D̂c are much smaller than
the feature dimension p and the dataset size n, we only need
to solve a problem (scaled-D∗ ) that may be much smaller
than problem (D∗ ) to exactly recover the optima w∗ (α, β)
and θ∗ (α, β) without sacrificing any accuracy.
However, we cannot directly apply the rules in (R) to identify subsets of F, R, and L, as they require the knowledge of w∗ (α, β) and θ∗ (α, β) that are usually unavailable.
Inspired by the idea in (El Ghaoui et al., 2012), we can
first estimate regions W and Θ that contain w∗ (α, β) and
θ∗ (α, β), respectively. Then, by denoting





1
(1)
F̂ := j ∈ [p] : max  [X̄θ]j  ≤ β ,
θ∈Θ
n


R̂ := i ∈ [n] : max {1 − hw, x̄i i} < 0 ,
(2)
w∈W


L̂ := i ∈ [n] : min {1 − hw, x̄i i} > γ ,
(3)
w∈W

L = {i ∈ [n] : 1 − hw∗ (α, β), x̄i i > γ},

since it is easy to know that F̂ ⊂ F, R̂ ⊂ R and L̂ ⊂ L,
the rules in (R) can be relaxed as follows:

which imply that
(i): i ∈ F ⇒ [w∗ (α, β)]i = 0,

i ∈ R ⇒ [θ∗ (α, β)]i = 0,
(ii):
i ∈ L ⇒ [θ∗ (α, β)]i = 1.

(iii) : Suppose that θ∗ (α, β) is known. Then,


1
1
∗
[w∗ (α, β)]F̂ c = Sβ
[
X̄]θ
(α,
β)
.
c
α
n F̂

(R)

(i): j ∈ F̂ ⇒ [w∗ (α, β)]j = 0,

i ∈ R̂ ⇒ [θ∗ (α, β)]i = 0,
(ii):
i ∈ L̂ ⇒ [θ∗ (α, β)]i = 1.

(R1)
(R2)

In view of R1 and R2, we sketch the development of SIFS
Thus, we call the j th feature inactive if j ∈ F. The samas follows.
ples in E are the so-called support vectors and we call the
samples in R and L inactive samples.
Step 1: Derive estimations W and Θ such that w∗ (α, β) ∈ W
and θ∗ (α, β) ∈ Θ, respectively.
Suppose that we are given subsets of F, R, and L, then
∗
by (R), we can see that many coefficients of w (α, β) and
Step 2: Develop SIFS by deriving the relaxed screening rules
θ∗ (α, β) are known. Thus, we may have much less unR1 and R2, i.e., by solving the optimization problems
knowns to solve and the problem size can be dramatically
in Eq. (1), Eq. (2) and Eq. (3).
reduced. We formalize this idea in Lemma 1.
Lemma 1. Given index sets F̂ ⊆ F, R̂ ⊆ R, and L̂ ⊆ L,
the followings hold
(i) : [w∗ (α, β)]F̂ = 0, [θ∗ (α, β)]R̂ = 0, [θ∗ (α, β)]L̂ = 1.
(ii) : Let D̂ = R̂ ∪ L̂, Ĝ1 = F̂ c [X̄]D̂c , and Ĝ2 = F̂ c [X̄]L̂ ,
where F̂ c = [p] \ F̂, D̂c = [n] \ D̂, and L̂c = [n] \ L̂. Then,
[θ∗ (α, β)]D̂c solves the following scaled dual problem:

2
n 1 


Sβ 1 Ĝ1 θ̂ + 1 Ĝ2 1  + γ kθ̂k2


n
n
2n
θ̂∈[0,1]|D̂c | 2α
o
1
− h1, θ̂i .
(scaled-D∗ )
n
min

3. Estimate the Primal and Dual Optima
In this section, we first show that the primal and dual optima admit closed form solutions for specific values of α
and β (see Section 3.1). Then, in Sections 3.2 and 3.3, we
present accurate estimations of the primal and dual optima,
respectively.
3.1. Effective Intervals of the Parameters α and β
We first show that, if the value of β is sufficiently large, no
matter what α is, the primal solution is 0.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Theorem 2. Let βmax = k n1 X̄1k∞ . Then, for α > 0 and
β ≥ βmax , we have
∗

w (α, β) = 0,

∗

θ (α, β) = 1.

For any β, the next result shows that, if α is large enough,
the primal and dual optima admit closed form solutions.
Theorem 3. If we denote
αmax (β) =


	
1
1
max hx̄i , Sβ ( X̄1)i ,
1 − γ i∈[n]
n

then for all α ∈ [max{αmax (β), 0}, ∞) ∩ (0, ∞), we have


1
1
∗
w (α, β) = Sβ
X̄1 , θ∗ (α, β) = 1.
(4)
α
n
By Theorems 2 and 3, we only need to consider the cases
with β ∈ (0, βmax ] and α ∈ (0, αmax (β)].
3.2. Primal Optimum Estimation
In Section 1, we mention that the proposed SIFS consists of
IFS and ISS, and an alternating application of IFS and ISS
can improve the estimation of the primal and dual optima,
which can in turn make ISS and IFS more effective in identifying inactive samples and features, respectively. Lemma
2 shows that discarding inactive features by IFS leads to a
more accurate estimation of the primal optimum.
Lemma 2. Suppose that the reference solution w∗ (α0 , β0 )
with β0 ∈ (0, βmax ] and α0 ∈ (0, αmax (β0 )] is known.
Consider problem (P∗ ) with parameters α > 0 and β0 . Let
F̂ be the index set of the inactive features identified by the
previous IFS steps, i.e., [w∗ (α, β0 )]F̂ = 0. We define
α0 + α ∗
[w (α0 , β0 )]F̂ c ,
2α
(α0 − α)2
r2 =
kw∗ (α0 , β0 )k2
4α2
(α0 + α)2
−
k[w∗ (α0 , β0 )]F̂ k2 .
4α2
c=

(5)

(6)

Then, the following holds:
∗

[w (α, β0 )]F̂ c ∈ W := {w : kw − ck ≤ r}.

(7)

As F̂ is the index set of identified inactive features, we
have [w∗ (α, β0 )]F̂ = 0. Hence, we only need to find an
accurate estimation of [w∗ (α, β0 )]F̂ c . Lemma 2 shows that
[w∗ (α, β0 )]F̂ c lies in a ball of radius r centered at c. Note
that, before we perform IFS, the set F̂ is empty and thus the
second term on the right hand side (RHS) of Eq. (6) is 0. If
we apply IFS multiple times (alternating with ISS), the set
F̂ will be monotonically increasing. Thus, Eq. (6) implies
that the radius will be monotonically decreasing, leading to
a more accurate primal optimum estimation.

3.3. Dual Optimum Estimation
Similar to Lemma 2, the next result shows that ISS can
improve the estimation of the dual optimum.
Lemma 3. Suppose that the reference solution θ∗ (α0 , β0 )
with β0 ∈ (0, βmax ] and α0 ∈ (0, αmax (β0 )] is known.
Consider problem (D∗ ) with parameters α > 0 and β0 .
Let R̂ and L̂ be the index sets of inactive samples identified by the previous ISS steps, i.e., [θ∗ (α, β0 )]R̂ = 0,
[θ∗ (α, β0 )]L̂ = 1, and D̂ = R̂ ∪ L̂. We define
α0 + α ∗
α − α0
(8)
1+
[θ (α0 , β0 )]D̂c ,
2γα
2α

2
1 
(α0 − α)2 
∗
2


r =
θ (α0 , β0 ) − γ 1
4α2

2
 (2γ − 1)α + α0

α0 + α ∗

−
1
−
[θ
(α
,
β
)]
0
0 L̂ 

2γα
2α

2
 α − α0

α0 + α ∗

−
(9)
1+
[θ (α0 , β0 )]R̂ 
 .
2γα
2α
c=

Then, the following holds:
[θ∗ (α, β0 )]D̂c ∈ Θ := kθ : θ − ck ≤ r.

(10)

Similar to Lemma 2, Lemma 3 also bounds [θ∗ (α, β0 )]D̂c
by a ball. In view of Eq. (9), a similar discussion of Lemma
2—that is, the index sets L̂ and R̂ monotonically increase
and thus the last two terms on the RHS of Eq. (9) monotonically increase when we perform ISS multiple times (alternating with IFS)—implies that the ISS steps can reduce the
radius and thus improve the dual optimum estimation.
Remark 1. To estimate w∗ (α, β0 ) and θ∗ (α, β0 ) by Lemmas 2 and 3, we have a free reference solution pair
w∗ (α0 , β0 ) and θ∗ (α0 , β0 ) with α0 = αmax (β0 ). From
Theorems 2 and 3, we know that in this setting, w∗ (α0 , β0 )
and θ∗ (α0 , β0 ) admit closed form solutions.

4. The Proposed SIFS Screening Rule
We first present the IFS and ISS rules in Sections 4.1 and
4.2, respectively. Then, in Section 4.3, we develop the SIFS
screening rule by an alternating application of IFS and ISS.
4.1. Inactive Feature Screening (IFS)
Suppose that w∗ (α0 , β0 ) and θ∗ (α0 , β0 ) are known, we derive IFS to identify inactive features for problem (P∗ ) at
(α, β0 ) by solving the optimization problem in Eq. (1) (see
Section E in the supplementary material):


1
i
i
i
s (α, β0 ) = max
|h[x̄ ]D̂c , θi + h[x̄ ]L̂ , 1i| , i ∈ F̂ c ,
θ∈Θ
n
(11)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

where Θ is given by Eq. (10) and F̂ and D̂ = R̂ ∪ L̂ are the
index sets of inactive features and samples that have been identified in previous screening processes, respectively. The
next result shows the closed form solution of problem (11).
Lemma 4. Consider problem (11). Let c and r be given by
Eq. (8) and Eq. (9). Then, for all i ∈ F̂ c , we have
si (α, β0 ) =

1
(|h[x̄i ]D̂c , ci + h[x̄i ]L̂ , 1i| + k[x̄i ]D̂c kr).
n

We are now ready to present the IFS rule.
Theorem 4. Consider problem (P∗ ). We suppose that
w∗ (α0 , β0 ) and θ∗ (α0 , β0 ) are known. Then,
(1): The feature screening rule IFS takes the form of
∗

i

s (α, β0 ) ≤ β0 ⇒ [w (α, β0 )]i = 0, ∀i ∈ F̂

c

(IFS)

(2): We update the index set F̂ by
F̂ ← F̂ ∪ {i : si ≤ β0 , i ∈ F̂ c }.

(12)

Recall that (Lemma 3), previous sample screening results
give us a more tighter dual estimation, i.e., a smaller feasible region Θ for problem (11), which results in a smaller
si (α, β0 ). It finally leads us to a more powerful feature
screening rule IFS. This is the so called synergy effect.
4.2. Inactive Sample Screening (ISS)
Similar to IFS, we derive ISS to identify inactive samples
by solving the optimization problems in Eq. (2) and Eq. (3)
(see Section G in the supplementary material for details):
ui (α, β0 ) = max {1 − h[x̄i ]F̂ c , wi}, i ∈ D̂c ,

(13)

li (α, β0 ) = min {1 − h[x̄i ]F̂ c , wi}, i ∈ D̂c ,

(14)

w∈W

w∈W

where W is given by Eq. (7) and F̂ and D̂ = R̂ ∪ L̂ are the
index sets of inactive features and samples that have been
identified in previous screening processes. We show that
problems (13) and (14) admit closed form solutions.
Lemma 5. Consider problems (13) and (14). Let c and r
be given by Eq. (5) and Eq. (6). Then,
ui (α, β0 ) = 1 − h[x̄i ]F̂ c , ci + k[x̄i ]F̂ c kr, i ∈ D̂c ,
li (α, β0 ) = 1 − h[x̄i ]F̂ c , ci − k[x̄i ]F̂ c kr, i ∈ D̂c .
We are now ready to present the ISS rule.
Theorem 5. Consider problem (D∗ ). We suppose that
w∗ (α0 , β0 ) and θ∗ (α0 , β0 ) are known. Then,
(1): The sample screening rule ISS takes the form of
ui (α, β0 ) < 0 ⇒ [θ∗ (α, β0 )]i = 0,
∀i ∈ D̂c (ISS)
li (α, β0 ) > γ ⇒ [θ∗ (α, β0 )]i = 1,

(2): We update the the index sets R̂ and L̂ by
R̂ ← R̂ ∪ {i : ui (α, β0 ) < 0, i ∈ D̂c },
c

L̂ ← L̂ ∪ {i : li (α, β0 ) > γ, i ∈ D̂ }.

(15)
(16)

The synergy effect also exists here. Recall that (Lemma 2),
previous feature screening results lead a smaller feasible
region W for the problems (13) and (14), which results in
smaller ui (α, β0 ) and bigger li (α, β0 ). It finally leads us to
a more accurate sample screening rule ISS.
4.3. The Proposed SIFS Rule by An Alternating
Application of IFS and ISS
In real applications, the optimal parameter values of α and
β are usually unknown. To determine appropriate parameter values, common approaches, like cross validation and
stability selection, need to solve the model over a grid of
parameter values {(αi,j , βj ) : i ∈ [M ], j ∈ [N ]} with
βmax > β1 > ... > βN > 0 and αmax (βj ) > α1,j > ... >
αM,j > 0. This can be very time-consuming. Inspired by
Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui
et al., 2012), we develop a sequential version of SIFS in
Algorithm 1. Specifically, given the primal and dual optiAlgorithm 1 SIFS
1: Input: βmax > β1 > ... > βN > 0 and αmax (βj ) =
α0,j > α1,j > ... > αM,j > 0.
2: for j = 1 to N do
3:
Compute the first reference solution w∗ (α0,j , βj )
and θ∗ (α0,j , βj ) using the close-form formula (4).
4:
for i = 1 to M do
5:
Initialization: F̂ = R̂ = L̂ = ∅
6:
repeat
7:
Run sample screening using rule ISS based on
w∗ (αi−1,j , βj ).
8:
Update R̂ and L̂ by Eq. (15) and Eq. (16), respectively.
9:
Run feature screening using rule IFS based on
θ∗ (αi−1,j , βj ).
10:
Update F̂ by Eq. (12).
11:
until No new inactive features or samples are identified
12:
Compute w∗ (αi,j , βj ) and θ∗ (αi,j , βj ) by solving the scaled problem.
13:
end for
14: end for
15: Output:w∗ (αi,j , βj ) and θ ∗ (αi,j , βj ), i ∈ [M ], j ∈
[N ].
ma w∗ (αi−1,j , βj ) and θ∗ (αi−1,j , βj ) at (αi−1,j , βj ), we
apply SIFS to identify the inactive features and samples for
problem (P∗ ) at (αi,j , βj ). Then, we perform optimization
on the reduced dataset and solve the primal and dual optima

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

at (αi,j , βj ). We repeat this process until we solve problem
(P∗ ) at all pairs of parameter values.

choose the state-of-art screening method for Sparse SVMs
in (Shibagaki et al., 2016) as a baseline in the experiments.

Note that we insert α0,j into every sequence {αi,j : i ∈
[M ]} ( see line 1 in Algorithm 1) to obtain a closed-form
solution as the first reference solution. In this way, we can
avoid solving problem at (α1,j , βj ), j ∈ [N ] directly (without screening), which is time consuming. At last, we would
like to point out that the values {(αi,j , βj ) : i ∈ [M ], j ∈
[N ]} in SIFS can be specified by users arbitrarily.

For each dataset, we solve problem (P∗ ) at a grid of turning
parameter values. Specifically, we first compute βmax by
Theorem 2 and then select 10 values of β that are equally
spaced on the logarithmic scale of β/βmax from 1 to 0.05.
Then, for each value of β, we first compute αmax (β) by
Theorem 3 and then select 100 values of α that are equally spaced on the logarithmic scale of α/αmax (β) from 1
to 0.01. Thus, for each dataset, we solve problem (P∗ ) at
1000 pairs of parameter values in total. We write the code
in C++ along with Eigen library for some numerical computations. We perform all the computations on a single core
of Intel(R) Core(TM) i7-5930K 3.50GHz, 128GB MEM.

Theorem 6. Given the optimal solutions w∗ (αi−1,j , βj )
and θ∗ (αi−1,j , βj ) at (αi−1,j , βj ) as the reference solution
pair at (αi,j , βj ) for SIFS, we assume SIFS with ISS first
stops after applying IFS and ISS for p times and denote
the identified inactive features and samples as F̂pA , R̂A
p and
A
L̂p . Similarly, when we apply IFS first, the results are deB
noted as F̂qB , R̂B
q and L̂q . Then, the followings hold:
B
A
A
B
A
(1) F̂p = F̂q , R̂p = R̂B
q and L̂p = L̂q .
(2) With different orders of applying ISS and IFS, the difference of the times of ISS and IFS we need to apply in SIFS
can never be larger than 1, that is, |p − q| ≤ 1.
Remark 2. From Remark 1, we can see that our SIFS can
also be applied to solve a single problem, due to the existence of the free reference solution pair.

5.1. Simulation Studies
We evaluate SIFS on 3 synthetic datasets named
syn1, syn2 and syn3 with sample and feature size
(n, p) ∈ {(10000, 1000), (10000, 10000), (1000, 10000)}.
We present each data point as x = [x1 ; x2 ] with x1 ∈
R0.02p and x2 ∈ R0.98p . We use Gaussian distributions
G1 = N (u, 0.75I), G2 = N (−u, 0.75I) and G3 = N (0, 1)
to generate the data points, where u = 1.51 and I ∈
R0.02p×0.02p is the identity matrix. To be precise, x1 for
positive and negative points are sampled from G1 and G2 ,
respectively. For each entry in x2 , it has chance η = 0.02
to be sampled from G3 and chance 1 − η to be 0.
log(β/βmax)

SIFS applies ISS and IFS in an alternating manner to reinforce their capability in identifying inactive samples and
features. In Algorithm 1, we apply ISS first. Of course, we
can also apply IFS first. The theorem below demonstrates
that the orders have no impact on the performance of SIFS.

0
-0.7
-1
-1.3
-2 -1.5 -1 -0.5

log(α/α

5. Experiments

max

)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5

log(α/α

max

)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5 0

log(α/α

max

1
0.95

)

Recall that, we can integrate SIFS with any solvers for
problem (P∗ ). In this experiment, we use Accelerated
Proximal Stochastic Dual Coordinate Ascent (AcceleratedProx-SDCA) (Shalev-Shwartz & Zhang, 2016), as it is one
of the state-of-the-arts. As we mentioned in the introduction section that screening differs greatly from features selection methods, it is not appropriate to make comparisons with feature selection methods. To this end, we only

max

log(β/β

0
-0.7
-1
-1.3
-2 -1.5 -1 -0.5

log(α/αmax)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5

log(α/αmax)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5 0

1
0.9
0.8

log(α/αmax)

max

)

(b) The scaling ratios of ISS, IFS, and SIFS on syn2.

log(β/β

We evaluate SIFS on both synthetic and real datasets in
terms of three measurements. The first one is the scaling
p̃)
ratio: 1− (n−ñ)(p−
, where ñ, p̃, n, and p are the numbers
np
of inactive samples and features identified by SIFS, sample
size, and feature dimension of the datasets. The second
measure is rejection ratios of each triggering of ISS and
IFS in SIFS: nñ0i and pp̃0i , where ñi and p̃i are the numbers
of inactive samples and features identified in i-th triggering
of ISS and IFS in SIFS. n0 and p0 are the numbers of inactive samples and features in the solution. The third measure
is speedup, i.e., the ratio of the running time of the solver
without screening to that with screening.

)

(a) The scaling ratios of ISS, IFS, and SIFS on syn1.

0
-0.7
-1
-1.3
-2 -1.5 -1 -0.5

log(α/αmax)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5

log(α/αmax)

0
-0.7
-1
-1.3
0
-2 -1.5 -1 -0.5 0

1
0.9
0.8

log(α/αmax)

(c) The scaling ratios of ISS, IFS, and SIFS on syn3.
Figure 1. Scaling ratios of ISS, IFS and SIFS (from left to right).

Fig. 1 shows the scaling ratios by ISS, IFS, and SIFS on
the synthetic datasets at 1000 parameter values. We can
see that IFS is more effective in scaling problem size than
ISS, with scaling ratios roughly 98% against 70 − 90%.
Moreover, SIFS, which is an alternating application of IFS
and ISS, significantly outperforms ISS and IFS, with scal-

0.4 1

0
0.01 0.03 0.1

α/αmax

Trigger 1
Trigger 2
Trigger 3

0.4 1

α/αmax

(e) β/βmax =0.05

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

1

0.5

0.4 1

α/αmax

(f) β/βmax =0.1

(d) β/βmax =0.9

1

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

α/αmax

(c) β/βmax =0.5

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

α/αmax

(b) β/βmax =0.1
Rejection Ratio

Rejection Ratio

1

0
0.01 0.03 0.1

0.4 1

0.5

α/αmax

(a) β/βmax =0.05

0.5

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0
0.01 0.03 0.1

0.5

1

Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.5

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

0.4 1

1

0.5

0
0.01 0.03 0.1

α/αmax

(g) β/βmax =0.5

Trigger 1
Trigger 2
Trigger 3

0.4 1

α/αmax

(h) β/βmax =0.9

Figure 2. Rejection ratios of SIFS on syn 2 (first row: Feature Screening, second row: Sample Screening).
Table 1. Running time (in seconds) for solving problem (P∗ ) at 1000 pairs of parameter values on three synthetic datasets.
ISS+Solver
IFS+Solver
SIFS+Solver
Data Solver
ISS
Solver Speedup
IFS
Solver Speedup SIFS Solver Speedup
syn1
499.1
4.9
27.8
15.3
2.3
42.6
11.1
8.6
6.0
34.2
syn2 8749.9 24.9 1496.6
5.8
23.0
288.1
28.1
92.6
70.3
53.7
syn3 1279.7
2.0
257.1
4.9
2.2
33.4
36.0
7.2
9.5
76.8

ing ratios roughly 99.9%. This high scaling ratios imply
that SIFS can lead to a significant speedup.
Due to the space limitation, we only report the rejection
ratios of SIFS on syn2. Other results can be found in the
supplementary material. Fig. 2 shows that SIFS can identify most of the inactive features and samples. However, few
features and samples are identified in the second and later
triggerings of ISS and IFS. The reason may be that the task
here is so simple that one triggering is enough.
Table 1 reports the running time of solver without and with
IFS, ISS and SIFS for solving problem (P∗ ) at 1000 pairs of
parameter values. We can see that SIFS leads to significant
speedups, that is, up to 76.8 times. Taking syn2 for example, without SIFS, the solver takes more than two hours to
solve problem (P∗ ) at 1000 pairs of parameter values. However, combined with SIFS, the solver only needs less than
three minutes for solving the same set of problems. From
the theoretical analysis in (Shalev-Shwartz & Zhang, 2016)
for Accelerated-Prox-SDCA, we can see that its computational complexity rises proportionately to the sample size
n and the feature dimension p. From this theoretical result,
we can see that the results in Figure 1 are roughly consistent with the speedups we achieved shown in Table 1.
5.2. Experiments on Real Datasets
In this experiment, we evaluate the performance of SIFS
on 5 large-scale real datasets: real-sim, rcv1-train, rcv1-

test, url, and kddb, which are all collected from the project
page of LibSVM (Chang & Lin, 2011). See Table 2 for a
brief summary. We note that, the kddb dataset has about 20
million samples with 30 million features.
Table 2. Statistics of the real datasets.
Dataset
real-sim
rcv1-train
rcv1-test
url
kddb

Feature size: p
20,958
47,236
47,236
3,231,961
29,890,095

Sample size:n
72,309
20,242
677, 399
2,396,130
19,264,097

Recall that, SIFS detects the inactive features and samples in a static manner, i.e., we perform SIFS only once
before the optimization and thus the size of the problem
we need to perform optimization on is fixed. However, the
method in (Shibagaki et al., 2016) detects inactive features
and samples in a dynamic manner (Bonnefoy et al., 2014),
i.e., they perform their method along with the optimization
and thus the size of the problem would keep decreasing
during the iterative process. Thus, comparing SIFS with
the method in (Shibagaki et al., 2016) in terms of rejection ratios is inapplicable. We compare the performance of
SIFS with the method in (Shibagaki et al., 2016) in terms of
speedup. Specifically, we compare the speedup gained by
SIFS and the method in (Shibagaki et al., 2016) for solving
problem (P∗ ) at 1000 pairs of parameter values. The code
of the method in (Shibagaki et al., 2016) is obtained from
(https://github.com/husk214/s3fs).

0.01 0.03 0.1

0.4 1

0
0.01 0.03 0.1

α/αmax

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

α/αmax

(e) β/βmax =0.05

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

α/αmax

(b) β/βmax =0.1
Rejection Ratio

Rejection Ratio

0.5

0.4 1

0.5

α/αmax

(a) β/βmax =0.05
1

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.985

0.5

1

0.4 1

α/αmax

(f) β/βmax =0.1

(c) β/βmax =0.5
1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

α/αmax

0.4 1

α/αmax

(g) β/βmax =0.5

(d) β/βmax =0.9
Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.99

1

Rejection Ratio

1
0.995

Rejection Ratio

Rejection Ratio

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

1
0.995
0.99

Trigger 1
Trigger 2
Trigger 3

0.985
0.01 0.03 0.1

0.4 1

α/αmax

(h) β/βmax =0.9

Figure 3. Rejection ratios of SIFS on the real-sim dataset (first row: Feature Screening, second row: Sample Screening).
Table 3. Running time (in seconds) for solving problem (P∗ ) at 1000 pairs of parameter values on five real datasets.
Data
Method in (Shibagaki et al., 2016)+Solver
SIFS+Solver
Solver
Set
Screen
Solver
Speedup
Screen
Solver
Speedup
real-sim
3.93E+04
24.10
4.94E+03
7.91
60.01
140.25
195.00
rcv1-train 2.98E+04
10.00
3.73E+03
7.90
27.11
80.11
277.10
rcv1-test
1.10E+06
398.00
1.35E+05
8.10
1.17E+03 2.55E+03
295.11
url
—
3.18E+04 8.60E+05
—
7.66E+03 2.91E+04
—
kddb
—
4.31E+04 1.16E+06
—
1.10E+04
3.6E+04
—

Fig. 3 shows the rejection ratios of SIFS on the real-sim
dataset (other results are in the supplementary material). In
Fig. 3, we can see that some inactive features and samples
are identified in the 2nd and 3rd triggering of ISS and IFS,
which verifies the necessity of the alternating application
of ISS and IFS. SIFS is efficient since it always stops in 3
times of triggering. In addition, most of (> 98%) the inactive features can be identified in the 1st triggering of IFS
while identifying inactive samples needs to apply ISS two
or more times. It may result from two reasons: 1) We run
ISS first, which reinforces the capability of IFS due to the
synergy effect (see Sections 4.1 and 4.2), see Section L.1 in
the supplementary material for further verification; 2) Feature screening here may be easier than sample screening.
Table 3 reports the running time of solver without and with
the method in (Shibagaki et al., 2016) and SIFS for solving problem (P∗ ) at 1000 pairs of parameter values on real
datasets. The speedup gained by SIFS is up to 300 times on
real-sim, rcv1-train and rcv1-test. Moreover, SIFS significantly outperforms the method in (Shibagaki et al., 2016)
in terms of speedup—by about 30 to 40 times faster on the
aforementioned three datasets. For datasets url and kddb,
we do not report the results of the solver as the sizes of the
datasets are huge and the computational cost is prohibitive.
Instead, we can see that the solver with SIFS is about 25

times faster than the solver with the method in (Shibagaki
et al., 2016) on both datasets url and kddb. Take the dataset
kddb as an example. The solver with SIFS takes about 13
hours to solve problem (P∗ ) for all 1000 pairs of parameter values, while the solver with the method in (Shibagaki
et al., 2016) needs 11 days to finish the same task.

6. Conclusion
In this paper, we develop a novel data reduction method
SIFS to simultaneously identify inactive features and samples for sparse SVM. Our major contribution is a novel
framework for an accurate estimation of the primal and dual optima based on strong convexity. To the best of our
knowledge, the proposed SIFS is the first static screening
method that is able to simultaneously identify inactive features and samples for sparse SVMs. An appealing feature
of SIFS is that all detected features and samples are guaranteed to be irrelevant to the outputs. Thus, the model learned
on the reduced data is identical to the one learned on the
full data. Experiments on both synthetic and real datasets
demonstrate that SIFS can dramatically reduce the problem
size and the resulting speedup can be orders of magnitude.
We plan to generalize SIFS to more complicated models,
e.g., SVM with a structured sparsity-inducing penalty.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Acknowledgements
This work was supported by the National Basic Research Program of China (973 Program) under Grant
2013CB336500, National Natural Science Foundation of
China under Grant 61233011 and National Youth Topnotch Talent Support Program.

References
Bi, Jinbo, Bennett, Kristin, Embrechts, Mark, Breneman,
Curt, and Song, Minghu. Dimensionality reduction via
sparse support vector machines. The Journal of Machine
Learning Research, 3:1229–1243, 2003.
Bonnefoy, Antoine, Emiya, Valentin, Ralaivola, Liva, and
Gribonval, Rémi. A dynamic screening principle for
the lasso. In Signal Processing Conference (EUSIPCO), 2014 Proceedings of the 22nd European, pp. 6–10.
IEEE, 2014.
Catanzaro, Bryan, Sundaram, Narayanan, and Keutzer,
Kurt. Fast support vector machine training and classification on graphics processors. In Proceedings of the
25th international conference on Machine learning, pp.
104–111. ACM, 2008.
Chang, Chih-Chung and Lin, Chih-Jen. Libsvm: a library
for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.

Kotsia, Irene and Pitas, Ioannis. Facial expression recognition in image sequences using geometric deformation
features and support vector machines. Image Processing,
IEEE Transactions on, 16(1):172–187, 2007.
Mohr, Johannes and Obermayer, Klaus. A topographic support vector machine: Classification using local label configurations. In Advances in Neural Information Processing Systems, pp. 929–936, 2004.
Narasimhan, Harikrishna and Agarwal, Shivani. Svm pauc
tight: a new support vector method for optimizing partial
auc based on a tight convex upper bound. In Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 167–175.
ACM, 2013.
Ndiaye, Eugene, Fercoq, Olivier, Gramfort, Alexandre, and
Salmon, Joseph. Gap safe screening rules for sparsegroup lasso. In Lee, D. D., Sugiyama, M., Luxburg,
U. V., Guyon, I., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems 29, pp. 388–
396. Curran Associates, Inc., 2016.
Ogawa, Kohei, Suzuki, Yoshiki, and Takeuchi, Ichiro. Safe
screening of non-support vectors in pathwise svm computation. In Proceedings of the 30th International Conference on Machine Learning, pp. 1382–1390, 2013.

El Ghaoui, Laurent, Viallon, Vivian, and Rabbani, Tarek.
Safe feature elimination in sparse supervised learning.
Pacific Journal of Optimization, 8:667–698, 2012.

Shalev-Shwartz, Shai and Zhang, Tong. Accelerated proximal stochastic dual coordinate ascent for regularized loss
minimization. Mathematical Programming, 155(1-2):
105–145, 2016.

Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,
Xiang-Rui, and Lin, Chih-Jen. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871–1874, 2008.

Shalev-Shwartz, Shai, Singer, Yoram, Srebro, Nathan, and
Cotter, Andrew. Pegasos: Primal estimated sub-gradient
solver for svm. Mathematical programming, 127(1):3–
30, 2011.

Hastie, Trevor, Rosset, Saharon, Tibshirani, Robert, and
Zhu, Ji. The entire regularization path for the support
vector machine. The Journal of Machine Learning Research, 5:1391–1415, 2004.

Shibagaki, Atsushi, Karasuyama, Masayuki, Hatano, Kohei, and Takeuchi, Ichiro. Simultaneous safe screening
of features and samples in doubly sparse modeling. In
Proceedings of The 33rd International Conference on
Machine Learning, 2016.

Hastie, Trevor, Tibshirani, Robert, and Wainwright, Martin. Statistical learning with sparsity: the lasso and generalizations. CRC Press, 2015.
Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, Sellamanickam. A dual
coordinate descent method for large-scale linear svm. In
Proceedings of the 25th international conference on Machine learning, pp. 408–415. ACM, 2008.
Joachims, Thorsten. Text categorization with support vector machines: Learning with many relevant features.
Springer, 1998.

Tibshirani, Robert, Bien, Jacob, Friedman, Jerome, Hastie,
Trevor, Simon, Noah, Taylor, Jonathan, and Tibshirani,
Ryan J. Strong rules for discarding predictors in lassotype problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(2):245–266,
2012.
Wang, Jie, Zhou, Jiayu, Wonka, Peter, and Ye, Jieping. Lasso screening rules via dual polytope projection. In Advances in Neural Information Processing Systems, pp.
1070–1078, 2013.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

Wang, Jie, Zhou, Jiayu, Liu, Jun, Wonka, Peter, and Ye,
Jieping. A safe screening rule for sparse logistic regression. In Advances in Neural Information Processing Systems, pp. 1053–1061, 2014.
Wang, Li, Zhu, Ji, and Zou, Hui. The doubly regularized
support vector machine. Statistica Sinica, pp. 589–615,
2006.
Xiang, Zhen James and Ramadge, Peter J. Fast lasso
screening tests based on correlations. In Acoustics,
Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pp. 2137–2140. IEEE, 2012.
Yoshikawa, Yuya, Iwata, Tomoharu, and Sawada, Hiroshi.
Latent support measure machines for bag-of-words data
classification. In Advances in Neural Information Processing Systems, pp. 1961–1969, 2014.

