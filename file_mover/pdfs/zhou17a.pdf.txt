Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values

Chaoxu Zhou ∗ 1 Wenbo Gao ∗ 1 Donald Goldfarb 1

Abstract

infeasible to analytically compute Ef (x, ξ). This can be
resolved by replacing the expectation Ef (x, ξ) by the estimate (2). The strong law of large numbers implies that
the sample mean L(x) converges almost surely to F (x) as
the number of samples m increases, provided that the samples yi are drawn independently from the distribution of ξ.
However, even the concrete problem (2) is not a good target
for classical optimization algorithms, as the amount of data
m is frequently extremely large. A better strategy when optimizing (2) is to consider subsamples of the data to reduce
the computational cost. This leads to stochastic algorithms
where the objective function changes at each iteration by
randomly selecting subsamples.

We propose a novel class of stochastic, adaptive
methods for minimizing self-concordant functions which can be expressed as an expected
value. These methods generate an estimate of
the true objective function by taking the empirical mean over a sample drawn at each step,
making the problem tractable. The use of adaptive step sizes eliminates the need for the user
to supply a step size. Methods in this class include extensions of gradient descent (GD) and
BFGS. We show that, given a suitable amount of
sampling, the stochastic adaptive GD attains linear convergence in expectation, and with further
sampling, the stochastic adaptive BFGS attains
R-superlinear convergence. We present experiments showing that these methods compare favorably to SGD.

1. Introduction
We are concerned with minimizing functions of the form
min F (x) = Eξ f (x, ξ)

x∈Rn

(1)

Many common problems in statistics and machine learning
can be put into this form. For instance, in the empirical
risk minimization framework, a model is learned from a set
{y1 , . . . , ym } of training data by minimizing an empirical
loss function of the form
m

min L(x) =
x

1 X
f (x, yi )
m i=1

(2)

It is easy to see that this formulation is equivalent to taking
ξ to be the uniform distribution on the points {y1 , . . . , ym }.
An objective function of the form (1) is often impractical,
as the distribution of ξ is generally unavailable, making it
*

Equal contribution 1 Dept. of Industrial Engineering and Operations Research, Columbia University. Correspondence to:
Chaoxu Zhou <cz2364@columbia.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Many stochastic algorithms have been proposed which use
this approach for solving (2), notably stochastic gradient descent (SGD) (Bottou, 2010), and variance-reduced
extensions of SGD, such as SVRG (Johnson & Zhang,
2013), SAG (Schmidt et al., 2013), and SAGA (Defazio
et al., 2014). These methods are first-order methods, extending gradient descent to the stochastic setting, and the
latter three (variance-reduced) methods can be shown to
converge linearly for strongly convex objectives. Linearly convergent stochastic Limited Memory BFGS algorithms (Byrd et al., 2016)(Moritz et al., 2016)(Gower et al.,
2016) have also been proposed. It is then natural to consider stochastic extensions of quasi-Newton and secondorder methods. One such method, the Newton Incremental Method (NIM) (Rodomanov & Kropotov, 2016), combines cyclic updating of a fixed collection of functions
f (x, y1 ), . . . , f (x, ym ) with Newton’s method, and attains
local superlinear convergence.
One of the key obstacles to developing stochastic extensions of quasi-Newton methods is the necessity of selecting appropriate step sizes. The analysis of the global convergence of the BFGS method (Powell, 1976) and other
members of Broyden’s convex class (Byrd et al., 1987)
assumes that Armijo-Wolfe inexact line search is used.
This is rather undesirable for a stochastic algorithm, as
line search is both computationally expensive and difficult to analyze in a probabilistic setting. However, there
is a special class of functions, the self-concordant functions, whose properties allow us to compute an adaptive
step size and thereby avoid performing line searches. In
(Gao & Goldfarb, 2016), it is shown that the BFGS method

Stochastic Adaptive Quasi-Newton Methods

(Broyden, 1967) (Fletcher, 1970)(Goldfarb, 1970)(Shanno,
1970) with adaptive step sizes converges superlinearly
when applied to self-concordant functions.
In this paper, our goal is to develop a stochastic quasiNewton algorithm for self-concordant functions. We propose an iterative method of the following form. At the k-th
iteration, we draw mk i.i.d samples ξ1 , . . . , ξmk . Define the
empirical objective function at the k-th iteration to be
Fk (x) =

mk
1 X
f (x, ξi )
mk i=1

(3)

Let Hk be a positive definite matrix. The next step direction
is given by
dk = −Hk ∇Fk (xk )
and the step size by
tk =

αk
1 + αk δk

where
T

∇Fk (xk ) Hk ∇Fk (xk )
δk2
q
δk = ∇Fk (xk )T Hk ∇2 Fk (xk )Hk ∇Fk (xk )

αk =

(4)

The motivation for this step size is described in Section 4.
A key feature of these methods is that the step size tk can
be computed analytically, using only local information, and
adapts itself to the local curvature. A fixed step size η is
typically used in variance-reduced first-order methods, and
this step size must be determined experimentally. The theoretical analysis that has been provided for these methods is
of little help in choosing η, as often η is constrained to be
impractically small, and moreover, is related to unknown
constants such as the Lipschitz parameter of the gradient.
Furthermore, a fixed η which was effective in one regime
may become ineffective as the algorithm progresses, and
enters regions of varying curvature.
Our new methods are also capable of solving general problems of the form (1). This is in contrast to popular
incremental-type methods such as SAG, SAGA, and NIM,
which, because of their stored updating scheme, can only
be applied to problems of the form (2) with a fixed data
set {y1 , . . . , ym }. This opens up new avenues for the solutions of problems where new data can be sampled as the
algorithm progresses, as opposed to having a fixed training
set throughout. In this respect, our new method is akin to
Streaming SVRG (Frostig et al., 2015), which is also able
to solve (1) with similar conditions on the growth of the
samples mk .
By choosing the matrices Hk appropriately, we obtain
stochastic extensions of classical methods. In particular,
two choices of Hk will be of interest:

1. Taking Hk = I yields the stochastic adaptive gradient
descent method (SA-GD).
2. Fixing H0 , and then taking Hk+1 to be the BFGS
update of Hk , yields the stochastic adaptive BFGS
method (SA-BFGS).
Our methods can be proven to converge under a suitable
sampling regime. When the number of samples mk is fixed,
and large enough, both SA-GD and SA-BFGS converge Rlinearly in expectation to an -optimal solution. To obtain
R-superlinear convergence, the number of samples must
be allowed to grow rapidly. This principle, of increasing
the number of samples to match the desired rate of convergence, was previously applied to R-linear convergence in
(Byrd et al., 2012). The SA-BFGS algorithm can be shown
to converge R-superlinearly almost surely to the true optimal solution if the number of samples is increased so that
m−1
k converges R-superlinearly to 0.
This paper is organized as follows. In Section 2, we introduce our notation, and the technical assumptions needed
for the analysis of our methods. In Section 3, we describe
the relevant results from stochastic analysis which motivate our algorithms. In Section 4, we briefly describe the
required theory of self-concordant functions. In Section 5,
we formally define stochastic adaptive methods, and state
the convergence results. In Section 6, we present preliminary numerical experiments, and conclude with a discussion in Section 7. Full proofs of the convergence theorems
can be found in the supplementary materials.

2. Assumptions and Notation
The number of variables is n. We write g(x) = ∇F (x)
and G(x) = ∇2 F (x), and gk (x) = ∇Fk (x) and Gk (x) =
∇2 Fk (x) for the gradient and Hessian of the empirical objective function. In the context of a sequence of iterates
{xk }∞
k=0 generated by an algorithm, we also write gk with
no argument to denote gk (xk ), and Gk for Gk (xk ). In the
context of BFGS, Hk denotes the approximation of the inverse Hessian, and Bk = Hk−1 . The step direction (generally −Hk gk ) is denoted dk , and the step size by tk , so the
actual step taken is sk = tk dk .
The optimal solution of minn F (x) is denoted x∗ , and the
x∈R

optimal solution of the empirical problem minn Fk (x) is
x∈R

denoted x∗k . Note that x∗k is a random variable.

Unless otherwise specified, the norm k · k is the 2-norm, or
the operator 2-norm. The Frobenius norm is indicated as
k · kF .
We make the following technical assumptions on F (x) and
f (x, ξ). We will explain the motivation for these assumptions at the relevant points in the discussion.

Stochastic Adaptive Quasi-Newton Methods

Assumptions:
1. There exist constants L ≥ ` > 0 such that for every
x ∈ Rn and every realization of ξ, the Hessian of f
with respect to x satisfies
`I 

∇2x f (x, ξ)

 LI

That is, f (x, ξ) is strongly convex for all ξ, with the
eigenvalues of ∇2x f (x, ξ) bounded by ` and L. The
condition number, denoted κ, is given by L/`.
2. Fk (x) is standard self-concordant for every possible
sampling ξ1 , . . . , ξmk .
3. There exist compact sets D0 and D with x∗ ∈ D and
D0 ⊆ D, such that if x0 is chosen in D0 , then for
all possible realizations of the samples ξ1 , . . . , ξmk for
every k, the sequence of iterates {xk }∞
k=0 produced
by the algorithm is contained within D. We use D =
sup{kx − yk : x, y ∈ D} for the diameter of D.
Furthermore, we assume that the objective values and
gradients are bounded:
u = sup sup f (x, ξ) < ∞
ξ

x∈D

l = inf inf f (x, ξ) > −∞
ξ x∈D

γ = sup sup k∇f (x, ξ)k < ∞
ξ

x∈D

4. (For BFGS only) The Hessian G(x) is Lipschitz continuous with constant LH .
Note that Assumption 1 is standard when analyzing
stochastic algorithms. The function f (x, ξ) is commonly
a loss function, and either f (x, ξ) is itself strongly convex,
or each f (x, ξ) is weakly convex and the strong convexity is ensured by adding a quadratic regularization term to
F (x).

3. Stochastic Framework
Our analysis is based on a uniform convergence law, a
standard technique in learning theory and empirical processes. The central idea is that Fk (x) is an empirical
mean estimating F (x), and we can closely control the error |Fk (x) − F (x)| over all x ∈ D by varying the sample
size mk . The relevant stochastic analysis can be found in
(W. van der Vaart & Wellner, 1996) and (Goldfarb et al.,
2017). These powerful techniques are required to analyze
second-order stochastic methods, since inverting the product of a sampled Hessian and sampled gradient generates
both dependence and non-linearity. In comparison, firstorder stochastic methods can be analyzed by evaluating the
expected value and variance of the sampled gradient. It

is also useful to compare this approach with that used in
(Byrd et al., 2012) and (Friedlander & Goh, 2013), where
the variance of the gradient is controlled by pointwise estimates or pointwise tail bounds.
Theorem 3.1 (Corollary 1, (Goldfarb et al., 2017)). Let
δ
}. Then
δ > 0 and 0 <  < min{D, 2L


mk (δ − 2L)2
P(sup |Fk (x)−F (x)| > δ) ≤ c() exp −
2(u − l)2
x∈D
where c() = 2nn/2 Dn −n . If mk ≥ 3, then
r
log mk
E sup |Fk (x) − F (x)| ≤ C
mk
x∈D
r
log mk
E|Fk (x∗k ) − F (x∗ )| ≤ C
mk
where C is a constant (depending only on F ) given by



√
u−l
n/2 n
4(|u|+|l|)n D exp −n log √
+(u−l) n + 1
2 2L
Assumption 3 is required for the uniform law of Theorem 3.1 to hold. The set D0 is assumed to be a region
where, if x0 is chosen within D0 , the path of the stochastic
algorithm will remain within the larger set D. For practical
purposes, we may take D to be an arbitrarily large bounded
set in order to ensure convergence, though this worsens the
complexity bounds provided by Theorem 3.1. We note that
the constant C is very much a ‘worst-case’ bound, and for
almost every problem arising in practice, the expected difference will be much smaller than Theorem 3.1 suggests.
Rather, the crucial implication
q is that the expected difference diminishes at the rate
of control by adjusting mk .

log mk
mk ,

allowing a sharp level

4. Self-Concordant Functions and Adaptive
Methods
A convex function f : Rn → R is self-concordant if there
exists a constant κ such that for every x ∈ Rn and every
h ∈ Rn , we have
|∇3 f (x)[h, h, h]| ≤ κ(∇2 f (x)[h, h])3/2
If κ = 2, f is standard self-concordant. Self-concordant
functions were introduced by Nesterov and Nemirovski
in the context of interior point methods (Nesterov & Nemirovski, 1994).
Many common problems have self-concordant formulations. At least one method, the DiSCO algorithm of Zhang
and Xiao (2015), is tailored for distributed self-concordant
optimization and has been applied to many regression problems. Convex quadratic objective functions have third

Stochastic Adaptive Quasi-Newton Methods

derivatives equal to zero, and are therefore trivially standard self-concordant. In particular, least squares regression is self-concordant. In (Zhang & Xiao, 2015), it is also
shown that regularized regression, with either logistic loss
or hinge loss, is self-concordant.
For self-concordant functions, the notion of a local norm
is especially useful. Given a convex function f , the local
norm with respect to f at the point x is given by
q
khkx = hT ∇2 f (x)h
Consider an iterative method for minimizing selfconcordant functions with steps given as follows. On the kth step, the step direction dk is given by dk = −Hk ∇f (xk )
for some positive definite matrix Hk , and the step size is
αk
, where δk = kdk kxk and αk =
given by tk = 1+α
k δk
T
gk
Hk g k
.
2
δk

Methods of this type have been analyzed in (Tran-Dinh
et al., 2015) and (Gao & Goldfarb, 2016). In (Gao & Goldfarb, 2016), the above choice of αk is shown to guarantees
a decrease in the function value.
Theorem 4.1 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let
ρk = ∇f (xk )T Hk ∇f (xk ). If αk is chosen to be αk = ρδ2k ,
k
then
f (xk + tk dk ) ≤ f (xk ) − ω(ηk ),
where ηk =

ρk
δk

and the function ω(z) = z − log(1 + z).

We make Assumption 2 in order to apply Theorem 4.1 to
the empirical objective functions Fk (x). A natural question
is whether Assumption 2 can be relaxed to the assumption
that f (x, ξ) is self-concordant for all ξ, and F (x) is standard self-concordant. In the case where ξ is finitely supported, it is possible to scale F (x) so that we may use this
weaker assumption.
Lemma 4.2. Suppose that ξ is finitely supported on
{a1 , . . . , am }, with pi = P(ξ = ai ). Suppose that f (x, ai )
max κ2
is self-concordant with constant κi . Let θ = 14 min pii . Then
the scaled function F (x) = E[θf (x, ξ)] is standard selfconcordant, and every empirical objective function F k (x)
is standard self-concordant.
Proof. Observe that θf (x, ai )pi is self-concordant with
−1/2
constant
θ−1/2 pi
κi ≤ 2. We deduce that EF (x) =
Pm
θf
(x,
a
)p
is
i
i
i=1
Pm standard self-concordant. Furthermore, since
= 1, we have min pi ≤
i=1 pi
1
1
.
Thus,
θf
(x,
a
)
is
self-concordant with constant
i
m
m
θ−1/2 m−1/2 κi ≤ 2, which implies that F k (x) is standard
self-concordant for every possible F k (x).
However, if we do not assume that each f (x, ξ) is selfconcordant, or if ξ is not compactly supported, it is un-

Algorithm 1 SA-GD
Input: x0 , {m0 , m1 , . . .}
for k = 0, 1, 2, . . . do
Sample ξ1 , . . . , ξmk i.i.d from ξ
Compute:
gk = ∇Fk (xk ), dk = −gk ,
q
δk = gkT Gk (xk )gk ,
αk
gkT gk
, tk =
,
2
δk
1 + α k δk
Pmk
where Fk (x) = m1k i=1
f (x, ξi ).
Set xk+1 = xk + tk gk .
end for
αk =

clear whether every possible Fk (x) will be standard selfconcordant, even when F (x) is standard self-concordant.
Thus, we impose Assumption 2 in our analysis, while observing that it is unnecessary in the practical case where ξ
is derived from a finite data set.

5. Stochastic Adaptive Methods
Our basic approach is to sample an empirical objective
function Fk (x) at each step, and then compute the step direction and adaptive step size (4) using Fk . For particular
choices of the matrices Hk , we recover analogues of classical methods such as gradient descent, L-BFGS, and BFGS.
5.1. Stochastic Adaptive GD
When Hk = I for every k, the resulting method is stochastic adaptive gradient descent (SA-GD), which is given as
Algorithm 1. The number of samples drawn at each iteration is left as an input to the algorithm, and (weak) bounds
on the required number mk can be inferred from the convergence analysis.
Theorem 5.1. Let  > 0. Suppose that at each iteration,
e
we draw m i.i.d samples. We may choose k = O(log
−1 )
e −2 log −1 ) so that the SA-GD method conand m = O(
verges in expectation to an -optimal solution after k steps.
e absorbs constants depending only on F , namely
Here, O(·)
C and κ.
A more precise quantitative form of Theorem 5.1, and its
proof, appear as Theorem B.1 in the supplementary materials. The argument can be sketched as follows. Theorem 4.1 implies that the adaptive step size tk produces
a linear decrease towards the optimal value of Fk ; that is,
Fk (xk+1 )−Fk (x∗k ) ≤ r(Fk (xk )−Fk (x∗k )). Here r is a deterministic constant depending only on `, L, and γ. By iterating, we can bound the true gap F (xk+1 )−F (x∗ ) in terms

Stochastic Adaptive Quasi-Newton Methods

Algorithm 2 SA-BFGS
Input: x0 , H0 , {m0 , m1 , . . .}, β < 1
for k = 0, 1, 2, . . . do
Sample ξ1 , . . . , ξmk i.i.d from ξ
Compute
gk = ∇Fk (xk ), dk = −Hk gk
q
δk = dTk Gk (xk )dk ,
αk
g T Hk gk
,
αk = k 2 , tk =
δk
1 + αk δk
Pmk
where Fk (x) = m1k i=1
f (x, ξi ).
Set gk+1 = ∇Fk (xk + tk dk )
Set yk = gk+1 − gk
T
if gk+1
dk < βgkT dk then
Set dk = gk
Recompute δk , αk , tk
Set Hk+1 = Hk
else
y sT
s sT
s yT
Set Hk+1 = (I − skT ykk )Hk (I − sTk ykk ) + sTk ykk
k
k
k
end if
Set xk+1 = xk + tk dk .
end for

of rk (F0 (x1 ) − F0 (x∗0 )) and rj sup |Fk+1−j (x) − F (x)|
x∈D

for 1 ≤ j ≤ k. The first term can be made smaller than  by
taking k to be O(log −1 ), and the second can be bounded
with Theorem 3.1.
5.2. Stochastic Adaptive BFGS
By updating Hk using the BFGS formula, we obtain the
stochastic adaptive BFGS (SA-BFGS) method, which is
given in Algorithm 2.
In Algorithm 2, we use the standard BFGS update with
yk = gk (xk+1 ) − gk (xk ). Another option is to replace yk
with the action of the Hessian on sk , so yk = Gk (xk )sk .
In general, we must compute Gk (xk )dk when finding the
adaptive step size, so we can re-use the result of that computation instead of computing an extra gradient gk (xk+1 ).
For technical reasons, our SA-BFGS procedure tests
whether the Wolfe condition is satisfied for the adaptive
step size tk . If not, we revert to taking a SA-GD step. This
is an artifact of our analysis, and under suitable conditions
on the growth of the samples mk , there will be some point
after which the Wolfe condition is necessarily satisfied on
every step. In practice, this test can be omitted.
There are also two possible ways to implement SA-BFGS.
For problems with n at most medium-sized, it is possible to
explicitly store the matrix Hk , and compute dk by a matrix

product −Hk gk . For n very large, it is infeasible to store
Hk , and we can instead store the pairs (sk , yk ) and compute
−Hk gk using a two-loop recursion (Nocedal, 1980). This
corresponds to stochastic adaptive L-BFGS (SA-LBFGS)
if we limit the number of past pairs (sk , yk ) to only the h
most recent, and to SA-BFGS if we store everything. The
amount of storage used by SA-LBFGS surpasses that of
SA-BFGS as h approaches n.
Theorem 5.1 holds for SA-LBFGS, since Theorem B.1
holds for any method where {Hk } has uniformly bounded
eigenvalues. Thus, SA-LBFGS also converges in expectae
tion to an -optimal solution after k = O(log
−1 ) steps
e −2 log −1 ), though now
given samples of size m = O(
the constants within the big-O are also dependent on h.
As with SA-GD, one can obtain an -optimal solution in
expectation from SA-BFGS with a fixed amount of sampling:
Theorem 5.2. Let  > 0. Suppose that at each iteration,
e
we draw m i.i.d samples. We may choose k = O(log
−1 )
−2
−1 3
e
and m = O( (log  ) ) so that the SA-BFGS method
converges in expectation to an -optimal solution after k
e absorbs constants depending only on F ,
steps. Here, O(·)
namely C and κ.
This theorem follows from Lemma C.9 in the supplementary materials. Note that the complexity bound is in fact
weaker than that of SA-GD. This occurs because the initial matrices H0 , . . . , Hk may be poor approximations of
(∇2 F (x))−1 , in which case the BFGS directions may perform poorly. We have little theoretical control over Hk except in the limit, and this is reflected in the weaker iteration
bounds for -optimality.
For convergence to the true optimal solution x∗ , it is necessary for the number of samples mk to increase over time.
In fact, by increasing the number of samples appropriately,
the SA-BFGS algorithm can be shown to converge to x∗
R-superlinearly with probability 1.
Theorem 5.3. Suppose that we draw mk samples on the kth step, where m−1
k converges R-superlinearly to 0. Then
the SA-BFGS algorithm converges R-superlinearly to the
optimal solution x∗ almost surely.
The complete proof is left to Lemma C.1 in the supplementary materials. We briefly sketch it here.
Taking mk to be an increasing sequence, we can guarantee
that the sequence {ηk = ρδkk } converges to 0. From basic
results on the adaptive step size, this implies that tk eventually satisfies the Armijo-Wolfe conditions, and therefore
the algorithm eventually uses the BFGS direction, and performs a BFGS update on Hk , at every step. Our test for
the Wolfe condition ensures that in every iteration, we either can control the progress by analyzing the BFGS up-

Stochastic Adaptive Quasi-Newton Methods

date Hk+1 , or by using our earlier results on SA-GD steps.
Together, these facts imply that SA-BFGS converges RlinearlyPalmost surely, and consequently, the sum of dis∞
tances k=0 kxk − x∗ k is finite almost surely.
Following the classical argument, we analyze the evolution
of Hk+1 to show that the steps taken by SA-BFGS converge to the Newton step. This is precisely the well-known
Dennis-Moré condition (Dennis Jr. & Moré, 1974) for Qsuperlinear convergence; however, in the stochastic setting,
we have an additional error term resulting from the variance in the sampling of the gradients and Hessians. We
apply Theorem 3.1 to show that gk (x) and G(x) rapidly
converge to the true gradient and Hessian, and thus SABFGS attains R-superlinear convergence.
Increasing mk at this rate is clearly infeasible in reality,
and it is non-trivial to determine a level of sampling which
produces (iteration-wise) superlinear convergence in a reasonable amount of real time. We note, for comparison, that
the Streaming SVRG method (Frostig et al., 2015) requires
sample sizes for the gradient that grow at a geometric rate,
and that the dynamic batch method (Byrd et al., 2012) obtains R-linear convergence in expectation by increasing the
sample sizes at a geometric rate. A superlinear increase in
the number of samples may be unavoidable as a condition
for superlinear convergence, given the statistical error of
the sample.
In practice, we are often uninterested in finding the true
minimizer, and instead aim to quickly find an approximate
solution. The theoretical R-superlinear rate of SA-BFGS
suggests that SA-BFGS or SA-LBFGS may offer practical
speedups over first-order stochastic methods, even without
using substantial sampling. One intuitive heuristic would
be to draw fewer samples in the early phase of the algorithm, when computing the gradient with high accuracy is
less valuable, and then increasing the number of samples as
the solutions approach optimality to reduce fluctuation.

6. Numerical Experiments
We compared several implementations of SA-GD and SABFGS against the original SGD method and the robust
SGD method (Nemirovski et al., 2009) on a penalized least
squares problem with random design. That is, the objective
function has the form
1
min L(w) = E(Y − X T w)2 + kwk22
2

w∈Rp

where X ∈ Rp and Y ∈ R are random variables with finite
second moments. We base our model on a standard linear
regression problem with Gaussian errors, so Y = X T β + 
for a deterministic vector β ∈ Rp and  ∼ N (0, 1) is a
noise component. X was drawn according to a multivariate
N (0, Σ(ρ)), where Σ(ρ) = (1 − ρ2 )Ip + ρ2 J (here J is the

all-ones matrix). By varying ρ, we control the condition
number of the expected Hessian. We tested problems of
size p = 100 and p = 500.
The following eight algorithms were tested:
SGD: SGD with fixed mk = p and diminishing step sizes
1
for problems with p = 100, and tk =
tk = k+1000
1
for
p
=
500.
k+5000
SA-GD: SA-GD with fixed mk = p.
SA-GD-I: SA-GD with increasing samples mk =
1.01k .

1
2p

+

SA-BFGS: SA-BFGS with fixed mk = p. We do not
test for the Wolfe condition at each step, and instead
always take the adaptive BFGS step and perform a
BFGS update.
SA-BFGS-I: SA-BFGS with increasing samples mk =
1
k
2 p + 1.01 . We do not test for the Wolfe condition at
each step, and instead always take the adaptive BFGS
step and perform a BFGS update.
SA-BFGS-GD: SA-BFGS with increasing samples mk =
1
k
2 p+1.01 . We test for the Wolfe condition and switch
to taking a SA-GD step when the adaptive step tk for
SA-BFGS fails the test.
R-S-GD-C: Robust SGD with constant step size tk =
√1 , where N is the pre-determined number of steps.
N
At the k-th iteration, output the average of the previous k/2 solutions.
R-S-GD-V: Robust SGD with diminishing step size tk =
√1 . At the k-th iteration, output the average of the
k
previous k/2 solutions.
We also tested the Streaming SVRG algorithm with the
hyperparameters specified in Corollary 4 (Frostig et al.,
2015). However, this algorithm was difficult to tune and
performed poorly, so we have omitted it from the figures.
The algorithms prefixed by “SA-” used the adaptive step
size. For SGD, we followed the standard practice of usa
ing a diminishing and non-summable step size tk = k+b
.
The values a = 1, b = {1000, 5000} in our test were chosen experimentally. The SA-BFGS algorithms were implemented using a two-loop recursion to compute Hk gk when
p = 500. This significantly improved their performance
compared to storing the matrix Hk explicitly.
Figure 1 shows the performance of each algorithm on a series of problems with varying problem size p and parameter ρ. The y-axis measures the gap log(f (x(t)) − f (x∗ ))
of the solution x(t) obtained by the algorithm after using t

Stochastic Adaptive Quasi-Newton Methods

seconds of CPU time. The algorithms were implemented in
Matlab 2015a, and the system was an Intel i5-5200U running Ubuntu.
The increasing sample sizes used in SA-GD-I do not appear
to reduce its variance, compared to SA-GD with mk fixed,
which goes against our initial expectations. In plots (a),
(b), (e), and (f), SA-GD-I appears to exhibit the same fluctuations as SA-GD when the points approach optimality.
In plot (c), SA-GD-I briefly surpasses SA-GD before the
objective value jumps again. It is only in plot (d) that SAGD-I appears to descend more consistently than SA-GD.
This suggests that, for these problem instances, the rate of
sampling mk = O(1.01k ) could even be increased further,
and that the tradeoff between taking more steps, versus taking fewer steps with more accurate estimates, favors more
accurate steps.
Likewise, SA-BFGS-I failed to consistently perform better than SA-BFGS with mk fixed. SA-BFGS-I seemed to
perform relatively better when the dimension was larger,
whereas SA-BFGS was faster when p = 100. One notable
example was plot (f), where SA-BFGS initially descends
rapidly before abruptly stalling, and is quickly surpassed
by SA-BFGS-I. The data going beyond 60 seconds of CPU
time verified that SA-BFGS continued to stall, and never
managed to obtain a log-error of less than 1. Our interpretation is that for this problem, the fixed sample size is
too small to obtain an accurate solution. It also appears
that SA-BFGS requires larger samples to obtain a stable
solution when the problem is particularly ill-conditioned.
In the plots (e), and particularly (f), where ρ = 0.9, SABFGS rapidly reaches the point where variance in the gradient estimates introduces too much noise to make further
progress.

Predictably, robust SGD was the most stable method (given
the lack of variance-reduction in the other methods). Both
R-S-GD-C and R-S-GD-V exhibited almost no fluctuations, while improving at a reasonable rate. The R-S-GD-C
method with a constant step size outperformed R-S-GD-V,
which we find counterintuitive given that R-S-GD-C used a
smaller step size than R-S-GD-V. This method of variancereduction by averaging the solutions seems to be effective
and can be applied to any iterative method.
Numerical results for ERM (empirical risk minimization)
problems can be found in section D in the supplementary
material.

7. Discussion
For self-concordant objective functions, adaptive methods
eliminate the need to choose a step size, which is a big
advantage. This is independent of a new difficulty which
arises for stochastic methods, which is the choice of the
sample size mk . From our experiments, we see that choosing mk appropriately is crucial. Unlike SGD and SVRG,
where it is often most effective to use mini-batches of size
1, SA-BFGS and SA-GD work best with comparatively
larger samples.
Note that SA-GD and SA-BFGS are not purely first-order
methods, as computing the adaptive step size requires calculating the Hessian-vector product Gk (xk )dk . However,
it is often possible to calculate Hessian-vector products efficiently, and at far less cost than computing the full Hessian
∇2 Fk (x). Consider, for example, a a logistic regression
problem where the empirical loss function is
m

L(x) =
What is also interesting is that SA-GD often outperforms
SA-BFGS if both algorithms use the same fixed sample
size. While somewhat disappointing, there is a natural reason for this. BFGS optimizes a local quadratic model of the
objective, and is performant when its approximation Hk
resembles the true Hessian. The Hessian exhibits greater
variance than the gradient, simply by virtue of having n2
components compared to n, and we generally expect that
more sampling is needed to accurately estimate the Hessian. With the same amount of sampling, SA-BFGS is
therefore ‘noisier’ than SA-GD relative to the true function.
We also see this reflected in the performance of SA-BFGSGD, which was able to outperform SA-BFGS and SABFGS-I when p = 100 despite the additional cost of checking the Wolfe condition. For p = 500, it is less clear
whether a better strategy is simply to perform SA-BFGSI. One strategy that might be effective is a fixed pattern of
‘switching’ between GD and BFGS steps, with a greater
number of GD steps in the early phase.

T
1 X
log(1 + e−yi ai x )
m i=1

for sampled data {(ai , yi ) : ai ∈ Rn , yi ∈ {−1, 1}}.
Let A denote the n × m matrix with columns ai . The
gradient is given by AT β, where β is the vector βi =
T
T
−yi e−yi ai x /(1 + e−yi ai x ), and the Hessian is given
by ABAT , where B is the diagonal matrix with entries
T
T
Bii = e−yi ai x /(1 + e−yi ai x )2 . ToP
compute the product
m
dT ∇2 L(x)d, it suffices to compute i=1 Bii (aTi d)2 , and
this requires approximately the same number of arithmetic
operations as computing ∇L(x). Thus, computing δk for
the adaptive step size requires roughly the same amount of
work as one additional gradient calculation. Also, as mentioned in Section 5, we may replace the BFGS update in
SA-BFGS with a modified update using the Hessian action
yk = Gk (xk )dk to save effort.
This work represents only a preliminary step in the development of stochastic quasi-Newton methods. There are
several key questions that remain open:

Stochastic Adaptive Quasi-Newton Methods

ρ = 0, p = 100

ρ = 0, p = 500

6

8
7

5

6

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

3

2

1

0

5
4
3
2
1
0

-1

-1

-2

-2
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

10

20

30

CPU time(s)

(a) ρ = 0, p = 100

60

50

60

50

60

ρ = 0.5, p = 500

6

8

5

7
6

log(f(x k ) - f(x * ))

4

log(f(x k ) - f(x * ))

50

(b) ρ = 0, p = 500

ρ = 0.5, p = 100

3

2

1

0

-1

5
4
3
2
1
0

-2

-1

-3

-2
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

10

20

30

CPU time(s)

40

CPU time(s)

(c) ρ = 0.5, p = 100

(d) ρ = 0.5, p = 500

ρ = 0.9, p = 100

ρ = 0.9, p = 500

6

7

5

6
5

log(f(x k ) - f(x * ))

4

log(f(x k ) - f(x * ))

40

CPU time(s)

3

2

1

0

-1

4
3
2
1
0
-1

-2

-2

-3

-3
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0

10

20

CPU time(s)

(e) ρ = 0.9, p = 100

S-GD

SA-GD

SA-GD-I

30

40

CPU time(s)

(f) ρ = 0.9, p = 500

SA-BFGS

SA-BFGS-I

SA-BFGS-GD

R-S-GD-C

R-S-GD-V

Figure 1. Experimental results for p = 100, 500 and varying ρ. The x-axis is the elapsed CPU time and the y-axis is log(f (x) − f (x∗ )).

1. Theorem 5.3 partially resolves a question posed by
Moritz et al. (Moritz et al., 2016), by proving superlinear convergence of a stochastic algorithm under
rather restrictive conditions. It would be strengthened
greatly if we could prove superlinear convergence under weaker conditions on mk .
2. The theory developed for adaptive step sizes only
applies to self-concordant functions, but the adaptive step size itself can be interpreted for non-selfconcordant functions, as an adjustment based on the
local curvature. It would be of great interest to extend
adaptive methods to general convex functions, thereby
replacing both inexact line search and fixed step sizes
on a large class of problems.

3. How can variance-reduction be applied to SA-GD and
SA-BFGS? The control variates used in SVRG are effective, though costly, and perhaps difficult to compute for functions of the form Ef (x, ξ). Another possible technique is to output an average of the solutions,
as is done by robust SGD, though this introduces an
additional hyperparameter (the number of past solutions to average). Better variance-reduction strategies
might compensate for noisy estimates, allowing us to
reduce the number of samples needed in practice.
4. What heuristics can be developed for the sample sizes
mk to improve the stability and speed up performance
of SA-GD and SA-BFGS?

Stochastic Adaptive Quasi-Newton Methods

References
Bottou, Léon. Large-scale machine learning with stochastic gradient descent.
In Proceedings of COMPSTAT’2010, pp. 177–186. Physica-Verlag HD, 2010.
Broyden, Charles G. Quasi-Newton methods and their application to function minimisation. Mathematics of Computation, 21(99):368–381, 1967.
Byrd, Richard H., Nocedal, Jorge, and Yuan, Ya-Xiang.
Global convergence of a class of quasi-Newton methods
on convex problems. Siam. J. Numer. Anal., (5):1171–
1190, 1987.
Byrd, Richard H, Chin, Gillian M, Nocedal, Jorge, and Wu,
Yuchen. Sample size selection in optimization methods
for machine learning. Mathematical Programming, 134
(1):127–155, 2012.
Byrd, Richard H., Hansen, S. L., Nocedal, Jorge, and
Singer, Yoram. A stochastic quasi-newton method for
large-scale optimization. SIAM Journal on Optimization,
26(2):1008–1031, 2016.
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
Saga: A fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pp.
1646–1654, 2014.
Dennis Jr., John E. and Moré, Jorge J. Characterization of superlinear convergence and its application to
quasi-Newton methods. Math. Comp., 28(106):549–560,
1974.
Fletcher, Roger. A new approach to variable metric algorithms. The Computer Journal, 13(3):317–322, 1970.
Friedlander, Michael P and Goh, Gabriel.
Tail
bounds for stochastic approximation. arXiv preprint
arXiv:1304.5586, 2013.
Frostig, Roy, Ge, Rong, Kakade, Sham M, and Sidford,
Aaron. Competing with the empirical risk minimizer in
a single pass. In COLT, pp. 728–763, 2015.

Gower, Robert, Goldfarb, Donald, and Richtárik, Peter.
Stochastic block BFGS : Squeezing more curvature out
of data. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1869–1878, 2016.
Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Advances in Neural Information Processing Systems, pp.
315–323, 2013.
Moritz, Philipp, Nishihara, Robert, and Jordan, Michael I.
A linearly-convergent stochastic L-BFGS algorithm. In
Proceedings of the Nineteenth International Conference
on Artificial Intelligence and Statistics, pp. 249–258,
2016.
Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and
Shapiro, Alexander. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on
Optimization, 19(4):1574–1609, 2009.
Nesterov, Yurii and Nemirovski, Arkadi. Interior-point
polynomial algorithms in convex programming. Society
for Industrial and Applied Mathematics, Philadelphia,
1994.
Nocedal, Jorge. Updating quasi-Newton matrices with limited storage. Math. Comp., 35(151):773–782, 1980.
Powell, Michael J. D. Some global convergence properties of a variable metric algorithm for minimization without exact line searches. In Cottle, Richard and Lemke,
C.E. (eds.), Nonlinear Programming, volume IX. SIAMAMS Proceedings, 1976.
Rodomanov, Anton and Kropotov, Dmitry.
A
superlinearly-convergent proximal newton-type method
for the optimization of finite sums. In Proceedings of the
33rd International Conference on Machine Learning,
pp. 2597–2605, 2016.
Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
Mathematical Programming, pp. 1–30, 2013.

Gao, Wenbo and Goldfarb, Donald. Quasi-Newton methods: Superlinear convergence without line search for
self-concordant functions. in review. arXiv:1612.06965,
2016.

Shanno, David F. Conditioning of quasi-Newton methods
for function minimization. Mathematics of Computation, 24(111):647–656, 1970.

Goldfarb, Donald. A family of variable-metric methods
derived by variational means. Mathematics of Computation, 24(109):23–26, 1970.

Tran-Dinh, Quoc, Kyrillidis, Anastasios, and Cevher,
Volkan. Composite self-concordant minimization. Journal of Machine Learning Research, 16(Mar):371–416,
2015.

Goldfarb, Donald, Iyengar, Garud, and Zhou, Chaoxu. Linear convergence of stochastic Frank-Wolfe variants. In
Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1066–1074, 2017.

W. van der Vaart, Aad. and Wellner, Jon A. Weak Convergence and Empirical Processes. Springer New York,
1996.

Stochastic Adaptive Quasi-Newton Methods

Zhang, Yuchen and Xiao, Lin. Disco: Distributed optimization for self-concordant empirical loss. In JMLR:
Workshop and Conference Proceedings, volume 32, pp.
362–370, 2015.

